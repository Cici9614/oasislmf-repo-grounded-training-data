{
  "repo": "OasisLMF",
  "repo_path": "D:\\HSBC\\oasislmf-repo-grounded-training-data\\data\\raw_repo",
  "source": "AST scan (public class/function with docstring)",
  "max_snippet_lines": 80,
  "chunks": [
    {
      "chunk_id": "setup.py::get_version@46",
      "source_type": "code",
      "path": "setup.py",
      "symbol_type": "function",
      "name": "get_version",
      "lineno": 46,
      "end_lineno": 51,
      "business_stage": "other",
      "docstring": "Return package version as listed in `__version__` in `init.py`.",
      "content": "# File: setup.py\n# function: get_version (lines 46-51)\n\ndef get_version():\n    \"\"\"\n    Return package version as listed in `__version__` in `init.py`.\n    \"\"\"\n    with io.open(os.path.join(SCRIPT_DIR, 'oasislmf', '__init__.py'), encoding='utf-8') as init_py:\n        return re.search('__version__ = [\\'\"]([^\\'\"]+)[\\'\"]', init_py.read()).group(1)\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturn package version as listed in `__version__` in `init.py`."
    },
    {
      "chunk_id": "setup.py::install_ktools@71",
      "source_type": "code",
      "path": "setup.py",
      "symbol_type": "function",
      "name": "install_ktools",
      "lineno": 71,
      "end_lineno": 90,
      "business_stage": "other",
      "docstring": "If system arch matches Ktools static build try to install from pre-build\nwith a fallback of compile ktools from source",
      "content": "# File: setup.py\n# function: install_ktools (lines 71-90)\n\n    def install_ktools(self):\n        '''\n        If system arch matches Ktools static build try to install from pre-build\n        with a fallback of compile ktools from source\n        '''\n        bin_install_kwargs = self.try_get_bin_install_kwargs()\n\n        if bin_install_kwargs:\n            # This only executes if 'KTOOLS_TAR_FILE_DIR' is set and the directory contains a correctly named ktools tar\n            if os.path.isfile(bin_install_kwargs.get('ktools_tar_override', '')):\n                self.install_ktools_local(**bin_install_kwargs)\n                return\n\n            try:\n                self.install_ktools_bin(**bin_install_kwargs)\n            except:\n                print('Fallback - building ktools from source')\n                self.install_ktools_source(**bin_install_kwargs)\n        else:\n            self.install_ktools_source()\n\n\"\"\"Docstring (excerpt)\"\"\"\nIf system arch matches Ktools static build try to install from pre-build\nwith a fallback of compile ktools from source"
    },
    {
      "chunk_id": "docker/entrypoint_benchmark.py::run_tests@80",
      "source_type": "code",
      "path": "docker/entrypoint_benchmark.py",
      "symbol_type": "function",
      "name": "run_tests",
      "lineno": 80,
      "end_lineno": 134,
      "business_stage": "other",
      "docstring": "Output of each run entry in `results`\n\nIn [3]: example_run\nOut[3]:\n{'total': 88.63,\n 'oasislmf.manager.__init__': 0.0,\n 'oasislmf.model_preparation.gul_inputs.get_gul_input_items': 16.05,\n 'oasislmf.model_preparation.gul_inputs.write_items_file': 3.84,\n 'oasislmf.model_preparation.gul_inputs.write_coverages_file': 1.88,\n 'oasislmf.model_preparation.gul_inputs.write_gul_input_files': 5.94,\n 'oasislmf.model_preparation.summaries.get_summary_mapping': 0.8,\n 'oasislmf.model_preparation.summaries.write_mapping_file': 6.77,\n 'oasislmf.model_preparation.il_inputs.get_il_input_items': 30.42,\n 'oasislmf.model_preparation.il_inputs.write_fm_policytc_file': 8.49,\n 'oasislmf.model_preparation.il_inputs.write_fm_profile_file': 1.59,\n 'oasislmf.model_preparation.il_inputs.write_fm_programme_file': 7.52,\n 'oasislmf.model_preparation.il_inputs.write_fm_xref_file': 2.98,\n 'oasislmf.model_preparation.il_inputs.write_il_input_files': 21.44}",
      "content": "# File: docker/entrypoint_benchmark.py\n# function: run_tests (lines 80-134)\n\ndef run_tests(test_dir, run_dir, log_fp, oasis_args, threshold=None):\n    '''\n    Output of each run entry in `results`\n\n    In [3]: example_run\n    Out[3]:\n    {'total': 88.63,\n     'oasislmf.manager.__init__': 0.0,\n     'oasislmf.model_preparation.gul_inputs.get_gul_input_items': 16.05,\n     'oasislmf.model_preparation.gul_inputs.write_items_file': 3.84,\n     'oasislmf.model_preparation.gul_inputs.write_coverages_file': 1.88,\n     'oasislmf.model_preparation.gul_inputs.write_gul_input_files': 5.94,\n     'oasislmf.model_preparation.summaries.get_summary_mapping': 0.8,\n     'oasislmf.model_preparation.summaries.write_mapping_file': 6.77,\n     'oasislmf.model_preparation.il_inputs.get_il_input_items': 30.42,\n     'oasislmf.model_preparation.il_inputs.write_fm_policytc_file': 8.49,\n     'oasislmf.model_preparation.il_inputs.write_fm_profile_file': 1.59,\n     'oasislmf.model_preparation.il_inputs.write_fm_programme_file': 7.52,\n     'oasislmf.model_preparation.il_inputs.write_fm_xref_file': 2.98,\n     'oasislmf.model_preparation.il_inputs.write_il_input_files': 21.44}\n    '''\n    sub_dirs = next(os.walk(test_dir))[1]\n    test_data = dict()\n    results = dict()\n\n    for d in sub_dirs:\n        loc_fp = os.path.join(test_dir, d, 'loc.csv')\n        acc_fp = os.path.join(test_dir, d, 'acc.csv')\n        keys_fp = os.path.join(test_dir, d, 'keys.csv')\n\n        n_sample = sum(1 for line in open(loc_fp)) - 1\n        cmd_str = f'oasislmf model generate-oasis-files -x {loc_fp} -y {acc_fp} -z {keys_fp} --oasis-files-dir {run_dir} {oasis_args} --verbose'\n        test_data[n_sample] = cmd_str\n\n    for t in sorted(test_data.keys()):\n        print('Running: ')\n        print(f\"cmd = {test_data[t]}\")\n        print(f'size = {t}')\n        print(f't_max = {threshold}')\n        stdout = run_command(test_data[t])\n        run = pasrse_gen_output(stdout)\n        results[t] = run\n        print(f\"t_total = {run['total']}\\n\")\n\n        # If given check that threshold isn't exceeded\n        if threshold:\n            if run['total'] > threshold:\n                print('FAILED\\n')\n                tabulate_data(results, log_fp)\n                sys.exit(1)\n            else:\n                print('PASSED\\n')\n\n    tabulate_data(results, log_fp)\n    return results\n\n\"\"\"Docstring (excerpt)\"\"\"\nOutput of each run entry in `results`\n\nIn [3]: example_run\nOut[3]:\n{'total': 88.63,\n 'oasislmf.manager.__init__': 0.0,\n 'oasislmf.model_preparation.gul_inputs.get_gul_input_items': 16.05,\n 'oasislmf.model_preparation.gul_inputs.write_items_file': 3.84,\n 'oasislmf.model_preparation.gul_inputs.write_coverages_file': 1.88,\n 'oasislmf.model_preparation.gul_inputs.write_gul_input_files': 5.94,\n 'oasislmf.model_preparation.summaries.get_summary_mapping': 0.8,\n 'oasislmf.model_preparation.summaries.write_mapping_file': 6.77,\n 'oasislmf.model_preparation.il_inputs.get_il_input_items': 30.42,\n 'oasislmf.model_preparation.il_inputs.write_fm_policytc_file': 8.49,\n 'oasislmf.model_preparation.il_inputs.write_fm_profile_file': 1.59,\n 'oasislmf.model_preparation.il_inputs.write_fm_programme_file': 7.52,\n 'oasislmf.model_preparation.il_inputs.write_fm_xref_file': 2.98,\n 'oasislmf.model_preparation.il_inputs.write_il_input_files': 21.44}"
    },
    {
      "chunk_id": "oasislmf/manager.py::computation_name_to_method@72",
      "source_type": "code",
      "path": "oasislmf/manager.py",
      "symbol_type": "function",
      "name": "computation_name_to_method",
      "lineno": 72,
      "end_lineno": 84,
      "business_stage": "other",
      "docstring": "generate the name of the method in manager for a given ComputationStep name\ntaken from https://stackoverflow.com/questions/1175208/elegant-python-function-to-convert-camelcase-to-snake-case\n\n>>> OasisManager.computation_name_to_method('ExposurePreAnalysis')\n'exposure_pre_analysis'\n>>> OasisManager.computation_name_to_method('EODFile')\n'eod_file'\n>>> OasisManager.computation_name_to_method('Model1Data')\n'model1_data'",
      "content": "# File: oasislmf/manager.py\n# function: computation_name_to_method (lines 72-84)\n\n    def computation_name_to_method(name):\n        \"\"\"\n        generate the name of the method in manager for a given ComputationStep name\n        taken from https://stackoverflow.com/questions/1175208/elegant-python-function-to-convert-camelcase-to-snake-case\n\n        >>> OasisManager.computation_name_to_method('ExposurePreAnalysis')\n        'exposure_pre_analysis'\n        >>> OasisManager.computation_name_to_method('EODFile')\n        'eod_file'\n        >>> OasisManager.computation_name_to_method('Model1Data')\n        'model1_data'\n        \"\"\"\n        return re.sub('((?<=[a-z0-9])[A-Z]|(?!^)[A-Z](?=[a-z]))', r'_\\1', name).lower()\n\n\"\"\"Docstring (excerpt)\"\"\"\ngenerate the name of the method in manager for a given ComputationStep name\ntaken from https://stackoverflow.com/questions/1175208/elegant-python-function-to-convert-camelcase-to-snake-case\n\n>>> OasisManager.computation_name_to_method('ExposurePreAnalysis')\n'exposure_pre_analysis'\n>>> OasisManager.computation_name_to_method('EODFile')\n'eod_file'\n>>> OasisManager.computation_name_to_method('Model1Data')\n'model1_data'"
    },
    {
      "chunk_id": "oasislmf/__init__.py::MyImport@33",
      "source_type": "code",
      "path": "oasislmf/__init__.py",
      "symbol_type": "class",
      "name": "MyImport",
      "lineno": 33,
      "end_lineno": 68,
      "business_stage": "other",
      "docstring": "Support alias of depreciated sub-modules\n\n* model_execution   -> execution\n* model_preparation -> preparation\n* api               -> platform\n\nExample:\n    `from oasislmf.model_execution.bash import genbash`\n        is the same as calling the new name\n    `from oasislmf.execution.bash import genbash`\n    https://docs.python.org/3/library/importlib.html#importlib.machinery.PathFinder",
      "content": "# File: oasislmf/__init__.py\n# class: MyImport (lines 33-68)\n\nclass MyImport(MetaPathFinder):\n    \"\"\" Support alias of depreciated sub-modules\n\n        * model_execution   -> execution\n        * model_preparation -> preparation\n        * api               -> platform\n\n        Example:\n            `from oasislmf.model_execution.bash import genbash`\n                is the same as calling the new name\n            `from oasislmf.execution.bash import genbash`\n            https://docs.python.org/3/library/importlib.html#importlib.machinery.PathFinder\n    \"\"\"\n\n    def __init__(self):\n        self.depricated_modules = {\n            \"model_execution\": \"execution\",\n            \"model_preparation\": \"preparation\",\n            \"api\": \"platform\",\n            \"platform\": \"platform_api\"\n        }\n\n    def find_spec(self, fullname, path=None, target=None):\n        import_path = fullname.split(\".\", 1)\n        if fullname.startswith(\"oasislmf\") and len(import_path) > 1:\n            import_path = import_path[1]\n            for deprecated in self.depricated_modules:\n                if deprecated == import_path or import_path.startswith(deprecated + '.'):\n                    with warnings.catch_warnings():\n                        warnings.simplefilter(\"always\")\n                        warnings.warn(\n                            f\"imports from 'oasislmf.{deprecated}' are deprecated. Import by using 'oasislmf.{self.depricated_modules[deprecated]}' instead.\"\n                        )\n                    import_path = import_path.replace(deprecated, self.depricated_modules[deprecated])\n\n            return spec_from_loader(fullname, MyLoader(import_path))\n\n\"\"\"Docstring (excerpt)\"\"\"\nSupport alias of depreciated sub-modules\n\n* model_execution   -> execution\n* model_preparation -> preparation\n* api               -> platform\n\nExample:\n    `from oasislmf.model_execution.bash import genbash`\n        is the same as calling the new name\n    `from oasislmf.execution.bash import genbash`\n    https://docs.python.org/3/library/importlib.html#importlib.machinery.PathFinder"
    },
    {
      "chunk_id": "oasislmf/cli/admin.py::EnableBashCompleteCmd@11",
      "source_type": "code",
      "path": "oasislmf/cli/admin.py",
      "symbol_type": "class",
      "name": "EnableBashCompleteCmd",
      "lineno": 11,
      "end_lineno": 17,
      "business_stage": "other",
      "docstring": "Adds required command to `.bashrc` Linux or .bash_profile for mac\nso that Command autocomplete works for oasislmf CLI",
      "content": "# File: oasislmf/cli/admin.py\n# class: EnableBashCompleteCmd (lines 11-17)\n\nclass EnableBashCompleteCmd(OasisComputationCommand):\n    \"\"\"\n    Adds required command to `.bashrc` Linux or .bash_profile for mac\n    so that Command autocomplete works for oasislmf CLI\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'HelperTabComplete'\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdds required command to `.bashrc` Linux or .bash_profile for mac\nso that Command autocomplete works for oasislmf CLI"
    },
    {
      "chunk_id": "oasislmf/cli/admin.py::AdminCmd@20",
      "source_type": "code",
      "path": "oasislmf/cli/admin.py",
      "symbol_type": "class",
      "name": "AdminCmd",
      "lineno": 20,
      "end_lineno": 27,
      "business_stage": "other",
      "docstring": "Admin subcommands::",
      "content": "# File: oasislmf/cli/admin.py\n# class: AdminCmd (lines 20-27)\n\nclass AdminCmd(OasisBaseCommand):\n    \"\"\"\n    Admin subcommands::\n\n    \"\"\"\n    sub_commands = {\n        'enable-bash-complete': EnableBashCompleteCmd\n    }\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdmin subcommands::"
    },
    {
      "chunk_id": "oasislmf/cli/api.py::ListApiCmd@5",
      "source_type": "code",
      "path": "oasislmf/cli/api.py",
      "symbol_type": "class",
      "name": "ListApiCmd",
      "lineno": 5,
      "end_lineno": 10,
      "business_stage": "other",
      "docstring": "Issue API GET requests via the command line",
      "content": "# File: oasislmf/cli/api.py\n# class: ListApiCmd (lines 5-10)\n\nclass ListApiCmd(OasisComputationCommand):\n    \"\"\"\n    Issue API GET requests via the command line\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'PlatformList'\n\n\"\"\"Docstring (excerpt)\"\"\"\nIssue API GET requests via the command line"
    },
    {
      "chunk_id": "oasislmf/cli/api.py::RunApiCmd@13",
      "source_type": "code",
      "path": "oasislmf/cli/api.py",
      "symbol_type": "class",
      "name": "RunApiCmd",
      "lineno": 13,
      "end_lineno": 18,
      "business_stage": "other",
      "docstring": "Run a model via the Oasis Platoform API end to end",
      "content": "# File: oasislmf/cli/api.py\n# class: RunApiCmd (lines 13-18)\n\nclass RunApiCmd(OasisComputationCommand):\n    \"\"\"\n    Run a model via the Oasis Platoform API end to end\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'PlatformRun'\n\n\"\"\"Docstring (excerpt)\"\"\"\nRun a model via the Oasis Platoform API end to end"
    },
    {
      "chunk_id": "oasislmf/cli/api.py::RunInputApiCmd@21",
      "source_type": "code",
      "path": "oasislmf/cli/api.py",
      "symbol_type": "class",
      "name": "RunInputApiCmd",
      "lineno": 21,
      "end_lineno": 26,
      "business_stage": "other",
      "docstring": "Run a model via the Oasis Platoform API end to end",
      "content": "# File: oasislmf/cli/api.py\n# class: RunInputApiCmd (lines 21-26)\n\nclass RunInputApiCmd(OasisComputationCommand):\n    \"\"\"\n    Run a model via the Oasis Platoform API end to end\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'PlatformRunInputs'\n\n\"\"\"Docstring (excerpt)\"\"\"\nRun a model via the Oasis Platoform API end to end"
    },
    {
      "chunk_id": "oasislmf/cli/api.py::RunLossApiCmd@29",
      "source_type": "code",
      "path": "oasislmf/cli/api.py",
      "symbol_type": "class",
      "name": "RunLossApiCmd",
      "lineno": 29,
      "end_lineno": 34,
      "business_stage": "other",
      "docstring": "Run a model via the Oasis Platoform API end to end",
      "content": "# File: oasislmf/cli/api.py\n# class: RunLossApiCmd (lines 29-34)\n\nclass RunLossApiCmd(OasisComputationCommand):\n    \"\"\"\n    Run a model via the Oasis Platoform API end to end\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'PlatformRunLosses'\n\n\"\"\"Docstring (excerpt)\"\"\"\nRun a model via the Oasis Platoform API end to end"
    },
    {
      "chunk_id": "oasislmf/cli/api.py::DeleteApiCmd@37",
      "source_type": "code",
      "path": "oasislmf/cli/api.py",
      "symbol_type": "class",
      "name": "DeleteApiCmd",
      "lineno": 37,
      "end_lineno": 42,
      "business_stage": "other",
      "docstring": "Delete items from the Platform API",
      "content": "# File: oasislmf/cli/api.py\n# class: DeleteApiCmd (lines 37-42)\n\nclass DeleteApiCmd(OasisComputationCommand):\n    \"\"\"\n    Delete items from the Platform API\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'PlatformDelete'\n\n\"\"\"Docstring (excerpt)\"\"\"\nDelete items from the Platform API"
    },
    {
      "chunk_id": "oasislmf/cli/api.py::GetApiCmd@45",
      "source_type": "code",
      "path": "oasislmf/cli/api.py",
      "symbol_type": "class",
      "name": "GetApiCmd",
      "lineno": 45,
      "end_lineno": 50,
      "business_stage": "other",
      "docstring": "Download files from the Oasis Platoform API",
      "content": "# File: oasislmf/cli/api.py\n# class: GetApiCmd (lines 45-50)\n\nclass GetApiCmd(OasisComputationCommand):\n    \"\"\"\n    Download files from the Oasis Platoform API\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'PlatformGet'\n\n\"\"\"Docstring (excerpt)\"\"\"\nDownload files from the Oasis Platoform API"
    },
    {
      "chunk_id": "oasislmf/cli/command.py::OasisBaseCommand@24",
      "source_type": "code",
      "path": "oasislmf/cli/command.py",
      "symbol_type": "class",
      "name": "OasisBaseCommand",
      "lineno": 24,
      "end_lineno": 232,
      "business_stage": "other",
      "docstring": "The base command to inherit from for each command.\n\n2 additional arguments (``--verbose`` and ``--config``) are added to\nthe parser so that they are available for all commands.",
      "content": "# File: oasislmf/cli/command.py\n# class: OasisBaseCommand (lines 24-232)\n\nclass OasisBaseCommand(BaseCommand):\n    \"\"\"\n    The base command to inherit from for each command.\n\n    2 additional arguments (``--verbose`` and ``--config``) are added to\n    the parser so that they are available for all commands.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self._logger = None\n        self.args = None\n        self.log_verbose = False\n        super(OasisBaseCommand, self).__init__(*args, **kwargs)\n\n    def add_args(self, parser):\n        \"\"\"\n        Adds arguments to the argument parser. This is used to modify\n        which arguments are processed by the command.\n\n        Enhanced logging arguments (--log-level, --log-format) added.\n        Legacy --verbose flag maintained for backward compatibility.\n\n        :param parser: The argument parser object\n        :type parser: ArgumentParser\n        \"\"\"\n        # Create temporary log config instance for dynamic choices\n        log_config = OasisLogConfig()\n\n        # Legacy verbose flag (backward compatibility with deprecation notice)\n        parser.add_argument(\n            \"-V\",\n            \"--verbose\",\n            action=\"store_true\",\n            help=\"Use verbose logging. (Deprecated: use --log-level=DEBUG)\",\n        )\n\n        # Enhanced logging arguments\n        parser.add_argument(\n            \"-L\",\n            \"--log-level\",\n            choices=log_config.get_available_levels(),\n            help=\"Set logging level (default: INFO)\",\n        )\n\n        parser.add_argument(\n            \"--log-format\",\n            choices=log_config.get_available_formats(),\n            help=\"Set log format template (default: standard)\",\n        )\n\n        # Configuration file argument\n        parser.add_argument(\n            \"-C\",\n            \"--config\",\n            required=False,\n            type=PathCleaner(\"MDK config. JSON file\", preexists=True),\n            help=\"MDK config. JSON file\",\n            default=\"./oasislmf.json\" if os.path.isfile(\"./oasislmf.json\") else None,\n        )\n\n    def parse_args(self):\n        \"\"\"\n        Parses the command line arguments and sets them in ``self.args``\n\n        :return: The arguments taken from the command line\n        \"\"\"\n        try:\n            self.args = super(OasisBaseCommand, self).parse_args()\n\n            # Handle backward compatibility with deprecation warning\n            if self.args.verbose:\n                warnings.warn(\n                    \"The --verbose flag is deprecated and will be removed in a future version. \"\n                    \"Use --log-level=DEBUG instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n            self.setup_logger()\n            return self.args\n\n\"\"\"Docstring (excerpt)\"\"\"\nThe base command to inherit from for each command.\n\n2 additional arguments (``--verbose`` and ``--config``) are added to\nthe parser so that they are available for all commands."
    },
    {
      "chunk_id": "oasislmf/cli/command.py::OasisComputationCommand@235",
      "source_type": "code",
      "path": "oasislmf/cli/command.py",
      "symbol_type": "class",
      "name": "OasisComputationCommand",
      "lineno": 235,
      "end_lineno": 328,
      "business_stage": "other",
      "docstring": "Eventually, the Parent class for all Oasis Computation Command\ncreate the command line interface from parameter define in the associated computation step",
      "content": "# File: oasislmf/cli/command.py\n# class: OasisComputationCommand (lines 235-328)\n\nclass OasisComputationCommand(OasisBaseCommand):\n    \"\"\"\n    Eventually, the Parent class for all Oasis Computation Command\n    create the command line interface from parameter define in the associated computation step\n    \"\"\"\n\n    def add_args(self, parser):\n        \"\"\"\n        Adds arguments to the argument parser.\n\n        :param parser: The argument parser object\n        :type parser: ArgumentParser\n        \"\"\"\n        super().add_args(parser)\n\n        for param in om.computations_params[self.computation_name]:\n            add_argument_kwargs = {\n                key: param.get(key)\n                for key in [\n                    \"action\",\n                    \"nargs\",\n                    \"const\",\n                    \"type\",\n                    \"choices\",\n                    \"help\",\n                    \"metavar\",\n                    \"dest\",\n                ]\n                if param.get(key) is not None\n            }\n            # If 'Help' is not set then this is a function only paramter, skip\n            if \"help\" in add_argument_kwargs:\n                arg_name = f\"--{param['name'].replace('_', '-')}\"\n                if param.get(\"flag\"):\n                    parser.add_argument(\n                        param.get(\"flag\"), arg_name, **add_argument_kwargs\n                    )\n                else:\n                    parser.add_argument(arg_name, **add_argument_kwargs)\n\n    @classmethod\n    def get_arguments(cls, args, manager_method):\n        inputs = InputValues(args)\n\n        def get_kwargs_item(param):\n            return param[\"name\"], inputs.get(\n                param[\"name\"],\n                required=param.get(\"required\"),\n                is_path=param.get(\"is_path\"),\n                dtype=param.get(\"type\"),\n            )\n\n        settings_args = {\n            param[\"name\"] for param in manager_method.get_params(param_type=\"settings\")\n        }\n\n        _kwargs = dict(\n            get_kwargs_item(param)\n            for param in manager_method.get_params()\n            if param[\"name\"] in settings_args\n        )\n\n        # read and merge computation settings files\n        computation_settings = Settings()\n        computation_settings.add_settings(inputs.config, ROOT_USER_ROLE)\n        for settings_info in manager_method.get_params(param_type=\"settings\"):\n            setting_fp = _kwargs.get(settings_info[\"name\"])\n            if setting_fp:\n                new_settings = settings_info[\"loader\"](setting_fp)\n                computation_settings.add_settings(\n                    new_settings.pop(\"computation_settings\", {}),\n                    settings_info.get(\"user_role\"),\n                )\n        inputs.config = computation_settings.get_settings()\n\n        return {\n            **dict(get_kwargs_item(param) for param in manager_method.get_params()),\n            **_kwargs,\n        }\n\n\"\"\"Docstring (excerpt)\"\"\"\nEventually, the Parent class for all Oasis Computation Command\ncreate the command line interface from parameter define in the associated computation step"
    },
    {
      "chunk_id": "oasislmf/cli/command.py::add_args@38",
      "source_type": "code",
      "path": "oasislmf/cli/command.py",
      "symbol_type": "function",
      "name": "add_args",
      "lineno": 38,
      "end_lineno": 82,
      "business_stage": "other",
      "docstring": "Adds arguments to the argument parser. This is used to modify\nwhich arguments are processed by the command.\n\nEnhanced logging arguments (--log-level, --log-format) added.\nLegacy --verbose flag maintained for backward compatibility.\n\n:param parser: The argument parser object\n:type parser: ArgumentParser",
      "content": "# File: oasislmf/cli/command.py\n# function: add_args (lines 38-82)\n\n    def add_args(self, parser):\n        \"\"\"\n        Adds arguments to the argument parser. This is used to modify\n        which arguments are processed by the command.\n\n        Enhanced logging arguments (--log-level, --log-format) added.\n        Legacy --verbose flag maintained for backward compatibility.\n\n        :param parser: The argument parser object\n        :type parser: ArgumentParser\n        \"\"\"\n        # Create temporary log config instance for dynamic choices\n        log_config = OasisLogConfig()\n\n        # Legacy verbose flag (backward compatibility with deprecation notice)\n        parser.add_argument(\n            \"-V\",\n            \"--verbose\",\n            action=\"store_true\",\n            help=\"Use verbose logging. (Deprecated: use --log-level=DEBUG)\",\n        )\n\n        # Enhanced logging arguments\n        parser.add_argument(\n            \"-L\",\n            \"--log-level\",\n            choices=log_config.get_available_levels(),\n            help=\"Set logging level (default: INFO)\",\n        )\n\n        parser.add_argument(\n            \"--log-format\",\n            choices=log_config.get_available_formats(),\n            help=\"Set log format template (default: standard)\",\n        )\n\n        # Configuration file argument\n        parser.add_argument(\n            \"-C\",\n            \"--config\",\n            required=False,\n            type=PathCleaner(\"MDK config. JSON file\", preexists=True),\n            help=\"MDK config. JSON file\",\n            default=\"./oasislmf.json\" if os.path.isfile(\"./oasislmf.json\") else None,\n        )\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdds arguments to the argument parser. This is used to modify\nwhich arguments are processed by the command.\n\nEnhanced logging arguments (--log-level, --log-format) added.\nLegacy --verbose flag maintained for backward compatibility.\n\n:param parser: The argument parser object\n:type parser: ArgumentParser"
    },
    {
      "chunk_id": "oasislmf/cli/command.py::parse_args@84",
      "source_type": "code",
      "path": "oasislmf/cli/command.py",
      "symbol_type": "function",
      "name": "parse_args",
      "lineno": 84,
      "end_lineno": 106,
      "business_stage": "other",
      "docstring": "Parses the command line arguments and sets them in ``self.args``\n\n:return: The arguments taken from the command line",
      "content": "# File: oasislmf/cli/command.py\n# function: parse_args (lines 84-106)\n\n    def parse_args(self):\n        \"\"\"\n        Parses the command line arguments and sets them in ``self.args``\n\n        :return: The arguments taken from the command line\n        \"\"\"\n        try:\n            self.args = super(OasisBaseCommand, self).parse_args()\n\n            # Handle backward compatibility with deprecation warning\n            if self.args.verbose:\n                warnings.warn(\n                    \"The --verbose flag is deprecated and will be removed in a future version. \"\n                    \"Use --log-level=DEBUG instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n            self.setup_logger()\n            return self.args\n        except Exception:\n            self.setup_logger()\n            raise\n\n\"\"\"Docstring (excerpt)\"\"\"\nParses the command line arguments and sets them in ``self.args``\n\n:return: The arguments taken from the command line"
    },
    {
      "chunk_id": "oasislmf/cli/command.py::setup_logger@145",
      "source_type": "code",
      "path": "oasislmf/cli/command.py",
      "symbol_type": "function",
      "name": "setup_logger",
      "lineno": 145,
      "end_lineno": 225,
      "business_stage": "other",
      "docstring": "Setup logger using OasisLogConfig for enhanced logging configuration.\n\nSupports configurable log levels, formats, and maintains backward compatibility.",
      "content": "# File: oasislmf/cli/command.py\n# function: setup_logger (lines 145-225)\n\n    def setup_logger(self):\n        \"\"\"\n        Setup logger using OasisLogConfig for enhanced logging configuration.\n\n        Supports configurable log levels, formats, and maintains backward compatibility.\n        \"\"\"\n        if not self._logger:\n            # Load configuration and create log config handler\n            config_dict = self._load_config_dict()\n            log_config = OasisLogConfig(config_dict)\n\n            # Validate configuration and show warnings\n            warnings_list = log_config.validate_config()\n            for warning in warnings_list:\n                print(f\"Warning: {warning}\", file=sys.stderr)\n\n            # Get effective log level and formatter\n            cli_level = (\n                getattr(self.args, \"log_level\", None)\n                if hasattr(self, \"args\") and self.args\n                else None\n            )\n            is_verbose = (\n                getattr(self.args, \"verbose\", False)\n                if hasattr(self, \"args\") and self.args\n                else False\n            )\n            cli_format = (\n                getattr(self.args, \"log_format\", None)\n                if hasattr(self, \"args\") and self.args\n                else None\n            )\n\n            log_level = log_config.get_log_level(cli_level, is_verbose)\n            formatter = log_config.create_formatter(cli_format)\n            ods_level = log_config.get_ods_tools_level(log_level)\n\n            # Setup main oasislmf logger\n            logger = logging.getLogger(\"oasislmf\")\n\n            # Remove existing handlers (preserve existing logic)\n            for handler in list(logger.handlers):\n                if handler.name == \"oasislmf\":\n                    logger.removeHandler(handler)\n                    break\n\n            # Setup ods_tools logger\n            ods_logger = logging.getLogger(\"ods_tools\")\n            ods_logger.setLevel(ods_level)\n            ods_logger.propagate = False\n\n            # Create and configure handler\n            ch = logging.StreamHandler(stream=sys.stdout)\n            ch.name = \"oasislmf\"\n            ch.setFormatter(formatter)\n\n            # Add handler to both loggers\n            logger.addHandler(ch)\n            ods_logger.addHandler(ch)\n            logger.setLevel(log_level)\n\n            # Set the logger and preserve backward compatibility\n            self._logger = logger\n            self.log_verbose = log_level <= logging.DEBUG\n\n            # Add debug info when running in debug mode\n            if log_level <= logging.DEBUG:\n                config_source = (\n                    getattr(self.args, \"config\", None)\n                    if hasattr(self, \"args\") and self.args\n                    else None\n                )\n                self._logger.debug(\n                    f\"Effective log level: {logging.getLevelName(log_level)}\"\n                )\n                self._logger.debug(\n                    f\"ods_tools level: {logging.getLevelName(ods_level)}\"\n                )\n                self._logger.debug(\n                    f\"Config source: {config_source if config_source else 'default'}\"\n\n\"\"\"Docstring (excerpt)\"\"\"\nSetup logger using OasisLogConfig for enhanced logging configuration.\n\nSupports configurable log levels, formats, and maintains backward compatibility."
    },
    {
      "chunk_id": "oasislmf/cli/command.py::add_args@241",
      "source_type": "code",
      "path": "oasislmf/cli/command.py",
      "symbol_type": "function",
      "name": "add_args",
      "lineno": 241,
      "end_lineno": 273,
      "business_stage": "other",
      "docstring": "Adds arguments to the argument parser.\n\n:param parser: The argument parser object\n:type parser: ArgumentParser",
      "content": "# File: oasislmf/cli/command.py\n# function: add_args (lines 241-273)\n\n    def add_args(self, parser):\n        \"\"\"\n        Adds arguments to the argument parser.\n\n        :param parser: The argument parser object\n        :type parser: ArgumentParser\n        \"\"\"\n        super().add_args(parser)\n\n        for param in om.computations_params[self.computation_name]:\n            add_argument_kwargs = {\n                key: param.get(key)\n                for key in [\n                    \"action\",\n                    \"nargs\",\n                    \"const\",\n                    \"type\",\n                    \"choices\",\n                    \"help\",\n                    \"metavar\",\n                    \"dest\",\n                ]\n                if param.get(key) is not None\n            }\n            # If 'Help' is not set then this is a function only paramter, skip\n            if \"help\" in add_argument_kwargs:\n                arg_name = f\"--{param['name'].replace('_', '-')}\"\n                if param.get(\"flag\"):\n                    parser.add_argument(\n                        param.get(\"flag\"), arg_name, **add_argument_kwargs\n                    )\n                else:\n                    parser.add_argument(arg_name, **add_argument_kwargs)\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdds arguments to the argument parser.\n\n:param parser: The argument parser object\n:type parser: ArgumentParser"
    },
    {
      "chunk_id": "oasislmf/cli/command.py::action@315",
      "source_type": "code",
      "path": "oasislmf/cli/command.py",
      "symbol_type": "function",
      "name": "action",
      "lineno": 315,
      "end_lineno": 328,
      "business_stage": "other",
      "docstring": "Generic method that call the correct manager method from the child class computation_name\n\n:param args: The arguments from the command line\n:type args: Namespace",
      "content": "# File: oasislmf/cli/command.py\n# function: action (lines 315-328)\n\n    def action(self, args):\n        \"\"\"\n        Generic method that call the correct manager method from the child class computation_name\n\n        :param args: The arguments from the command line\n        :type args: Namespace\n        \"\"\"\n        manager_method = getattr(\n            om(), om.computation_name_to_method(self.computation_name)\n        )\n        _kwargs = self.get_arguments(args, manager_method)\n\n        self.logger.info(f\"\\nStarting oasislmf command - {self.computation_name}\")\n        manager_method(**_kwargs)\n\n\"\"\"Docstring (excerpt)\"\"\"\nGeneric method that call the correct manager method from the child class computation_name\n\n:param args: The arguments from the command line\n:type args: Namespace"
    },
    {
      "chunk_id": "oasislmf/cli/config.py::ConfigUpdateCmd@9",
      "source_type": "code",
      "path": "oasislmf/cli/config.py",
      "symbol_type": "class",
      "name": "ConfigUpdateCmd",
      "lineno": 9,
      "end_lineno": 51,
      "business_stage": "other",
      "docstring": "Read in an MDK config file and writes an updated file, replacing deprecated keys\nwith newer ones compatible with the current MDK release",
      "content": "# File: oasislmf/cli/config.py\n# class: ConfigUpdateCmd (lines 9-51)\n\nclass ConfigUpdateCmd(OasisBaseCommand):\n    \"\"\"\n    Read in an MDK config file and writes an updated file, replacing deprecated keys\n    with newer ones compatible with the current MDK release\n    \"\"\"\n\n    def add_args(self, parser):\n        \"\"\"\n        Adds arguments to the argument parser.\n\n        :param parser: The argument parser object\n        :type parser: ArgumentParser\n        \"\"\"\n\n        super(self.__class__, self).add_args(parser)\n\n        parser.add_argument('-o', '--output-config', default=None, help='File path to write an updated MDK config file')\n        parser.add_argument('-y', '--no-confirm', default=False, help='No confirmation prompt before file write')\n\n    def action(self, args):\n        \"\"\"\n        :param args: The arguments from the command line\n        :type args: Namespace\n        \"\"\"\n\n        inputs = InputValues(args, update_keys=False)\n\n        if inputs.obsolete_keys:\n            inputs.list_obsolete_keys(fix_warning=False)\n            inputs.update_config_keys()\n            new_config_fp = inputs.get('output_config') if inputs.get('output_config') else inputs.config_fp\n\n            if inputs.get('no_confirm'):\n                inputs.write_config_file(new_config_fp)\n            else:\n                msg = 'Write updated config file to \"{}\"?'.format(new_config_fp)\n                if inputs.confirm_action(msg):\n                    inputs.write_config_file(new_config_fp)\n        else:\n            self.logger.info('File \"{}\" is up to date with version {}'.format(\n                inputs.config_fp,\n                __version__,\n            ))\n\n\"\"\"Docstring (excerpt)\"\"\"\nRead in an MDK config file and writes an updated file, replacing deprecated keys\nwith newer ones compatible with the current MDK release"
    },
    {
      "chunk_id": "oasislmf/cli/config.py::ConfigCmd@54",
      "source_type": "code",
      "path": "oasislmf/cli/config.py",
      "symbol_type": "class",
      "name": "ConfigCmd",
      "lineno": 54,
      "end_lineno": 90,
      "business_stage": "other",
      "docstring": "Describes the format of the configuration (JSON) file to use the MDK\n``model run`` command for running models end-to-end.\n\nOne file will need to be defined per model, usually in the model repository\nand with an indicative name.\n\nThe path-related keys should be strings, given relative to the location of\nconfiguration file. Optional arguments for a command are usually defaulted\nwith appropriate values, but can be overridden by providing a runtime flag.\n\n:analysis_settings_json: Analysis settings (JSON) file path\n:lookup_data_dir: Model lookup/keys data path (optional)\n:lookup_config_json: Model built-in lookup config. (JSON) file path (optional)\n:lookup_package_dir: Model custom lookup package path (optional)\n:model_version_csv: Model version (CSV) file path (optional)\n:model_data_dir: Model data path\n:model_package_dir: Path to the directory to use as the model specific package (optional)\n:model_run_dir: Model run directory (optional)\n:oed_location_csv: Source OED exposure (CSV) file path\n:oed_accounts_csv: Source OED accounts (CSV) file path (optional)\n:oed_info_csv: Reinsurance (RI) info. file path (optional)\n:oed_scope_csv: RI scope file path (optional)\n:profile_location_csv: Source OED exposure (JSON) profile describing the financial terms contained in the source exposure file (optional)\n:profile_accounts_json: Source OED accouns (JSON) profile describing the financial terms contained in the source accounts file (optional)\n:summarise_exposure: Generates an exposure summary report in JSON\n:ktools_num_processes: The number of concurrent processes used by ktools during model execution - default is ``2``\n:ktools_fifo_relative: Whether to create ktools FIFO queues under the ``./fifo`` subfolder (in the model run directory)\n:ktools_alloc_rule_gul: Override the allocation used in ``fmcalc`` - default is ``1``\n:ktools_alloc_rule_il: Override the allocation used in ``fmcalc`` - default is ``2``",
      "content": "# File: oasislmf/cli/config.py\n# class: ConfigCmd (lines 54-90)\n\nclass ConfigCmd(OasisBaseCommand):\n    \"\"\"\n    Describes the format of the configuration (JSON) file to use the MDK\n    ``model run`` command for running models end-to-end.\n\n    One file will need to be defined per model, usually in the model repository\n    and with an indicative name.\n\n    The path-related keys should be strings, given relative to the location of\n    configuration file. Optional arguments for a command are usually defaulted\n    with appropriate values, but can be overridden by providing a runtime flag.\n\n    :analysis_settings_json: Analysis settings (JSON) file path\n    :lookup_data_dir: Model lookup/keys data path (optional)\n    :lookup_config_json: Model built-in lookup config. (JSON) file path (optional)\n    :lookup_package_dir: Model custom lookup package path (optional)\n    :model_version_csv: Model version (CSV) file path (optional)\n    :model_data_dir: Model data path\n    :model_package_dir: Path to the directory to use as the model specific package (optional)\n    :model_run_dir: Model run directory (optional)\n    :oed_location_csv: Source OED exposure (CSV) file path\n    :oed_accounts_csv: Source OED accounts (CSV) file path (optional)\n    :oed_info_csv: Reinsurance (RI) info. file path (optional)\n    :oed_scope_csv: RI scope file path (optional)\n    :profile_location_csv: Source OED exposure (JSON) profile describing the financial terms contained in the source exposure file (optional)\n    :profile_accounts_json: Source OED accouns (JSON) profile describing the financial terms contained in the source accounts file (optional)\n    :summarise_exposure: Generates an exposure summary report in JSON\n    :ktools_num_processes: The number of concurrent processes used by ktools during model execution - default is ``2``\n    :ktools_fifo_relative: Whether to create ktools FIFO queues under the ``./fifo`` subfolder (in the model run directory)\n    :ktools_alloc_rule_gul: Override the allocation used in ``fmcalc`` - default is ``1``\n    :ktools_alloc_rule_il: Override the allocation used in ``fmcalc`` - default is ``2``\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n\n    sub_commands = {\n        'update': ConfigUpdateCmd,\n    }\n\n\"\"\"Docstring (excerpt)\"\"\"\nDescribes the format of the configuration (JSON) file to use the MDK\n``model run`` command for running models end-to-end.\n\nOne file will need to be defined per model, usually in the model repository\nand with an indicative name.\n\nThe path-related keys should be strings, given relative to the location of\nconfiguration file. Optional arguments for a command are usually defaulted\nwith appropriate values, but can be overridden by providing a runtime flag.\n\n:analysis_settings_json: Analysis settings (JSON) file path\n:lookup_data_dir: Model lookup/keys data path (optional)\n:lookup_config_json: Model built-in lookup config. (JSON) file path (optional)\n:lookup_package_dir: Model custom lookup package path (optional)\n:model_version_csv: Model version (CSV) file path (optional)\n:model_data_dir: Model data path\n:model_package_dir: Path to the directory to use as the model specific package (optional)\n:model_run_dir: Model run directory (optional)\n:oed_location_csv: Source OED exposure (CSV) file path\n:oed_accounts_csv: Source OED accounts (CSV) file path (optional)\n:oed_info_csv: Reinsurance (RI) info. file path (optional)\n:oed_scope_csv: RI scope file path (optional)\n:profile_location_csv: Source OED exposure (JSON) profile describing the financial terms contained in the source exposure file (optional)\n:profile_accounts_json: Source OED accouns (JSON) profile describing the financial terms contained in the source accounts file (optional)\n:summarise_exposure: Generates an exposure summary report in JSON\n:ktools_num_processes: The number of concurrent processes used by ktools during model execution - default is ``2``\n:ktools_fifo_relative: Whether to create ktools FIFO queues under the ``./fifo`` subfolder (in the model run directory)\n:ktools_alloc_rule_gul: Override the allocation used in ``fmcalc`` - default is ``1``\n:ktools_alloc_rule_il: Override the allocation used in ``fmcalc`` - default is ``2``"
    },
    {
      "chunk_id": "oasislmf/cli/config.py::add_args@15",
      "source_type": "code",
      "path": "oasislmf/cli/config.py",
      "symbol_type": "function",
      "name": "add_args",
      "lineno": 15,
      "end_lineno": 26,
      "business_stage": "other",
      "docstring": "Adds arguments to the argument parser.\n\n:param parser: The argument parser object\n:type parser: ArgumentParser",
      "content": "# File: oasislmf/cli/config.py\n# function: add_args (lines 15-26)\n\n    def add_args(self, parser):\n        \"\"\"\n        Adds arguments to the argument parser.\n\n        :param parser: The argument parser object\n        :type parser: ArgumentParser\n        \"\"\"\n\n        super(self.__class__, self).add_args(parser)\n\n        parser.add_argument('-o', '--output-config', default=None, help='File path to write an updated MDK config file')\n        parser.add_argument('-y', '--no-confirm', default=False, help='No confirmation prompt before file write')\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdds arguments to the argument parser.\n\n:param parser: The argument parser object\n:type parser: ArgumentParser"
    },
    {
      "chunk_id": "oasislmf/cli/config.py::action@28",
      "source_type": "code",
      "path": "oasislmf/cli/config.py",
      "symbol_type": "function",
      "name": "action",
      "lineno": 28,
      "end_lineno": 51,
      "business_stage": "other",
      "docstring": ":param args: The arguments from the command line\n:type args: Namespace",
      "content": "# File: oasislmf/cli/config.py\n# function: action (lines 28-51)\n\n    def action(self, args):\n        \"\"\"\n        :param args: The arguments from the command line\n        :type args: Namespace\n        \"\"\"\n\n        inputs = InputValues(args, update_keys=False)\n\n        if inputs.obsolete_keys:\n            inputs.list_obsolete_keys(fix_warning=False)\n            inputs.update_config_keys()\n            new_config_fp = inputs.get('output_config') if inputs.get('output_config') else inputs.config_fp\n\n            if inputs.get('no_confirm'):\n                inputs.write_config_file(new_config_fp)\n            else:\n                msg = 'Write updated config file to \"{}\"?'.format(new_config_fp)\n                if inputs.confirm_action(msg):\n                    inputs.write_config_file(new_config_fp)\n        else:\n            self.logger.info('File \"{}\" is up to date with version {}'.format(\n                inputs.config_fp,\n                __version__,\n            ))\n\n\"\"\"Docstring (excerpt)\"\"\"\n:param args: The arguments from the command line\n:type args: Namespace"
    },
    {
      "chunk_id": "oasislmf/cli/exposure.py::RunCmd@11",
      "source_type": "code",
      "path": "oasislmf/cli/exposure.py",
      "symbol_type": "class",
      "name": "RunCmd",
      "lineno": 11,
      "end_lineno": 21,
      "business_stage": "exposure",
      "docstring": "Generates deterministic losses using the installed ktools framework given\ndirect Oasis files (GUL + optionally IL and RI input files).\n\nThe command line arguments can be supplied in the configuration file\n(``oasislmf.json`` by default or specified with the ``--config`` flag).\nRun ``oasislmf config --help`` for more information.",
      "content": "# File: oasislmf/cli/exposure.py\n# class: RunCmd (lines 11-21)\n\nclass RunCmd(OasisComputationCommand):\n    \"\"\"\n    Generates deterministic losses using the installed ktools framework given\n    direct Oasis files (GUL + optionally IL and RI input files).\n\n    The command line arguments can be supplied in the configuration file\n    (``oasislmf.json`` by default or specified with the ``--config`` flag).\n    Run ``oasislmf config --help`` for more information.\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'RunExposure'\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerates deterministic losses using the installed ktools framework given\ndirect Oasis files (GUL + optionally IL and RI input files).\n\nThe command line arguments can be supplied in the configuration file\n(``oasislmf.json`` by default or specified with the ``--config`` flag).\nRun ``oasislmf config --help`` for more information."
    },
    {
      "chunk_id": "oasislmf/cli/exposure.py::ExposureCmd@24",
      "source_type": "code",
      "path": "oasislmf/cli/exposure.py",
      "symbol_type": "class",
      "name": "ExposureCmd",
      "lineno": 24,
      "end_lineno": 32,
      "business_stage": "exposure",
      "docstring": "Exposure subcommands::\n\n    * generate - and optionally, validate - deterministic losses (GUL, IL or RIL)",
      "content": "# File: oasislmf/cli/exposure.py\n# class: ExposureCmd (lines 24-32)\n\nclass ExposureCmd(OasisBaseCommand):\n    \"\"\"\n    Exposure subcommands::\n\n        * generate - and optionally, validate - deterministic losses (GUL, IL or RIL)\n    \"\"\"\n    sub_commands = {\n        'run': RunCmd\n    }\n\n\"\"\"Docstring (excerpt)\"\"\"\nExposure subcommands::\n\n    * generate - and optionally, validate - deterministic losses (GUL, IL or RIL)"
    },
    {
      "chunk_id": "oasislmf/cli/model.py::GenerateExposurePreAnalysisCmd@20",
      "source_type": "code",
      "path": "oasislmf/cli/model.py",
      "symbol_type": "class",
      "name": "GenerateExposurePreAnalysisCmd",
      "lineno": 20,
      "end_lineno": 26,
      "business_stage": "other",
      "docstring": "Generate a new EOD from original one by specifying a model specific pre-analysis hook for exposure modification\nsee ExposurePreAnalysis for more detail",
      "content": "# File: oasislmf/cli/model.py\n# class: GenerateExposurePreAnalysisCmd (lines 20-26)\n\nclass GenerateExposurePreAnalysisCmd(OasisComputationCommand):\n    \"\"\"\n    Generate a new EOD from original one by specifying a model specific pre-analysis hook for exposure modification\n    see ExposurePreAnalysis for more detail\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'ExposurePreAnalysis'\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate a new EOD from original one by specifying a model specific pre-analysis hook for exposure modification\nsee ExposurePreAnalysis for more detail"
    },
    {
      "chunk_id": "oasislmf/cli/model.py::GeneratePostFileGenCmd@29",
      "source_type": "code",
      "path": "oasislmf/cli/model.py",
      "symbol_type": "class",
      "name": "GeneratePostFileGenCmd",
      "lineno": 29,
      "end_lineno": 35,
      "business_stage": "other",
      "docstring": "Generate a new EOD from original one by specifying a model specific pre-analysis hook for exposure modification\nsee ExposurePreAnalysis for more detail",
      "content": "# File: oasislmf/cli/model.py\n# class: GeneratePostFileGenCmd (lines 29-35)\n\nclass GeneratePostFileGenCmd(OasisComputationCommand):\n    \"\"\"\n    Generate a new EOD from original one by specifying a model specific pre-analysis hook for exposure modification\n    see ExposurePreAnalysis for more detail\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'PostFileGen'\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate a new EOD from original one by specifying a model specific pre-analysis hook for exposure modification\nsee ExposurePreAnalysis for more detail"
    },
    {
      "chunk_id": "oasislmf/cli/model.py::GeneratePrelossCmd@38",
      "source_type": "code",
      "path": "oasislmf/cli/model.py",
      "symbol_type": "class",
      "name": "GeneratePrelossCmd",
      "lineno": 38,
      "end_lineno": 44,
      "business_stage": "other",
      "docstring": "Generate a new EOD from original one by specifying a model specific pre-analysis hook for exposure modification\nsee ExposurePreAnalysis for more detail",
      "content": "# File: oasislmf/cli/model.py\n# class: GeneratePrelossCmd (lines 38-44)\n\nclass GeneratePrelossCmd(OasisComputationCommand):\n    \"\"\"\n    Generate a new EOD from original one by specifying a model specific pre-analysis hook for exposure modification\n    see ExposurePreAnalysis for more detail\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'PreLoss'\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate a new EOD from original one by specifying a model specific pre-analysis hook for exposure modification\nsee ExposurePreAnalysis for more detail"
    },
    {
      "chunk_id": "oasislmf/cli/model.py::GenerateKeysCmd@47",
      "source_type": "code",
      "path": "oasislmf/cli/model.py",
      "symbol_type": "class",
      "name": "GenerateKeysCmd",
      "lineno": 47,
      "end_lineno": 52,
      "business_stage": "other",
      "docstring": "Generates keys from a model lookup, and write Oasis keys and keys error files.",
      "content": "# File: oasislmf/cli/model.py\n# class: GenerateKeysCmd (lines 47-52)\n\nclass GenerateKeysCmd(OasisComputationCommand):\n    \"\"\"\n    Generates keys from a model lookup, and write Oasis keys and keys error files.\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'GenerateKeys'\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerates keys from a model lookup, and write Oasis keys and keys error files."
    },
    {
      "chunk_id": "oasislmf/cli/model.py::GenerateOasisFilesCmd@55",
      "source_type": "code",
      "path": "oasislmf/cli/model.py",
      "symbol_type": "class",
      "name": "GenerateOasisFilesCmd",
      "lineno": 55,
      "end_lineno": 61,
      "business_stage": "other",
      "docstring": "Generates the standard Oasis GUL input files + optionally the IL/FM input\nfiles and the RI input files.",
      "content": "# File: oasislmf/cli/model.py\n# class: GenerateOasisFilesCmd (lines 55-61)\n\nclass GenerateOasisFilesCmd(OasisComputationCommand):\n    \"\"\"\n    Generates the standard Oasis GUL input files + optionally the IL/FM input\n    files and the RI input files.\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'GenerateOasisFiles'\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerates the standard Oasis GUL input files + optionally the IL/FM input\nfiles and the RI input files."
    },
    {
      "chunk_id": "oasislmf/cli/model.py::GenerateLossesCmd@64",
      "source_type": "code",
      "path": "oasislmf/cli/model.py",
      "symbol_type": "class",
      "name": "GenerateLossesCmd",
      "lineno": 64,
      "end_lineno": 70,
      "business_stage": "other",
      "docstring": "Generates the standard Oasis GUL input files + optionally the IL/FM input\nfiles and the RI input files.",
      "content": "# File: oasislmf/cli/model.py\n# class: GenerateLossesCmd (lines 64-70)\n\nclass GenerateLossesCmd(OasisComputationCommand):\n    \"\"\"\n    Generates the standard Oasis GUL input files + optionally the IL/FM input\n    files and the RI input files.\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'GenerateOasisLosses'\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerates the standard Oasis GUL input files + optionally the IL/FM input\nfiles and the RI input files."
    },
    {
      "chunk_id": "oasislmf/cli/model.py::GenerateLossesPartialCmd@73",
      "source_type": "code",
      "path": "oasislmf/cli/model.py",
      "symbol_type": "class",
      "name": "GenerateLossesPartialCmd",
      "lineno": 73,
      "end_lineno": 78,
      "business_stage": "other",
      "docstring": "Distributed Oasis CMD: desc todo",
      "content": "# File: oasislmf/cli/model.py\n# class: GenerateLossesPartialCmd (lines 73-78)\n\nclass GenerateLossesPartialCmd(OasisComputationCommand):\n    \"\"\"\n    Distributed Oasis CMD: desc todo\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'GenerateLossesPartial'\n\n\"\"\"Docstring (excerpt)\"\"\"\nDistributed Oasis CMD: desc todo"
    },
    {
      "chunk_id": "oasislmf/cli/model.py::GenerateLossesOutputCmd@81",
      "source_type": "code",
      "path": "oasislmf/cli/model.py",
      "symbol_type": "class",
      "name": "GenerateLossesOutputCmd",
      "lineno": 81,
      "end_lineno": 86,
      "business_stage": "other",
      "docstring": "Distributed Oasis CMD: desc todo",
      "content": "# File: oasislmf/cli/model.py\n# class: GenerateLossesOutputCmd (lines 81-86)\n\nclass GenerateLossesOutputCmd(OasisComputationCommand):\n    \"\"\"\n    Distributed Oasis CMD: desc todo\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'GenerateLossesOutput'\n\n\"\"\"Docstring (excerpt)\"\"\"\nDistributed Oasis CMD: desc todo"
    },
    {
      "chunk_id": "oasislmf/cli/model.py::GenerateDocumentationCmd@89",
      "source_type": "code",
      "path": "oasislmf/cli/model.py",
      "symbol_type": "class",
      "name": "GenerateDocumentationCmd",
      "lineno": 89,
      "end_lineno": 97,
      "business_stage": "other",
      "docstring": "Generate Documentation for model from the config file\n\nThe command line arguments can be supplied in the configuration file\n(``oasislmf.json`` by default or specified with the ``--config`` flag).",
      "content": "# File: oasislmf/cli/model.py\n# class: GenerateDocumentationCmd (lines 89-97)\n\nclass GenerateDocumentationCmd(OasisComputationCommand):\n    \"\"\"\n    Generate Documentation for model from the config file\n\n    The command line arguments can be supplied in the configuration file\n    (``oasislmf.json`` by default or specified with the ``--config`` flag).\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'GenerateDocumentation'\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Documentation for model from the config file\n\nThe command line arguments can be supplied in the configuration file\n(``oasislmf.json`` by default or specified with the ``--config`` flag)."
    },
    {
      "chunk_id": "oasislmf/cli/model.py::RunCmd@100",
      "source_type": "code",
      "path": "oasislmf/cli/model.py",
      "symbol_type": "class",
      "name": "RunCmd",
      "lineno": 100,
      "end_lineno": 108,
      "business_stage": "other",
      "docstring": "Run models end to end.\n\nThe command line arguments can be supplied in the configuration file\n(``oasislmf.json`` by default or specified with the ``--config`` flag).",
      "content": "# File: oasislmf/cli/model.py\n# class: RunCmd (lines 100-108)\n\nclass RunCmd(OasisComputationCommand):\n    \"\"\"\n    Run models end to end.\n\n    The command line arguments can be supplied in the configuration file\n    (``oasislmf.json`` by default or specified with the ``--config`` flag).\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'RunModel'\n\n\"\"\"Docstring (excerpt)\"\"\"\nRun models end to end.\n\nThe command line arguments can be supplied in the configuration file\n(``oasislmf.json`` by default or specified with the ``--config`` flag)."
    },
    {
      "chunk_id": "oasislmf/cli/model.py::RunPostAnalysisCmd@111",
      "source_type": "code",
      "path": "oasislmf/cli/model.py",
      "symbol_type": "class",
      "name": "RunPostAnalysisCmd",
      "lineno": 111,
      "end_lineno": 119,
      "business_stage": "other",
      "docstring": "Run the output postprocessing step.\n\nThe command line arguments can be supplied in the configuration file\n(``oasislmf.json`` by default or specified with the ``--config`` flag).",
      "content": "# File: oasislmf/cli/model.py\n# class: RunPostAnalysisCmd (lines 111-119)\n\nclass RunPostAnalysisCmd(OasisComputationCommand):\n    \"\"\"\n    Run the output postprocessing step.\n\n    The command line arguments can be supplied in the configuration file\n    (``oasislmf.json`` by default or specified with the ``--config`` flag).\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'PostAnalysis'\n\n\"\"\"Docstring (excerpt)\"\"\"\nRun the output postprocessing step.\n\nThe command line arguments can be supplied in the configuration file\n(``oasislmf.json`` by default or specified with the ``--config`` flag)."
    },
    {
      "chunk_id": "oasislmf/cli/model.py::GenerateComputationSettingsJsonSchema@122",
      "source_type": "code",
      "path": "oasislmf/cli/model.py",
      "symbol_type": "class",
      "name": "GenerateComputationSettingsJsonSchema",
      "lineno": 122,
      "end_lineno": 127,
      "business_stage": "other",
      "docstring": "Generate a json schema to validate the computation settings part of oed settings",
      "content": "# File: oasislmf/cli/model.py\n# class: GenerateComputationSettingsJsonSchema (lines 122-127)\n\nclass GenerateComputationSettingsJsonSchema(OasisComputationCommand):\n    \"\"\"\n    Generate a json schema to validate the computation settings part of oed settings\n    \"\"\"\n    formatter_class = RawDescriptionHelpFormatter\n    computation_name = 'GenerateComputationSettingsJsonSchema'\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate a json schema to validate the computation settings part of oed settings"
    },
    {
      "chunk_id": "oasislmf/cli/model.py::ModelCmd@130",
      "source_type": "code",
      "path": "oasislmf/cli/model.py",
      "symbol_type": "class",
      "name": "ModelCmd",
      "lineno": 130,
      "end_lineno": 153,
      "business_stage": "other",
      "docstring": "Model subcommands::\n\n    * generating keys files from model lookups\n    * generating Oasis input CSV files (GUL [+ IL, RI])\n    * generating losses from a preexisting set of Oasis input CSV files\n    * generating deterministic losses (no model)\n    * running a model end-to-end",
      "content": "# File: oasislmf/cli/model.py\n# class: ModelCmd (lines 130-153)\n\nclass ModelCmd(OasisBaseCommand):\n    \"\"\"\n    Model subcommands::\n\n        * generating keys files from model lookups\n        * generating Oasis input CSV files (GUL [+ IL, RI])\n        * generating losses from a preexisting set of Oasis input CSV files\n        * generating deterministic losses (no model)\n        * running a model end-to-end\n    \"\"\"\n    sub_commands = {\n        'generate-exposure-pre-analysis': GenerateExposurePreAnalysisCmd,\n        'generate-keys': GenerateKeysCmd,\n        'generate-oasis-files': GenerateOasisFilesCmd,\n        'generate-post-file-gen': GeneratePostFileGenCmd,\n        'generate-pre-loss': GeneratePrelossCmd,\n        'generate-losses': GenerateLossesCmd,\n        'generate-losses-chunk': GenerateLossesPartialCmd,\n        'generate-losses-output': GenerateLossesOutputCmd,\n        'generate-doc': GenerateDocumentationCmd,\n        'generate-computation-settings-json-schema': GenerateComputationSettingsJsonSchema,\n        'run': RunCmd,\n        'run-postanalysis': RunPostAnalysisCmd,\n    }\n\n\"\"\"Docstring (excerpt)\"\"\"\nModel subcommands::\n\n    * generating keys files from model lookups\n    * generating Oasis input CSV files (GUL [+ IL, RI])\n    * generating losses from a preexisting set of Oasis input CSV files\n    * generating deterministic losses (no model)\n    * running a model end-to-end"
    },
    {
      "chunk_id": "oasislmf/cli/root.py::RootCmd@14",
      "source_type": "code",
      "path": "oasislmf/cli/root.py",
      "symbol_type": "class",
      "name": "RootCmd",
      "lineno": 14,
      "end_lineno": 49,
      "business_stage": "other",
      "docstring": "Tool for managing oasislmf models.",
      "content": "# File: oasislmf/cli/root.py\n# class: RootCmd (lines 14-49)\n\nclass RootCmd(OasisBaseCommand):\n    \"\"\"\n    Tool for managing oasislmf models.\n    \"\"\"\n    sub_commands = {\n        'admin': AdminCmd,\n        'test': TestCmd,\n        'version': VersionCmd,\n        'model': ModelCmd,\n        'exposure': ExposureCmd,\n        'api': ApiCmd,\n        'config': ConfigCmd\n    }\n\n    def run(self, args=None):\n        \"\"\"\n        Runs the command passing in the parsed arguments. If an ``OasisException`` is\n        raised the exception is caught, the error is logged and the process exits with\n        an error code of 1.\n\n        :param args: The arguments to run the command with. If ``None`` the arguments\n            are gathered from the argument parser. This is automatically set when calling\n            sub commands and in most cases should not be set for the root command.\n        :type args: Namespace\n\n        :return: The status code of the action (0 on success)\n        \"\"\"\n        try:\n            return super(OasisBaseCommand, self).run(args=args)\n        except OasisException as e:\n            if self.log_verbose:\n                # Log with traceback\n                self.logger.exception(str(e))\n            else:\n                self.logger.error(str(e))\n            return 1\n\n\"\"\"Docstring (excerpt)\"\"\"\nTool for managing oasislmf models."
    },
    {
      "chunk_id": "oasislmf/cli/root.py::main@52",
      "source_type": "code",
      "path": "oasislmf/cli/root.py",
      "symbol_type": "function",
      "name": "main",
      "lineno": 52,
      "end_lineno": 54,
      "business_stage": "other",
      "docstring": "CLI entrypoint for running the whole RootCmd",
      "content": "# File: oasislmf/cli/root.py\n# function: main (lines 52-54)\n\ndef main():\n    \"\"\"CLI entrypoint for running the whole RootCmd\"\"\"\n    sys.exit(RootCmd().run())\n\n\"\"\"Docstring (excerpt)\"\"\"\nCLI entrypoint for running the whole RootCmd"
    },
    {
      "chunk_id": "oasislmf/cli/root.py::run@28",
      "source_type": "code",
      "path": "oasislmf/cli/root.py",
      "symbol_type": "function",
      "name": "run",
      "lineno": 28,
      "end_lineno": 49,
      "business_stage": "other",
      "docstring": "Runs the command passing in the parsed arguments. If an ``OasisException`` is\nraised the exception is caught, the error is logged and the process exits with\nan error code of 1.\n\n:param args: The arguments to run the command with. If ``None`` the arguments\n    are gathered from the argument parser. This is automatically set when calling\n    sub commands and in most cases should not be set for the root command.\n:type args: Namespace\n\n:return: The status code of the action (0 on success)",
      "content": "# File: oasislmf/cli/root.py\n# function: run (lines 28-49)\n\n    def run(self, args=None):\n        \"\"\"\n        Runs the command passing in the parsed arguments. If an ``OasisException`` is\n        raised the exception is caught, the error is logged and the process exits with\n        an error code of 1.\n\n        :param args: The arguments to run the command with. If ``None`` the arguments\n            are gathered from the argument parser. This is automatically set when calling\n            sub commands and in most cases should not be set for the root command.\n        :type args: Namespace\n\n        :return: The status code of the action (0 on success)\n        \"\"\"\n        try:\n            return super(OasisBaseCommand, self).run(args=args)\n        except OasisException as e:\n            if self.log_verbose:\n                # Log with traceback\n                self.logger.exception(str(e))\n            else:\n                self.logger.error(str(e))\n            return 1\n\n\"\"\"Docstring (excerpt)\"\"\"\nRuns the command passing in the parsed arguments. If an ``OasisException`` is\nraised the exception is caught, the error is logged and the process exits with\nan error code of 1.\n\n:param args: The arguments to run the command with. If ``None`` the arguments\n    are gathered from the argument parser. This is automatically set when calling\n    sub commands and in most cases should not be set for the root command.\n:type args: Namespace\n\n:return: The status code of the action (0 on success)"
    },
    {
      "chunk_id": "oasislmf/cli/version.py::VersionCmd@5",
      "source_type": "code",
      "path": "oasislmf/cli/version.py",
      "symbol_type": "class",
      "name": "VersionCmd",
      "lineno": 5,
      "end_lineno": 17,
      "business_stage": "other",
      "docstring": "Prints the installed package version",
      "content": "# File: oasislmf/cli/version.py\n# class: VersionCmd (lines 5-17)\n\nclass VersionCmd(OasisBaseCommand):\n    \"\"\"\n    Prints the installed package version\n    \"\"\"\n\n    def action(self, args):\n        \"\"\"\n        Prints the version number to the console.\n\n        :param args: The arguments from the command line\n        :type args: Namespace\n        \"\"\"\n        print(__version__)\n\n\"\"\"Docstring (excerpt)\"\"\"\nPrints the installed package version"
    },
    {
      "chunk_id": "oasislmf/cli/version.py::action@10",
      "source_type": "code",
      "path": "oasislmf/cli/version.py",
      "symbol_type": "function",
      "name": "action",
      "lineno": 10,
      "end_lineno": 17,
      "business_stage": "other",
      "docstring": "Prints the version number to the console.\n\n:param args: The arguments from the command line\n:type args: Namespace",
      "content": "# File: oasislmf/cli/version.py\n# function: action (lines 10-17)\n\n    def action(self, args):\n        \"\"\"\n        Prints the version number to the console.\n\n        :param args: The arguments from the command line\n        :type args: Namespace\n        \"\"\"\n        print(__version__)\n\n\"\"\"Docstring (excerpt)\"\"\"\nPrints the version number to the console.\n\n:param args: The arguments from the command line\n:type args: Namespace"
    },
    {
      "chunk_id": "oasislmf/computation/base.py::ComputationStep@20",
      "source_type": "code",
      "path": "oasislmf/computation/base.py",
      "symbol_type": "class",
      "name": "ComputationStep",
      "lineno": 20,
      "end_lineno": 227,
      "business_stage": "other",
      "docstring": "\"Abstract\" Class for all Computation Step (ExposurePreAnalysis, GulCalc, ...)\ninitialise the object with all specified param un step_param and sub- ComputationStep\nprovide a generic interface to get the all those parameter definitions (get_params)\n\nthe Run method must be implemented and contain le business execution logic.",
      "content": "# File: oasislmf/computation/base.py\n# class: ComputationStep (lines 20-227)\n\nclass ComputationStep:\n    \"\"\"\n    \"Abstract\" Class for all Computation Step (ExposurePreAnalysis, GulCalc, ...)\n    initialise the object with all specified param un step_param and sub- ComputationStep\n    provide a generic interface to get the all those parameter definitions (get_params)\n\n    the Run method must be implemented and contain le business execution logic.\n\n    \"\"\"\n\n    step_params = []\n    chained_commands = []\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        initialise the ComputationStep objects:\n         - do the basic check for required parameter (required)\n         - provide default value if defined (default)\n         - check path existence (pre_exist)\n         - create necessary directories (is_dir, is_path)\n        \"\"\"\n        self.logger = logging.getLogger(__name__)\n        self.kwargs = kwargs\n        self.logger.debug(f\"{self.__class__.__name__}: \" + json.dumps(self.kwargs, indent=4, default=str))\n        self.run = oasis_log(self.run)\n\n        # set initial value for mdk params\n        for param in self.get_params():\n            param_value = self._get_init_value(param, kwargs)\n            setattr(self, param['name'], param_value)\n\n        # read and merge settings files\n        settings = Settings()\n        for settings_info in self.get_params(param_type=\"settings\"):\n            setting_fp = kwargs.get(settings_info['name'])\n            if setting_fp and pathlib.Path(setting_fp).exists():\n                new_settings = settings_info['loader'](setting_fp)\n                settings.add_settings(new_settings, settings_info.get('user_role'))\n        self.settings = settings.get_settings()\n\n    def _get_init_value(self, param, kwargs):\n        param_value = kwargs.get(param['name'])\n        if param_value in [None, \"\"]:\n            if param.get('required'):\n                raise OasisException(f\"parameter {param['name']} is required \"\n                                     f\"for Computation Step {self.__class__.__name__}\")\n            else:\n                param_value = param.get('default')\n\n        if (getattr(param.get('type'), '__name__', None) == 'str2bool') and (not isinstance(param_value, bool)):\n            try:\n                param_value = str2bool(param_value)\n            except ArgumentTypeError:\n                raise OasisException(\n                    f\"The parameter '{param.get('name')}' has an invalid value '{param_value}' for boolean. Valid strings are (case insensitive):\"\n                    \"\\n  True:  ['yes', 'true', 't', 'y', '1']\"\n                    \"\\n  False: ['no', 'false', 'f', 'n', '0']\"\n                )\n\n        if (param.get('is_path')\n                and param_value is not None\n                and not isinstance(param_value, OedSource)):\n            if param.get('pre_exist') and not os.path.exists(param_value):\n                raise OasisException(\n                    f\"The path {param_value} ({param['help']}) \"\n                    f\"must exist for Computation Step {self.__class__.__name__}\")\n            else:\n                if param.get('is_dir'):\n                    pathlib.Path(param_value).mkdir(parents=True, exist_ok=True)\n                else:\n                    pathlib.Path(os.path.dirname(param_value)).mkdir(parents=True, exist_ok=True)\n            param_value = str(param_value)\n        return param_value\n\n    @classmethod\n    def get_default_run_dir(cls):\n        return os.path.join(os.getcwd(), 'runs', f'{cls.run_dir_key}-{get_utctimestamp(fmt=\"%Y%m%d%H%M%S\")}')\n\n    @classmethod\n    def get_params(cls, param_type=\"step\"):\n\n\"\"\"Docstring (excerpt)\"\"\"\n\"Abstract\" Class for all Computation Step (ExposurePreAnalysis, GulCalc, ...)\ninitialise the object with all specified param un step_param and sub- ComputationStep\nprovide a generic interface to get the all those parameter definitions (get_params)\n\nthe Run method must be implemented and contain le business execution logic."
    },
    {
      "chunk_id": "oasislmf/computation/base.py::get_params@99",
      "source_type": "code",
      "path": "oasislmf/computation/base.py",
      "symbol_type": "function",
      "name": "get_params",
      "lineno": 99,
      "end_lineno": 119,
      "business_stage": "other",
      "docstring": "return all the params of the computation step defined in step_params\nand the params from the sub_computation step in chained_commands\nif two params have the same name, return the param definition of the first param found only\nthis allow to overwrite the param definition of sub step if necessary.",
      "content": "# File: oasislmf/computation/base.py\n# function: get_params (lines 99-119)\n\n    def get_params(cls, param_type=\"step\"):\n        \"\"\"\n        return all the params of the computation step defined in step_params\n        and the params from the sub_computation step in chained_commands\n        if two params have the same name, return the param definition of the first param found only\n        this allow to overwrite the param definition of sub step if necessary.\n        \"\"\"\n        params = {}\n\n        def all_params():\n            for _param in getattr(cls, f\"{param_type}_params\", []):\n                yield _param\n            for command in cls.chained_commands:\n                for _param in command.get_params(param_type=param_type):\n                    yield _param\n\n        for param in all_params():\n            if param['name'] not in params:\n                params[param['name']] = param\n\n        return list(params.values())\n\n\"\"\"Docstring (excerpt)\"\"\"\nreturn all the params of the computation step defined in step_params\nand the params from the sub_computation step in chained_commands\nif two params have the same name, return the param definition of the first param found only\nthis allow to overwrite the param definition of sub step if necessary."
    },
    {
      "chunk_id": "oasislmf/computation/base.py::get_arguments@122",
      "source_type": "code",
      "path": "oasislmf/computation/base.py",
      "symbol_type": "function",
      "name": "get_arguments",
      "lineno": 122,
      "end_lineno": 149,
      "business_stage": "other",
      "docstring": "Return a list of default arguments values for the functions parameters\nIf given arg values in 'kwargs' these will override the defaults",
      "content": "# File: oasislmf/computation/base.py\n# function: get_arguments (lines 122-149)\n\n    def get_arguments(cls, **kwargs):\n        \"\"\"\n        Return a list of default arguments values for the functions parameters\n        If given arg values in 'kwargs' these will override the defaults\n        \"\"\"\n\n        func_args = {el['name']: el.get('default', None) for el in cls.get_params()}\n        type_map = {el['name']: el.get('type', None) for el in cls.get_params()}\n\n        func_kwargs = update_config(kwargs)\n        env_override = str2bool(os.getenv('OASIS_ENV_OVERRIDE', default=False))\n\n        for param in func_args:\n            if env_override and has_oasis_env(param):\n                func_args[param] = get_oasis_env(param, type_map[param])\n            elif param in func_kwargs:\n                func_args[param] = func_kwargs[param]\n\n        computation_settings = Settings()\n        computation_settings.add_settings(func_args, ROOT_USER_ROLE)\n        for settings_info in cls.get_params(param_type=\"settings\"):\n            setting_fp = func_args.get(settings_info[\"name\"])\n            if setting_fp and pathlib.Path(setting_fp).exists():\n                new_settings = settings_info[\"loader\"](setting_fp)\n                computation_settings.add_settings(\n                    new_settings.pop(\"computation_settings\", {}), {'admin'}\n                )\n        return computation_settings.get_settings()\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturn a list of default arguments values for the functions parameters\nIf given arg values in 'kwargs' these will override the defaults"
    },
    {
      "chunk_id": "oasislmf/computation/base.py::get_signature@152",
      "source_type": "code",
      "path": "oasislmf/computation/base.py",
      "symbol_type": "function",
      "name": "get_signature",
      "lineno": 152,
      "end_lineno": 174,
      "business_stage": "other",
      "docstring": "Create a function signature based on the 'get_params()' return",
      "content": "# File: oasislmf/computation/base.py\n# function: get_signature (lines 152-174)\n\n    def get_signature(cls):\n        \"\"\" Create a function signature based on the 'get_params()' return\n        \"\"\"\n        try:\n            # Create keyword params (without default values)\n            params = [\"{}=None\".format(p.get('name')) for p in cls.get_params() if not p.get('default')]\n\n            # Create keyword params (with default values)\n            for p in [p for p in cls.get_params() if p.get('default')]:\n                if isinstance(p.get('default'), str):\n                    params.append(\"{}='{}'\".format(p.get('name'), p.get('default')))\n                elif isinstance(p.get('default'), dict):\n                    params.append(\"{}=dict()\".format(p.get('name'), p.get('default')))\n                elif isinstance(p.get('default'), OrderedDict):\n                    params.append(\"{}=OrderedDict()\".format(p.get('name'), p.get('default')))\n                else:\n                    params.append(\"{}={}\".format(p.get('name'), p.get('default')))\n\n            exec('def func_sig({}): pass'.format(\", \".join(params)))\n            return inspect.signature(locals()['func_sig'])\n        except Exception:\n            # ignore any errors in signature creation and return blank\n            return None\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreate a function signature based on the 'get_params()' return"
    },
    {
      "chunk_id": "oasislmf/computation/base.py::get_computation_settings_json_schema@177",
      "source_type": "code",
      "path": "oasislmf/computation/base.py",
      "symbol_type": "function",
      "name": "get_computation_settings_json_schema",
      "lineno": 177,
      "end_lineno": 223,
      "business_stage": "other",
      "docstring": "return a json schema equivalent to validate the input of the command line",
      "content": "# File: oasislmf/computation/base.py\n# function: get_computation_settings_json_schema (lines 177-223)\n\n    def get_computation_settings_json_schema(cls):\n        \"\"\"\n            return a json schema equivalent to validate the input of the command line\n        \"\"\"\n\n        arg_type_to_json_type = {\n            str: \"string\",\n            int: \"number\",\n            float: \"number\",\n            str2bool: \"boolean\",\n        }\n\n        def get_json_type(_param):\n            if _param.get('type') in arg_type_to_json_type:\n                return arg_type_to_json_type[_param.get('type')]\n            elif _param.get('is_path'):\n                return \"string\"\n            elif _param.get('default') in [True, False]:\n                return \"boolean\"\n            elif isinstance(_param.get('default'), dict):\n                return \"object\"\n            elif isinstance(_param.get('default'), list):\n                return \"array\"\n            elif isinstance(_param.get('default'), str):\n                return \"string\"\n            else:\n                return \"string\"\n\n        json_schema = {\n            \"$schema\": \"http://oasislmf.org/computation_settings/draft/schema#\",\n            \"type\": \"object\",\n            \"title\": \"Computation settings.\",\n            \"description\": \"Specifies the computation settings and outputs for an analysis.\",\n            \"additionalProperties\": False,\n            \"properties\": {}\n        }\n        settings_param_names = [param['name'] for param in cls.get_params(param_type=\"settings\")]\n        for param in cls.get_params():\n            if param['name'] in settings_param_names:  # param is a json settings and therefore cannot be in the settings schema\n                continue\n            param_schema = {\"type\": get_json_type(param)}\n            if param.get('help'):\n                param_schema[\"description\"] = param['help']\n            if param.get('choices'):\n                param_schema[\"enum\"] = param.get('choices')\n            json_schema[\"properties\"][param['name']] = param_schema\n        return json_schema\n\n\"\"\"Docstring (excerpt)\"\"\"\nreturn a json schema equivalent to validate the input of the command line"
    },
    {
      "chunk_id": "oasislmf/computation/base.py::run@225",
      "source_type": "code",
      "path": "oasislmf/computation/base.py",
      "symbol_type": "function",
      "name": "run",
      "lineno": 225,
      "end_lineno": 227,
      "business_stage": "other",
      "docstring": "method that will be call by all the interface to execute the computation step",
      "content": "# File: oasislmf/computation/base.py\n# function: run (lines 225-227)\n\n    def run(self):\n        \"\"\"method that will be call by all the interface to execute the computation step\"\"\"\n        raise NotImplemented(f'Methode run must be implemented in {self.__class__.__name__}')\n\n\"\"\"Docstring (excerpt)\"\"\"\nmethod that will be call by all the interface to execute the computation step"
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::ModelFile@29",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "ModelFile",
      "lineno": 29,
      "end_lineno": 107,
      "business_stage": "other",
      "docstring": "Base class for all dummy model files.\n\nEach dummy model file is a class that inherits from this base class. The\ntypical order of execution is as follows:\n    1. Initialise class attributes and methods (__init__).\n    2. Set random seed (seed_rng).\n    3. Generate random data (generate_data).\n    4. Convert random data to binary format and write to file. This step is\n    done as each line of data is generated to minimise memory use\n    (write_file).\n\nAttributes:\n    seed_rng: Seed random number generator.\n    write_file: Write data to output file in binary format.\n    debug_write_file: Write data to screen in csv format.\n    generate_data: Generate dummy model data.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: ModelFile (lines 29-107)\n\nclass ModelFile:\n    \"\"\"\n    Base class for all dummy model files.\n\n    Each dummy model file is a class that inherits from this base class. The\n    typical order of execution is as follows:\n        1. Initialise class attributes and methods (__init__).\n        2. Set random seed (seed_rng).\n        3. Generate random data (generate_data).\n        4. Convert random data to binary format and write to file. This step is\n        done as each line of data is generated to minimise memory use\n        (write_file).\n\n    Attributes:\n        seed_rng: Seed random number generator.\n        write_file: Write data to output file in binary format.\n        debug_write_file: Write data to screen in csv format.\n        generate_data: Generate dummy model data.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def seed_rng(self):\n        \"\"\"\n        Seed random number generator.\n\n        Assign different random number generator seed to generate each\n        randomised dummy model file data. Pollute seeds with salt to prevent\n        all random number generators starting with the same seed.\n        \"\"\"\n        if self.random_seed == 0:\n            np.random.seed()\n        elif self.random_seed == -1:\n            # Add salt to random seed using name of child class\n            salt = int.from_bytes(type(self).__name__.encode(), 'little')\n            np.random.seed((1234 + salt) % 0xFFFFFFFF)\n        else:\n            # Add salt to random seed using name of child class\n            salt = int.from_bytes(type(self).__name__.encode(), 'little')\n            np.random.seed((self.random_seed + salt) % 0xFFFFFFFF)\n\n    def write_file(self):\n        \"\"\"\n        Write data to output file in binary format.\n\n        General method to convert generated data to binary format and write to\n        file. Calls chlid class-specific generate_data method.\n        \"\"\"\n        with open(self.file_name, 'wb') as f:\n            if self.start_stats:\n                for stat in self.start_stats:\n                    f.write(struct.pack(stat['dtype'], stat['value']))\n            dtypes_list = ''.join(self.dtypes.values())\n            for line in self.generate_data():\n                f.write(struct.pack('=' + dtypes_list, *(line)))\n\n    def debug_write_file(self):\n        \"\"\"\n        Write data to screen in csv format.\n\n        Used for debugging file output.\n        \"\"\"\n        if self.start_stats:\n            for stat in self.start_stats:\n                print('{} = {}'.format(stat['desc'], stat['value']))\n        line_format = '{}' + ',{}' * (len(self.dtypes) - 1)\n        print(line_format.format(*self.dtypes.keys()))\n        for line in self.generate_data():\n            print(line_format.format(*line))\n\n    def generate_data(self):\n        \"\"\"\n        Generate dummy model data.\n\n        Class specific method to generate randomised data. Is called by\n        write_file method.\n        \"\"\"\n        pass\n\n\"\"\"Docstring (excerpt)\"\"\"\nBase class for all dummy model files.\n\nEach dummy model file is a class that inherits from this base class. The\ntypical order of execution is as follows:\n    1. Initialise class attributes and methods (__init__).\n    2. Set random seed (seed_rng).\n    3. Generate random data (generate_data).\n    4. Convert random data to binary format and write to file. This step is\n    done as each line of data is generated to minimise memory use\n    (write_file).\n\nAttributes:\n    seed_rng: Seed random number generator.\n    write_file: Write data to output file in binary format.\n    debug_write_file: Write data to screen in csv format.\n    generate_data: Generate dummy model data."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::VulnerabilityFile@110",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "VulnerabilityFile",
      "lineno": 110,
      "end_lineno": 185,
      "business_stage": "other",
      "docstring": "Generate random data for Vulnerability dummy model file.\n\nThis file shows the conditional distributions of damage for each intensity\nbin and for each vulnerability ID.\n\nAttributes:\n    generate_data: Generate Vulnerability dummy model file data.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: VulnerabilityFile (lines 110-185)\n\nclass VulnerabilityFile(ModelFile):\n    \"\"\"\n    Generate random data for Vulnerability dummy model file.\n\n    This file shows the conditional distributions of damage for each intensity\n    bin and for each vulnerability ID.\n\n    Attributes:\n        generate_data: Generate Vulnerability dummy model file data.\n    \"\"\"\n\n    def __init__(\n        self, num_vulnerabilities, num_intensity_bins, num_damage_bins,\n        vulnerability_sparseness, random_seed, directory\n    ):\n        \"\"\"\n        Initialise VulnerabilityFile class.\n\n        Args:\n            num_vulnerabilities (int): number of vulnerabilities.\n            num_intensity_bins (int): number of intensity bins.\n            num_damage_bins(int): number of damage bins.\n            vulnerability_sparseness (float): percentage of bins normalised to\n                range [0,1] impacted for a vulnerability at an intensity level.\n            random_seed (float): random seed for random number generator.\n            directory (str): dummy model file destination.\n        \"\"\"\n        self.num_vulnerabilities = num_vulnerabilities\n        self.num_intensity_bins = num_intensity_bins\n        self.num_damage_bins = num_damage_bins\n        self.vulnerability_sparseness = vulnerability_sparseness\n        self.dtypes = OrderedDict([\n            ('vulnerability_id', 'i'), ('intensity_bin_index', 'i'),\n            ('damage_bin_index', 'i'), ('prob', 'f')\n        ])\n        self.start_stats = [\n            {\n                'desc': 'Number of damage bins', 'value': num_damage_bins,\n                'dtype': 'i'\n            }\n        ]\n        self.random_seed = random_seed\n        self.data_length = num_vulnerabilities * num_intensity_bins * num_damage_bins\n        self.file_name = os.path.join(directory, 'vulnerability.bin')\n\n    def generate_data(self):\n        \"\"\"\n        Generate Vulnerability dummy model file data.\n\n        Yields:\n            vulnerability (int): vulnerability ID.\n            intensity_bin (int): intensity bin ID.\n            damage_bin (int): damage bin ID.\n            probability (float): impact probability.\n        \"\"\"\n        super().seed_rng()\n        for vulnerability in range(self.num_vulnerabilities):\n            for intensity_bin in range(self.num_intensity_bins):\n\n                # Generate probabalities according to vulnerability sparseness\n                # and normalise\n                triggers = np.random.uniform(size=self.num_damage_bins)\n                probabilities = np.apply_along_axis(\n                    lambda x: np.where(\n                        x < self.vulnerability_sparseness,\n                        np.random.uniform(size=x.shape), 0.0\n                    ), 0, triggers\n                )\n                total_probability = np.sum(probabilities)\n                if (total_probability == 0):\n                    probabilities[0] = 1.0   # First damage bin is always zero-loss\n                else:\n                    probabilities /= total_probability\n\n                for damage_bin, probability in enumerate(probabilities):\n                    yield vulnerability + 1, intensity_bin + 1, damage_bin + 1, probability\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate random data for Vulnerability dummy model file.\n\nThis file shows the conditional distributions of damage for each intensity\nbin and for each vulnerability ID.\n\nAttributes:\n    generate_data: Generate Vulnerability dummy model file data."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::EventsFile@188",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "EventsFile",
      "lineno": 188,
      "end_lineno": 219,
      "business_stage": "other",
      "docstring": "Generate random data for Events dummy model file.\n\nThis file lists event IDs to be run.\n\nAttributes:\n    generate_data: Generate Events dummy model file data.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: EventsFile (lines 188-219)\n\nclass EventsFile(ModelFile):\n    \"\"\"\n    Generate random data for Events dummy model file.\n\n    This file lists event IDs to be run.\n\n    Attributes:\n        generate_data: Generate Events dummy model file data.\n    \"\"\"\n\n    def __init__(self, num_events, directory):\n        \"\"\"\n        Initialise VulnerabilityFile class.\n\n        Args:\n            num_events (int): number of events.\n            directory (str): dummy model file destination.\n        \"\"\"\n        self.num_events = num_events\n        self.dtypes = {'event_id': 'i'}\n        self.start_stats = None\n        self.data_length = num_events\n        self.file_name = os.path.join(directory, 'events.bin')\n\n    def generate_data(self):\n        \"\"\"\n        Generate Events dummy model file data.\n\n        Yields:\n            event (int): event ID.\n        \"\"\"\n        return (tuple([event]) for event in range(1, self.num_events + 1))\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate random data for Events dummy model file.\n\nThis file lists event IDs to be run.\n\nAttributes:\n    generate_data: Generate Events dummy model file data."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::LossFactorsFile@222",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "LossFactorsFile",
      "lineno": 222,
      "end_lineno": 294,
      "business_stage": "other",
      "docstring": "Generate data for Loss Factors dummy model file.\n\nThis file maps post loss amplification/reduction loss factors to\nevent ID-amplification ID pairs.\n\nAttributes:\n    generate_data: Geenrate Loss Factors dummy model file data.\n    write_file: Write data to Loss Factors dummy model file in binary\n      format.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: LossFactorsFile (lines 222-294)\n\nclass LossFactorsFile(ModelFile):\n    \"\"\"\n    Generate data for Loss Factors dummy model file.\n\n    This file maps post loss amplification/reduction loss factors to\n    event ID-amplification ID pairs.\n\n    Attributes:\n        generate_data: Geenrate Loss Factors dummy model file data.\n        write_file: Write data to Loss Factors dummy model file in binary\n          format.\n    \"\"\"\n\n    def __init__(\n        self, num_events, num_amplifications, min_pla_factor, max_pla_factor,\n        random_seed, directory\n    ):\n        \"\"\"\n        Initialise LossFactorsFile class.\n\n        Args:\n            num_events (int): number of events.\n            num_amplifications (int): number of amplification IDs.\n            min_pla_factor (float): minimum post loss amplification/reduction\n              factor.\n            max_pla_factor (float): maximum post loss amplification/reduction\n              factor.\n            random_seed (float): random seed for random number generator.\n            directory (str): dummy model file destination.\n        \"\"\"\n        self.num_events = num_events\n        self.num_amplifications = num_amplifications\n        self.min_pla_factor = min_pla_factor\n        self.delta_pla_factor = max_pla_factor - min_pla_factor\n        self.random_seed = random_seed\n        self.file_name = os.path.join(directory, 'lossfactors.bin')\n        self.start_stats = [\n            {\n                'desc': 'Reserved for future use', 'value': 0, 'dtype': 'i'\n            }\n        ]\n        self.dtypes = OrderedDict([\n            ('event_id', 'i'), ('amplification_id', 'i'), ('factor', 'f')\n        ])\n\n    def generate_data(self):\n        \"\"\"\n        Generate Loss Factors dummy model file data.\n\n        Yields:\n            event (int): event ID\n            amplification (int): amplification ID\n            factor (float): post loss amplification/reduction factor\n        \"\"\"\n        super().seed_rng()\n        for event in range(self.num_events):\n            for amplification in range(self.num_amplifications):\n                factor = np.random.random() * self.delta_pla_factor + self.min_pla_factor\n                factor = np.round(factor, decimals=2)\n                if factor == 1.0:\n                    continue   # Default loss factor = 1.0\n                yield event + 1, amplification + 1, factor\n\n    def write_file(self):\n        \"\"\"\n        Write data to output Loss Factors file in binary format.\n\n        Checks number of amplifications are greater than 0 before calling base\n        class method.\n        \"\"\"\n        if not self.num_amplifications:\n            return\n        super().write_file()\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate data for Loss Factors dummy model file.\n\nThis file maps post loss amplification/reduction loss factors to\nevent ID-amplification ID pairs.\n\nAttributes:\n    generate_data: Geenrate Loss Factors dummy model file data.\n    write_file: Write data to Loss Factors dummy model file in binary\n      format."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::FootprintIdxFile@297",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "FootprintIdxFile",
      "lineno": 297,
      "end_lineno": 338,
      "business_stage": "other",
      "docstring": "Generate data for Footprint index dummy model file.\n\nThe binary footprint file footprint.bin requires the index file\nfootprint.idx.\n\nAttributes:\n    write_file: Write data to Footprint index file in binary format.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: FootprintIdxFile (lines 297-338)\n\nclass FootprintIdxFile(ModelFile):\n    \"\"\"\n    Generate data for Footprint index dummy model file.\n\n    The binary footprint file footprint.bin requires the index file\n    footprint.idx.\n\n    Attributes:\n        write_file: Write data to Footprint index file in binary format.\n    \"\"\"\n\n    def __init__(self, directory):\n        \"\"\"\n        Initialise Footprint index file class.\n\n        Args:\n            directory (str): dummy model file destination.\n        \"\"\"\n        self.dtypes = OrderedDict([\n            ('event_id', 'i'), ('offset', 'q'), ('size', 'q')\n        ])\n        self.dtypes_list = ''.join(self.dtypes.values())\n        self.file_name = os.path.join(directory, 'footprint.idx')\n\n    def write_file(self, event_id, offset, event_size):\n        \"\"\"\n        Write data to output Footprint index file in binary format.\n\n        Overrides method in base class. Converts data to arguments to binary and\n        writes to file. Called by FootprintBinFile.generate_data().\n\n        Args:\n            event_id (int): event ID.\n            offset (long long): position of data for event ID in generated\n                Footprint binary file relative to beginning of that file.\n            size (long long): size of data corresponding to event ID in\n                generated Footprint binary file, long long.\n        \"\"\"\n        with open(self.file_name, 'ab') as f:\n            f.write(struct.pack(\n                '=' + self.dtypes_list, event_id, offset, event_size)\n            )\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate data for Footprint index dummy model file.\n\nThe binary footprint file footprint.bin requires the index file\nfootprint.idx.\n\nAttributes:\n    write_file: Write data to Footprint index file in binary format."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::FootprintBinFile@341",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "FootprintBinFile",
      "lineno": 341,
      "end_lineno": 461,
      "business_stage": "other",
      "docstring": "Generate data for Footprint binary dummy model file.\n\nThis file shows the intensity of a given event-areaperil combination. The\nbinary footprint file footprint.bin requires the index file footprint.idx.\n\nAttributes:\n    generate_data: Generate Footprint binary dummy model file data.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: FootprintBinFile (lines 341-461)\n\nclass FootprintBinFile(ModelFile):\n    \"\"\"\n    Generate data for Footprint binary dummy model file.\n\n    This file shows the intensity of a given event-areaperil combination. The\n    binary footprint file footprint.bin requires the index file footprint.idx.\n\n    Attributes:\n        generate_data: Generate Footprint binary dummy model file data.\n    \"\"\"\n\n    def __init__(\n        self, num_events, num_areaperils, areaperils_per_event,\n        num_intensity_bins, intensity_sparseness, no_intensity_uncertainty,\n        random_seed, directory\n    ):\n        \"\"\"\n        Initialise Footprint binary file class.\n\n        Args:\n            num_events (int): number of events.\n            num_areaperils (int): number of areaperils.\n            areaperils_per_event (int): number of areaperils impacted per event.\n            num_intensity_bins (int): number of intensity bins.\n            intensity_sparseness (float): percentage of bins normalised to\n                range [0,1] impacted for an event and areaperil.\n            no_intensity_uncertainty (bool): flag to indicate whether more than\n                one intensity bin can be impacted, bool.\n            random_seed (float): random seed for random number generator.\n            directory (str): dummy model file destination.\n        \"\"\"\n        self.num_events = num_events\n        self.num_areaperils = num_areaperils\n        self.areaperils_per_event = areaperils_per_event\n        self.num_intensity_bins = num_intensity_bins\n        self.intensity_sparseness = intensity_sparseness\n        self.no_intensity_uncertainty = no_intensity_uncertainty\n        self.random_seed = random_seed\n        self.file_name = os.path.join(directory, 'footprint.bin')\n        self.event_id = 0\n        self.start_stats = [\n            {\n                'desc': 'Number of intensity bins',\n                'value': self.num_intensity_bins, 'dtype': 'i'\n            },\n            {\n                'desc': 'Has Intensity Uncertainty',\n                'value': not self.no_intensity_uncertainty, 'dtype': 'i'\n            }\n        ]\n        self.dtypes = OrderedDict(\n            [\n                ('areaperil_id', 'i'),\n                ('intensity_bin_id', 'i'),\n                ('probability', 'f')\n            ]\n        )\n        self.idx_file = FootprintIdxFile(directory)\n        # Size of data is the same for all events\n        self.size = 0\n        for dtype in self.dtypes.values():\n            self.size += struct.calcsize(dtype)\n        if not self.no_intensity_uncertainty:\n            self.size *= self.num_intensity_bins\n        # Set initial offset\n        self.offset = 0\n        for stat in self.start_stats:\n            self.offset += struct.calcsize(stat['dtype'])\n\n    def generate_data(self):\n        \"\"\"\n        Generate Footprint binary dummy model file data.\n\n        Yields:\n            areaperil (int): areaperil ID.\n            intensity_bin (int): intensity bin ID.\n            probability (float): impact probability.\n        \"\"\"\n        super().seed_rng()\n        for event in range(self.num_events):\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate data for Footprint binary dummy model file.\n\nThis file shows the intensity of a given event-areaperil combination. The\nbinary footprint file footprint.bin requires the index file footprint.idx.\n\nAttributes:\n    generate_data: Generate Footprint binary dummy model file data."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::DamageBinDictFile@464",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "DamageBinDictFile",
      "lineno": 464,
      "end_lineno": 528,
      "business_stage": "other",
      "docstring": "Generate data for Damage Bin Dictionary dummy model file.\n\nThis file shows the discretisation of the effective damageability cumulative\ndistribution function.\n\nAttributes:\n    generate_data: Generate Damage Bin Dictionary dummy model file data.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: DamageBinDictFile (lines 464-528)\n\nclass DamageBinDictFile(ModelFile):\n    \"\"\"\n    Generate data for Damage Bin Dictionary dummy model file.\n\n    This file shows the discretisation of the effective damageability cumulative\n    distribution function.\n\n    Attributes:\n        generate_data: Generate Damage Bin Dictionary dummy model file data.\n    \"\"\"\n\n    def __init__(self, num_damage_bins, directory):\n        \"\"\"\n        Initialise Damage Bin Dictionary file class.\n\n        Args:\n            num_damage_bins (int): number of damage bins.\n            directory (str): dummy model file destination.\n        \"\"\"\n        self.num_damage_bins = num_damage_bins\n        self.dtypes = OrderedDict([\n            ('bin_index', 'i'), ('bin_from', 'f'), ('bin_to', 'f'),\n            ('interpolation', 'f'), ('damage_type', 'i')\n        ])\n        self.start_stats = None\n        self.data_length = num_damage_bins\n        self.file_name = os.path.join(directory, 'damage_bin_dict.bin')\n\n    def generate_data(self):\n        \"\"\"\n        Generate Damage Bin Dictionary dummy model file data.\n\n        First bin always runs from 0 to 0, i.e. has a midpoint (interpolation)\n        of 0. Last bin always runs from 0 to 0, i.e. has a midpoint\n        (interpolation) of 1.\n\n        Yields:\n            bin_id (int): damage bin ID.\n            bin_from (float): damage bin lower limit.\n            bin_to (float): damage bin upper limit.\n            interpolation (float): damage bin midpoint.\n            damage_type (int): damage_type.\n        \"\"\"\n        # Exclude first and last bins for now\n        bin_indexes = np.arange(self.num_damage_bins - 2)\n        bin_from_values = bin_indexes / (self.num_damage_bins - 2)\n        bin_to_values = (bin_indexes + 1) / (self.num_damage_bins - 2)\n        # Set interpolation in middle of bin\n        interpolations = (0.5 + bin_indexes) / (self.num_damage_bins - 2)\n        # Insert first and last bins\n        bin_indexes += 2\n        bin_indexes = np.insert(bin_indexes, 0, 1)\n        bin_indexes = np.append(bin_indexes, self.num_damage_bins)\n        fields = [bin_from_values, bin_to_values, interpolations]\n        for i, field in enumerate(fields):\n            fields[i] = np.insert(field, 0, 0)\n            fields[i] = np.append(fields[i], 1)\n        bin_from_values, bin_to_values, interpolations = fields\n        # Set damage_type for all bins to 0 (unused)\n        damage_type = 0\n\n        for bin_id, bin_from, bin_to, interpolation in zip(\n            bin_indexes, bin_from_values, bin_to_values, interpolations\n        ):\n            yield bin_id, bin_from, bin_to, interpolation, damage_type\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate data for Damage Bin Dictionary dummy model file.\n\nThis file shows the discretisation of the effective damageability cumulative\ndistribution function.\n\nAttributes:\n    generate_data: Generate Damage Bin Dictionary dummy model file data."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::OccurrenceFile@531",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "OccurrenceFile",
      "lineno": 531,
      "end_lineno": 672,
      "business_stage": "other",
      "docstring": "Generate data for Occurrence dummy model file.\n\nThis file maps events to periods, which can represent any length of time.\n\nAttributes:\n    get_num_periods_from_truncated_normal_cdf: Get number of periods on\n        event-by-event basis.\n    get_num_periods_from_truncated_normal_cdf: Get number of periods from\n        truncated normal cumulative distribution function.\n    set_occ_date_id: Set date of occurrence in ktools format.\n    generate_data: Generate Occurrence dummy model file data.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: OccurrenceFile (lines 531-672)\n\nclass OccurrenceFile(ModelFile):\n    \"\"\"\n    Generate data for Occurrence dummy model file.\n\n    This file maps events to periods, which can represent any length of time.\n\n    Attributes:\n        get_num_periods_from_truncated_normal_cdf: Get number of periods on\n            event-by-event basis.\n        get_num_periods_from_truncated_normal_cdf: Get number of periods from\n            truncated normal cumulative distribution function.\n        set_occ_date_id: Set date of occurrence in ktools format.\n        generate_data: Generate Occurrence dummy model file data.\n    \"\"\"\n\n    def __init__(\n            self, num_events, num_periods, random_seed, directory, mean, stddev\n    ):\n        \"\"\"\n        Initialise Occurrence file class.\n\n        Args:\n            num_events (int): number of events.\n            num_periods (int): total number of periods.\n            random_seed (float): random seed for random number generator.\n            directory (str): dummy model file destination.\n            mean (float): mean of truncated normal distribution sampled to\n                determine number of periods per event.\n            stddev (float): standard deviation of truncated normal distribution\n                sampled to determine number of periods per event.\n        \"\"\"\n        self.num_events = num_events\n        self.num_periods = num_periods\n        self.dtypes = OrderedDict([\n            ('event_id', 'i'), ('period_no', 'i'), ('occ_date_id', 'i')\n        ])\n        self.date_algorithm = 1\n        self.start_stats = [\n            {\n                'desc': 'Date algorithm', 'value': self.date_algorithm,\n                'dtype': 'i'\n            },\n            {\n                'desc': 'Number of periods', 'value': self.num_periods,\n                'dtype': 'i'\n            }\n        ]\n        self.random_seed = random_seed\n        self.data_length = num_events\n        self.mean = mean\n        self.stddev = stddev\n        self.file_name = os.path.join(directory, 'occurrence.bin')\n\n    def get_num_periods_from_truncated_normal_cdf(self):\n        \"\"\"\n        Get number of periods from truncated normal cumulative distribution\n        function.\n\n        Events can occur mupltiple times over multiple periods in the occurrence\n        file. The number of periods per event is modelled by sampling from a\n        truncated normal distribution with mean self.mean and standard deviation\n        self.stddev. The lower tail of the distribution is truncated at 0.5 and\n        the cumulative distribution function is given by:\n\n        F(x) = [Phi(g(x)) - Phi(g(a))] / [Phi(g(b)) - Phi(g(a))]\n        g(y) = (y - mean) / standard_deviation\n        Phi(g(y)) = 1/2 * (1 + erf(g(y) / sqrt(2)))\n        a = lower boundary = 0.5, b = upper boundary = infinity\n          therefore g(b) -> infinity ===> Phi(g(b)) -> 1\n\n        Returns:\n            bound_a (int): lower boundary, when converted to an integer gives\n                number of periods for this event.\n        \"\"\"\n        alpha = (0.5 - self.mean) / self.stddev\n        phi_alpha = 0.5 * (1 + erf(alpha / np.sqrt(2)))\n        rand_no = np.random.random()\n        bound_a = 0.5\n        while True:\n            xi = (bound_a - self.mean) / self.stddev\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate data for Occurrence dummy model file.\n\nThis file maps events to periods, which can represent any length of time.\n\nAttributes:\n    get_num_periods_from_truncated_normal_cdf: Get number of periods on\n        event-by-event basis.\n    get_num_periods_from_truncated_normal_cdf: Get number of periods from\n        truncated normal cumulative distribution function.\n    set_occ_date_id: Set date of occurrence in ktools format.\n    generate_data: Generate Occurrence dummy model file data."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::RandomFile@675",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "RandomFile",
      "lineno": 675,
      "end_lineno": 710,
      "business_stage": "other",
      "docstring": "Generate data for Random Numbers dummy model file.\n\nThis optional file contains random numbers for ground up loss sampling.\n\nAttributes:\n    generate_data: Generate Random Numbers dummy model file data.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: RandomFile (lines 675-710)\n\nclass RandomFile(ModelFile):\n    \"\"\"\n    Generate data for Random Numbers dummy model file.\n\n    This optional file contains random numbers for ground up loss sampling.\n\n    Attributes:\n        generate_data: Generate Random Numbers dummy model file data.\n    \"\"\"\n\n    def __init__(self, num_randoms, random_seed, directory):\n        \"\"\"\n        Initialise Random Numbers file class.\n\n        Args:\n            num_randoms (int): number of random numbers.\n            random_seed (float): random seed for random number generator.\n            directory (str): dummy model file destination.\n        \"\"\"\n        self.num_randoms = num_randoms\n        self.dtypes = {'random_no': 'f'}\n        self.start_stats = None\n        self.random_seed = random_seed\n        self.data_length = num_randoms\n        self.file_name = os.path.join(directory, 'random.bin')\n\n    def generate_data(self):\n        \"\"\"\n        Generate Random Numbers dummy model file data.\n\n        Yields:\n            random number (float): random number.\n        \"\"\"\n        super().seed_rng()\n        # First random number is 0\n        return (tuple([np.random.uniform()]) if i != 0 else (0,) for i in range(self.num_randoms))\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate data for Random Numbers dummy model file.\n\nThis optional file contains random numbers for ground up loss sampling.\n\nAttributes:\n    generate_data: Generate Random Numbers dummy model file data."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::CoveragesFile@713",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "CoveragesFile",
      "lineno": 713,
      "end_lineno": 756,
      "business_stage": "other",
      "docstring": "Generate data for Coverages dummy model Oasis file.\n\nThis file maps coverage IDs to Total Insured Values.\n\nAttributes:\n    generate_data: Generate Coverages dummy model Oasis file data.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: CoveragesFile (lines 713-756)\n\nclass CoveragesFile(ModelFile):\n    \"\"\"\n    Generate data for Coverages dummy model Oasis file.\n\n    This file maps coverage IDs to Total Insured Values.\n\n    Attributes:\n        generate_data: Generate Coverages dummy model Oasis file data.\n    \"\"\"\n\n    def __init__(\n        self, num_locations, coverages_per_location, random_seed, directory\n    ):\n        \"\"\"\n        Initialise Coverages file class.\n\n        Args:\n            num_locations (int): number of locations.\n            coverages_per_location (int): number of coverage types per location.\n            random_seed (float): random seed for random number generator.\n            directory (str): dummy model file destination.\n        \"\"\"\n        self.num_locations = num_locations\n        self.coverages_per_location = coverages_per_location\n        self.dtypes = {'tiv': 'f'}\n        self.start_stats = None\n        self.random_seed = random_seed\n        self.data_length = num_locations * coverages_per_location\n        self.file_name = os.path.join(directory, 'coverages.bin')\n\n    def generate_data(self):\n        \"\"\"\n        Generate Coverages dummy model file data.\n\n        Yields:\n            total insured value (float): Total Insured Value (TIV).\n        \"\"\"\n        super().seed_rng()\n        # Assume 1-1 mapping between item and coverage IDs\n        return (\n            tuple([np.random.uniform(1, 1000000)]) for _ in range(\n                self.num_locations * self.coverages_per_location\n            )\n        )\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate data for Coverages dummy model Oasis file.\n\nThis file maps coverage IDs to Total Insured Values.\n\nAttributes:\n    generate_data: Generate Coverages dummy model Oasis file data."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::ItemsFile@759",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "ItemsFile",
      "lineno": 759,
      "end_lineno": 822,
      "business_stage": "other",
      "docstring": "Generate data for Items dummy model Oasis file.\n\nThis file lists the exposure items for which ground up loss will be sampled.\n\nAttributes:\n    generate_data: Generate Items dummy model Oasis file data.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: ItemsFile (lines 759-822)\n\nclass ItemsFile(ModelFile):\n    \"\"\"\n    Generate data for Items dummy model Oasis file.\n\n    This file lists the exposure items for which ground up loss will be sampled.\n\n    Attributes:\n        generate_data: Generate Items dummy model Oasis file data.\n    \"\"\"\n\n    def __init__(\n        self, num_locations, coverages_per_location, num_areaperils,\n        num_vulnerabilities, random_seed, directory\n    ):\n        \"\"\"\n        Initialise Items file class.\n\n        Args:\n            num_locations (int): number of locations.\n            coverages_per_location (int): number of coverage types per location.\n            num_areaperils (int): number of areaperils.\n            num_vulnerabilities (int): number of vulnerabilities.\n            random_seed (float): random seed for random number generator.\n            directory (str): dummy model file destination.\n        \"\"\"\n        self.num_locations = num_locations\n        self.coverages_per_location = coverages_per_location\n        self.num_areaperils = num_areaperils\n        self.num_vulnerabilities = num_vulnerabilities\n        self.dtypes = OrderedDict([\n            ('item_id', 'i'), ('coverage_id', 'i'), ('areaperil_id', 'i'),\n            ('vulnerability_id', 'i'), ('group_id', 'i'),\n        ])\n        self.start_stats = None\n        self.random_seed = random_seed\n        self.data_length = num_locations * coverages_per_location\n        self.file_name = os.path.join(directory, 'items.bin')\n\n    def generate_data(self):\n        \"\"\"\n        Generate Items dummy model file data.\n\n        Yields:\n            item (int): item ID.\n            item (int): coverage ID = item ID (1-1 mapping).\n            areaperils[coverage] (int): areaperil ID corresponding to\n                coverage ID.\n            vulnerabilities[coverage] (int): vulnerability ID corresponding to\n                coverage ID.\n            location (int): group ID mapped to location ID.\n        \"\"\"\n        super().seed_rng()\n        for location in range(self.num_locations):\n            areaperils = np.random.randint(\n                1, self.num_areaperils + 1, size=self.coverages_per_location\n            )\n            vulnerabilities = np.random.randint(\n                1, self.num_vulnerabilities + 1, size=self.coverages_per_location\n            )\n            for coverage in range(self.coverages_per_location):\n                item = self.coverages_per_location * location + coverage + 1\n                # Assume 1-1 mapping between item and coverage IDs\n                # Assume group ID mapped to location\n                yield item, item, areaperils[coverage], vulnerabilities[coverage], location + 1\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate data for Items dummy model Oasis file.\n\nThis file lists the exposure items for which ground up loss will be sampled.\n\nAttributes:\n    generate_data: Generate Items dummy model Oasis file data."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::AmplificationsFile@825",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "AmplificationsFile",
      "lineno": 825,
      "end_lineno": 884,
      "business_stage": "other",
      "docstring": "Generate data for Amplifications dummy model Oasis file.\n\nThis file maps exposure items to amplification IDs.\n\nAttributes:\n    generate_data: Generate Amplifications dummy model Oasis file data.\n    write_file: Write data to Amplifications dummy model Oasis file in\n      binary format.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: AmplificationsFile (lines 825-884)\n\nclass AmplificationsFile(ModelFile):\n    \"\"\"\n    Generate data for Amplifications dummy model Oasis file.\n\n    This file maps exposure items to amplification IDs.\n\n    Attributes:\n        generate_data: Generate Amplifications dummy model Oasis file data.\n        write_file: Write data to Amplifications dummy model Oasis file in\n          binary format.\n    \"\"\"\n\n    def __init__(\n        self, num_locations, coverages_per_location, num_amplifications,\n        random_seed, directory\n    ):\n        \"\"\"\n        Initialise AmplificationsFile class.\n\n        Args:\n            num_locations (int): number of locations.\n            coverages_per_location (int): number of coverage types per location.\n            num_amplifications (int): number of amplifications.\n            random_ssed (float): random seed for random number generator.\n            directory (str): dummy model file destination.\n        \"\"\"\n        self.num_items = coverages_per_location * num_locations\n        self.num_amplifications = num_amplifications\n        self.random_seed = random_seed\n        self.file_name = os.path.join(directory, 'amplifications.bin')\n        self.start_stats = [\n            {\n                'desc': 'Reserved for fuiure use', 'value': 0, 'dtype': 'i'\n            }\n        ]\n        self.dtypes = OrderedDict([('item_id', 'i'), ('amplification_id', 'i')])\n\n    def generate_data(self):\n        \"\"\"\n        Generate Amplifications dummy model Oasis file data.\n\n        Yields:\n            item (int): item ID\n            amplification (int): amplification ID\n        \"\"\"\n        super().seed_rng()\n        for item in range(self.num_items):\n            amplification = np.random.randint(1, self.num_amplifications + 1)\n            yield item + 1, amplification\n\n    def write_file(self):\n        \"\"\"\n        Write data to output Amplifications file in binary format.\n\n        Checks number of amplifications are greater than 0 before calling base\n        class method.\n        \"\"\"\n        if not self.num_amplifications:\n            return\n        super().write_file()\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate data for Amplifications dummy model Oasis file.\n\nThis file maps exposure items to amplification IDs.\n\nAttributes:\n    generate_data: Generate Amplifications dummy model Oasis file data.\n    write_file: Write data to Amplifications dummy model Oasis file in\n      binary format."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::FMFile@887",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "FMFile",
      "lineno": 887,
      "end_lineno": 902,
      "business_stage": "other",
      "docstring": "Parent class for generating random data for Financial Model files.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: FMFile (lines 887-902)\n\nclass FMFile(ModelFile):\n    \"\"\"\n    Parent class for generating random data for Financial Model files.\n    \"\"\"\n\n    def __init__(self, num_locations, coverages_per_location):\n        \"\"\"\n        Initialise Financial Model files classes.\n\n        Args:\n            num_locations (int): number of locations.\n            coverages_per_location (int): number of coverage types per location.\n        \"\"\"\n        self.num_locations = num_locations\n        self.coverages_per_location = coverages_per_location\n        self.start_stats = None\n\n\"\"\"Docstring (excerpt)\"\"\"\nParent class for generating random data for Financial Model files."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::FMProgrammeFile@905",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "FMProgrammeFile",
      "lineno": 905,
      "end_lineno": 952,
      "business_stage": "other",
      "docstring": "Generate data for Financial Model Programme dummy model Oasis file.\n\nThis file shows the level hierarchy.\n\nAttributes:\n    generate_data: Generate Financial Model Programme dummy model Oasis file\n        data.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: FMProgrammeFile (lines 905-952)\n\nclass FMProgrammeFile(FMFile):\n    \"\"\"\n    Generate data for Financial Model Programme dummy model Oasis file.\n\n    This file shows the level hierarchy.\n\n    Attributes:\n        generate_data: Generate Financial Model Programme dummy model Oasis file\n            data.\n    \"\"\"\n\n    def __init__(self, num_locations, coverages_per_location, directory):\n        \"\"\"\n        Initialise Financial Model Programme file class.\n\n        Args:\n            num_locations (int): number of locations.\n            coverages_per_location (int): number of coverage types per location.\n            directory (str): dummy model file destination.\n        \"\"\"\n        super().__init__(num_locations, coverages_per_location)\n        self.dtypes = OrderedDict([\n            ('from_agg_id', 'i'), ('level_id', 'i'), ('to_agg_id', 'i')\n        ])\n        self.data_length = num_locations * coverages_per_location * 2   # 2 from number of levels\n        self.file_name = os.path.join(directory, 'fm_programme.bin')\n\n    def generate_data(self):\n        \"\"\"\n        Generate Financial Model Programme dummy model file data.\n\n        Yields:\n            agg_id (int): from aggregate ID.\n            level (int): level ID.\n            agg_id (int): to aggregate ID.\n        \"\"\"\n        levels = [1, 10]\n        levels = range(1, len(levels) + 1)\n        for level in levels:\n            for agg_id in range(\n                1, self.num_locations * self.coverages_per_location + 1\n            ):\n                # Site coverage FM level\n                if level == 1:\n                    yield agg_id, level, agg_id\n                # Policy layer FM level\n                elif level == len(levels):\n                    yield agg_id, level, 1\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate data for Financial Model Programme dummy model Oasis file.\n\nThis file shows the level hierarchy.\n\nAttributes:\n    generate_data: Generate Financial Model Programme dummy model Oasis file\n        data."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::FMPolicyTCFile@955",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "FMPolicyTCFile",
      "lineno": 955,
      "end_lineno": 1015,
      "business_stage": "other",
      "docstring": "Generate data for Financial Model Policy dummy model Oasis file.\n\nThis file shows the calculation rule (from the Financial Model Policy file)\nthat should be applied to aggregations of loss at a particular level.\n\nAttributes:\n    generate_data: Generate Financial Model Policy dummy model Oasis file\n        data.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: FMPolicyTCFile (lines 955-1015)\n\nclass FMPolicyTCFile(FMFile):\n    \"\"\"\n    Generate data for Financial Model Policy dummy model Oasis file.\n\n    This file shows the calculation rule (from the Financial Model Policy file)\n    that should be applied to aggregations of loss at a particular level.\n\n    Attributes:\n        generate_data: Generate Financial Model Policy dummy model Oasis file\n            data.\n    \"\"\"\n\n    def __init__(\n        self, num_locations, coverages_per_location, num_layers, directory\n    ):\n        \"\"\"\n        Initialise Financial Model Policy file class.\n\n        Args:\n            num_locations (int): number of locations.\n            coverages_per_location (int): number of coverage types per location.\n            num_layers (int): number of layers.\n            directory (str): dummy model file destination.\n        \"\"\"\n        super().__init__(num_locations, coverages_per_location)\n        self.num_layers = num_layers\n        self.dtypes = OrderedDict([\n            ('level_id', 'i'), ('agg_id', 'i'), ('layer_id', 'i'),\n            ('profile_id', 'i')\n        ])\n        self.data_length = num_locations * coverages_per_location + num_layers\n        self.file_name = os.path.join(directory, 'fm_policytc.bin')\n\n    def generate_data(self):\n        \"\"\"\n        Generate Financial Model Policy dummy model file data.\n\n        Yields:\n            level (int): level ID.\n            agg_id (int): aggregate ID.\n            layer (int): layer ID.\n            profile_id (int): profile ID.\n        \"\"\"\n        # Site coverage #1 & policy layer #10 FM levels\n        levels = [1, 10]\n        levels = range(1, len(levels) + 1)\n        profile_id = 1\n        for level in levels:\n            # Site coverage FM level\n            if level == 1:\n                for agg_id in range(\n                    1, self.num_locations * self.coverages_per_location + 1\n                ):\n                    # One layer in site coverage FM level\n                    yield level, agg_id, 1, profile_id\n                profile_id += 1   # Next profile_id\n            # Policy layer FM level\n            elif level == len(levels):\n                for layer in range(self.num_layers):\n                    yield level, 1, layer + 1, profile_id\n                    profile_id += 1   # Next profile_id\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate data for Financial Model Policy dummy model Oasis file.\n\nThis file shows the calculation rule (from the Financial Model Policy file)\nthat should be applied to aggregations of loss at a particular level.\n\nAttributes:\n    generate_data: Generate Financial Model Policy dummy model Oasis file\n        data."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::FMProfileFile@1018",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "FMProfileFile",
      "lineno": 1018,
      "end_lineno": 1086,
      "business_stage": "other",
      "docstring": "Generate data for Financial Model Profile dummy model Oasis file.\n\nThis file contains the list of calculation rules with profile values used\nto generate insurance losses.\n\nAttributes:\n    generate_data: Generate Financial Model Profile dummy model Oasis file\n        data.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: FMProfileFile (lines 1018-1086)\n\nclass FMProfileFile(ModelFile):\n    \"\"\"\n    Generate data for Financial Model Profile dummy model Oasis file.\n\n    This file contains the list of calculation rules with profile values used\n    to generate insurance losses.\n\n    Attributes:\n        generate_data: Generate Financial Model Profile dummy model Oasis file\n            data.\n    \"\"\"\n\n    def __init__(self, num_layers, directory):\n        \"\"\"\n        Initialise Financial Model Profile file class.\n\n        Args:\n            num_layers (int): number of layers.\n            directory (str): dummy model file destination.\n        \"\"\"\n        self.num_layers = num_layers\n        self.dtypes = OrderedDict([\n            ('profile_id', 'i'), ('calcrule_id', 'i'), ('deductible1', 'f'),\n            ('deductible2', 'f'), ('deductible3', 'f'), ('attachment1', 'f'),\n            ('limit1', 'f'), ('share1', 'f'), ('share2', 'f'), ('share3', 'f')\n        ])\n        self.start_stats = None\n        self.data_length = 1 + num_layers   # 1 from pass through at level 1\n        self.file_name = os.path.join(directory, 'fm_profile.bin')\n\n    def generate_data(self):\n        \"\"\"\n        Generate Financial Model Profile dummy model file data.\n\n        Yields:\n            profile_id (int): profile ID.\n            calculation rule ID (int): calculation rule ID (2 or 100).\n            first deductible (float): first deductible (fixed at 0.0).\n            second deductible (float): second deductible (fixed at 0.0).\n            third deductible (float): third deductible (fixed at 0.0).\n            attachment1 (float): attachment point/excess.\n            limit1 (float): limit.\n            first proportional share (float): first proportional\n                share (0.0 or 0.3).\n            second proportional share (float): second proportional\n                share (fixed at 0.0).\n            third proportional share (float): third proportional\n                share (fixed at 0.0).\n        \"\"\"\n        # Pass through for level 1\n        profile_rows = [(1, 100, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)]\n        # First policy\n        init_profile_id = 2\n        init_attachment1 = 500000.0\n        attachment1_offset = 5000000.0\n        max_limit1 = 100000000.0\n        for layer in range(self.num_layers):\n            profile_id = init_profile_id + layer\n            attachment1 = init_attachment1 + attachment1_offset * layer\n            # Set limit1 at maximum for last layer\n            if (layer + 1) == self.num_layers:\n                limit1 = max_limit1\n            else:\n                limit1 = attachment1_offset * (layer + 1)\n            profile_rows.append(\n                (profile_id, 2, 0.0, 0.0, 0.0, attachment1, limit1, 0.3, 0.0, 0.0)\n            )\n        for row in profile_rows:\n            yield row\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate data for Financial Model Profile dummy model Oasis file.\n\nThis file contains the list of calculation rules with profile values used\nto generate insurance losses.\n\nAttributes:\n    generate_data: Generate Financial Model Profile dummy model Oasis file\n        data."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::FMXrefFile@1089",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "FMXrefFile",
      "lineno": 1089,
      "end_lineno": 1137,
      "business_stage": "other",
      "docstring": "Generate data for Financial Model Cross Reference dummy model Oasis file.\n\nThis file shows the mapping between the financial model output ID, and\naggregate and layer IDs.\n\nAttributes:\n    generate_data: Generate Financial Model Cross Reference dummy model\n        Oasis file data.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: FMXrefFile (lines 1089-1137)\n\nclass FMXrefFile(FMFile):\n    \"\"\"\n    Generate data for Financial Model Cross Reference dummy model Oasis file.\n\n    This file shows the mapping between the financial model output ID, and\n    aggregate and layer IDs.\n\n    Attributes:\n        generate_data: Generate Financial Model Cross Reference dummy model\n            Oasis file data.\n    \"\"\"\n\n    def __init__(\n        self, num_locations, coverages_per_location, num_layers, directory\n    ):\n        \"\"\"\n        Initialise Financial Model Cross Reference file class.\n\n        Args:\n            num_locations (int): number of locations.\n            coverages_per_location (int): number of coverage types per location.\n            num_layers (int): number of layers.\n            directory (str): dummy model file destination.\n        \"\"\"\n        super().__init__(num_locations, coverages_per_location)\n        self.num_layers = num_layers\n        self.dtypes = OrderedDict([\n            ('output', 'i'), ('agg_id', 'i'), ('layer_id', 'i')\n        ])\n        self.data_length = num_locations * coverages_per_location * num_layers\n        self.file_name = os.path.join(directory, 'fm_xref.bin')\n\n    def generate_data(self):\n        \"\"\"\n        Generate Financial Model Cross Reference dummy model file data.\n\n        Yields:\n            output_count (int): output ID.\n            agg_id (int): aggregate ID.\n            layer (int): layer ID.\n        \"\"\"\n        layers = range(1, self.num_layers + 1)\n        output_count = 1\n        for agg_id in range(\n            1, self.num_locations * self.coverages_per_location + 1\n        ):\n            for layer in layers:\n                yield output_count, agg_id, layer\n                output_count += 1\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate data for Financial Model Cross Reference dummy model Oasis file.\n\nThis file shows the mapping between the financial model output ID, and\naggregate and layer IDs.\n\nAttributes:\n    generate_data: Generate Financial Model Cross Reference dummy model\n        Oasis file data."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::GULSummaryXrefFile@1140",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "GULSummaryXrefFile",
      "lineno": 1140,
      "end_lineno": 1181,
      "business_stage": "other",
      "docstring": "Generate data for Ground Up Losses Summary Cross Reference dummy model Oasis\nfile.\n\nThis file shows how item ground up losses are summed together at various\nsummary levels in summarycalc.\n\nAttributes:\n    generate_data: Generate Ground Up Losses Summary Cross Reference dummy\n        model Oasis file data.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: GULSummaryXrefFile (lines 1140-1181)\n\nclass GULSummaryXrefFile(FMFile):\n    \"\"\"\n    Generate data for Ground Up Losses Summary Cross Reference dummy model Oasis\n    file.\n\n    This file shows how item ground up losses are summed together at various\n    summary levels in summarycalc.\n\n    Attributes:\n        generate_data: Generate Ground Up Losses Summary Cross Reference dummy\n            model Oasis file data.\n    \"\"\"\n\n    def __init__(self, num_locations, coverages_per_location, directory):\n        \"\"\"\n        Initialise Ground Up Losses Summary Cross Reference file class.\n\n        Args:\n            num_locations (int): number of locations.\n            coverages_per_location (int): number of coverage types per location.\n            directory (str): dummy model file destination.\n        \"\"\"\n        super().__init__(num_locations, coverages_per_location)\n        self.dtypes = OrderedDict([\n            ('item_id', 'i'), ('summary_id', 'i'), ('summaryset_id', 'i')\n        ])\n        self.data_length = num_locations * coverages_per_location\n        self.file_name = os.path.join(directory, 'gulsummaryxref.bin')\n\n    def generate_data(self):\n        \"\"\"\n        Generate Ground Up Losses Summary Cross Reference dummy model file data.\n\n        Yields:\n            item (int): item ID.\n            summary_id (int): summary ID.\n            summaryset_id (int): summary set ID.\n        \"\"\"\n        summary_id = 1\n        summaryset_id = 1\n        for item in range(self.num_locations * self.coverages_per_location):\n            yield item + 1, summary_id, summaryset_id\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate data for Ground Up Losses Summary Cross Reference dummy model Oasis\nfile.\n\nThis file shows how item ground up losses are summed together at various\nsummary levels in summarycalc.\n\nAttributes:\n    generate_data: Generate Ground Up Losses Summary Cross Reference dummy\n        model Oasis file data."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::FMSummaryXrefFile@1184",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "class",
      "name": "FMSummaryXrefFile",
      "lineno": 1184,
      "end_lineno": 1231,
      "business_stage": "other",
      "docstring": "Generate data for Financial Model Summary Cross Reference dummy model Oasis\nfile.\n\nThis file shows how insurance losses are summed together at various levels\nby summarycalc.\n\nAttributes:\n    generate_data: Generate Financial Model Summary Cross Reference dummy\n        model Oasis file data.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# class: FMSummaryXrefFile (lines 1184-1231)\n\nclass FMSummaryXrefFile(FMFile):\n    \"\"\"\n    Generate data for Financial Model Summary Cross Reference dummy model Oasis\n    file.\n\n    This file shows how insurance losses are summed together at various levels\n    by summarycalc.\n\n    Attributes:\n        generate_data: Generate Financial Model Summary Cross Reference dummy\n            model Oasis file data.\n    \"\"\"\n\n    def __init__(\n        self, num_locations, coverages_per_location, num_layers, directory\n    ):\n        \"\"\"\n        Initialise Financial Model Summary Cross Reference file class.\n\n        Args:\n            num_locations (int): number of locations.\n            coverages_per_location (int): number of coverage types per location.\n            num_layers (int): number of layers.\n            directory (str): dummy model file destination.\n        \"\"\"\n        super().__init__(num_locations, coverages_per_location)\n        self.num_layers = num_layers\n        self.dtypes = OrderedDict([\n            ('output_id', 'i'), ('summary_id', 'i'), ('summaryset_id', 'i')\n        ])\n        self.data_length = num_locations * coverages_per_location * num_layers\n        self.file_name = os.path.join(directory, 'fmsummaryxref.bin')\n\n    def generate_data(self):\n        \"\"\"\n        Generate Financial Model Summary Cross Reference dummy model file data.\n\n        Yields:\n            output_id (int): output ID.\n            summary_id (int): summary ID.\n            summaryset_id (int): summary set ID.\n        \"\"\"\n        summary_id = 1\n        summaryset_id = 1\n        for output_id in range(\n            self.num_locations * self.coverages_per_location * self.num_layers\n        ):\n            yield output_id + 1, summary_id, summaryset_id\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate data for Financial Model Summary Cross Reference dummy model Oasis\nfile.\n\nThis file shows how insurance losses are summed together at various levels\nby summarycalc.\n\nAttributes:\n    generate_data: Generate Financial Model Summary Cross Reference dummy\n        model Oasis file data."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::seed_rng@52",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "seed_rng",
      "lineno": 52,
      "end_lineno": 69,
      "business_stage": "other",
      "docstring": "Seed random number generator.\n\nAssign different random number generator seed to generate each\nrandomised dummy model file data. Pollute seeds with salt to prevent\nall random number generators starting with the same seed.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: seed_rng (lines 52-69)\n\n    def seed_rng(self):\n        \"\"\"\n        Seed random number generator.\n\n        Assign different random number generator seed to generate each\n        randomised dummy model file data. Pollute seeds with salt to prevent\n        all random number generators starting with the same seed.\n        \"\"\"\n        if self.random_seed == 0:\n            np.random.seed()\n        elif self.random_seed == -1:\n            # Add salt to random seed using name of child class\n            salt = int.from_bytes(type(self).__name__.encode(), 'little')\n            np.random.seed((1234 + salt) % 0xFFFFFFFF)\n        else:\n            # Add salt to random seed using name of child class\n            salt = int.from_bytes(type(self).__name__.encode(), 'little')\n            np.random.seed((self.random_seed + salt) % 0xFFFFFFFF)\n\n\"\"\"Docstring (excerpt)\"\"\"\nSeed random number generator.\n\nAssign different random number generator seed to generate each\nrandomised dummy model file data. Pollute seeds with salt to prevent\nall random number generators starting with the same seed."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::write_file@71",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "write_file",
      "lineno": 71,
      "end_lineno": 84,
      "business_stage": "other",
      "docstring": "Write data to output file in binary format.\n\nGeneral method to convert generated data to binary format and write to\nfile. Calls chlid class-specific generate_data method.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: write_file (lines 71-84)\n\n    def write_file(self):\n        \"\"\"\n        Write data to output file in binary format.\n\n        General method to convert generated data to binary format and write to\n        file. Calls chlid class-specific generate_data method.\n        \"\"\"\n        with open(self.file_name, 'wb') as f:\n            if self.start_stats:\n                for stat in self.start_stats:\n                    f.write(struct.pack(stat['dtype'], stat['value']))\n            dtypes_list = ''.join(self.dtypes.values())\n            for line in self.generate_data():\n                f.write(struct.pack('=' + dtypes_list, *(line)))\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrite data to output file in binary format.\n\nGeneral method to convert generated data to binary format and write to\nfile. Calls chlid class-specific generate_data method."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::debug_write_file@86",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "debug_write_file",
      "lineno": 86,
      "end_lineno": 98,
      "business_stage": "other",
      "docstring": "Write data to screen in csv format.\n\nUsed for debugging file output.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: debug_write_file (lines 86-98)\n\n    def debug_write_file(self):\n        \"\"\"\n        Write data to screen in csv format.\n\n        Used for debugging file output.\n        \"\"\"\n        if self.start_stats:\n            for stat in self.start_stats:\n                print('{} = {}'.format(stat['desc'], stat['value']))\n        line_format = '{}' + ',{}' * (len(self.dtypes) - 1)\n        print(line_format.format(*self.dtypes.keys()))\n        for line in self.generate_data():\n            print(line_format.format(*line))\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrite data to screen in csv format.\n\nUsed for debugging file output."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::generate_data@100",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "generate_data",
      "lineno": 100,
      "end_lineno": 107,
      "business_stage": "other",
      "docstring": "Generate dummy model data.\n\nClass specific method to generate randomised data. Is called by\nwrite_file method.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: generate_data (lines 100-107)\n\n    def generate_data(self):\n        \"\"\"\n        Generate dummy model data.\n\n        Class specific method to generate randomised data. Is called by\n        write_file method.\n        \"\"\"\n        pass\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate dummy model data.\n\nClass specific method to generate randomised data. Is called by\nwrite_file method."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::generate_data@155",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "generate_data",
      "lineno": 155,
      "end_lineno": 185,
      "business_stage": "other",
      "docstring": "Generate Vulnerability dummy model file data.\n\nYields:\n    vulnerability (int): vulnerability ID.\n    intensity_bin (int): intensity bin ID.\n    damage_bin (int): damage bin ID.\n    probability (float): impact probability.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: generate_data (lines 155-185)\n\n    def generate_data(self):\n        \"\"\"\n        Generate Vulnerability dummy model file data.\n\n        Yields:\n            vulnerability (int): vulnerability ID.\n            intensity_bin (int): intensity bin ID.\n            damage_bin (int): damage bin ID.\n            probability (float): impact probability.\n        \"\"\"\n        super().seed_rng()\n        for vulnerability in range(self.num_vulnerabilities):\n            for intensity_bin in range(self.num_intensity_bins):\n\n                # Generate probabalities according to vulnerability sparseness\n                # and normalise\n                triggers = np.random.uniform(size=self.num_damage_bins)\n                probabilities = np.apply_along_axis(\n                    lambda x: np.where(\n                        x < self.vulnerability_sparseness,\n                        np.random.uniform(size=x.shape), 0.0\n                    ), 0, triggers\n                )\n                total_probability = np.sum(probabilities)\n                if (total_probability == 0):\n                    probabilities[0] = 1.0   # First damage bin is always zero-loss\n                else:\n                    probabilities /= total_probability\n\n                for damage_bin, probability in enumerate(probabilities):\n                    yield vulnerability + 1, intensity_bin + 1, damage_bin + 1, probability\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Vulnerability dummy model file data.\n\nYields:\n    vulnerability (int): vulnerability ID.\n    intensity_bin (int): intensity bin ID.\n    damage_bin (int): damage bin ID.\n    probability (float): impact probability."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::generate_data@212",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "generate_data",
      "lineno": 212,
      "end_lineno": 219,
      "business_stage": "other",
      "docstring": "Generate Events dummy model file data.\n\nYields:\n    event (int): event ID.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: generate_data (lines 212-219)\n\n    def generate_data(self):\n        \"\"\"\n        Generate Events dummy model file data.\n\n        Yields:\n            event (int): event ID.\n        \"\"\"\n        return (tuple([event]) for event in range(1, self.num_events + 1))\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Events dummy model file data.\n\nYields:\n    event (int): event ID."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::generate_data@267",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "generate_data",
      "lineno": 267,
      "end_lineno": 283,
      "business_stage": "other",
      "docstring": "Generate Loss Factors dummy model file data.\n\nYields:\n    event (int): event ID\n    amplification (int): amplification ID\n    factor (float): post loss amplification/reduction factor",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: generate_data (lines 267-283)\n\n    def generate_data(self):\n        \"\"\"\n        Generate Loss Factors dummy model file data.\n\n        Yields:\n            event (int): event ID\n            amplification (int): amplification ID\n            factor (float): post loss amplification/reduction factor\n        \"\"\"\n        super().seed_rng()\n        for event in range(self.num_events):\n            for amplification in range(self.num_amplifications):\n                factor = np.random.random() * self.delta_pla_factor + self.min_pla_factor\n                factor = np.round(factor, decimals=2)\n                if factor == 1.0:\n                    continue   # Default loss factor = 1.0\n                yield event + 1, amplification + 1, factor\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Loss Factors dummy model file data.\n\nYields:\n    event (int): event ID\n    amplification (int): amplification ID\n    factor (float): post loss amplification/reduction factor"
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::write_file@285",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "write_file",
      "lineno": 285,
      "end_lineno": 294,
      "business_stage": "other",
      "docstring": "Write data to output Loss Factors file in binary format.\n\nChecks number of amplifications are greater than 0 before calling base\nclass method.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: write_file (lines 285-294)\n\n    def write_file(self):\n        \"\"\"\n        Write data to output Loss Factors file in binary format.\n\n        Checks number of amplifications are greater than 0 before calling base\n        class method.\n        \"\"\"\n        if not self.num_amplifications:\n            return\n        super().write_file()\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrite data to output Loss Factors file in binary format.\n\nChecks number of amplifications are greater than 0 before calling base\nclass method."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::write_file@321",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "write_file",
      "lineno": 321,
      "end_lineno": 338,
      "business_stage": "other",
      "docstring": "Write data to output Footprint index file in binary format.\n\nOverrides method in base class. Converts data to arguments to binary and\nwrites to file. Called by FootprintBinFile.generate_data().\n\nArgs:\n    event_id (int): event ID.\n    offset (long long): position of data for event ID in generated\n        Footprint binary file relative to beginning of that file.\n    size (long long): size of data corresponding to event ID in\n        generated Footprint binary file, long long.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: write_file (lines 321-338)\n\n    def write_file(self, event_id, offset, event_size):\n        \"\"\"\n        Write data to output Footprint index file in binary format.\n\n        Overrides method in base class. Converts data to arguments to binary and\n        writes to file. Called by FootprintBinFile.generate_data().\n\n        Args:\n            event_id (int): event ID.\n            offset (long long): position of data for event ID in generated\n                Footprint binary file relative to beginning of that file.\n            size (long long): size of data corresponding to event ID in\n                generated Footprint binary file, long long.\n        \"\"\"\n        with open(self.file_name, 'ab') as f:\n            f.write(struct.pack(\n                '=' + self.dtypes_list, event_id, offset, event_size)\n            )\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrite data to output Footprint index file in binary format.\n\nOverrides method in base class. Converts data to arguments to binary and\nwrites to file. Called by FootprintBinFile.generate_data().\n\nArgs:\n    event_id (int): event ID.\n    offset (long long): position of data for event ID in generated\n        Footprint binary file relative to beginning of that file.\n    size (long long): size of data corresponding to event ID in\n        generated Footprint binary file, long long."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::generate_data@410",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "generate_data",
      "lineno": 410,
      "end_lineno": 461,
      "business_stage": "other",
      "docstring": "Generate Footprint binary dummy model file data.\n\nYields:\n    areaperil (int): areaperil ID.\n    intensity_bin (int): intensity bin ID.\n    probability (float): impact probability.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: generate_data (lines 410-461)\n\n    def generate_data(self):\n        \"\"\"\n        Generate Footprint binary dummy model file data.\n\n        Yields:\n            areaperil (int): areaperil ID.\n            intensity_bin (int): intensity bin ID.\n            probability (float): impact probability.\n        \"\"\"\n        super().seed_rng()\n        for event in range(self.num_events):\n            event_size = 0\n\n            if self.areaperils_per_event == self.num_areaperils:\n                selected_areaperils = np.arange(1, self.num_areaperils + 1)\n            else:\n                selected_areaperils = np.random.choice(\n                    self.num_areaperils, self.areaperils_per_event,\n                    replace=False\n                )\n                selected_areaperils += 1\n                selected_areaperils = np.sort(selected_areaperils)\n\n            for areaperil in selected_areaperils:\n                if self.no_intensity_uncertainty:\n                    intensity_bin = np.random.randint(\n                        1, self.num_intensity_bins + 1\n                    )\n                    probability = 1.0\n                    event_size += self.size\n                    yield areaperil, intensity_bin, probability\n                else:\n                    # Generate probabalities according to intensity sparseness\n                    # and normalise\n                    triggers = np.random.uniform(size=self.num_intensity_bins)\n                    probabilities = np.apply_along_axis(\n                        lambda x: np.where(\n                            x < self.intensity_sparseness,\n                            np.random.uniform(size=x.shape), 0.0\n                        ), 0, triggers\n                    )\n                    total_probability = np.sum(probabilities)\n                    if total_probability == 0:\n                        continue   # No impacted intensity bins\n                    event_size += self.size\n                    probabilities /= total_probability\n\n                    for intensity_bin, probability in enumerate(probabilities):\n                        yield areaperil, intensity_bin + 1, probability\n\n            self.idx_file.write_file(event + 1, self.offset, event_size)\n            self.offset += event_size\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Footprint binary dummy model file data.\n\nYields:\n    areaperil (int): areaperil ID.\n    intensity_bin (int): intensity bin ID.\n    probability (float): impact probability."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::generate_data@492",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "generate_data",
      "lineno": 492,
      "end_lineno": 528,
      "business_stage": "other",
      "docstring": "Generate Damage Bin Dictionary dummy model file data.\n\nFirst bin always runs from 0 to 0, i.e. has a midpoint (interpolation)\nof 0. Last bin always runs from 0 to 0, i.e. has a midpoint\n(interpolation) of 1.\n\nYields:\n    bin_id (int): damage bin ID.\n    bin_from (float): damage bin lower limit.\n    bin_to (float): damage bin upper limit.\n    interpolation (float): damage bin midpoint.\n    damage_type (int): damage_type.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: generate_data (lines 492-528)\n\n    def generate_data(self):\n        \"\"\"\n        Generate Damage Bin Dictionary dummy model file data.\n\n        First bin always runs from 0 to 0, i.e. has a midpoint (interpolation)\n        of 0. Last bin always runs from 0 to 0, i.e. has a midpoint\n        (interpolation) of 1.\n\n        Yields:\n            bin_id (int): damage bin ID.\n            bin_from (float): damage bin lower limit.\n            bin_to (float): damage bin upper limit.\n            interpolation (float): damage bin midpoint.\n            damage_type (int): damage_type.\n        \"\"\"\n        # Exclude first and last bins for now\n        bin_indexes = np.arange(self.num_damage_bins - 2)\n        bin_from_values = bin_indexes / (self.num_damage_bins - 2)\n        bin_to_values = (bin_indexes + 1) / (self.num_damage_bins - 2)\n        # Set interpolation in middle of bin\n        interpolations = (0.5 + bin_indexes) / (self.num_damage_bins - 2)\n        # Insert first and last bins\n        bin_indexes += 2\n        bin_indexes = np.insert(bin_indexes, 0, 1)\n        bin_indexes = np.append(bin_indexes, self.num_damage_bins)\n        fields = [bin_from_values, bin_to_values, interpolations]\n        for i, field in enumerate(fields):\n            fields[i] = np.insert(field, 0, 0)\n            fields[i] = np.append(fields[i], 1)\n        bin_from_values, bin_to_values, interpolations = fields\n        # Set damage_type for all bins to 0 (unused)\n        damage_type = 0\n\n        for bin_id, bin_from, bin_to, interpolation in zip(\n            bin_indexes, bin_from_values, bin_to_values, interpolations\n        ):\n            yield bin_id, bin_from, bin_to, interpolation, damage_type\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Damage Bin Dictionary dummy model file data.\n\nFirst bin always runs from 0 to 0, i.e. has a midpoint (interpolation)\nof 0. Last bin always runs from 0 to 0, i.e. has a midpoint\n(interpolation) of 1.\n\nYields:\n    bin_id (int): damage bin ID.\n    bin_from (float): damage bin lower limit.\n    bin_to (float): damage bin upper limit.\n    interpolation (float): damage bin midpoint.\n    damage_type (int): damage_type."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::get_num_periods_from_truncated_normal_cdf@584",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "get_num_periods_from_truncated_normal_cdf",
      "lineno": 584,
      "end_lineno": 614,
      "business_stage": "other",
      "docstring": "Get number of periods from truncated normal cumulative distribution\nfunction.\n\nEvents can occur mupltiple times over multiple periods in the occurrence\nfile. The number of periods per event is modelled by sampling from a\ntruncated normal distribution with mean self.mean and standard deviation\nself.stddev. The lower tail of the distribution is truncated at 0.5 and\nthe cumulative distribution function is given by:\n\nF(x) = [Phi(g(x)) - Phi(g(a))] / [Phi(g(b)) - Phi(g(a))]\ng(y) = (y - mean) / standard_deviation\nPhi(g(y)) = 1/2 * (1 + erf(g(y) / sqrt(2)))\na = lower boundary = 0.5, b = upper boundary = infinity\n  therefore g(b) -> infinity ===> Phi(g(b)) -> 1\n\nReturns:\n    bound_a (int): lower boundary, when converted to an integer gives\n        number of periods for this event.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: get_num_periods_from_truncated_normal_cdf (lines 584-614)\n\n    def get_num_periods_from_truncated_normal_cdf(self):\n        \"\"\"\n        Get number of periods from truncated normal cumulative distribution\n        function.\n\n        Events can occur mupltiple times over multiple periods in the occurrence\n        file. The number of periods per event is modelled by sampling from a\n        truncated normal distribution with mean self.mean and standard deviation\n        self.stddev. The lower tail of the distribution is truncated at 0.5 and\n        the cumulative distribution function is given by:\n\n        F(x) = [Phi(g(x)) - Phi(g(a))] / [Phi(g(b)) - Phi(g(a))]\n        g(y) = (y - mean) / standard_deviation\n        Phi(g(y)) = 1/2 * (1 + erf(g(y) / sqrt(2)))\n        a = lower boundary = 0.5, b = upper boundary = infinity\n          therefore g(b) -> infinity ===> Phi(g(b)) -> 1\n\n        Returns:\n            bound_a (int): lower boundary, when converted to an integer gives\n                number of periods for this event.\n        \"\"\"\n        alpha = (0.5 - self.mean) / self.stddev\n        phi_alpha = 0.5 * (1 + erf(alpha / np.sqrt(2)))\n        rand_no = np.random.random()\n        bound_a = 0.5\n        while True:\n            xi = (bound_a - self.mean) / self.stddev\n            phi_xi = 0.5 * (1 + erf(xi / np.sqrt(2)))\n            if rand_no < ((phi_xi - phi_alpha) / (1 - phi_alpha)):\n                return int(bound_a)\n            bound_a += 1\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet number of periods from truncated normal cumulative distribution\nfunction.\n\nEvents can occur mupltiple times over multiple periods in the occurrence\nfile. The number of periods per event is modelled by sampling from a\ntruncated normal distribution with mean self.mean and standard deviation\nself.stddev. The lower tail of the distribution is truncated at 0.5 and\nthe cumulative distribution function is given by:\n\nF(x) = [Phi(g(x)) - Phi(g(a))] / [Phi(g(b)) - Phi(g(a))]\ng(y) = (y - mean) / standard_deviation\nPhi(g(y)) = 1/2 * (1 + erf(g(y) / sqrt(2)))\na = lower boundary = 0.5, b = upper boundary = infinity\n  therefore g(b) -> infinity ===> Phi(g(b)) -> 1\n\nReturns:\n    bound_a (int): lower boundary, when converted to an integer gives\n        number of periods for this event."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::get_num_periods_per_event@616",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "get_num_periods_per_event",
      "lineno": 616,
      "end_lineno": 630,
      "business_stage": "other",
      "docstring": "Get number of periods on event-by-event basis.\n\nDetermines whether sampling of truncated normal cumulative distribution\nfunction is required to obtain number of periods for this event.\n\nReturns:\n    mean|bound_a (int): Number of periods for this event.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: get_num_periods_per_event (lines 616-630)\n\n    def get_num_periods_per_event(self):\n        \"\"\"\n        Get number of periods on event-by-event basis.\n\n        Determines whether sampling of truncated normal cumulative distribution\n        function is required to obtain number of periods for this event.\n\n        Returns:\n            mean|bound_a (int): Number of periods for this event.\n        \"\"\"\n        # Return mean if standard deviation is 0\n        if self.stddev == 0:\n            return self.mean\n        else:\n            return self.get_num_periods_from_truncated_normal_cdf()\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet number of periods on event-by-event basis.\n\nDetermines whether sampling of truncated normal cumulative distribution\nfunction is required to obtain number of periods for this event.\n\nReturns:\n    mean|bound_a (int): Number of periods for this event."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::set_occ_date_id@632",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "set_occ_date_id",
      "lineno": 632,
      "end_lineno": 649,
      "business_stage": "other",
      "docstring": "Set date of occurrence in ktools format.\n\nReduce year, month and day information to a single integer.\n\nArgs:\n    year (int): year.\n    month (int): month.\n    day (int): day.\n\nReturns:\n    date (int): date in ktools format.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: set_occ_date_id (lines 632-649)\n\n    def set_occ_date_id(self, year, month, day):\n        \"\"\"\n        Set date of occurrence in ktools format.\n\n        Reduce year, month and day information to a single integer.\n\n        Args:\n            year (int): year.\n            month (int): month.\n            day (int): day.\n\n        Returns:\n            date (int): date in ktools format.\n        \"\"\"\n        # Set date relative to epoch\n        month = (month + 9) % 12\n        year = year - month // 10\n        return 365 * year + year // 4 - year // 100 + year // 400 + (306 * month + 5) // 10 + (day - 1)\n\n\"\"\"Docstring (excerpt)\"\"\"\nSet date of occurrence in ktools format.\n\nReduce year, month and day information to a single integer.\n\nArgs:\n    year (int): year.\n    month (int): month.\n    day (int): day.\n\nReturns:\n    date (int): date in ktools format."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::generate_data@651",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "generate_data",
      "lineno": 651,
      "end_lineno": 672,
      "business_stage": "other",
      "docstring": "Generate Occurrence dummy model file data.\n\nYields:\n    event (int): event ID.\n    period_no (int): period number.\n    date (int): date in ktools format.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: generate_data (lines 651-672)\n\n    def generate_data(self):\n        \"\"\"\n        Generate Occurrence dummy model file data.\n\n        Yields:\n            event (int): event ID.\n            period_no (int): period number.\n            date (int): date in ktools format.\n        \"\"\"\n        super().seed_rng()\n        months = np.arange(1, 13)\n        days_per_month = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n        months_weights = np.array(days_per_month, dtype=float)\n        months_weights /= months_weights.sum()   # Normalise\n        for event in range(self.num_events):\n            for _ in range(self.get_num_periods_per_event()):\n                period_no = np.random.randint(1, self.num_periods + 1)\n                occ_year = period_no   # Assume one period represents one year\n                occ_month = np.random.choice(months, p=months_weights)\n                occ_day = np.random.randint(1, days_per_month[occ_month - 1])\n                occ_date = self.set_occ_date_id(occ_year, occ_month, occ_day)\n                yield event + 1, period_no, occ_date\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Occurrence dummy model file data.\n\nYields:\n    event (int): event ID.\n    period_no (int): period number.\n    date (int): date in ktools format."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::generate_data@701",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "generate_data",
      "lineno": 701,
      "end_lineno": 710,
      "business_stage": "other",
      "docstring": "Generate Random Numbers dummy model file data.\n\nYields:\n    random number (float): random number.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: generate_data (lines 701-710)\n\n    def generate_data(self):\n        \"\"\"\n        Generate Random Numbers dummy model file data.\n\n        Yields:\n            random number (float): random number.\n        \"\"\"\n        super().seed_rng()\n        # First random number is 0\n        return (tuple([np.random.uniform()]) if i != 0 else (0,) for i in range(self.num_randoms))\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Random Numbers dummy model file data.\n\nYields:\n    random number (float): random number."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::generate_data@743",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "generate_data",
      "lineno": 743,
      "end_lineno": 756,
      "business_stage": "other",
      "docstring": "Generate Coverages dummy model file data.\n\nYields:\n    total insured value (float): Total Insured Value (TIV).",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: generate_data (lines 743-756)\n\n    def generate_data(self):\n        \"\"\"\n        Generate Coverages dummy model file data.\n\n        Yields:\n            total insured value (float): Total Insured Value (TIV).\n        \"\"\"\n        super().seed_rng()\n        # Assume 1-1 mapping between item and coverage IDs\n        return (\n            tuple([np.random.uniform(1, 1000000)]) for _ in range(\n                self.num_locations * self.coverages_per_location\n            )\n        )\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Coverages dummy model file data.\n\nYields:\n    total insured value (float): Total Insured Value (TIV)."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::generate_data@797",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "generate_data",
      "lineno": 797,
      "end_lineno": 822,
      "business_stage": "other",
      "docstring": "Generate Items dummy model file data.\n\nYields:\n    item (int): item ID.\n    item (int): coverage ID = item ID (1-1 mapping).\n    areaperils[coverage] (int): areaperil ID corresponding to\n        coverage ID.\n    vulnerabilities[coverage] (int): vulnerability ID corresponding to\n        coverage ID.\n    location (int): group ID mapped to location ID.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: generate_data (lines 797-822)\n\n    def generate_data(self):\n        \"\"\"\n        Generate Items dummy model file data.\n\n        Yields:\n            item (int): item ID.\n            item (int): coverage ID = item ID (1-1 mapping).\n            areaperils[coverage] (int): areaperil ID corresponding to\n                coverage ID.\n            vulnerabilities[coverage] (int): vulnerability ID corresponding to\n                coverage ID.\n            location (int): group ID mapped to location ID.\n        \"\"\"\n        super().seed_rng()\n        for location in range(self.num_locations):\n            areaperils = np.random.randint(\n                1, self.num_areaperils + 1, size=self.coverages_per_location\n            )\n            vulnerabilities = np.random.randint(\n                1, self.num_vulnerabilities + 1, size=self.coverages_per_location\n            )\n            for coverage in range(self.coverages_per_location):\n                item = self.coverages_per_location * location + coverage + 1\n                # Assume 1-1 mapping between item and coverage IDs\n                # Assume group ID mapped to location\n                yield item, item, areaperils[coverage], vulnerabilities[coverage], location + 1\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Items dummy model file data.\n\nYields:\n    item (int): item ID.\n    item (int): coverage ID = item ID (1-1 mapping).\n    areaperils[coverage] (int): areaperil ID corresponding to\n        coverage ID.\n    vulnerabilities[coverage] (int): vulnerability ID corresponding to\n        coverage ID.\n    location (int): group ID mapped to location ID."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::generate_data@862",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "generate_data",
      "lineno": 862,
      "end_lineno": 873,
      "business_stage": "other",
      "docstring": "Generate Amplifications dummy model Oasis file data.\n\nYields:\n    item (int): item ID\n    amplification (int): amplification ID",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: generate_data (lines 862-873)\n\n    def generate_data(self):\n        \"\"\"\n        Generate Amplifications dummy model Oasis file data.\n\n        Yields:\n            item (int): item ID\n            amplification (int): amplification ID\n        \"\"\"\n        super().seed_rng()\n        for item in range(self.num_items):\n            amplification = np.random.randint(1, self.num_amplifications + 1)\n            yield item + 1, amplification\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Amplifications dummy model Oasis file data.\n\nYields:\n    item (int): item ID\n    amplification (int): amplification ID"
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::write_file@875",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "write_file",
      "lineno": 875,
      "end_lineno": 884,
      "business_stage": "other",
      "docstring": "Write data to output Amplifications file in binary format.\n\nChecks number of amplifications are greater than 0 before calling base\nclass method.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: write_file (lines 875-884)\n\n    def write_file(self):\n        \"\"\"\n        Write data to output Amplifications file in binary format.\n\n        Checks number of amplifications are greater than 0 before calling base\n        class method.\n        \"\"\"\n        if not self.num_amplifications:\n            return\n        super().write_file()\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrite data to output Amplifications file in binary format.\n\nChecks number of amplifications are greater than 0 before calling base\nclass method."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::generate_data@932",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "generate_data",
      "lineno": 932,
      "end_lineno": 952,
      "business_stage": "other",
      "docstring": "Generate Financial Model Programme dummy model file data.\n\nYields:\n    agg_id (int): from aggregate ID.\n    level (int): level ID.\n    agg_id (int): to aggregate ID.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: generate_data (lines 932-952)\n\n    def generate_data(self):\n        \"\"\"\n        Generate Financial Model Programme dummy model file data.\n\n        Yields:\n            agg_id (int): from aggregate ID.\n            level (int): level ID.\n            agg_id (int): to aggregate ID.\n        \"\"\"\n        levels = [1, 10]\n        levels = range(1, len(levels) + 1)\n        for level in levels:\n            for agg_id in range(\n                1, self.num_locations * self.coverages_per_location + 1\n            ):\n                # Site coverage FM level\n                if level == 1:\n                    yield agg_id, level, agg_id\n                # Policy layer FM level\n                elif level == len(levels):\n                    yield agg_id, level, 1\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Financial Model Programme dummy model file data.\n\nYields:\n    agg_id (int): from aggregate ID.\n    level (int): level ID.\n    agg_id (int): to aggregate ID."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::generate_data@988",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "generate_data",
      "lineno": 988,
      "end_lineno": 1015,
      "business_stage": "other",
      "docstring": "Generate Financial Model Policy dummy model file data.\n\nYields:\n    level (int): level ID.\n    agg_id (int): aggregate ID.\n    layer (int): layer ID.\n    profile_id (int): profile ID.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: generate_data (lines 988-1015)\n\n    def generate_data(self):\n        \"\"\"\n        Generate Financial Model Policy dummy model file data.\n\n        Yields:\n            level (int): level ID.\n            agg_id (int): aggregate ID.\n            layer (int): layer ID.\n            profile_id (int): profile ID.\n        \"\"\"\n        # Site coverage #1 & policy layer #10 FM levels\n        levels = [1, 10]\n        levels = range(1, len(levels) + 1)\n        profile_id = 1\n        for level in levels:\n            # Site coverage FM level\n            if level == 1:\n                for agg_id in range(\n                    1, self.num_locations * self.coverages_per_location + 1\n                ):\n                    # One layer in site coverage FM level\n                    yield level, agg_id, 1, profile_id\n                profile_id += 1   # Next profile_id\n            # Policy layer FM level\n            elif level == len(levels):\n                for layer in range(self.num_layers):\n                    yield level, 1, layer + 1, profile_id\n                    profile_id += 1   # Next profile_id\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Financial Model Policy dummy model file data.\n\nYields:\n    level (int): level ID.\n    agg_id (int): aggregate ID.\n    layer (int): layer ID.\n    profile_id (int): profile ID."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::generate_data@1048",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "generate_data",
      "lineno": 1048,
      "end_lineno": 1086,
      "business_stage": "other",
      "docstring": "Generate Financial Model Profile dummy model file data.\n\nYields:\n    profile_id (int): profile ID.\n    calculation rule ID (int): calculation rule ID (2 or 100).\n    first deductible (float): first deductible (fixed at 0.0).\n    second deductible (float): second deductible (fixed at 0.0).\n    third deductible (float): third deductible (fixed at 0.0).\n    attachment1 (float): attachment point/excess.\n    limit1 (float): limit.\n    first proportional share (float): first proportional\n        share (0.0 or 0.3).\n    second proportional share (float): second proportional\n        share (fixed at 0.0).\n    third proportional share (float): third proportional\n        share (fixed at 0.0).",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: generate_data (lines 1048-1086)\n\n    def generate_data(self):\n        \"\"\"\n        Generate Financial Model Profile dummy model file data.\n\n        Yields:\n            profile_id (int): profile ID.\n            calculation rule ID (int): calculation rule ID (2 or 100).\n            first deductible (float): first deductible (fixed at 0.0).\n            second deductible (float): second deductible (fixed at 0.0).\n            third deductible (float): third deductible (fixed at 0.0).\n            attachment1 (float): attachment point/excess.\n            limit1 (float): limit.\n            first proportional share (float): first proportional\n                share (0.0 or 0.3).\n            second proportional share (float): second proportional\n                share (fixed at 0.0).\n            third proportional share (float): third proportional\n                share (fixed at 0.0).\n        \"\"\"\n        # Pass through for level 1\n        profile_rows = [(1, 100, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)]\n        # First policy\n        init_profile_id = 2\n        init_attachment1 = 500000.0\n        attachment1_offset = 5000000.0\n        max_limit1 = 100000000.0\n        for layer in range(self.num_layers):\n            profile_id = init_profile_id + layer\n            attachment1 = init_attachment1 + attachment1_offset * layer\n            # Set limit1 at maximum for last layer\n            if (layer + 1) == self.num_layers:\n                limit1 = max_limit1\n            else:\n                limit1 = attachment1_offset * (layer + 1)\n            profile_rows.append(\n                (profile_id, 2, 0.0, 0.0, 0.0, attachment1, limit1, 0.3, 0.0, 0.0)\n            )\n        for row in profile_rows:\n            yield row\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Financial Model Profile dummy model file data.\n\nYields:\n    profile_id (int): profile ID.\n    calculation rule ID (int): calculation rule ID (2 or 100).\n    first deductible (float): first deductible (fixed at 0.0).\n    second deductible (float): second deductible (fixed at 0.0).\n    third deductible (float): third deductible (fixed at 0.0).\n    attachment1 (float): attachment point/excess.\n    limit1 (float): limit.\n    first proportional share (float): first proportional\n        share (0.0 or 0.3).\n    second proportional share (float): second proportional\n        share (fixed at 0.0).\n    third proportional share (float): third proportional\n        share (fixed at 0.0)."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::generate_data@1121",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "generate_data",
      "lineno": 1121,
      "end_lineno": 1137,
      "business_stage": "other",
      "docstring": "Generate Financial Model Cross Reference dummy model file data.\n\nYields:\n    output_count (int): output ID.\n    agg_id (int): aggregate ID.\n    layer (int): layer ID.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: generate_data (lines 1121-1137)\n\n    def generate_data(self):\n        \"\"\"\n        Generate Financial Model Cross Reference dummy model file data.\n\n        Yields:\n            output_count (int): output ID.\n            agg_id (int): aggregate ID.\n            layer (int): layer ID.\n        \"\"\"\n        layers = range(1, self.num_layers + 1)\n        output_count = 1\n        for agg_id in range(\n            1, self.num_locations * self.coverages_per_location + 1\n        ):\n            for layer in layers:\n                yield output_count, agg_id, layer\n                output_count += 1\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Financial Model Cross Reference dummy model file data.\n\nYields:\n    output_count (int): output ID.\n    agg_id (int): aggregate ID.\n    layer (int): layer ID."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::generate_data@1169",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "generate_data",
      "lineno": 1169,
      "end_lineno": 1181,
      "business_stage": "other",
      "docstring": "Generate Ground Up Losses Summary Cross Reference dummy model file data.\n\nYields:\n    item (int): item ID.\n    summary_id (int): summary ID.\n    summaryset_id (int): summary set ID.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: generate_data (lines 1169-1181)\n\n    def generate_data(self):\n        \"\"\"\n        Generate Ground Up Losses Summary Cross Reference dummy model file data.\n\n        Yields:\n            item (int): item ID.\n            summary_id (int): summary ID.\n            summaryset_id (int): summary set ID.\n        \"\"\"\n        summary_id = 1\n        summaryset_id = 1\n        for item in range(self.num_locations * self.coverages_per_location):\n            yield item + 1, summary_id, summaryset_id\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Ground Up Losses Summary Cross Reference dummy model file data.\n\nYields:\n    item (int): item ID.\n    summary_id (int): summary ID.\n    summaryset_id (int): summary set ID."
    },
    {
      "chunk_id": "oasislmf/computation/data/dummy_model/generate.py::generate_data@1217",
      "source_type": "code",
      "path": "oasislmf/computation/data/dummy_model/generate.py",
      "symbol_type": "function",
      "name": "generate_data",
      "lineno": 1217,
      "end_lineno": 1231,
      "business_stage": "other",
      "docstring": "Generate Financial Model Summary Cross Reference dummy model file data.\n\nYields:\n    output_id (int): output ID.\n    summary_id (int): summary ID.\n    summaryset_id (int): summary set ID.",
      "content": "# File: oasislmf/computation/data/dummy_model/generate.py\n# function: generate_data (lines 1217-1231)\n\n    def generate_data(self):\n        \"\"\"\n        Generate Financial Model Summary Cross Reference dummy model file data.\n\n        Yields:\n            output_id (int): output ID.\n            summary_id (int): summary ID.\n            summaryset_id (int): summary set ID.\n        \"\"\"\n        summary_id = 1\n        summaryset_id = 1\n        for output_id in range(\n            self.num_locations * self.coverages_per_location * self.num_layers\n        ):\n            yield output_id + 1, summary_id, summaryset_id\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Financial Model Summary Cross Reference dummy model file data.\n\nYields:\n    output_id (int): output ID.\n    summary_id (int): summary ID.\n    summaryset_id (int): summary set ID."
    },
    {
      "chunk_id": "oasislmf/computation/generate/doc.py::GenerateModelDocumentation@11",
      "source_type": "code",
      "path": "oasislmf/computation/generate/doc.py",
      "symbol_type": "class",
      "name": "GenerateModelDocumentation",
      "lineno": 11,
      "end_lineno": 88,
      "business_stage": "other",
      "docstring": "Generates Model Documentation from schema provided in the model config file",
      "content": "# File: oasislmf/computation/generate/doc.py\n# class: GenerateModelDocumentation (lines 11-88)\n\nclass GenerateModelDocumentation(ComputationStep):\n    \"\"\"\n    Generates Model Documentation from schema provided in the model config file\n    \"\"\"\n    # Command line options\n    step_params = [\n        {'name': 'doc_out_dir', 'flag': '-o', 'is_path': True, 'pre_exist': False,\n         'help': 'Path to the directory in which to generate the Documentation files', 'default': '.'},\n        {'name': 'doc_json', 'flag': '-d', 'is_path': True, 'pre_exist': True, 'required': True,\n         'help': 'The json file containing model meta-data for documentation'},\n        {'name': 'doc_schema_info', 'flag': '-s', 'is_path': True, 'pre_exist': True, 'required': False,\n         'help': 'The schema for the model meta-data json'},\n\n    ]\n    chained_commands = []\n\n    def validate_doc_schema(self, schema_path, docjson_path):\n        \"\"\"Validates docjson_path file with schema_path file\n        Args:\n            schema_path (str | os.PathLike): Schema path file\n            docjson_path (str | os.PathLike): Documentation JSON path file\n        Returns:\n            docjson (Dict): Json data loaded as a dictionary\n        \"\"\"\n        with open(schema_path, \"r\") as f:\n            schema = json.load(f)\n\n        with open(docjson_path, \"r\") as f:\n            docjson = json.load(f)\n\n        if \"datasets\" not in docjson:\n            raise ValidationError(f\"key \\'datasets\\' not found inside {docjson_path}\")\n\n        datasets = docjson[\"datasets\"]\n        for i, dataset in enumerate(datasets):\n            try:\n                validate(instance=dataset, schema=schema)\n            except ValidationError as e:\n                raise ValidationError(f\"doc schema validation error for dataset idx {i}: {e.message}\")\n\n        return docjson, schema\n\n    def json_to_mdtxt(self, json_data, full_schema, data_path, doc_out_dir):\n        \"\"\"Convert json data to markdown text with schemas provided\n        Args:\n            json_data (dict): Json data as dictionary\n            full_schema (dict): Full schema file as dictionary\n            data_path (str | os.PathLike): Path to data folder for any relative file paths\n            doc_out_dir (str | os.PathLike): Path to documentation file output folder for any relative file paths\n        \"\"\"\n        schema_id = full_schema[\"$id\"]\n        json_to_md_generator = DefaultJsonToMarkdownGenerator\n        if schema_id == \"https://docs.riskdatalibrary.org/en/0__2__0/rdls_schema.json\":\n            # RDLS v0.2\n            json_to_md_generator = RDLS_0_2_0_JsonToMarkdownGenerator\n        else:\n            self.logger.warning(f\"WARN: Unsupported formatting for following schema: {schema_id}. Using DefaultJsonToMarkdownGenerator output\")\n        gen = json_to_md_generator(full_schema, data_path, doc_out_dir)\n        return gen.generate(json_data, generate_toc=True)\n\n    def run(self):\n        if not os.path.exists(self.doc_json):\n            raise FileNotFoundError(f'Could not locate doc_json file: {self.doc_json}, Cannot generate documentation')\n        if not self.doc_schema_info:\n            self.doc_schema_info = resources.files('rdls').joinpath('rdls_schema.json')\n            if not os.path.exists(self.doc_schema_info):\n                raise FileNotFoundError(f'Could not locate doc_schema_info file: {self.doc_schema_info}, Cannot generate documentation')\n\n        doc_out_dir = Path(self.doc_out_dir)\n        doc_json = Path(self.doc_json)\n        data_path = doc_json.parent\n        doc_schema_info = Path(self.doc_schema_info)\n        doc_file = Path(doc_out_dir, 'doc.md')\n        json_data, schema = self.validate_doc_schema(doc_schema_info, doc_json)\n\n        with open(doc_file, \"w\") as f:\n            mdtxt = self.json_to_mdtxt(json_data, schema, data_path, doc_out_dir)\n            f.write(mdtxt)\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerates Model Documentation from schema provided in the model config file"
    },
    {
      "chunk_id": "oasislmf/computation/generate/doc.py::validate_doc_schema@27",
      "source_type": "code",
      "path": "oasislmf/computation/generate/doc.py",
      "symbol_type": "function",
      "name": "validate_doc_schema",
      "lineno": 27,
      "end_lineno": 51,
      "business_stage": "other",
      "docstring": "Validates docjson_path file with schema_path file\nArgs:\n    schema_path (str | os.PathLike): Schema path file\n    docjson_path (str | os.PathLike): Documentation JSON path file\nReturns:\n    docjson (Dict): Json data loaded as a dictionary",
      "content": "# File: oasislmf/computation/generate/doc.py\n# function: validate_doc_schema (lines 27-51)\n\n    def validate_doc_schema(self, schema_path, docjson_path):\n        \"\"\"Validates docjson_path file with schema_path file\n        Args:\n            schema_path (str | os.PathLike): Schema path file\n            docjson_path (str | os.PathLike): Documentation JSON path file\n        Returns:\n            docjson (Dict): Json data loaded as a dictionary\n        \"\"\"\n        with open(schema_path, \"r\") as f:\n            schema = json.load(f)\n\n        with open(docjson_path, \"r\") as f:\n            docjson = json.load(f)\n\n        if \"datasets\" not in docjson:\n            raise ValidationError(f\"key \\'datasets\\' not found inside {docjson_path}\")\n\n        datasets = docjson[\"datasets\"]\n        for i, dataset in enumerate(datasets):\n            try:\n                validate(instance=dataset, schema=schema)\n            except ValidationError as e:\n                raise ValidationError(f\"doc schema validation error for dataset idx {i}: {e.message}\")\n\n        return docjson, schema\n\n\"\"\"Docstring (excerpt)\"\"\"\nValidates docjson_path file with schema_path file\nArgs:\n    schema_path (str | os.PathLike): Schema path file\n    docjson_path (str | os.PathLike): Documentation JSON path file\nReturns:\n    docjson (Dict): Json data loaded as a dictionary"
    },
    {
      "chunk_id": "oasislmf/computation/generate/doc.py::json_to_mdtxt@53",
      "source_type": "code",
      "path": "oasislmf/computation/generate/doc.py",
      "symbol_type": "function",
      "name": "json_to_mdtxt",
      "lineno": 53,
      "end_lineno": 69,
      "business_stage": "other",
      "docstring": "Convert json data to markdown text with schemas provided\nArgs:\n    json_data (dict): Json data as dictionary\n    full_schema (dict): Full schema file as dictionary\n    data_path (str | os.PathLike): Path to data folder for any relative file paths\n    doc_out_dir (str | os.PathLike): Path to documentation file output folder for any relative file paths",
      "content": "# File: oasislmf/computation/generate/doc.py\n# function: json_to_mdtxt (lines 53-69)\n\n    def json_to_mdtxt(self, json_data, full_schema, data_path, doc_out_dir):\n        \"\"\"Convert json data to markdown text with schemas provided\n        Args:\n            json_data (dict): Json data as dictionary\n            full_schema (dict): Full schema file as dictionary\n            data_path (str | os.PathLike): Path to data folder for any relative file paths\n            doc_out_dir (str | os.PathLike): Path to documentation file output folder for any relative file paths\n        \"\"\"\n        schema_id = full_schema[\"$id\"]\n        json_to_md_generator = DefaultJsonToMarkdownGenerator\n        if schema_id == \"https://docs.riskdatalibrary.org/en/0__2__0/rdls_schema.json\":\n            # RDLS v0.2\n            json_to_md_generator = RDLS_0_2_0_JsonToMarkdownGenerator\n        else:\n            self.logger.warning(f\"WARN: Unsupported formatting for following schema: {schema_id}. Using DefaultJsonToMarkdownGenerator output\")\n        gen = json_to_md_generator(full_schema, data_path, doc_out_dir)\n        return gen.generate(json_data, generate_toc=True)\n\n\"\"\"Docstring (excerpt)\"\"\"\nConvert json data to markdown text with schemas provided\nArgs:\n    json_data (dict): Json data as dictionary\n    full_schema (dict): Full schema file as dictionary\n    data_path (str | os.PathLike): Path to data folder for any relative file paths\n    doc_out_dir (str | os.PathLike): Path to documentation file output folder for any relative file paths"
    },
    {
      "chunk_id": "oasislmf/computation/generate/files.py::GenerateFiles@63",
      "source_type": "code",
      "path": "oasislmf/computation/generate/files.py",
      "symbol_type": "class",
      "name": "GenerateFiles",
      "lineno": 63,
      "end_lineno": 432,
      "business_stage": "other",
      "docstring": "Generates the standard Oasis GUL input files + optionally the IL/FM input\nfiles and the RI input files.",
      "content": "# File: oasislmf/computation/generate/files.py\n# class: GenerateFiles (lines 63-432)\n\nclass GenerateFiles(ComputationStep):\n    \"\"\"\n    Generates the standard Oasis GUL input files + optionally the IL/FM input\n    files and the RI input files.\n    \"\"\"\n    settings_params = [{'name': 'analysis_settings_json', 'loader': analysis_settings_loader, 'user_role': 'user'},\n                       {'name': 'model_settings_json', 'loader': model_settings_loader}]\n\n    step_params = [\n        # Command line options\n        {'name': 'oasis_files_dir', 'flag': '-o', 'is_path': True, 'pre_exist': False,\n         'help': 'Path to the directory in which to generate the Oasis files'},\n        {'name': 'keys_data_csv', 'flag': '-z', 'is_path': True, 'pre_exist': True, 'help': 'Pre-generated keys CSV file path'},\n        {'name': 'analysis_settings_json', 'flag': '-a', 'is_path': True, 'pre_exist': True, 'required': False,\n         'help': 'Analysis settings JSON file path'},\n        {'name': 'keys_errors_csv', 'is_path': True, 'pre_exist': True, 'help': 'Pre-generated keys errors CSV file path'},\n        {'name': 'profile_loc_json', 'is_path': True, 'pre_exist': True, 'help': 'Source (OED) exposure profile JSON path'},\n        {'name': 'profile_acc_json', 'flag': '-b', 'is_path': True, 'pre_exist': True, 'help': 'Source (OED) accounts profile JSON path'},\n        {'name': 'profile_fm_agg_json', 'is_path': True, 'pre_exist': True, 'help': 'FM (OED) aggregation profile path'},\n        {'name': 'oed_schema_info', 'help': 'Takes a version of OED schema to use in the form \"v1.2.3\" or a path to an OED schema json'},\n        {'name': 'currency_conversion_json', 'is_path': True, 'pre_exist': True, 'help': 'settings to perform currency conversion of oed files'},\n        {'name': 'reporting_currency', 'help': 'currency to use in the results reported'},\n        {'name': 'oed_location_csv', 'flag': '-x', 'is_path': True, 'pre_exist': True, 'help': 'Source location CSV file path'},\n        {'name': 'oed_accounts_csv', 'flag': '-y', 'is_path': True, 'pre_exist': True, 'help': 'Source accounts CSV file path'},\n        {'name': 'oed_info_csv', 'flag': '-i', 'is_path': True, 'pre_exist': True, 'help': 'Reinsurance info. CSV file path'},\n        {'name': 'oed_scope_csv', 'flag': '-s', 'is_path': True, 'pre_exist': True, 'help': 'Reinsurance scope CSV file path'},\n        {'name': 'check_oed', 'type': str2bool, 'const': True, 'nargs': '?', 'default': True, 'help': 'if True check input oed files'},\n        {'name': 'disable_summarise_exposure', 'flag': '-S', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n         'help': 'Disables creation of an exposure summary report'},\n        {'name': 'damage_group_id_cols', 'flag': '-G', 'nargs': '+', 'help': 'Columns from loc file to set group_id', 'default': DAMAGE_GROUP_ID_COLS},\n        {'name': 'hazard_group_id_cols', 'flag': '-H', 'nargs': '+', 'help': 'Columns from loc file to set hazard_group_id', 'default': HAZARD_GROUP_ID_COLS},\n        {'name': 'lookup_multiprocessing', 'type': str2bool, 'const': False, 'nargs': '?', 'default': False,\n         'help': 'Flag to enable/disable lookup multiprocessing'},\n        {'name': 'do_disaggregation', 'type': str2bool, 'const': True, 'nargs': '?', 'default': True, 'help': 'if True run the oasis disaggregation.'},\n\n        # Manager only options (pass data directy instead of filepaths)\n        {'name': 'lookup_config'},\n        {'name': 'lookup_complex_config'},\n        {'name': 'write_ri_tree', 'default': False},\n        {'name': 'verbose', 'default': False},\n        {'name': 'write_chunksize', 'type': int, 'default': WRITE_CHUNKSIZE},\n        {'name': 'oasis_files_prefixes', 'default': OASIS_FILES_PREFIXES},\n        {'name': 'profile_loc', 'default': get_default_exposure_profile()},\n        {'name': 'profile_acc', 'default': get_default_accounts_profile()},\n        {'name': 'profile_fm_agg', 'default': get_default_fm_aggregation_profile()},\n        {'name': 'location', 'type': str, 'nargs': '+', 'help': 'A set of locations to include in the files'},\n        {'name': 'portfolio', 'type': str, 'nargs': '+', 'help': 'A set of portfolios to include in the files'},\n        {'name': 'account', 'type': str, 'nargs': '+', 'help': 'A set of locations to include in the files'},\n        {'name': 'base_df_engine', 'type': str, 'default': 'oasis_data_manager.df_reader.reader.OasisPandasReader',\n         'help': 'The default dataframe reading engine to use when loading files'},\n        {'name': 'exposure_df_engine', 'type': str, 'default': None,\n         'help': 'The dataframe reading engine to use when loading exposure files'},\n        {'name': 'oed_backend_dtype', 'type': str, 'default': 'pd_dtype',\n         'help': \"define what type dtype the oed column will be (pd_dtype or pa_dtype)\"},\n    ]\n\n    chained_commands = [\n        GenerateKeys\n    ]\n\n    def _get_output_dir(self):\n        if self.oasis_files_dir:\n            return self.oasis_files_dir\n        utcnow = get_utctimestamp(fmt='%Y%m%d%H%M%S')\n        return os.path.join(os.getcwd(), 'runs', 'files-{}'.format(utcnow))\n\n    def get_exposure_data_config(self):\n        return {\n            'location': self.oed_location_csv,\n            'account': self.oed_accounts_csv,\n            'ri_info': self.oed_info_csv,\n            'ri_scope': self.oed_scope_csv,\n            'oed_schema_info': self.oed_schema_info,\n            'currency_conversion': self.currency_conversion_json,\n            'check_oed': self.check_oed,\n            'use_field': True,\n            'location_numbers': self.location,\n            'portfolio_numbers': self.portfolio,\n            'account_numbers': self.account,\n            'base_df_engine': self.base_df_engine,\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerates the standard Oasis GUL input files + optionally the IL/FM input\nfiles and the RI input files."
    },
    {
      "chunk_id": "oasislmf/computation/generate/files.py::GenerateDummyModelFiles@435",
      "source_type": "code",
      "path": "oasislmf/computation/generate/files.py",
      "symbol_type": "class",
      "name": "GenerateDummyModelFiles",
      "lineno": 435,
      "end_lineno": 573,
      "business_stage": "other",
      "docstring": "Generates dummy model files.",
      "content": "# File: oasislmf/computation/generate/files.py\n# class: GenerateDummyModelFiles (lines 435-573)\n\nclass GenerateDummyModelFiles(ComputationStep):\n    \"\"\"\n    Generates dummy model files.\n    \"\"\"\n\n    # Command line options\n    step_params = [\n        {'name': 'target_dir', 'flag': '-o', 'is_path': True, 'pre_exist': False,\n         'help': 'Path to the directory in which to generate the Model files'},\n        {'name': 'num_vulnerabilities', 'flag': '-v', 'required': True, 'type': int, 'help': 'Number of vulnerabilities'},\n        {'name': 'num_intensity_bins', 'flag': '-i', 'required': True, 'type': int, 'help': 'Number of intensity bins'},\n        {'name': 'num_damage_bins', 'flag': '-d', 'required': True, 'type': int, 'help': 'Number of damage bins'},\n        {'name': 'vulnerability_sparseness', 'flag': '-s', 'required': False, 'type': float, 'default': 1.0,\n         'help': 'Percentage of bins normalised to range [0,1] impacted for a vulnerability at an intensity level'},\n        {'name': 'num_events', 'flag': '-e', 'required': True, 'type': int, 'help': 'Number of events'},\n        {'name': 'num_areaperils', 'flag': '-a', 'required': True, 'type': int, 'help': 'Number of areaperils'},\n        {'name': 'areaperils_per_event', 'flag': '-A', 'required': False, 'type': int, 'default': None,\n         'help': 'Number of areaperils impacted per event'},\n        {'name': 'intensity_sparseness', 'flag': '-S', 'required': False, 'type': float, 'default': 1.0,\n         'help': 'Percentage of bins normalised to range [0,1] impacted for an event and areaperil'},\n        {'name': 'no_intensity_uncertainty', 'flag': '-u', 'required': False,\n         'default': False, 'action': 'store_true', 'help': 'No intensity uncertainty flag'},\n        {'name': 'num_periods', 'flag': '-p', 'required': True, 'type': int, 'help': 'Number of periods'},\n        {'name': 'periods_per_event_mean', 'flag': '-P', 'required': False, 'type': int, 'default': 1,\n         'help': 'Mean of truncated normal distribution sampled to determine number of periods per event'},\n        {'name': 'periods_per_event_stddev', 'flag': '-Q', 'required': False, 'type': float, 'default': 0.0,\n         'help': 'Standard deviation of truncated normal distribution sampled to determine number of periods per event'},\n        {'name': 'num_amplifications', 'flag': '-m', 'required': False, 'type': int, 'default': 0, 'help': 'Number of amplifications'},\n        {'name': 'min_pla_factor', 'flag': '-f', 'required': False, 'type': float, 'default': 0.875,\n         'help': 'Minimum Post Loss Amplification Factor'},\n        {'name': 'max_pla_factor', 'flag': '-F', 'required': False, 'type': float, 'default': 1.5,\n         'help': 'Maximum Post Loss Amplification Factor'},\n        {'name': 'num_randoms', 'flag': '-r', 'required': False, 'type': int, 'default': 0, 'help': 'Number of random numbers'},\n        {'name': 'random_seed', 'flag': '-R', 'required': False, 'type': int, 'default': -\n         1, 'help': 'Random seed (-1 for 1234 (default), 0 for current system time'}\n    ]\n\n    def _validate_input_arguments(self):\n        if self.vulnerability_sparseness > 1.0 or self.vulnerability_sparseness < 0.0:\n            raise OasisException('Invalid value for --vulnerability-sparseness')\n        if self.intensity_sparseness > 1.0 or self.intensity_sparseness < 0.0:\n            raise OasisException('Invalid value for --intensity-sparseness')\n        if not self.areaperils_per_event:\n            self.areaperils_per_event = self.num_areaperils\n        if self.areaperils_per_event > self.num_areaperils:\n            raise OasisException('Number of areaperils per event exceeds total number of areaperils')\n        if self.num_amplifications < 0.0:\n            raise OasisException('Invalid value for --num-amplifications')\n        if self.max_pla_factor < self.min_pla_factor:\n            raise OasisException('Value for --max-pla-factor must be greater than that for --min-pla-factor')\n        if self.min_pla_factor < 0:\n            raise OasisException('Invalid value for --min-pla-factor')\n        if self.random_seed < -1:\n            raise OasisException('Invalid random seed')\n\n    def _create_target_directory(self, label):\n        utcnow = get_utctimestamp(fmt='%Y%m%d%H%M%S')\n        if not self.target_dir:\n            self.target_dir = os.path.join(os.getcwd(), 'runs', f'test-{label}-{utcnow}')\n\n        self.target_dir = create_target_directory(\n            self.target_dir, 'target test model files directory'\n        )\n\n    def _prepare_run_directory(self):\n        self.input_dir = os.path.join(self.target_dir, 'input')\n        self.static_dir = os.path.join(self.target_dir, 'static')\n        directories = [\n            self.input_dir, self.static_dir\n        ]\n        for directory in directories:\n            if not os.path.exists(directory):\n                Path(directory).mkdir(parents=True, exist_ok=True)\n\n    def _set_footprint_files_inputs(self):\n        self.footprint_files_inputs = {\n            'num_events': self.num_events,\n            'num_areaperils': self.num_areaperils,\n            'areaperils_per_event': self.areaperils_per_event,\n            'num_intensity_bins': self.num_intensity_bins,\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerates dummy model files."
    },
    {
      "chunk_id": "oasislmf/computation/generate/files.py::GenerateDummyOasisFiles@576",
      "source_type": "code",
      "path": "oasislmf/computation/generate/files.py",
      "symbol_type": "class",
      "name": "GenerateDummyOasisFiles",
      "lineno": 576,
      "end_lineno": 657,
      "business_stage": "other",
      "docstring": "Generates dummy model and Oasis GUL input files + optionally the IL/FM\ninput files.",
      "content": "# File: oasislmf/computation/generate/files.py\n# class: GenerateDummyOasisFiles (lines 576-657)\n\nclass GenerateDummyOasisFiles(GenerateDummyModelFiles):\n    \"\"\"\n    Generates dummy model and Oasis GUL input files + optionally the IL/FM\n    input files.\n    \"\"\"\n\n    step_params = [\n        {'name': 'num_locations', 'flag': '-l', 'required': True, 'type': int, 'help': 'Number of locations'},\n        {'name': 'coverages_per_location', 'flag': '-c', 'required': True, 'type': int, 'help': 'Number of coverage types per location'},\n        {'name': 'num_layers', 'required': False, 'type': int, 'default': 1, 'help': 'Number of layers'}\n    ]\n    chained_commands = [GenerateDummyModelFiles]\n\n    def _validate_input_arguments(self):\n        super()._validate_input_arguments()\n        if self.coverages_per_location > 4 or self.coverages_per_location < 1:\n            raise OasisException('Number of supported coverage types is 1 to 4')\n\n    def _get_gul_file_objects(self):\n\n        # coverages.bin, items.bin and gulsummaryxref.bin\n        self.gul_files = [\n            CoveragesFile(\n                self.num_locations, self.coverages_per_location,\n                self.random_seed, self.input_dir\n            ),\n            ItemsFile(\n                self.num_locations, self.coverages_per_location,\n                self.num_areaperils, self.num_vulnerabilities,\n                self.random_seed, self.input_dir\n            ),\n            GULSummaryXrefFile(\n                self.num_locations, self.coverages_per_location, self.input_dir\n            )\n        ]\n        if self.num_amplifications > 0:\n            self.gul_files += [\n                AmplificationsFile(\n                    self.num_locations, self.coverages_per_location,\n                    self.num_amplifications, self.random_seed, self.input_dir\n                )\n            ]\n\n    def _get_fm_file_objects(self):\n\n        # fm_programme.bin, fm_policytc.bin, fm_profile.bin, fm_xref.bin and\n        # fmsummaryxref.bin\n        self.fm_files = [\n            FMProgrammeFile(\n                self.num_locations, self.coverages_per_location, self.input_dir\n            ),\n            FMPolicyTCFile(\n                self.num_locations, self.coverages_per_location,\n                self.num_layers, self.input_dir\n            ),\n            FMProfileFile(self.num_layers, self.input_dir),\n            FMXrefFile(\n                self.num_locations, self.coverages_per_location,\n                self.num_layers, self.input_dir\n            ),\n            FMSummaryXrefFile(\n                self.num_locations, self.coverages_per_location,\n                self.num_layers, self.input_dir\n            )\n        ]\n\n    def run(self):\n        self.logger.info('\\nProcessing arguments - Creating Model & Test Oasis Files')\n\n        self._validate_input_arguments()\n        self._create_target_directory(label='files')\n        self._prepare_run_directory()\n        self._get_model_file_objects()\n        self._get_gul_file_objects()\n        self._get_fm_file_objects()\n\n        output_files = self.model_files + self.gul_files + self.fm_files\n        for output_file in output_files:\n            self.logger.info(f'Writing {output_file.file_name}')\n            output_file.write_file()\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerates dummy model and Oasis GUL input files + optionally the IL/FM\ninput files."
    },
    {
      "chunk_id": "oasislmf/computation/generate/keys.py::GenerateKeys@28",
      "source_type": "code",
      "path": "oasislmf/computation/generate/keys.py",
      "symbol_type": "class",
      "name": "GenerateKeys",
      "lineno": 28,
      "end_lineno": 165,
      "business_stage": "other",
      "docstring": "Generates keys from a model lookup, and write Oasis keys and keys error files.\n\nThe model lookup, which is normally independently implemented by the model\nsupplier, should generate keys as dicts with the following format\n::\n\n    {\n        \"id\": <loc. ID>,\n        \"peril_id\": <OED sub-peril ID>,\n        \"coverage_type\": <OED coverage type ID>,\n        \"area_peril_id\": <area peril ID>,\n        \"vulnerability_id\": <vulnerability ID>,\n        \"message\": <loc. lookup status message>,\n        \"status\": <loc. lookup status flag indicating success, failure or no-match>\n    }\n\nThe keys generation command can generate these dicts, and write them to\nfile. It can also be used to write these to an Oasis keys file (which is a\nrequirement for model execution), which has the following format.::\n\n    LocID,PerilID,CoverageTypeID,AreaPerilID,VulnerabilityID\n    ..\n    ..\nThis file only lists the locations for which there has been a successful\nlookup. The keys errors file lists all the locations with failing or\nnon-matching lookups and has the following format::\n\n    LocID,PerilID,CoverageTypeID,Message\n    ..\n    ..",
      "content": "# File: oasislmf/computation/generate/keys.py\n# class: GenerateKeys (lines 28-165)\n\nclass GenerateKeys(KeyComputationStep):\n    \"\"\"\n    Generates keys from a model lookup, and write Oasis keys and keys error files.\n\n    The model lookup, which is normally independently implemented by the model\n    supplier, should generate keys as dicts with the following format\n    ::\n\n        {\n            \"id\": <loc. ID>,\n            \"peril_id\": <OED sub-peril ID>,\n            \"coverage_type\": <OED coverage type ID>,\n            \"area_peril_id\": <area peril ID>,\n            \"vulnerability_id\": <vulnerability ID>,\n            \"message\": <loc. lookup status message>,\n            \"status\": <loc. lookup status flag indicating success, failure or no-match>\n        }\n\n    The keys generation command can generate these dicts, and write them to\n    file. It can also be used to write these to an Oasis keys file (which is a\n    requirement for model execution), which has the following format.::\n\n        LocID,PerilID,CoverageTypeID,AreaPerilID,VulnerabilityID\n        ..\n        ..\n    This file only lists the locations for which there has been a successful\n    lookup. The keys errors file lists all the locations with failing or\n    non-matching lookups and has the following format::\n\n        LocID,PerilID,CoverageTypeID,Message\n        ..\n        ..\n    \"\"\"\n    settings_params = [{'name': 'lookup_complex_config_json', 'loader': analysis_settings_loader, 'user_role': 'user'},\n                       {'name': 'model_settings_json', 'loader': model_settings_loader}]\n\n    step_params = [\n        {'name': 'oed_location_csv', 'flag': '-x', 'is_path': True, 'pre_exist': True, 'help': 'Source location CSV file path'},\n        {'name': 'oed_schema_info', 'help': 'Takes a version of OED schema to use in the form \"v1.2.3\" or a path to an OED schema json'},\n        {'name': 'check_oed', 'type': str2bool, 'const': True, 'nargs': '?', 'default': True, 'help': 'if True check input oed files'},\n        {'name': 'keys_data_csv', 'flag': '-k', 'is_path': True, 'pre_exist': False, 'help': 'Generated keys CSV output path'},\n        {'name': 'keys_errors_csv', 'flag': '-e', 'is_path': True, 'pre_exist': False, 'help': 'Generated keys errors CSV output path'},\n        {'name': 'keys_format', 'flag': '-f', 'help': 'Keys files output format', 'choices': ['oasis', 'json'], 'default': 'oasis'},\n        {'name': 'lookup_config_json', 'flag': '-g', 'is_path': True, 'pre_exist': False, 'help': 'Lookup config JSON file path'},\n        {'name': 'lookup_data_dir', 'is_path': True, 'pre_exist': True, 'help': 'Model lookup/keys data directory path'},\n        {'name': 'lookup_module_path', 'flag': '-l', 'is_path': True, 'pre_exist': False, 'help': 'Model lookup module path'},\n        {'name': 'lookup_complex_config_json', 'is_path': True, 'pre_exist': False, 'help': 'Complex lookup config JSON file path'},\n        {'name': 'lookup_num_processes', 'type': int, 'default': -1, 'help': 'Number of workers in multiprocess pools'},\n        {'name': 'lookup_num_chunks', 'type': int, 'default': -1, 'help': 'Number of chunks to split the location file into for multiprocessing'},\n        {'name': 'model_version_csv', 'flag': '-v', 'is_path': True, 'pre_exist': False, 'help': 'Model version CSV file path'},\n        {'name': 'model_settings_json', 'flag': '-M', 'is_path': True, 'pre_exist': True, 'help': 'Model settings JSON file path'},\n        {'name': 'user_data_dir', 'flag': '-D', 'is_path': True, 'pre_exist': False,\n         'help': 'Directory containing additional model data files which varies between analysis runs'},\n        {'name': 'lookup_multiprocessing', 'type': str2bool, 'const': True, 'nargs': '?', 'default': True,\n         'help': 'Flag to enable/disable lookup multiprocessing'},\n        {'name': 'disable_oed_version_update', 'type': str2bool, 'const': True, 'nargs': '?', 'default': False,\n         'help': 'Flag to enable/disable conversion to latest compatible OED version. Must be present in model settings.'},\n        {'name': 'oed_backend_dtype', 'type': str, 'default': 'pd_dtype',\n         'help': \"define what type dtype the oed column will be (pd_dtype or pa_dtype)\"},\n\n        # Manager only options\n        {'name': 'verbose', 'default': False},\n    ]\n\n    def _get_output_dir(self):\n        if self.keys_data_csv:\n            return os.path.dirname(self.keys_data_csv)\n        utcnow = get_utctimestamp(fmt='%Y%m%d%H%M%S')\n        return os.path.join(os.getcwd(), 'runs', 'keys-{}'.format(utcnow))\n\n    def run(self):\n        if not (self.lookup_config_json or (self.lookup_data_dir and self.model_version_csv and self.lookup_module_path)):\n            raise OasisException(\n                'No pre-generated keys file provided, and no lookup assets '\n                'provided to generate a keys file - if you do not have a '\n                'pre-generated keys file then lookup assets must be provided - '\n                'for a built-in lookup the lookup config. JSON file path must '\n                'be provided, or for custom lookups the keys data path + model '\n                'version file path + lookup package path must be provided'\n            )\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerates keys from a model lookup, and write Oasis keys and keys error files.\n\nThe model lookup, which is normally independently implemented by the model\nsupplier, should generate keys as dicts with the following format\n::\n\n    {\n        \"id\": <loc. ID>,\n        \"peril_id\": <OED sub-peril ID>,\n        \"coverage_type\": <OED coverage type ID>,\n        \"area_peril_id\": <area peril ID>,\n        \"vulnerability_id\": <vulnerability ID>,\n        \"message\": <loc. lookup status message>,\n        \"status\": <loc. lookup status flag indicating success, failure or no-match>\n    }\n\nThe keys generation command can generate these dicts, and write them to\nfile. It can also be used to write these to an Oasis keys file (which is a\nrequirement for model execution), which has the following format.::\n\n    LocID,PerilID,CoverageTypeID,AreaPerilID,VulnerabilityID\n    ..\n    ..\nThis file only lists the locations for which there has been a successful\nlookup. The keys errors file lists all the locations with failing or\nnon-matching lookups and has the following format::\n\n    LocID,PerilID,CoverageTypeID,Message\n    ..\n    .."
    },
    {
      "chunk_id": "oasislmf/computation/generate/losses.py::GenerateLossesBase@59",
      "source_type": "code",
      "path": "oasislmf/computation/generate/losses.py",
      "symbol_type": "class",
      "name": "GenerateLossesBase",
      "lineno": 59,
      "end_lineno": 170,
      "business_stage": "gul",
      "docstring": "Base class for Loss generation functions\n\nIncludes methods useful across all GenerateLoss functions\nintended as a common inherited class",
      "content": "# File: oasislmf/computation/generate/losses.py\n# class: GenerateLossesBase (lines 59-170)\n\nclass GenerateLossesBase(ComputationStep):\n    \"\"\"\n    Base class for Loss generation functions\n\n    Includes methods useful across all GenerateLoss functions\n    intended as a common inherited class\n    \"\"\"\n\n    def _get_output_dir(self):\n        \"\"\"\n        Set the model run directory to '<cwd>/runs/losses-<timestamp>' if not set\n        in arguments\n\n        :return: (str) the model run directory, either given or generated\n        \"\"\"\n        if not self.model_run_dir:\n            utcnow = get_utctimestamp(fmt='%Y%m%d%H%M%S')\n            self.model_run_dir = os.path.join(os.getcwd(), 'runs', 'losses-{}'.format(utcnow))\n        if not os.path.exists(self.model_run_dir):\n            Path(self.model_run_dir).mkdir(parents=True, exist_ok=True)\n        return self.model_run_dir\n\n    def _get_model_runner(self):\n        \"\"\"\n        Returns the model runner module, by default this is imported from `oasislmf/execution/runner.py`\n        but can be overridden from the conf option `model_package_dir`\n\n        :return: (object) The model runner module, (str) Package Name\n        \"\"\"\n        package_name = None\n        if self.model_package_dir and os.path.exists(os.path.join(self.model_package_dir, 'supplier_model_runner.py')):\n            path, package_name = os.path.split(self.model_package_dir)\n            sys.path.append(path)\n            return importlib.import_module('{}.supplier_model_runner'.format(package_name)), package_name\n        else:\n            return runner, package_name\n\n    def _check_ktool_rules(self):\n        \"\"\"\n        Check the given ktool allocation rules are within valid ranges\n        Raises an `OasisException` if a rules is invalid\n        \"\"\"\n        rule_ranges = {\n            'ktools_alloc_rule_gul': KTOOLS_ALLOC_GUL_MAX,\n            'ktools_alloc_rule_il': KTOOLS_ALLOC_FM_MAX,\n            'ktools_alloc_rule_ri': KTOOLS_ALLOC_FM_MAX,\n            'ktools_event_shuffle': EVE_STD_SHUFFLE,\n            'gulpy_random_generator': 1}\n\n        for rule in rule_ranges:\n            rule_val = int(getattr(self, rule))\n            if (rule_val < 0) or (rule_val > rule_ranges[rule]):\n                raise OasisException(f'Error: {rule}={rule_val} - Not within valid ranges [0..{rule_ranges[rule]}]')\n\n    def _store_run_settings(self, analysis_settings, target_dir):\n        \"\"\"\n        Writes the analysis settings file to the `target_dir` path\n        \"\"\"\n        with io.open(os.path.join(target_dir, 'analysis_settings.json'), 'w', encoding='utf-8') as f:\n            f.write(json.dumps(analysis_settings, ensure_ascii=False, indent=4))\n\n    def _is_run_settings_stored(self, target_dir):\n        \"\"\"\n        Checks if analysis settings file is under target_dir path\n        \"\"\"\n        return os.path.isfile(os.path.join(target_dir, 'analysis_settings.json'))\n\n    def _get_num_ri_layers(self, analysis_settings, model_run_fp):\n        \"\"\"\n        Find the number of Reinsurance layers based on `'ri_layers.json'`, returns pos int()\n        \"\"\"\n        ri_layers = 0\n        if analysis_settings.get('ri_output', False) or analysis_settings.get('rl_output', False):\n            ri_layers = len(get_json(os.path.join(model_run_fp, 'input', 'ri_layers.json')))\n        return ri_layers\n\n    def _get_peril_filter(self, analysis_settings):\n        \"\"\"\n        Check the 'analysis_settings' for user set peril filter, if empty return the MDK peril filter\n        option\n\n\"\"\"Docstring (excerpt)\"\"\"\nBase class for Loss generation functions\n\nIncludes methods useful across all GenerateLoss functions\nintended as a common inherited class"
    },
    {
      "chunk_id": "oasislmf/computation/generate/losses.py::GenerateLossesDir@173",
      "source_type": "code",
      "path": "oasislmf/computation/generate/losses.py",
      "symbol_type": "class",
      "name": "GenerateLossesDir",
      "lineno": 173,
      "end_lineno": 415,
      "business_stage": "gul",
      "docstring": "Prepare the loss generation directory\n\n* converts input `csv` files to ktools binary types\n* links model data to the static directory to the run locations\n* Validates and updates the `analysis_settings.json`\n* Stores the analysis_settings.json in the output directory\n\n:return: (dict) Updated analysis_settings",
      "content": "# File: oasislmf/computation/generate/losses.py\n# class: GenerateLossesDir (lines 173-415)\n\nclass GenerateLossesDir(GenerateLossesBase):\n    \"\"\"\n    Prepare the loss generation directory\n\n    * converts input `csv` files to ktools binary types\n    * links model data to the static directory to the run locations\n    * Validates and updates the `analysis_settings.json`\n    * Stores the analysis_settings.json in the output directory\n\n    :return: (dict) Updated analysis_settings\n    \"\"\"\n    settings_params = [{'name': 'analysis_settings_json', 'loader': analysis_settings_loader, 'user_role': 'user'},\n                       {'name': 'model_settings_json', 'loader': model_settings_loader}]\n\n    step_params = [\n        # Command line options\n        {'name': 'oasis_files_dir', 'flag': '-o', 'is_path': True, 'pre_exist': True,\n         'required': True, 'help': 'Path to the directory in which to generate the Oasis files'},\n        {'name': 'check_oed', 'type': str2bool, 'const': True, 'nargs': '?', 'default': True, 'help': 'if True check input oed files'},\n        {'name': 'analysis_settings_json', 'flag': '-a', 'is_path': True, 'pre_exist': True, 'required': True,\n         'help': 'Analysis settings JSON file path'},\n        {'name': 'model_storage_json', 'is_path': True, 'pre_exist': True, 'required': False,\n         'help': 'Model data storage settings JSON file path'},\n        {'name': 'model_settings_json', 'flag': '-M', 'is_path': True, 'pre_exist': False, 'required': False,\n         'help': 'Model settings JSON file path'},\n        {'name': 'user_data_dir', 'flag': '-D', 'is_path': True, 'pre_exist': False,\n         'help': 'Directory containing additional model data files which varies between analysis runs'},\n        {'name': 'model_data_dir', 'flag': '-d', 'is_path': True, 'pre_exist': True, 'help': 'Model data directory path'},\n        {'name': 'copy_model_data', 'default': False, 'type': str2bool, 'help': 'Copy model data instead of creating symbolic links to it.'},\n        {'name': 'model_run_dir', 'flag': '-r', 'is_path': True, 'pre_exist': False, 'help': 'Model run directory path'},\n        {'name': 'model_package_dir', 'flag': '-p', 'is_path': True, 'pre_exist': False, 'help': 'Path containing model specific package'},\n        {'name': 'ktools_legacy_stream', 'type': str2bool, 'const': True, 'nargs': '?', 'default': KTOOLS_GUL_LEGACY_STREAM,\n         'help': 'Run Ground up losses using the older stream type (Compatibility option)'},\n        {'name': 'fmpy', 'default': True, 'type': str2bool, 'const': True, 'nargs': '?', 'help': 'use fmcalc python version instead of c++ version'},\n        {'name': 'ktools_alloc_rule_il', 'default': KTOOLS_ALLOC_IL_DEFAULT, 'type': int,\n         'help': 'Set the fmcalc allocation rule used in direct insured loss'},\n        {'name': 'ktools_alloc_rule_ri', 'default': KTOOLS_ALLOC_RI_DEFAULT, 'type': int,\n         'help': 'Set the fmcalc allocation rule used in reinsurance'},\n        {'name': 'summarypy', 'default': False, 'type': str2bool, 'const': True,\n            'nargs': '?', 'help': 'use summarycalc python version instead of c++ version'},\n        {'name': 'check_missing_inputs', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n         'help': 'Fail an analysis run if IL/RI is requested without the required generated files.'},\n\n        # Manager only options (pass data directy instead of filepaths)\n        {'name': 'verbose', 'default': KTOOLS_DEBUG},\n\n    ]\n\n    def _get_storage_manager(self):\n        model_storage = get_storage_from_config_path(\n            self.model_storage_json,\n            os.path.join(self.model_run_dir, \"static\"),\n        )\n\n        # if not local test the connection to remote storage FS\n        if not isinstance(model_storage, LocalStorage):\n            try:\n                model_storage.listdir()\n            except Exception as e:\n                raise OasisException('Error: Storage Manager connection issue', e)\n\n        return model_storage\n\n    def __check_for_parquet_output(self, analysis_settings, runtypes):\n        \"\"\"\n        Private method to check whether ktools components were linked to third\n        party parquet libraries during compilation if user requests parquet\n        output.\n        \"\"\"\n        for runtype in runtypes:\n            for summary in analysis_settings.get(f'{runtype}_summaries', {}):\n                if summary.get('ord_output', {}).get('parquet_format'):\n                    katparquet_output = subprocess.run(\n                        ['katparquet', '-v'],\n                        stdout=subprocess.PIPE,\n                        stderr=subprocess.PIPE\n                    )\n                    if 'Parquet output enabled' not in katparquet_output.stderr.decode():\n                        raise OasisException(\n                            'Parquet output format requested but not supported by ktools components. '\n\n\"\"\"Docstring (excerpt)\"\"\"\nPrepare the loss generation directory\n\n* converts input `csv` files to ktools binary types\n* links model data to the static directory to the run locations\n* Validates and updates the `analysis_settings.json`\n* Stores the analysis_settings.json in the output directory\n\n:return: (dict) Updated analysis_settings"
    },
    {
      "chunk_id": "oasislmf/computation/generate/losses.py::GenerateLossesPartial@418",
      "source_type": "code",
      "path": "oasislmf/computation/generate/losses.py",
      "symbol_type": "class",
      "name": "GenerateLossesPartial",
      "lineno": 418,
      "end_lineno": 572,
      "business_stage": "gul",
      "docstring": "Runs a single analysis event chunk",
      "content": "# File: oasislmf/computation/generate/losses.py\n# class: GenerateLossesPartial (lines 418-572)\n\nclass GenerateLossesPartial(GenerateLossesDir):\n    \"\"\"\n    Runs a single analysis event chunk\n    \"\"\"\n    step_params = GenerateLossesDir.step_params + [\n        {'name': 'ktools_num_processes', 'flag': '-n', 'type': int, 'default': KTOOLS_NUM_PROCESSES,\n         'help': 'Number of ktools calculation processes to use'},\n        {'name': 'ktools_event_shuffle', 'default': EVE_DEFAULT_SHUFFLE, 'type': int,\n         'help': 'Set rule for event shuffling between eve partions, 0 - No shuffle, 1 - round robin (output elts sorted), 2 - Fisher-Yates shuffle, 3 - std::shuffle (previous default in oasislmf<1.14.0) '},\n        {'name': 'ktools_alloc_rule_gul', 'default': KTOOLS_ALLOC_GUL_DEFAULT, 'type': int, 'help': 'Set the allocation used in gulcalc'},\n        {'name': 'ktools_alloc_rule_il', 'default': KTOOLS_ALLOC_IL_DEFAULT, 'type': int,\n         'help': 'Set the fmcalc allocation rule used in direct insured loss'},\n        {'name': 'ktools_alloc_rule_ri', 'default': KTOOLS_ALLOC_RI_DEFAULT, 'type': int,\n         'help': 'Set the fmcalc allocation rule used in reinsurance'},\n        {'name': 'ktools_num_gul_per_lb', 'default': KTOOL_N_GUL_PER_LB, 'type': int,\n         'help': 'Number of gul per load balancer (0 means no load balancer)'},\n        {'name': 'ktools_num_fm_per_lb', 'default': KTOOL_N_FM_PER_LB, 'type': int,\n         'help': 'Number of fm per load balancer (0 means no load balancer)'},\n        {'name': 'ktools_disable_guard', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n         'help': 'Disables error handling in the ktools run script (abort on non-zero exitcode or output on stderr)'},\n        {'name': 'ktools_fifo_relative', 'default': False, 'type': str2bool, 'const': True,\n         'nargs': '?', 'help': 'Create ktools fifo queues under the ./fifo dir'},\n        {'name': 'evepy', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n         'help': 'use eve python version instead of c++ version'},\n        {'name': 'modelpy', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n         'help': 'use getmodel python version instead of c++ version'},\n        {'name': 'gulpy', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n         'help': 'use gulcalc python version instead of c++ version'},\n        {'name': 'gulpy_random_generator', 'default': 1, 'type': int,\n         'help': 'set the random number generator in gulpy (0: Mersenne-Twister, 1: Latin Hypercube. Default: 1).'},\n        {'name': 'gulmc', 'default': True, 'type': str2bool, 'const': True, 'nargs': '?', 'help': 'use full Monte Carlo gulcalc python version'},\n        {'name': 'gulmc_random_generator', 'default': 1, 'type': int,\n         'help': 'set the random number generator in gulmc (0: Mersenne-Twister, 1: Latin Hypercube. Default: 1).'},\n        {'name': 'gulmc_effective_damageability', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n         'help': 'use the effective damageability to draw loss samples instead of the full Monte Carlo method. Default: False'},\n        {'name': 'gulmc_vuln_cache_size', 'default': 200, 'type': int,\n         'help': 'Size in MB of the cache for the vulnerability calculations. Default: 200'},\n        {'name': 'fmpy', 'default': True, 'type': str2bool, 'const': True, 'nargs': '?', 'help': 'use fmcalc python version instead of c++ version'},\n        {'name': 'fmpy_low_memory', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n         'help': 'use memory map instead of RAM to store loss array (may decrease performance but reduce RAM usage drastically)'},\n        {'name': 'fmpy_sort_output', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?', 'help': 'order fmpy output by item_id'},\n        {'name': 'model_custom_gulcalc', 'default': None, 'help': 'Custom gulcalc binary name to call in the model losses step'},\n        {'name': 'peril_filter', 'default': [], 'nargs': '+', 'help': 'Peril specific run'},\n        {'name': 'summarypy', 'default': False, 'type': str2bool, 'const': True,\n            'nargs': '?', 'help': 'use summarycalc python version instead of c++ version'},\n        {'name': 'join_summary_info', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n            'help': 'join summary id information to outputcalc csvs'},\n        {'name': 'eltpy', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n            'help': 'use eltpy python version instead of eltcalc c++ version'},\n        {'name': 'pltpy', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n            'help': 'use pltpy python version instead of pltcalc c++ version'},\n        {'name': 'aalpy', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n            'help': 'use aalpy python version instead of aalcalc c++ version'},\n        {'name': 'lecpy', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n            'help': 'use lecpy python version instead of ordleccalc c++ version'},\n        {'name': 'base_df_engine', 'default': \"oasis_data_manager.df_reader.reader.OasisPandasReader\", 'help': 'The engine to use when loading dataframes'},\n        {'name': 'exposure_df_engine', 'default': None,\n            'help': 'The engine to use when loading dataframes exposure data (default: same as --base-df-engine)'},\n        {'name': 'model_df_engine', 'default': None,\n            'help': 'The engine to use when loading dataframes model data (default: same as --base-df-engine)'},\n        {'name': 'dynamic_footprint', 'default': False,\n            'help': 'Dynamic Footprint'},\n\n        # New vars for chunked loss generation\n        {'name': 'analysis_settings', 'default': None},\n        {'name': 'script_fp', 'default': None},\n        {'name': 'process_number', 'default': None, 'type': int, 'help': 'Partition number to run, if not set then run all in a single script'},\n        {'name': 'max_process_id', 'default': -1, 'type': int, 'help': 'Max number of loss chunks, defaults to `ktools_num_processes` if not set'},\n        {'name': 'ktools_fifo_queue_dir', 'default': None, 'is_path': True, 'help': 'Override the path used for fifo processing'},\n    ]\n\n    def run(self):\n        GenerateLossesDir._check_ktool_rules(self)\n        model_run_fp = GenerateLossesDir._get_output_dir(self)\n\n        # distributed worker will pass in run settings as JSON, if not given load settings\n        # and re-load input dir.\n        if not self._is_run_settings_stored(os.path.join(model_run_fp, 'output')):\n            GenerateLossesDir.run(self)\n\n\"\"\"Docstring (excerpt)\"\"\"\nRuns a single analysis event chunk"
    },
    {
      "chunk_id": "oasislmf/computation/generate/losses.py::GenerateLossesOutput@575",
      "source_type": "code",
      "path": "oasislmf/computation/generate/losses.py",
      "symbol_type": "class",
      "name": "GenerateLossesOutput",
      "lineno": 575,
      "end_lineno": 628,
      "business_stage": "gul",
      "docstring": "Runs the output reports generation on a set of event chunks",
      "content": "# File: oasislmf/computation/generate/losses.py\n# class: GenerateLossesOutput (lines 575-628)\n\nclass GenerateLossesOutput(GenerateLossesDir):\n    \"\"\"\n    Runs the output reports generation on a set of event chunks\n    \"\"\"\n    step_params = GenerateLossesDir.step_params + [\n        {'name': 'analysis_settings_json', 'flag': '-a', 'is_path': True, 'pre_exist': True, 'required': True,\n         'help': 'Analysis settings JSON file path'},\n        {'name': 'ktools_num_processes', 'flag': '-n', 'type': int, 'default': KTOOLS_NUM_PROCESSES,\n         'help': 'Number of ktools calculation processes to use'},\n        {'name': 'ktools_disable_guard', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n         'help': 'Disables error handling in the ktools run script (abort on non-zero exitcode or output on stderr)'},\n        {'name': 'ktools_fifo_relative', 'default': False, 'type': str2bool, 'const': True,\n         'nargs': '?', 'help': 'Create ktools fifo queues under the ./fifo dir'},\n\n        # New vars for chunked loss generation\n        {'name': 'analysis_settings', 'default': None},\n        {'name': 'script_fp', 'default': None},\n        {'name': 'remove_working_file', 'default': False, 'help': 'Delete files in the \"work/\" dir onces outputs have completed'},\n        {'name': 'max_process_id', 'default': -1, 'type': int, 'help': 'Max number of loss chunks, defaults to `ktools_num_processes` if not set'},\n    ]\n\n    def run(self):\n        model_run_fp = GenerateLossesDir._get_output_dir(self)\n        if not self._is_run_settings_stored(os.path.join(model_run_fp, 'output')):\n            GenerateLossesDir.run(self)\n\n        model_runner_module, _ = self._get_model_runner()\n        ri_layers = self._get_num_ri_layers(self.settings, model_run_fp)\n\n        if not self.script_fp:\n            self.script_fp = os.path.join(os.path.abspath(model_run_fp), 'run_outputs.sh')\n\n        if os.path.isfile(self.script_fp):\n            os.remove(self.script_fp)\n\n        bash_params = bash.bash_params(\n            self.settings,\n            number_of_processes=self.ktools_num_processes,\n            num_reinsurance_iterations=ri_layers,\n            filename=self.script_fp,\n            bash_trace=self.verbose,\n            stderr_guard=not self.ktools_disable_guard,\n            fifo_tmp_dir=not self.ktools_fifo_relative,\n            remove_working_file=self.remove_working_file,\n            max_process_id=self.max_process_id,\n        )\n        with setcwd(model_run_fp):\n            try:\n                self.logger.info('Generating Loss outputs in {}'.format(model_run_fp))\n                return model_runner_module.run_outputs(**bash_params)\n            except CalledProcessError as e:\n                log_fp = os.path.join(model_run_fp, 'log', 'out')\n                self._print_error_logs(log_fp, e)\n        return model_run_fp\n\n\"\"\"Docstring (excerpt)\"\"\"\nRuns the output reports generation on a set of event chunks"
    },
    {
      "chunk_id": "oasislmf/computation/generate/losses.py::GenerateLosses@631",
      "source_type": "code",
      "path": "oasislmf/computation/generate/losses.py",
      "symbol_type": "class",
      "name": "GenerateLosses",
      "lineno": 631,
      "end_lineno": 842,
      "business_stage": "gul",
      "docstring": "Runs the GenerateLosses workflow as a single bash script (Default for the MDK)\n\nGenerates losses using the installed ktools framework given Oasis files,\nmodel analysis settings JSON file, model data and model package data.\n\nThe command line arguments can be supplied in the configuration file\n(``oasislmf.json`` by default or specified with the ``--config`` flag).\nRun ``oasislmf config --help`` for more information.\n\nThe script creates a time-stamped folder in the model run directory and\nsets that as the new model run directory, copies the analysis settings\nJSON file into the run directory and creates the following folder\nstructure\n::\n\n    |-- analysis_settings.json\n    |-- fifo\n    |-- input\n        |-- RI_1\n    |-- output\n    |-- ri_layers.json\n    |-- run_ktools.sh\n    |-- static\n    `-- work\n\nDepending on the OS type the model data is symlinked (Linux, Darwin) or\ncopied (Cygwin, Windows) into the ``static`` subfolder. The input files\nare kept in the ``input`` subfolder and the losses are generated as CSV\nfiles in the ``output`` subfolder.",
      "content": "# File: oasislmf/computation/generate/losses.py\n# class: GenerateLosses (lines 631-842)\n\nclass GenerateLosses(GenerateLossesDir):\n    \"\"\"\n    Runs the GenerateLosses workflow as a single bash script (Default for the MDK)\n\n    Generates losses using the installed ktools framework given Oasis files,\n    model analysis settings JSON file, model data and model package data.\n\n    The command line arguments can be supplied in the configuration file\n    (``oasislmf.json`` by default or specified with the ``--config`` flag).\n    Run ``oasislmf config --help`` for more information.\n\n    The script creates a time-stamped folder in the model run directory and\n    sets that as the new model run directory, copies the analysis settings\n    JSON file into the run directory and creates the following folder\n    structure\n    ::\n\n        |-- analysis_settings.json\n        |-- fifo\n        |-- input\n            |-- RI_1\n        |-- output\n        |-- ri_layers.json\n        |-- run_ktools.sh\n        |-- static\n        `-- work\n\n    Depending on the OS type the model data is symlinked (Linux, Darwin) or\n    copied (Cygwin, Windows) into the ``static`` subfolder. The input files\n    are kept in the ``input`` subfolder and the losses are generated as CSV\n    files in the ``output`` subfolder.\n    \"\"\"\n    step_params = GenerateLossesDir.step_params + [\n        {'name': 'ktools_num_processes', 'flag': '-n', 'type': int, 'default': KTOOLS_NUM_PROCESSES,\n         'help': 'Number of ktools calculation processes to use'},\n        {'name': 'ktools_event_shuffle', 'default': EVE_DEFAULT_SHUFFLE, 'type': int,\n         'help': 'Set rule for event shuffling between eve partions, 0 - No shuffle, 1 - round robin (output elts sorted), 2 - Fisher-Yates shuffle, 3 - std::shuffle (previous default in oasislmf<1.14.0) '},\n        {'name': 'ktools_alloc_rule_gul', 'default': KTOOLS_ALLOC_GUL_DEFAULT, 'type': int, 'help': 'Set the allocation used in gulcalc'},\n        {'name': 'ktools_alloc_rule_il', 'default': KTOOLS_ALLOC_IL_DEFAULT, 'type': int,\n         'help': 'Set the fmcalc allocation rule used in direct insured loss'},\n        {'name': 'ktools_alloc_rule_ri', 'default': KTOOLS_ALLOC_RI_DEFAULT, 'type': int,\n         'help': 'Set the fmcalc allocation rule used in reinsurance'},\n        {'name': 'ktools_num_gul_per_lb', 'default': KTOOL_N_GUL_PER_LB, 'type': int,\n         'help': 'Number of gul per load balancer (0 means no load balancer)'},\n        {'name': 'ktools_num_fm_per_lb', 'default': KTOOL_N_FM_PER_LB, 'type': int,\n         'help': 'Number of fm per load balancer (0 means no load balancer)'},\n        {'name': 'ktools_disable_guard', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n         'help': 'Disables error handling in the ktools run script (abort on non-zero exitcode or output on stderr)'},\n        {'name': 'ktools_fifo_relative', 'default': False, 'type': str2bool, 'const': True,\n         'nargs': '?', 'help': 'Create ktools fifo queues under the ./fifo dir'},\n        {'name': 'modelpy', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n         'help': 'use getmodel python version instead of c++ version'},\n        {'name': 'evepy', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n         'help': 'use eve python version instead of c++ version'},\n        {'name': 'gulpy', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n         'help': 'use gulcalc python version instead of c++ version'},\n        {'name': 'gulpy_random_generator', 'default': 1, 'type': int,\n         'help': 'set the random number generator in gulpy (0: Mersenne-Twister, 1: Latin Hypercube. Default: 1).'},\n        {'name': 'gulmc', 'default': True, 'type': str2bool, 'const': True, 'nargs': '?', 'help': 'use full Monte Carlo gulcalc python version'},\n        {'name': 'gulmc_random_generator', 'default': 1, 'type': int,\n         'help': 'set the random number generator in gulmc (0: Mersenne-Twister, 1: Latin Hypercube. Default: 1).'},\n        {'name': 'gulmc_effective_damageability', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n         'help': 'use the effective damageability to draw loss samples instead of the full Monte Carlo method. Default: False'},\n        {'name': 'gulmc_vuln_cache_size', 'default': 200, 'type': int,\n         'help': 'Size in MB of the cache for the vulnerability calculations. Default: 200'},\n        {'name': 'fmpy', 'default': True, 'type': str2bool, 'const': True, 'nargs': '?', 'help': 'use fmcalc python version instead of c++ version'},\n        {'name': 'fmpy_low_memory', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n         'help': 'use memory map instead of RAM to store loss array (may decrease performance but reduce RAM usage drastically)'},\n        {'name': 'fmpy_sort_output', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?', 'help': 'order fmpy output by item_id'},\n        {'name': 'model_custom_gulcalc', 'default': None, 'help': 'Custom gulcalc binary name to call in the model losses step'},\n        {'name': 'model_py_server', 'default': False, 'type': str2bool, 'help': 'running the data server for modelpy'},\n        {'name': 'peril_filter', 'default': [], 'nargs': '+', 'help': 'Peril specific run'},\n        {'name': 'summarypy', 'default': False, 'type': str2bool, 'const': True,\n            'nargs': '?', 'help': 'use summarycalc python version instead of c++ version'},\n        {'name': 'join_summary_info', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n            'help': 'join summary id information to outputcalc csvs'},\n        {'name': 'eltpy', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n            'help': 'use eltpy python version instead of eltcalc c++ version'},\n        {'name': 'pltpy', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n            'help': 'use pltpy python version instead of pltcalc c++ version'},\n\n\"\"\"Docstring (excerpt)\"\"\"\nRuns the GenerateLosses workflow as a single bash script (Default for the MDK)\n\nGenerates losses using the installed ktools framework given Oasis files,\nmodel analysis settings JSON file, model data and model package data.\n\nThe command line arguments can be supplied in the configuration file\n(``oasislmf.json`` by default or specified with the ``--config`` flag).\nRun ``oasislmf config --help`` for more information.\n\nThe script creates a time-stamped folder in the model run directory and\nsets that as the new model run directory, copies the analysis settings\nJSON file into the run directory and creates the following folder\nstructure\n::\n\n    |-- analysis_settings.json\n    |-- fifo\n    |-- input\n        |-- RI_1\n    |-- output\n    |-- ri_layers.json\n    |-- run_ktools.sh\n    |-- static\n    `-- work\n\nDepending on the OS type the model data is symlinked (Linux, Darwin) or\ncopied (Cygwin, Windows) into the ``static`` subfolder. The input files\nare kept in the ``input`` subfolder and the losses are generated as CSV\nfiles in the ``output`` subfolder."
    },
    {
      "chunk_id": "oasislmf/computation/helper/autocomplete.py::HelperTabComplete@13",
      "source_type": "code",
      "path": "oasislmf/computation/helper/autocomplete.py",
      "symbol_type": "class",
      "name": "HelperTabComplete",
      "lineno": 13,
      "end_lineno": 83,
      "business_stage": "other",
      "docstring": "Adds required command to `.bashrc` Linux or .bash_profile for mac\nso that Command autocomplete works for oasislmf CLI",
      "content": "# File: oasislmf/computation/helper/autocomplete.py\n# class: HelperTabComplete (lines 13-83)\n\nclass HelperTabComplete(ComputationStep):\n    \"\"\"\n    Adds required command to `.bashrc` Linux or .bash_profile for mac\n    so that Command autocomplete works for oasislmf CLI\n    \"\"\"\n    step_params = [\n        {'name': 'bash_rc_file', 'flag': '-p', 'help': 'Path to bash configuration RC file, \"~/.bashrc\". '},\n        {'name': 'no_confirm', 'flag': '-y', 'action': 'store_true', 'default': False, 'help': 'Skip the confirmation prompt'},\n    ]\n\n    def confirm_action(self, question_str, no_confirm=False):\n        self.logger.debug('Prompt user for confirmation')\n        if no_confirm:\n            return True\n        try:\n            check = str(input(\"%s (Y/N): \" % question_str)).lower().strip()\n            if check[:1] == 'y':\n                return True\n            elif check[:1] == 'n':\n                return False\n            else:\n                self.logger.error('Enter \"y\" for Yes, \"n\" for No or Ctrl-C to exit.\\n')\n                return self.confirm_action(question_str)\n        except KeyboardInterrupt:\n            self.logger.error('\\nexiting.')\n\n    def install_autocomplete(self, target_file=None):\n        msg_success = 'Auto-Complete installed.'\n        msg_failed = 'install failed'\n        msg_installed = 'Auto-Complete feature is already enabled.'\n        msg_reload_bash = '\\n To activate reload bash by running: \\n     source {}'.format(target_file)\n        cmd_header = '# Added by OasisLMF\\n'\n        cmd_autocomplete = 'complete -C completer_oasislmf oasislmf\\n'\n\n        try:\n            if os.path.isfile(target_file):\n                # Check command is in file\n                with open(target_file, \"r\") as rc:\n                    if cmd_autocomplete in rc.read():\n                        self.logger.info(msg_installed)\n                        self.logger.info(msg_reload_bash)\n                        sys.exit(0)\n            else:\n                # create new file at set location\n                basedir = os.path.dirname(target_file)\n                if not os.path.isdir(basedir):\n                    os.makedirs(basedir)\n\n            # Add complete command\n            with open(target_file, \"a\") as rc:\n                rc.write(cmd_header)\n                rc.write(cmd_autocomplete)\n                self.logger.info(msg_success)\n                self.logger.info(msg_reload_bash)\n        except Exception as e:\n            self.logger.error('{}: {}'.format(msg_failed, e))\n\n    def run(self):\n\n        # select default bashrc if not set\n        if not self.bash_rc_file:\n            default_file = '.bash_profile' if system == 'Darwin' else '.bashrc'\n            self.bash_rc_file = os.path.join(\n                os.path.expanduser('~'),\n                default_file\n            )\n\n        # Prompt user, and then install\n        msg_user = 'Running this will append a command to the following file:\\n'\n        if self.confirm_action(\"{} {}\".format(msg_user, self.bash_rc_file), self.no_confirm):\n            self.install_autocomplete(self.bash_rc_file)\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdds required command to `.bashrc` Linux or .bash_profile for mac\nso that Command autocomplete works for oasislmf CLI"
    },
    {
      "chunk_id": "oasislmf/computation/hooks/post_analysis.py::PostAnalysis@11",
      "source_type": "code",
      "path": "oasislmf/computation/hooks/post_analysis.py",
      "symbol_type": "class",
      "name": "PostAnalysis",
      "lineno": 11,
      "end_lineno": 53,
      "business_stage": "other",
      "docstring": "Computation step that is called after loss calculations.\n\nIt passes the output directory to a customisable function that might modify or add to the\nstandard output files.",
      "content": "# File: oasislmf/computation/hooks/post_analysis.py\n# class: PostAnalysis (lines 11-53)\n\nclass PostAnalysis(ComputationStep):\n    \"\"\"Computation step that is called after loss calculations.\n\n    It passes the output directory to a customisable function that might modify or add to the\n    standard output files.\n    \"\"\"\n    settings_params = [{'name': 'analysis_settings_json', 'loader': analysis_settings_loader, 'user_role': 'user'},\n                       {'name': 'model_settings_json', 'loader': model_settings_loader}]\n\n    step_params = [\n        {'name': 'post_analysis_module', 'required': True, 'is_path': True, 'pre_exist': True,\n         'help': 'Post-Analysis module path'},\n        {'name': 'post_analysis_class_name', 'default': 'PostAnalysis',\n         'help': 'Name of the class to use for the post_analysis'},\n        {'name': 'model_run_dir', 'flag': '-r', 'is_path': True, 'pre_exist': False,\n         'help': 'Model run directory path'},\n        {'name': 'model_data_dir', 'flag': '-d', 'is_path': True, 'pre_exist': True,\n         'help': 'Model data directory path'},\n        {'name': 'analysis_settings_json', 'flag': '-a', 'is_path': True, 'pre_exist': True,\n         'help': 'Analysis settings JSON file path'},\n        {'name': 'user_data_dir', 'flag': '-D', 'is_path': True, 'pre_exist': False,\n         'help': 'Directory containing additional model data files which varies between analysis runs'},\n    ]\n\n    run_dir_key = 'post-analysis'\n\n    def run(self):\n        kwargs = {\n            'model_data_dir': self.model_data_dir,\n            'analysis_settings_json': self.analysis_settings_json,\n            'settings': self.settings,\n            'model_run_dir': self.model_run_dir,\n            'user_data_dir': self.user_data_dir,\n        }\n\n        _module = get_custom_module(self.post_analysis_module, 'Post-Analysis module path')\n        try:\n            _class = getattr(_module, self.post_analysis_class_name)\n        except AttributeError as e:\n            raise OasisException(f\"Class {self.post_analysis_class_name} \"\n                                 f\"is not defined in module {self.post_analysis_module}\") from e.__cause__\n\n        _class(**kwargs).run()\n\n\"\"\"Docstring (excerpt)\"\"\"\nComputation step that is called after loss calculations.\n\nIt passes the output directory to a customisable function that might modify or add to the\nstandard output files."
    },
    {
      "chunk_id": "oasislmf/computation/hooks/post_file_gen.py::PostFileGen@15",
      "source_type": "code",
      "path": "oasislmf/computation/hooks/post_file_gen.py",
      "symbol_type": "class",
      "name": "PostFileGen",
      "lineno": 15,
      "end_lineno": 114,
      "business_stage": "other",
      "docstring": "Computation step that will be call just after oasis file generation.\nOn the platform it will be called on a single machine before the files are copied on the several worker for the loss calculation\nAdd the ability to specify a model specific step that will modify or expand on the loss calculation input file",
      "content": "# File: oasislmf/computation/hooks/post_file_gen.py\n# class: PostFileGen (lines 15-114)\n\nclass PostFileGen(ComputationStep):\n    \"\"\"\n    Computation step that will be call just after oasis file generation.\n    On the platform it will be called on a single machine before the files are copied on the several worker for the loss calculation\n    Add the ability to specify a model specific step that will modify or expand on the loss calculation input file\n    \"\"\"\n    settings_params = [{'name': 'analysis_settings_json', 'loader': analysis_settings_loader, 'user_role': 'user'},\n                       {'name': 'model_settings_json', 'loader': model_settings_loader}]\n\n    step_params = [{'name': 'post_file_gen_module', 'required': True, 'is_path': True, 'pre_exist': True,\n                    'help': 'post file generation lookup module path'},\n                   {'name': 'post_file_gen_class_name', 'default': 'PostFileGen',\n                    'help': 'Name of the class to use for the pre loss calculation'},\n                   {'name': 'post_file_gen_setting_json', 'is_path': True, 'pre_exist': True,\n                    'help': 'post file generation config JSON file path'},\n                   {'name': 'oed_schema_info', 'help': 'Takes a version of OED schema to use in the form \"v1.2.3\" or a path to an OED schema json'},\n                   {'name': 'oed_location_csv', 'flag': '-x', 'is_path': True, 'pre_exist': True, 'help': 'Source location CSV file path'},\n                   {'name': 'oed_accounts_csv', 'flag': '-y', 'is_path': True, 'pre_exist': True, 'help': 'Source accounts CSV file path'},\n                   {'name': 'oed_info_csv', 'flag': '-i', 'is_path': True, 'pre_exist': True, 'help': 'Reinsurance info. CSV file path'},\n                   {'name': 'oed_scope_csv', 'flag': '-s', 'is_path': True, 'pre_exist': True, 'help': 'Reinsurance scope CSV file path'},\n                   {'name': 'check_oed', 'type': str2bool, 'const': True, 'nargs': '?', 'default': True, 'help': 'if True check input oed files'},\n                   {'name': 'oasis_files_dir', 'flag': '-o', 'is_path': True, 'pre_exist': False,\n                    'help': 'Path to the directory in which to generate the Oasis files'},\n                   {'name': 'location', 'type': str, 'nargs': '+', 'help': 'A set of locations to include in the files'},\n                   {'name': 'portfolio', 'type': str, 'nargs': '+', 'help': 'A set of portfolios to include in the files'},\n                   {'name': 'account', 'type': str, 'nargs': '+', 'help': 'A set of locations to include in the files'},\n                   {'name': 'base_df_engine', 'type': str, 'default': 'oasis_data_manager.df_reader.reader.OasisPandasReader',\n                    'help': 'The default dataframe reading engine to use when loading files'},\n                   {'name': 'exposure_df_engine', 'type': str, 'default': None,\n                    'help': 'The dataframe reading engine to use when loading exposure files'},\n                   {'name': 'model_df_engine', 'type': str, 'default': None, 'help': 'The dataframe reading engine to use when loading model files'},\n                   {'name': 'model_data_dir', 'flag': '-d', 'is_path': True, 'pre_exist': True, 'help': 'Model data directory path'},\n                   {'name': 'analysis_settings_json', 'flag': '-a', 'is_path': True, 'pre_exist': True,\n                    'help': 'Analysis settings JSON file path'},\n                   {'name': 'user_data_dir', 'flag': '-D', 'is_path': True, 'pre_exist': False,\n                    'help': 'Directory containing additional model data files which varies between analysis runs'},\n                   {'name': 'oed_backend_dtype', 'type': str, 'default': 'pd_dtype',\n                    'help': \"define what type dtype the oed column will be (pd_dtype or pa_dtype)\"},\n                   ]\n\n    run_dir_key = 'pre-loss'\n\n    def get_exposure_data_config(self):\n        return {\n            'location': self.oed_location_csv,\n            'account': self.oed_accounts_csv,\n            'ri_info': self.oed_info_csv,\n            'ri_scope': self.oed_scope_csv,\n            'oed_schema_info': self.oed_schema_info,\n            'check_oed': self.check_oed,\n            'use_field': True,\n            'location_numbers': self.location,\n            'portfolio_numbers': self.portfolio,\n            'account_numbers': self.account,\n            'base_df_engine': self.base_df_engine,\n            'exposure_df_engine': self.exposure_df_engine,\n            'backend_dtype': self.oed_backend_dtype,\n        }\n\n    def run(self):\n        \"\"\"\n        import post_file_gen_module and call the run method\n        \"\"\"\n        exposure_data = get_exposure_data(self, add_internal_col=True)\n        kwargs = dict()\n\n        # If given a value for 'oasis_files_dir' then use that directly\n        if self.oasis_files_dir:\n            input_dir = self.oasis_files_dir\n        else:\n            input_dir = self.get_default_run_dir()\n            pathlib.Path(input_dir).mkdir(parents=True, exist_ok=True)\n\n        kwargs['exposure_data'] = exposure_data\n        kwargs['input_dir'] = input_dir\n        kwargs['model_data_dir'] = self.model_data_dir\n        kwargs['analysis_settings_json'] = self.analysis_settings_json\n        kwargs['settings'] = self.settings\n        kwargs['user_data_dir'] = self.user_data_dir\n        kwargs['logger'] = self.logger\n\n\"\"\"Docstring (excerpt)\"\"\"\nComputation step that will be call just after oasis file generation.\nOn the platform it will be called on a single machine before the files are copied on the several worker for the loss calculation\nAdd the ability to specify a model specific step that will modify or expand on the loss calculation input file"
    },
    {
      "chunk_id": "oasislmf/computation/hooks/post_file_gen.py::run@74",
      "source_type": "code",
      "path": "oasislmf/computation/hooks/post_file_gen.py",
      "symbol_type": "function",
      "name": "run",
      "lineno": 74,
      "end_lineno": 114,
      "business_stage": "other",
      "docstring": "import post_file_gen_module and call the run method",
      "content": "# File: oasislmf/computation/hooks/post_file_gen.py\n# function: run (lines 74-114)\n\n    def run(self):\n        \"\"\"\n        import post_file_gen_module and call the run method\n        \"\"\"\n        exposure_data = get_exposure_data(self, add_internal_col=True)\n        kwargs = dict()\n\n        # If given a value for 'oasis_files_dir' then use that directly\n        if self.oasis_files_dir:\n            input_dir = self.oasis_files_dir\n        else:\n            input_dir = self.get_default_run_dir()\n            pathlib.Path(input_dir).mkdir(parents=True, exist_ok=True)\n\n        kwargs['exposure_data'] = exposure_data\n        kwargs['input_dir'] = input_dir\n        kwargs['model_data_dir'] = self.model_data_dir\n        kwargs['analysis_settings_json'] = self.analysis_settings_json\n        kwargs['settings'] = self.settings\n        kwargs['user_data_dir'] = self.user_data_dir\n        kwargs['logger'] = self.logger\n\n        if self.post_file_gen_setting_json:\n            with open(self.post_file_gen_setting_json) as post_file_gen_setting_file:\n                kwargs['post_file_gen_setting'] = json.load(post_file_gen_setting_file)\n        else:\n            kwargs['post_file_gen_setting'] = {}\n\n        _module = get_custom_module(self.post_file_gen_module, 'post file gen module path')\n\n        try:\n            _class = getattr(_module, self.post_file_gen_class_name)\n        except AttributeError as e:\n            raise OasisException(f\"class {self.post_file_gen_class_name} \"\n                                 f\"is not defined in module {self.post_file_gen_module}\") from e.__cause__\n\n        _class_return = _class(**kwargs).run()\n\n        return {\n            \"class\": _class_return\n        }\n\n\"\"\"Docstring (excerpt)\"\"\"\nimport post_file_gen_module and call the run method"
    },
    {
      "chunk_id": "oasislmf/computation/hooks/pre_analysis.py::ExposurePreAnalysis@16",
      "source_type": "code",
      "path": "oasislmf/computation/hooks/pre_analysis.py",
      "symbol_type": "class",
      "name": "ExposurePreAnalysis",
      "lineno": 16,
      "end_lineno": 141,
      "business_stage": "other",
      "docstring": "Computation step that will be call before the gulcalc.\nAdd the ability to specify a model specific pre-analysis hook for exposure modification,\nAllows OED to be processed by some custom code.\nExample of usage include geo-coding, exposure enhancement, or dis-aggregation...\n\nwhen the run method is call it will :\n- load the module specified at exposure_pre_analysis_module\n- init the class named exposure_pre_analysis_class_name with all the non null args in step_params as key arguments\n- call the method run of the object\n- return the output of the method\n\nyou can find an example of such custom module in OasisPyWind/custom_module/exposure_pre_analysis.py",
      "content": "# File: oasislmf/computation/hooks/pre_analysis.py\n# class: ExposurePreAnalysis (lines 16-141)\n\nclass ExposurePreAnalysis(ComputationStep):\n    \"\"\"\n    Computation step that will be call before the gulcalc.\n    Add the ability to specify a model specific pre-analysis hook for exposure modification,\n    Allows OED to be processed by some custom code.\n    Example of usage include geo-coding, exposure enhancement, or dis-aggregation...\n\n    when the run method is call it will :\n    - load the module specified at exposure_pre_analysis_module\n    - init the class named exposure_pre_analysis_class_name with all the non null args in step_params as key arguments\n    - call the method run of the object\n    - return the output of the method\n\n    you can find an example of such custom module in OasisPyWind/custom_module/exposure_pre_analysis.py\n\n    \"\"\"\n    settings_params = [{'name': 'analysis_settings_json', 'loader': analysis_settings_loader, 'user_role': 'user'},\n                       {'name': 'model_settings_json', 'loader': model_settings_loader}]\n\n    step_params = [{'name': 'exposure_pre_analysis_module', 'required': True, 'is_path': True, 'pre_exist': True,\n                    'help': 'Exposure Pre-Analysis lookup module path'},\n                   {'name': 'exposure_pre_analysis_class_name', 'default': 'ExposurePreAnalysis',\n                    'help': 'Name of the class to use for the exposure_pre_analysis'},\n                   {'name': 'exposure_pre_analysis_setting_json', 'is_path': True, 'pre_exist': True,\n                    'help': 'Exposure Pre-Analysis config JSON file path'},\n                   {'name': 'oed_schema_info', 'help': 'Takes a version of OED schema to use in the form \"v1.2.3\" or a path to an OED schema json'},\n                   {'name': 'oed_location_csv', 'flag': '-x', 'is_path': True, 'pre_exist': True, 'help': 'Source location CSV file path'},\n                   {'name': 'oed_accounts_csv', 'flag': '-y', 'is_path': True, 'pre_exist': True, 'help': 'Source accounts CSV file path'},\n                   {'name': 'oed_info_csv', 'flag': '-i', 'is_path': True, 'pre_exist': True, 'help': 'Reinsurance info. CSV file path'},\n                   {'name': 'oed_scope_csv', 'flag': '-s', 'is_path': True, 'pre_exist': True, 'help': 'Reinsurance scope CSV file path'},\n                   {'name': 'check_oed', 'type': str2bool, 'const': True, 'nargs': '?', 'default': True, 'help': 'if True check input oed files'},\n                   {'name': 'oasis_files_dir', 'flag': '-o', 'is_path': True, 'pre_exist': False,\n                    'help': 'Path to the directory in which to generate the Oasis files'},\n                   {'name': 'location', 'type': str, 'nargs': '+', 'help': 'A set of locations to include in the files'},\n                   {'name': 'portfolio', 'type': str, 'nargs': '+', 'help': 'A set of portfolios to include in the files'},\n                   {'name': 'account', 'type': str, 'nargs': '+', 'help': 'A set of locations to include in the files'},\n                   {'name': 'base_df_engine', 'type': str, 'default': 'oasis_data_manager.df_reader.reader.OasisPandasReader',\n                    'help': 'The default dataframe reading engine to use when loading files'},\n                   {'name': 'exposure_df_engine', 'type': str, 'default': None,\n                    'help': 'The dataframe reading engine to use when loading exposure files'},\n                   {'name': 'model_df_engine', 'type': str, 'default': None, 'help': 'The dataframe reading engine to use when loading model files'},\n                   {'name': 'model_data_dir', 'flag': '-d', 'is_path': True, 'pre_exist': True, 'help': 'Model data directory path'},\n                   {'name': 'analysis_settings_json', 'flag': '-a', 'is_path': True, 'pre_exist': True,\n                    'help': 'Analysis settings JSON file path'},\n                   {'name': 'user_data_dir', 'flag': '-D', 'is_path': True, 'pre_exist': False,\n                    'help': 'Directory containing additional model data files which varies between analysis runs'},\n                   {'name': 'oed_backend_dtype', 'type': str, 'default': 'pd_dtype',\n                    'help': \"define what type dtype the oed column will be (pd_dtype or pa_dtype)\"},\n                   ]\n\n    run_dir_key = 'pre-analysis'\n\n    def get_exposure_data_config(self):\n        return {\n            'location': self.oed_location_csv,\n            'account': self.oed_accounts_csv,\n            'ri_info': self.oed_info_csv,\n            'ri_scope': self.oed_scope_csv,\n            'oed_schema_info': self.oed_schema_info,\n            'check_oed': self.check_oed,\n            'use_field': True,\n            'location_numbers': self.location,\n            'portfolio_numbers': self.portfolio,\n            'account_numbers': self.account,\n            'base_df_engine': self.base_df_engine,\n            'exposure_df_engine': self.exposure_df_engine,\n            'backend_dtype': self.oed_backend_dtype,\n        }\n\n    def run(self):\n        \"\"\"\n        import exposure_pre_analysis_module and call the run method\n        \"\"\"\n        exposure_data = get_exposure_data(self, add_internal_col=True)\n        kwargs = dict()\n\n        # If given a value for 'oasis_files_dir' then use that directly\n        if self.oasis_files_dir:\n            input_dir = self.oasis_files_dir\n        else:\n\n\"\"\"Docstring (excerpt)\"\"\"\nComputation step that will be call before the gulcalc.\nAdd the ability to specify a model specific pre-analysis hook for exposure modification,\nAllows OED to be processed by some custom code.\nExample of usage include geo-coding, exposure enhancement, or dis-aggregation...\n\nwhen the run method is call it will :\n- load the module specified at exposure_pre_analysis_module\n- init the class named exposure_pre_analysis_class_name with all the non null args in step_params as key arguments\n- call the method run of the object\n- return the output of the method\n\nyou can find an example of such custom module in OasisPyWind/custom_module/exposure_pre_analysis.py"
    },
    {
      "chunk_id": "oasislmf/computation/hooks/pre_analysis.py::run@85",
      "source_type": "code",
      "path": "oasislmf/computation/hooks/pre_analysis.py",
      "symbol_type": "function",
      "name": "run",
      "lineno": 85,
      "end_lineno": 141,
      "business_stage": "other",
      "docstring": "import exposure_pre_analysis_module and call the run method",
      "content": "# File: oasislmf/computation/hooks/pre_analysis.py\n# function: run (lines 85-141)\n\n    def run(self):\n        \"\"\"\n        import exposure_pre_analysis_module and call the run method\n        \"\"\"\n        exposure_data = get_exposure_data(self, add_internal_col=True)\n        kwargs = dict()\n\n        # If given a value for 'oasis_files_dir' then use that directly\n        if self.oasis_files_dir:\n            input_dir = self.oasis_files_dir\n        else:\n            input_dir = self.get_default_run_dir()\n            pathlib.Path(input_dir).mkdir(parents=True, exist_ok=True)\n\n        ids_option = {'loc_id': UnknownColumnSaveOption.DELETE,\n                      'loc_idx': UnknownColumnSaveOption.DELETE}\n        exposure_data.save(path=input_dir, version_name='raw', save_config=True, unknown_columns=ids_option)\n        kwargs['exposure_data'] = exposure_data\n        kwargs['input_dir'] = input_dir\n        kwargs['model_data_dir'] = self.model_data_dir\n        kwargs['user_data_dir'] = self.user_data_dir\n        kwargs['analysis_settings_json'] = self.analysis_settings_json\n        kwargs['settings'] = self.settings\n\n        if self.exposure_pre_analysis_setting_json:\n            with open(self.exposure_pre_analysis_setting_json) as exposure_pre_analysis_setting_file:\n                kwargs['exposure_pre_analysis_setting'] = json.load(exposure_pre_analysis_setting_file)\n\n        _module = get_custom_module(self.exposure_pre_analysis_module, 'Exposure Pre-Analysis lookup module path')\n\n        try:\n            _class = getattr(_module, self.exposure_pre_analysis_class_name)\n        except AttributeError as e:\n            raise OasisException(f\"class {self.exposure_pre_analysis_class_name} \"\n                                 f\"is not defined in module {self.exposure_pre_analysis_module}\") from e.__cause__\n\n        original_files = {oed_source.oed_name: str(oed_source.current_source['filepath']) for oed_source in exposure_data.get_oed_sources()}\n        self.logger.info('\\nPre-analysis original files: {}'.format(\n            json.dumps(original_files, indent=4)))\n\n        print(kwargs)\n        print(_class(**kwargs))\n        _class_return = _class(**kwargs).run()\n\n        exposure_data.save(path=input_dir, version_name='', save_config=True, unknown_columns=ids_option)\n        # regenerate ids\n        exposure_data.location.dataframe = exposure_data.location.dataframe.drop(columns=['loc_id', 'loc_idx'])\n        prepare_oed_exposure(exposure_data)\n\n        modified_files = {oed_source.oed_name: str(oed_source.current_source['filepath']) for oed_source in exposure_data.get_oed_sources()}\n        self.logger.info('\\nPre-analysis modified files: {}'.format(\n            json.dumps(modified_files, indent=4)))\n        return {\n            \"class\": _class_return,\n            \"modified\": modified_files,\n            \"original\": original_files,\n        }\n\n\"\"\"Docstring (excerpt)\"\"\"\nimport exposure_pre_analysis_module and call the run method"
    },
    {
      "chunk_id": "oasislmf/computation/hooks/pre_loss.py::PreLoss@15",
      "source_type": "code",
      "path": "oasislmf/computation/hooks/pre_loss.py",
      "symbol_type": "class",
      "name": "PreLoss",
      "lineno": 15,
      "end_lineno": 114,
      "business_stage": "gul",
      "docstring": "Computation step that will be call just before loss compuation.\nOn the platform it will be called on each machine performing the loss calculation,\nAdd the ability to specify a model specific step that will modify or expand on the loss calculation input file on each worker.",
      "content": "# File: oasislmf/computation/hooks/pre_loss.py\n# class: PreLoss (lines 15-114)\n\nclass PreLoss(ComputationStep):\n    \"\"\"\n    Computation step that will be call just before loss compuation.\n    On the platform it will be called on each machine performing the loss calculation,\n    Add the ability to specify a model specific step that will modify or expand on the loss calculation input file on each worker.\n    \"\"\"\n    settings_params = [{'name': 'analysis_settings_json', 'loader': analysis_settings_loader, 'user_role': 'user'},\n                       {'name': 'model_settings_json', 'loader': model_settings_loader}]\n\n    step_params = [{'name': 'pre_loss_module', 'required': True, 'is_path': True, 'pre_exist': True,\n                    'help': 'pre loss calculation lookup module path'},\n                   {'name': 'pre_loss_class_name', 'default': 'PreLoss',\n                    'help': 'Name of the class to use for the pre loss calculation'},\n                   {'name': 'pre_loss_setting_json', 'is_path': True, 'pre_exist': True,\n                    'help': 'pre loss calculation config JSON file path'},\n                   {'name': 'oed_schema_info', 'help': 'Takes a version of OED schema to use in the form \"v1.2.3\" or a path to an OED schema json'},\n                   {'name': 'oed_location_csv', 'flag': '-x', 'is_path': True, 'pre_exist': True, 'help': 'Source location CSV file path'},\n                   {'name': 'oed_accounts_csv', 'flag': '-y', 'is_path': True, 'pre_exist': True, 'help': 'Source accounts CSV file path'},\n                   {'name': 'oed_info_csv', 'flag': '-i', 'is_path': True, 'pre_exist': True, 'help': 'Reinsurance info. CSV file path'},\n                   {'name': 'oed_scope_csv', 'flag': '-s', 'is_path': True, 'pre_exist': True, 'help': 'Reinsurance scope CSV file path'},\n                   {'name': 'check_oed', 'type': str2bool, 'const': True, 'nargs': '?', 'default': True, 'help': 'if True check input oed files'},\n                   {'name': 'oasis_files_dir', 'flag': '-o', 'is_path': True, 'pre_exist': False,\n                    'help': 'Path to the directory in which to generate the Oasis files'},\n                   {'name': 'location', 'type': str, 'nargs': '+', 'help': 'A set of locations to include in the files'},\n                   {'name': 'portfolio', 'type': str, 'nargs': '+', 'help': 'A set of portfolios to include in the files'},\n                   {'name': 'account', 'type': str, 'nargs': '+', 'help': 'A set of locations to include in the files'},\n                   {'name': 'base_df_engine', 'type': str, 'default': 'oasis_data_manager.df_reader.reader.OasisPandasReader',\n                    'help': 'The default dataframe reading engine to use when loading files'},\n                   {'name': 'exposure_df_engine', 'type': str, 'default': None,\n                    'help': 'The dataframe reading engine to use when loading exposure files'},\n                   {'name': 'model_df_engine', 'type': str, 'default': None, 'help': 'The dataframe reading engine to use when loading model files'},\n                   {'name': 'model_data_dir', 'flag': '-d', 'is_path': True, 'pre_exist': True, 'help': 'Model data directory path'},\n                   {'name': 'analysis_settings_json', 'flag': '-a', 'is_path': True, 'pre_exist': True,\n                    'help': 'Analysis settings JSON file path'},\n                   {'name': 'user_data_dir', 'flag': '-D', 'is_path': True, 'pre_exist': False,\n                    'help': 'Directory containing additional model data files which varies between analysis runs'},\n                   {'name': 'oed_backend_dtype', 'type': str, 'default': 'pd_dtype',\n                    'help': \"define what type dtype the oed column will be (pd_dtype or pa_dtype)\"},\n                   ]\n\n    run_dir_key = 'pre-loss'\n\n    def get_exposure_data_config(self):\n        return {\n            'location': self.oed_location_csv,\n            'account': self.oed_accounts_csv,\n            'ri_info': self.oed_info_csv,\n            'ri_scope': self.oed_scope_csv,\n            'oed_schema_info': self.oed_schema_info,\n            'check_oed': self.check_oed,\n            'use_field': True,\n            'location_numbers': self.location,\n            'portfolio_numbers': self.portfolio,\n            'account_numbers': self.account,\n            'base_df_engine': self.base_df_engine,\n            'exposure_df_engine': self.exposure_df_engine,\n            'backend_dtype': self.oed_backend_dtype,\n        }\n\n    def run(self):\n        \"\"\"\n        import pre_loss_module and call the run method\n        \"\"\"\n        exposure_data = get_exposure_data(self, add_internal_col=True)\n        kwargs = dict()\n\n        # If given a value for 'oasis_files_dir' then use that directly\n        if self.oasis_files_dir:\n            input_dir = self.oasis_files_dir\n        else:\n            input_dir = self.get_default_run_dir()\n            pathlib.Path(input_dir).mkdir(parents=True, exist_ok=True)\n\n        kwargs['exposure_data'] = exposure_data\n        kwargs['input_dir'] = input_dir\n        kwargs['model_data_dir'] = self.model_data_dir\n        kwargs['analysis_settings_json'] = self.analysis_settings_json\n        kwargs['settings'] = self.settings\n        kwargs['user_data_dir'] = self.user_data_dir\n        kwargs['logger'] = self.logger\n\n\"\"\"Docstring (excerpt)\"\"\"\nComputation step that will be call just before loss compuation.\nOn the platform it will be called on each machine performing the loss calculation,\nAdd the ability to specify a model specific step that will modify or expand on the loss calculation input file on each worker."
    },
    {
      "chunk_id": "oasislmf/computation/hooks/pre_loss.py::run@74",
      "source_type": "code",
      "path": "oasislmf/computation/hooks/pre_loss.py",
      "symbol_type": "function",
      "name": "run",
      "lineno": 74,
      "end_lineno": 114,
      "business_stage": "gul",
      "docstring": "import pre_loss_module and call the run method",
      "content": "# File: oasislmf/computation/hooks/pre_loss.py\n# function: run (lines 74-114)\n\n    def run(self):\n        \"\"\"\n        import pre_loss_module and call the run method\n        \"\"\"\n        exposure_data = get_exposure_data(self, add_internal_col=True)\n        kwargs = dict()\n\n        # If given a value for 'oasis_files_dir' then use that directly\n        if self.oasis_files_dir:\n            input_dir = self.oasis_files_dir\n        else:\n            input_dir = self.get_default_run_dir()\n            pathlib.Path(input_dir).mkdir(parents=True, exist_ok=True)\n\n        kwargs['exposure_data'] = exposure_data\n        kwargs['input_dir'] = input_dir\n        kwargs['model_data_dir'] = self.model_data_dir\n        kwargs['analysis_settings_json'] = self.analysis_settings_json\n        kwargs['settings'] = self.settings\n        kwargs['user_data_dir'] = self.user_data_dir\n        kwargs['logger'] = self.logger\n\n        if self.pre_loss_setting_json:\n            with open(self.pre_loss_setting_json) as pre_loss_setting_file:\n                kwargs['pre_loss_setting'] = json.load(pre_loss_setting_file)\n        else:\n            kwargs['pre_loss_setting'] = {}\n\n        _module = get_custom_module(self.pre_loss_module, 'pre loss calculation module path')\n\n        try:\n            _class = getattr(_module, self.pre_loss_class_name)\n        except AttributeError as e:\n            raise OasisException(f\"class {self.pre_loss_class_name} \"\n                                 f\"is not defined in module {self.pre_loss_module}\") from e.__cause__\n\n        _class_return = _class(**kwargs).run()\n\n        return {\n            \"class\": _class_return\n        }\n\n\"\"\"Docstring (excerpt)\"\"\"\nimport pre_loss_module and call the run method"
    },
    {
      "chunk_id": "oasislmf/computation/run/exposure.py::RunExposure@30",
      "source_type": "code",
      "path": "oasislmf/computation/run/exposure.py",
      "symbol_type": "class",
      "name": "RunExposure",
      "lineno": 30,
      "end_lineno": 289,
      "business_stage": "exposure",
      "docstring": "Generates insured losses from preexisting Oasis files with specified\nloss factors (loss % of TIV).",
      "content": "# File: oasislmf/computation/run/exposure.py\n# class: RunExposure (lines 30-289)\n\nclass RunExposure(ComputationStep):\n    \"\"\"\n    Generates insured losses from preexisting Oasis files with specified\n    loss factors (loss % of TIV).\n    \"\"\"\n    step_params = [\n        {'name': 'src_dir', 'flag': '-s', 'is_path': True, 'pre_exist': True, 'help': ''},\n        {'name': 'run_dir', 'flag': '-r', 'is_path': True, 'pre_exist': False, 'help': ''},\n        {'name': 'check_oed', 'type': str2bool, 'const': True, 'nargs': '?', 'default': True, 'help': 'if True check input oed files'},\n        {'name': 'output_file', 'flag': '-f', 'is_path': True, 'pre_exist': False, 'help': '', 'type': str},\n        {'name': 'loss_factor', 'flag': '-l', 'type': float, 'nargs': '+', 'help': '', 'default': [1.0]},\n        {'name': 'oed_schema_info', 'help': 'Takes a version of OED schema to use in the form \"v1.2.3\" or a path to an OED schema json'},\n        {'name': 'currency_conversion_json', 'is_path': True, 'pre_exist': True, 'help': 'settings to perform currency conversion of oed files'},\n        {'name': 'reporting_currency', 'help': 'currency to use in the results reported'},\n        {'name': 'ktools_alloc_rule_il', 'flag': '-a', 'default': KTOOLS_ALLOC_IL_DEFAULT, 'type': int,\n         'help': 'Set the fmcalc allocation rule used in direct insured loss'},\n        {'name': 'ktools_alloc_rule_ri', 'flag': '-A', 'default': KTOOLS_ALLOC_RI_DEFAULT, 'type': int,\n         'help': 'Set the fmcalc allocation rule used in reinsurance'},\n        {'name': 'output_level', 'flag': '-o', 'help': 'Keys files output format', 'choices': ['item', 'loc', 'pol', 'acc', 'port'],\n         'default': 'item'},\n        {'name': 'extra_summary_cols', 'nargs': '+', 'help': 'extra column to include in the summary', 'default': []},\n        {'name': 'fmpy', 'default': True, 'type': str2bool, 'const': True, 'nargs': '?', 'help': 'use fmcalc python version instead of c++ version'},\n        {'name': 'fmpy_low_memory', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n         'help': 'use memory map instead of RAM to store loss array (may decrease performance but reduce RAM usage drastically)'},\n        {'name': 'fmpy_sort_output', 'default': True, 'type': str2bool, 'const': True, 'nargs': '?', 'help': 'order fmpy output by item_id'},\n        {'name': 'stream_type', 'flag': '-t', 'default': 2, 'type': int,\n         'help': 'Set the IL input stream type, 2 = default loss stream, 1 = deprecated cov/item stream'},\n        {'name': 'net_ri', 'default': True},\n        {'name': 'include_loss_factor', 'default': True},\n        {'name': 'print_summary', 'default': True},\n        {'name': 'do_disaggregation', 'type': str2bool, 'const': True, 'nargs': '?', 'default': True, 'help': 'if True run the oasis disaggregation.'},\n        {'name': 'oed_backend_dtype', 'type': str, 'default': 'pd_dtype',\n         'help': \"define what type dtype the oed column will be (pd_dtype or pa_dtype)\"},\n    ]\n\n    chained_commands = [GenerateKeysDeterministic]\n\n    def _check_alloc_rules(self):\n        alloc_ranges = {\n            'ktools_alloc_rule_il': KTOOLS_ALLOC_FM_MAX,\n            'ktools_alloc_rule_ri': KTOOLS_ALLOC_FM_MAX}\n        for rule in alloc_ranges:\n            alloc_val = getattr(self, rule)\n            if (alloc_val < 0) or (alloc_val > alloc_ranges[rule]):\n                raise OasisException(f'Error: {rule}={alloc_val} - Not withing valid range [0..{alloc_ranges[rule]}]')\n\n    def run(self):\n        tmp_dir = None\n        src_dir = self.src_dir if self.src_dir else os.getcwd()\n\n        if self.run_dir:\n            run_dir = self.run_dir\n        else:\n            tmp_dir = tempfile.TemporaryDirectory()\n            run_dir = tmp_dir.name\n\n        include_loss_factor = not (len(self.loss_factor) == 1)\n\n        self._check_alloc_rules()\n\n        self.oasis_files_dir = src_dir\n        exposure_data = get_exposure_data(self, add_internal_col=True)\n\n        il = bool(exposure_data.account)\n        ril = all([exposure_data.ri_info, exposure_data.ri_scope, il])\n\n        self.logger.debug('\\nRunning deterministic losses (GUL=True, IL={}, RIL={})\\n'.format(il, ril))\n\n        if not os.path.exists(run_dir):\n            os.makedirs(run_dir)\n\n        # 1. Create Deterministic keys file\n        keys_fp = os.path.join(run_dir, 'keys.csv')\n        GenerateKeysDeterministic(**{**self.kwargs, **{\"keys_data_csv\": keys_fp, \"exposure_data\": exposure_data}}).run()\n\n        # 2. Start Oasis files generation\n        GenerateFiles(\n            oasis_files_dir=run_dir,\n            exposure_data=exposure_data,\n            keys_data_csv=keys_fp,\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerates insured losses from preexisting Oasis files with specified\nloss factors (loss % of TIV)."
    },
    {
      "chunk_id": "oasislmf/computation/run/exposure.py::RunFmTest@292",
      "source_type": "code",
      "path": "oasislmf/computation/run/exposure.py",
      "symbol_type": "class",
      "name": "RunFmTest",
      "lineno": 292,
      "end_lineno": 476,
      "business_stage": "exposure",
      "docstring": "Runs an FM test case and validates generated\nlosses against expected losses.\n\nonly use 'update_expected' for debugging\nit replaces the expected file with generated",
      "content": "# File: oasislmf/computation/run/exposure.py\n# class: RunFmTest (lines 292-476)\n\nclass RunFmTest(ComputationStep):\n    \"\"\"\n    Runs an FM test case and validates generated\n    losses against expected losses.\n\n    only use 'update_expected' for debugging\n    it replaces the expected file with generated\n    \"\"\"\n\n    step_params = [\n        {'name': 'test_case_name', 'flag': '-c', 'type': str,\n         'help': 'Runs a specific test sub-directory from \"test_case_dir\". If not set then run all tests found.'},\n        {'name': 'list_tests', 'flag': '-l', 'action': 'store_true', 'help': 'List the valid test cases in the test directory rather than running'},\n        {'name': 'test_case_dir', 'flag': '-t', 'default': os.getcwd(), 'is_path': True, 'pre_exist': True,\n         'help': 'Test directory - should contain test directories containing OED files and expected results'},\n        {'name': 'run_dir', 'flag': '-r', 'help': 'Run directory - where files should be generated. If not set temporary files will not be saved.'},\n        {'name': 'test_tolerance', 'type': float, 'help': 'Relative tolerance between expected values and results, default is \"1e-4\" or 0.0001',\n         'default': 1e-4},\n        {'name': 'model_perils_covered', 'nargs': '+', 'default': ['AA1'],\n         'help': 'List of peril covered by the model'},\n        {'name': 'fmpy', 'default': True, 'type': str2bool, 'const': True, 'nargs': '?', 'help': 'use fmcalc python version instead of c++ version'},\n        {'name': 'fmpy_low_memory', 'default': False, 'type': str2bool, 'const': True, 'nargs': '?',\n         'help': 'use memory map instead of RAM to store loss array (may decrease performance but reduce RAM usage drastically)'},\n        {'name': 'fmpy_sort_output', 'default': True, 'type': str2bool, 'const': True, 'nargs': '?', 'help': 'order fmpy output by item_id'},\n        {'name': 'update_expected', 'default': False},\n        {'name': 'expected_output_dir', 'default': \"expected\"},\n    ]\n\n    def search_test_cases(self):\n        case_names = []\n        for test_case in os.listdir(path=self.test_case_dir):\n            if os.path.exists(\n                    os.path.join(self.test_case_dir, test_case, self.expected_output_dir)\n            ):\n                case_names.append(test_case)\n        case_names.sort()\n        return case_names, len(case_names)\n\n    def _case_dir_is_valid_test(self):\n        src_contents = [fn.lower() for fn in os.listdir(self.test_case_dir)]\n        return 'location.csv' and 'account.csv' and 'expected' in src_contents\n\n    def run(self):\n        # Run test case given on CLI\n        if self.test_case_name:\n            return self.execute_test_case(self.test_case_name)\n\n        # If 'test_case_dir' is a valid test run that dir directly\n        if self._case_dir_is_valid_test():\n            return self.execute_test_case('')\n\n        # Search for valid cases in sub-dirs and run all found\n        case_names, case_num = self.search_test_cases()\n\n        # If '--list-tests' is selected print found cases and exit\n        if self.list_tests:\n            for name in case_names:\n                self.logger.info(name)\n            exit(0)\n\n        if case_num < 1:\n            raise OasisException(f'No vaild FM test cases found in \"{self.test_case_dir}\"')\n        else:\n            # If test_case not selected run all cases\n            self.logger.info(f\"Running: {case_num} Tests from '{self.test_case_dir}'\")\n            self.logger.info(f'Test names: {case_names}')\n            failed_tests = []\n            exit_status = 0\n            for case in case_names:\n                test_result = self.execute_test_case(case)\n\n                if not test_result:\n                    failed_tests.append(case)\n                    exit_status = 1\n\n            if len(failed_tests) == 0:\n                self.logger.info(\"All tests passed\")\n            else:\n                self.logger.info(\"{} test failed: \".format(len(failed_tests)))\n                [self.logger.info(n) for n in failed_tests]\n\n\"\"\"Docstring (excerpt)\"\"\"\nRuns an FM test case and validates generated\nlosses against expected losses.\n\nonly use 'update_expected' for debugging\nit replaces the expected file with generated"
    },
    {
      "chunk_id": "oasislmf/computation/run/generate_documentation.py::GenerateDocumentation@12",
      "source_type": "code",
      "path": "oasislmf/computation/run/generate_documentation.py",
      "symbol_type": "class",
      "name": "GenerateDocumentation",
      "lineno": 12,
      "end_lineno": 38,
      "business_stage": "other",
      "docstring": "Generate Documentation for model from the config file",
      "content": "# File: oasislmf/computation/run/generate_documentation.py\n# class: GenerateDocumentation (lines 12-38)\n\nclass GenerateDocumentation(ComputationStep):\n    \"\"\"\n    Generate Documentation for model from the config file\n    \"\"\"\n\n    # Override params\n    step_params = []\n    # Add params from each sub command not in 'step_params'\n    chained_commands = [\n        GenerateModelDocumentation,\n    ]\n\n    def run(self):\n        #  setup documentation output dir\n        if not self.doc_out_dir:\n            self.doc_out_dir = GenerateModelDocumentation._get_output_dir(self)\n\n        # create documentation output dir\n        if not os.path.exists(self.doc_out_dir):\n            os.makedirs(self.doc_out_dir)\n\n        # generate Model Documentation\n        self.kwargs['doc_out_dir'] = self.doc_out_dir\n        GenerateModelDocumentation(**self.kwargs).run()\n\n        # logger info\n        self.logger.info('\\nGenerate Documentation completed successfully in {}'.format(self.doc_out_dir))\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Documentation for model from the config file"
    },
    {
      "chunk_id": "oasislmf/computation/run/generate_files.py::GenerateOasisFiles@17",
      "source_type": "code",
      "path": "oasislmf/computation/run/generate_files.py",
      "symbol_type": "class",
      "name": "GenerateOasisFiles",
      "lineno": 17,
      "end_lineno": 80,
      "business_stage": "other",
      "docstring": "Run Oasis file geneartion with optional PreAnalysis hook.",
      "content": "# File: oasislmf/computation/run/generate_files.py\n# class: GenerateOasisFiles (lines 17-80)\n\nclass GenerateOasisFiles(ComputationStep):\n    \"\"\"\n    Run Oasis file geneartion with optional PreAnalysis hook.\n    \"\"\"\n\n    # Override params\n    step_params = [\n        {'name': 'exposure_pre_analysis_module', 'required': False, 'is_path': True,\n            'pre_exist': True, 'help': 'Exposure Pre-Analysis lookup module path'},\n        {'name': 'post_file_gen_module', 'required': False, 'is_path': True,\n         'pre_exist': True, 'help': 'post-file gen hook module path'},\n    ]\n    # Add params from each sub command not in 'step_params'\n    chained_commands = [\n        PostFileGen,\n        GenerateFiles,\n        ExposurePreAnalysis,\n    ]\n\n    def get_exposure_data_config(self):\n        return {\n            'location': self.oed_location_csv,\n            'account': self.oed_accounts_csv,\n            'ri_info': self.oed_info_csv,\n            'ri_scope': self.oed_scope_csv,\n            'oed_schema_info': self.oed_schema_info,\n            'currency_conversion': self.currency_conversion_json,\n            'check_oed': self.check_oed,\n            'use_field': True,\n            'location_numbers': self.location,\n            'portfolio_numbers': self.portfolio,\n            'account_numbers': self.account,\n            'base_df_engine': self.base_df_engine,\n            'exposure_df_engine': self.exposure_df_engine or self.base_df_engine,\n            'backend_dtype': self.oed_backend_dtype,\n        }\n\n    def run(self):\n        # setup input dir\n        if not self.oasis_files_dir:\n            self.oasis_files_dir = GenerateFiles._get_output_dir(self)\n\n        # create input dir\n        if not os.path.exists(self.oasis_files_dir):\n            os.makedirs(self.oasis_files_dir)\n\n        self.kwargs['oasis_files_dir'] = self.oasis_files_dir\n        self.kwargs['exposure_data'] = get_exposure_data(self, add_internal_col=True)\n\n        # Run chain\n        if self.exposure_pre_analysis_module:\n            cmds = [(ExposurePreAnalysis, self.kwargs), (GenerateFiles, self.kwargs)]\n        else:\n            cmds = [(GenerateFiles, self.kwargs)]\n\n        if self.post_file_gen_module:\n            cmds += [(PostFileGen, self.kwargs)]\n\n        with tqdm(total=len(cmds)) as pbar:\n            for cmd in cmds:\n                cmd[0](**cmd[1]).run()\n                pbar.update(1)\n\n        self.logger.info('\\nGenerate Files completed successfully in {}'.format(self.oasis_files_dir))\n\n\"\"\"Docstring (excerpt)\"\"\"\nRun Oasis file geneartion with optional PreAnalysis hook."
    },
    {
      "chunk_id": "oasislmf/computation/run/generate_losses.py::GenerateOasisLosses@14",
      "source_type": "code",
      "path": "oasislmf/computation/run/generate_losses.py",
      "symbol_type": "class",
      "name": "GenerateOasisLosses",
      "lineno": 14,
      "end_lineno": 54,
      "business_stage": "gul",
      "docstring": "Run Oasis file geneartion with optional PreAnalysis hook.",
      "content": "# File: oasislmf/computation/run/generate_losses.py\n# class: GenerateOasisLosses (lines 14-54)\n\nclass GenerateOasisLosses(ComputationStep):\n    \"\"\"\n    Run Oasis file geneartion with optional PreAnalysis hook.\n    \"\"\"\n\n    # Override params\n    step_params = [\n        {'name': 'pre_loss_module', 'required': False, 'is_path': True, 'pre_exist': True,\n         'help': 'Pre-Loss module path'},\n        {'name': 'post_analysis_module', 'required': False, 'is_path': True, 'pre_exist': True,\n         'help': 'Post-Analysis module path'},\n    ]\n    # Add params from each sub command not in 'step_params'\n    chained_commands = [\n        PreLoss,\n        GenerateLosses,\n        PostAnalysis,\n    ]\n\n    def run(self):\n\n        # setup output dir\n        if not self.model_run_dir:\n            self.model_run_dir = GenerateLosses._get_output_dir(self)\n        self.kwargs['model_run_dir'] = self.model_run_dir\n\n        # Run chain\n        if self.pre_loss_module:\n            cmds = [(PreLoss, self.kwargs)]\n        else:\n            cmds = []\n        cmds += [(GenerateLosses, self.kwargs)]\n        if self.post_analysis_module:\n            cmds += [(PostAnalysis, self.kwargs)]\n\n        with tqdm(total=len(cmds)) as pbar:\n            for cmd in cmds:\n                cmd[0](**cmd[1]).run()\n                pbar.update(1)\n\n        self.logger.info(f'Losses generated in {self.model_run_dir}')\n\n\"\"\"Docstring (excerpt)\"\"\"\nRun Oasis file geneartion with optional PreAnalysis hook."
    },
    {
      "chunk_id": "oasislmf/computation/run/model.py::RunModel@23",
      "source_type": "code",
      "path": "oasislmf/computation/run/model.py",
      "symbol_type": "class",
      "name": "RunModel",
      "lineno": 23,
      "end_lineno": 96,
      "business_stage": "other",
      "docstring": "Run models end to end.",
      "content": "# File: oasislmf/computation/run/model.py\n# class: RunModel (lines 23-96)\n\nclass RunModel(ComputationStep):\n    \"\"\"\n    Run models end to end.\n    \"\"\"\n\n    # Override params\n    step_params = [\n        {'name': 'oasis_files_dir', 'flag': '-o', 'is_path': True, 'pre_exist': False,\n            'help': 'Path to the directory in which to generate the Oasis files'},\n        {'name': 'exposure_pre_analysis_module', 'required': False, 'is_path': True,\n            'pre_exist': True, 'help': 'Exposure Pre-Analysis lookup module path'},\n        {'name': 'post_analysis_module', 'required': False, 'is_path': True, 'pre_exist': True,\n         'help': 'Post-Analysis module path'},\n        {'name': 'pre_loss_module', 'required': False, 'is_path': True,\n         'pre_exist': True, 'help': 'pre-loss hook module path'},\n        {'name': 'post_file_gen_module', 'required': False, 'is_path': True,\n         'pre_exist': True, 'help': 'post-file gen hook module path'}\n    ]\n    # Add params from each sub command not in 'step_params'\n    chained_commands = [\n        GenerateLosses,\n        PostFileGen,\n        PreLoss,\n        GenerateFiles,\n        ExposurePreAnalysis,\n        PostAnalysis,\n    ]\n\n    def get_exposure_data_config(self):\n        return {\n            'location': self.oed_location_csv,\n            'account': self.oed_accounts_csv,\n            'ri_info': self.oed_info_csv,\n            'ri_scope': self.oed_scope_csv,\n            'oed_schema_info': self.oed_schema_info,\n            'currency_conversion': self.currency_conversion_json,\n            'check_oed': self.check_oed,\n            'use_field': True,\n            'backend_dtype': self.oed_backend_dtype,\n        }\n\n    def run(self):\n        # setup output dir\n        if not self.model_run_dir:\n            self.model_run_dir = GenerateLosses._get_output_dir(self)\n        if os.path.exists(self.model_run_dir):\n            empty_dir(self.model_run_dir)\n        os.makedirs(os.path.join(self.model_run_dir, 'input'))\n\n        self.kwargs['model_run_dir'] = self.model_run_dir\n        # TODO: input oasis_files_dir is actually not use in the code\n        self.kwargs['oasis_files_dir'] = os.path.join(self.model_run_dir, 'input')\n        self.oasis_files_dir = self.kwargs['oasis_files_dir']\n\n        self.kwargs['exposure_data'] = get_exposure_data(self, add_internal_col=True)\n\n        # Run chain\n        cmds = []\n        if self.exposure_pre_analysis_module:\n            cmds += [(ExposurePreAnalysis, self.kwargs)]\n        cmds += [(GenerateFiles, self.kwargs)]\n        if self.post_file_gen_module:\n            cmds += [(PostFileGen, self.kwargs)]\n        if self.pre_loss_module:\n            cmds += [(PreLoss, self.kwargs)]\n        cmds += [(GenerateLosses, self.kwargs)]\n        if self.post_analysis_module:\n            cmds += [(PostAnalysis, self.kwargs)]\n        with tqdm(total=len(cmds)) as pbar:\n            for cmd in cmds:\n                cmd[0](**cmd[1]).run()\n                pbar.update(1)\n\n        self.logger.info('\\nModel run completed successfully in {}'.format(self.model_run_dir))\n\n\"\"\"Docstring (excerpt)\"\"\"\nRun models end to end."
    },
    {
      "chunk_id": "oasislmf/computation/run/platform.py::PlatformBase@27",
      "source_type": "code",
      "path": "oasislmf/computation/run/platform.py",
      "symbol_type": "class",
      "name": "PlatformBase",
      "lineno": 27,
      "end_lineno": 198,
      "business_stage": "other",
      "docstring": "Base platform class to handle opening a client connection",
      "content": "# File: oasislmf/computation/run/platform.py\n# class: PlatformBase (lines 27-198)\n\nclass PlatformBase(ComputationStep):\n    \"\"\"\n    Base platform class to handle opening a client connection\n    \"\"\"\n    step_params = [\n        {'name': 'server_login_json', 'required': False, 'default': None, 'is_path': True,\n            'pre_exist': False, 'help': 'Server login credentials json string'},\n        {'name': 'server_url', 'default': 'http://localhost:8000', 'help': 'URL to Oasis Platform server, default is localhost'},\n        {'name': 'server_version', 'default': 'v2', 'help': \"Version prefix for OasisPlatform server, 'v1' = single server run, 'v2' = distributed on cluster\"},\n    ]\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.server = self.open_connection()\n\n    def load_credentials(self, login_arg):\n        \"\"\"\n        Load credentials from JSON file or Prompt for username / password\n\n        Options:\n            1.'--server-login ./APIcredentials.json'\n            2. Load credentials from default config file '-C oasislmf.json'\n        \"\"\"\n        if isinstance(login_arg, str):\n            with io.open(login_arg, encoding='utf-8') as f:\n                return json.load(f)\n\n        while True:\n            user_response = input(\"Use simple JWT [Y/n]: \").lower().strip()\n            if user_response in [\"y\", \"yes\"]:\n                use_simple = True\n                break\n            elif user_response in [\"n\", \"no\"]:\n                use_simple = False\n                break\n\n        self.logger.info('API Login:')\n        api_login = {}\n        if use_simple:\n            api_login['username'] = input('Username: ')\n            api_login['password'] = getpass.getpass('Password: ')\n        else:\n            api_login['client_id'] = input('Client ID: ')\n            api_login['client_secret'] = getpass.getpass('Client Secret: ')\n\n        return api_login\n\n    def try_connection(self, fail_safe=True, **kwargs):\n        \"\"\"Helper to safely try connecting and return None if unauthorized.\"\"\"\n        try:\n            return APIClient(\n                api_url=self.server_url,\n                api_ver=self.server_version,\n                **kwargs\n            )\n        except OasisException as e:\n            if not fail_safe:\n                raise e\n            orig_excep = e.original_exception\n            if (isinstance(orig_excep, HTTPError) and orig_excep.response.status_code == 401):\n                self.logger.debug(\"Login attempt failed, reason: Unauthorized (401) for credentials: %s\", list(kwargs.keys()))\n                return None\n            elif isinstance(orig_excep, HTTPError) and orig_excep.response.status_code == 400 and \"flow is disabled\" in orig_excep.response.text.lower():\n                self.logger.debug(\n                    \"Login attempt failed, reason: Validation Error (400) for credentials, invalid flow used, must be one of username/password or client_id/client_secret: %s\", list(\n                        kwargs.keys())\n                )\n                return None\n            else:\n                raise e  # Some other error  propagate\n\n    def open_connection(self):\n        \"\"\"\n        Attempts connection in this order:\n        1. API_EXAMPLE_AUTH username/password\n        2. API_EXAMPLE_AUTH client_id/client_secret\n        3. Prompt or load credentials\n        \"\"\"\n        if not isinstance(self.server_login_json, str):\n            # 1. Try example username/password\n\n\"\"\"Docstring (excerpt)\"\"\"\nBase platform class to handle opening a client connection"
    },
    {
      "chunk_id": "oasislmf/computation/run/platform.py::PlatformList@201",
      "source_type": "code",
      "path": "oasislmf/computation/run/platform.py",
      "symbol_type": "class",
      "name": "PlatformList",
      "lineno": 201,
      "end_lineno": 242,
      "business_stage": "other",
      "docstring": "Return status and details from an Oasis Platform API server",
      "content": "# File: oasislmf/computation/run/platform.py\n# class: PlatformList (lines 201-242)\n\nclass PlatformList(PlatformBase):\n    \"\"\" Return status and details from an Oasis Platform API server\n    \"\"\"\n    step_params = PlatformBase.step_params + [\n        {'name': 'models', 'flag': '-m', 'type': int, 'nargs': '+', 'help': 'List of model ids to print in detail'},\n        {'name': 'portfolios', 'flag': '-p', 'type': int, 'nargs': '+', 'help': 'List of portfolio ids to print in detail'},\n        {'name': 'analyses', 'flag': '-a', 'type': int, 'nargs': '+', 'help': 'List of analyses ids to print in detail'},\n    ]\n\n    def run(self):\n        # Default to printing summary of API status\n        if not any([self.models, self.portfolios, self.analyses]):\n            self.print_endpoint('models', ['id', 'supplier_id', 'model_id', 'version_id'])\n            self.print_endpoint('portfolios', ['id', 'name', 'location_file', 'accounts_file', 'reinsurance_info_file', 'reinsurance_scope_file'])\n            self.print_endpoint('analyses', ['id', 'name', 'model', 'portfolio', 'status', 'input_file', 'output_file', 'run_log_file'])\n\n        if self.models:\n            for Id in self.models:\n                msg = f'Model (id={Id}): \\n'\n                try:\n                    rsp = self.server.models.get(Id)\n                    self.logger.info(msg + json.dumps(rsp.json(), indent=4, sort_keys=True))\n                except HTTPError as e:\n                    self.logger.info(msg + e.response.text)\n\n        if self.portfolios:\n            for Id in self.portfolios:\n                msg = f'Portfolio (id={Id}): \\n'\n                try:\n                    rsp = self.server.portfolios.get(Id)\n                    self.logger.info(msg + json.dumps(rsp.json(), indent=4, sort_keys=True))\n                except HTTPError as e:\n                    self.logger.info(msg + e.response.text)\n\n        if self.analyses:\n            for Id in self.analyses:\n                msg = f'Analysis (id={Id}): \\n'\n                try:\n                    rsp = self.server.analyses.get(Id)\n                    self.logger.info(msg + json.dumps(rsp.json(), indent=4, sort_keys=True))\n                except HTTPError as e:\n                    self.logger.info(msg + e.response.text)\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturn status and details from an Oasis Platform API server"
    },
    {
      "chunk_id": "oasislmf/computation/run/platform.py::PlatformRunInputs@245",
      "source_type": "code",
      "path": "oasislmf/computation/run/platform.py",
      "symbol_type": "class",
      "name": "PlatformRunInputs",
      "lineno": 245,
      "end_lineno": 316,
      "business_stage": "other",
      "docstring": "run generate inputs via the Oasis Platoform API",
      "content": "# File: oasislmf/computation/run/platform.py\n# class: PlatformRunInputs (lines 245-316)\n\nclass PlatformRunInputs(PlatformBase):\n    \"\"\" run generate inputs via the Oasis Platoform API\n    \"\"\"\n    step_params = PlatformBase.step_params + [\n        {'name': 'model_id', 'type': int, 'help': 'API `id` of a model to run an analysis with'},\n        {'name': 'portfolio_id', 'type': int, 'help': 'API `id` of a portfolio to run an analysis with'},\n        {'name': 'analysis_id', 'type': int, 'help': 'API `id` of an analysis to run'},\n\n        {'name': 'analysis_settings_json', 'flag': '-a', 'is_path': True, 'pre_exist': True, 'help': 'Analysis settings JSON file path'},\n        {'name': 'oed_location_csv', 'flag': '-x', 'is_path': True, 'pre_exist': True, 'help': 'Source location CSV file path'},\n        {'name': 'oed_accounts_csv', 'flag': '-y', 'is_path': True, 'pre_exist': True, 'help': 'Source accounts CSV file path'},\n        {'name': 'oed_info_csv', 'flag': '-i', 'is_path': True, 'pre_exist': True, 'help': 'Reinsurance info. CSV file path'},\n        {'name': 'oed_scope_csv', 'flag': '-s', 'is_path': True, 'pre_exist': True, 'help': 'Reinsurance scope CSV file path'},\n    ]\n\n    def run(self):\n        # Run Input geneneration from ID\n        if self.analysis_id:\n            try:\n                status = self.server.analyses.status(self.analysis_id)\n                if status in ['RUN_QUEUED', 'RUN_STARTED']:\n                    self.server.cancel_analysis(self.analysis_id)\n                elif status in ['INPUTS_GENERATION_QUEUED', 'INPUTS_GENERATION_STARTED']:\n                    self.server.cancel_generate(self.analysis_id)\n                self.server.run_generate(self.analysis_id)\n                return self.analysis_id\n            except HTTPError as e:\n                raise OasisException(f'Error running analysis ({self.analysis_id}) - {e}')\n\n        # Create Portfolio and Ananlysis, then run\n        if not (self.portfolio_id or self.oed_location_csv or self.oed_accounts_csv):\n            raise OasisException('Error: At least one of the following inputs is required [portfolio_id, oed_location_csv, oed_accounts_csv]')\n\n        # when no model is selected prompt user for choice\n        if not self.model_id:\n            models = self.server.models.get().json()\n            model_count = len(models)\n\n            if model_count < 1:\n                raise OasisException(f'No models found in API: {self.server_url}')\n            if model_count == 1:\n                self.model_id = models[0]['id']\n            if model_count > 1:\n                models_table = self.print_endpoint('models', ['id', 'supplier_id', 'model_id', 'version_id'])\n                self.model_id = self.select_id('models', models_table['id'])\n            if self.model_id < 0:\n                raise OasisException(' Model selection cancelled')\n\n        # Select or create a portfilo\n        if self.portfolio_id:\n            portfolios = self.server.portfolios.get().json()\n            if self.portfolio_id not in [p['id'] for p in portfolios]:\n                raise OasisException(f'Portfolio \"{self.portfolio_id}\" not found in API: {self.server_url}')\n        else:\n            portfolio = self.server.upload_inputs(\n                portfolio_id=None,\n                location_fp=self.oed_location_csv,\n                accounts_fp=self.oed_accounts_csv,\n                ri_info_fp=self.oed_info_csv,\n                ri_scope_fp=self.oed_scope_csv,\n            )\n            self.portfolio_id = portfolio['id']\n        analysis = self.server.create_analysis(\n            portfolio_id=self.portfolio_id,\n            model_id=self.model_id,\n            analysis_settings_fp=self.analysis_settings_json,\n        )\n        self.analysis_id = analysis['id']\n\n        # Execure run\n        self.server.run_generate(self.analysis_id)\n        return self.analysis_id\n\n\"\"\"Docstring (excerpt)\"\"\"\nrun generate inputs via the Oasis Platoform API"
    },
    {
      "chunk_id": "oasislmf/computation/run/platform.py::PlatformRunLosses@319",
      "source_type": "code",
      "path": "oasislmf/computation/run/platform.py",
      "symbol_type": "class",
      "name": "PlatformRunLosses",
      "lineno": 319,
      "end_lineno": 331,
      "business_stage": "other",
      "docstring": "run generate losses via the Oasis Platoform API",
      "content": "# File: oasislmf/computation/run/platform.py\n# class: PlatformRunLosses (lines 319-331)\n\nclass PlatformRunLosses(PlatformBase):\n    \"\"\" run generate losses via the Oasis Platoform API\n    \"\"\"\n    step_params = PlatformBase.step_params + [\n        {'name': 'analysis_id', 'type': int, 'required': True, 'help': 'API `id` of an analysis to run'},\n        {'name': 'output_dir', 'flag': '-o', 'is_path': True, 'pre_exist': True,\n            'help': 'Output data directory for results data (absolute or relative file path)', 'default': './'},\n        {'name': 'analysis_settings_json', 'flag': '-a', 'is_path': True, 'pre_exist': True, 'help': 'Analysis settings JSON file path'},\n    ]\n\n    def run(self):\n        self.server.run_analysis(self.analysis_id, self.analysis_settings_json)\n        self.server.download_output(self.analysis_id, self.output_dir)\n\n\"\"\"Docstring (excerpt)\"\"\"\nrun generate losses via the Oasis Platoform API"
    },
    {
      "chunk_id": "oasislmf/computation/run/platform.py::PlatformRun@334",
      "source_type": "code",
      "path": "oasislmf/computation/run/platform.py",
      "symbol_type": "class",
      "name": "PlatformRun",
      "lineno": 334,
      "end_lineno": 341,
      "business_stage": "other",
      "docstring": "End to End - run model via the Oasis Platoform API",
      "content": "# File: oasislmf/computation/run/platform.py\n# class: PlatformRun (lines 334-341)\n\nclass PlatformRun(PlatformBase):\n    \"\"\" End to End - run model via the Oasis Platoform API\n    \"\"\"\n    chained_commands = [PlatformRunInputs, PlatformRunLosses]\n\n    def run(self):\n        self.kwargs['analysis_id'] = PlatformRunInputs(**self.kwargs).run()\n        PlatformRunLosses(**self.kwargs).run()\n\n\"\"\"Docstring (excerpt)\"\"\"\nEnd to End - run model via the Oasis Platoform API"
    },
    {
      "chunk_id": "oasislmf/computation/run/platform.py::PlatformDelete@344",
      "source_type": "code",
      "path": "oasislmf/computation/run/platform.py",
      "symbol_type": "class",
      "name": "PlatformDelete",
      "lineno": 344,
      "end_lineno": 380,
      "business_stage": "other",
      "docstring": "Delete either a 'model', 'portfolio' or an 'analysis' from the API's Database",
      "content": "# File: oasislmf/computation/run/platform.py\n# class: PlatformDelete (lines 344-380)\n\nclass PlatformDelete(PlatformBase):\n    \"\"\" Delete either a 'model', 'portfolio' or an 'analysis' from the API's Database\n    \"\"\"\n    step_params = PlatformBase.step_params + [\n        {'name': 'models', 'flag': '-m', 'type': int, 'nargs': '+', 'help': 'List of model ids to Delete.'},\n        {'name': 'portfolios', 'flag': '-p', 'type': int, 'nargs': '+', 'help': 'List of Portfolio ids to Delete'},\n        {'name': 'analyses', 'flag': '-a', 'type': int, 'nargs': '+', 'help': 'List of Analyses ids to Detele'},\n    ]\n\n    def delete_list(self, attr, id_list):\n        if not all(isinstance(ID, int) for ID in id_list):\n            raise OasisException(f\"Invalid input, '{attr}', must be a list of type Int, not {id_list}\")\n\n        api_endpoint = getattr(self.server, attr)\n        for Id in id_list:\n            try:\n                if api_endpoint.delete(Id).ok:\n                    self.logger.info(f'Deleted {attr}_id={Id}')\n            except HTTPError as e:\n                self.logger.error('Delete error {}_id={} - {}'.format(attr, Id, e))\n                continue\n\n    def run(self):\n\n        if not any([self.models, self.portfolios, self.analyses]):\n            raise OasisException(\"\"\"Select item(s) to delete, list of either:\n                --models MODEL_ID [MODEL_ID ... n],\n                --portfolios PORTFOLIOS_ID [PORTFOLIO_ID ... n]\n                --analyses ANALYSES_ID [ANALYSES_ID ... n]\n                \"\"\")\n\n        if self.models:\n            self.delete_list('models', self.models)\n        if self.portfolios:\n            self.delete_list('portfolios', self.portfolios)\n        if self.analyses:\n            self.delete_list('analyses', self.analyses)\n\n\"\"\"Docstring (excerpt)\"\"\"\nDelete either a 'model', 'portfolio' or an 'analysis' from the API's Database"
    },
    {
      "chunk_id": "oasislmf/computation/run/platform.py::PlatformGet@383",
      "source_type": "code",
      "path": "oasislmf/computation/run/platform.py",
      "symbol_type": "class",
      "name": "PlatformGet",
      "lineno": 383,
      "end_lineno": 446,
      "business_stage": "other",
      "docstring": "Download file(s) from the api",
      "content": "# File: oasislmf/computation/run/platform.py\n# class: PlatformGet (lines 383-446)\n\nclass PlatformGet(PlatformBase):\n    \"\"\" Download file(s) from the api\n    \"\"\"\n    step_params = PlatformBase.step_params + [\n        {'name': 'output_dir', 'flag': '-o', 'is_path': True, 'pre_exist': True,\n            'help': 'Output data directory for results data (absolute or relative file path)', 'default': './'},\n        # Files for models object\n        {'name': 'model_settings', 'type': int, 'nargs': '+', 'help': 'Model ids to download settings file.'},\n        {'name': 'model_versions', 'type': int, 'nargs': '+', 'help': 'Model ids to download versions file'},\n        # Files from portfolio\n        {'name': 'portfolio_location_file', 'type': int, 'nargs': '+', 'help': 'Portfolio ids to download Location file'},\n        {'name': 'portfolio_accounts_file', 'type': int, 'nargs': '+', 'help': 'Portfolio ids to download Accounts file'},\n        {'name': 'portfolio_reinsurance_scope_file', 'type': int, 'nargs': '+', 'help': 'Portfolio ids to download RI scope file.'},\n        {'name': 'portfolio_reinsurance_info_file', 'type': int, 'nargs': '+', 'help': 'Portfolio ids to download RI info file.'},\n        # Files from an analyses\n        {'name': 'analyses_settings_file', 'type': int, 'nargs': '+', 'help': 'Analyses ids to download settings file'},\n        {'name': 'analyses_run_traceback_file', 'type': int, 'nargs': '+', 'help': 'Analyses ids to download traceback logs'},\n        {'name': 'analyses_run_log_file', 'type': int, 'nargs': '+', 'help': 'Analyses ids to download Ktools run logs'},\n        {'name': 'analyses_input_generation_traceback_file', 'type': int, 'nargs': '+',\n            'help': 'Analyses ids to download input_generation traceback logs'},\n        {'name': 'analyses_input_file', 'type': int, 'nargs': '+', 'help': 'Analyses ids to download Generated inputs tar'},\n        {'name': 'analyses_output_file', 'type': int, 'nargs': '+', 'help': 'Analyses ids to download Output losses tar'},\n        {'name': 'analyses_summary_levels_file', 'type': int, 'nargs': '+', 'help': 'Analyses ids to download summary levels file'},\n        {'name': 'analyses_lookup_validation_file', 'type': int, 'nargs': '+', 'help': 'Analyses ids to download exposure summary'},\n        {'name': 'analyses_lookup_success_file', 'type': int, 'nargs': '+', 'help': 'Analyses ids to download successful lookups'},\n        {'name': 'analyses_lookup_errors_file', 'type': int, 'nargs': '+', 'help': 'Analyses ids to download summary of failed lookups'},\n    ]\n\n    def extract_args(self, param_suffix):\n        return {k.replace(param_suffix, ''): v for k, v in self.kwargs.items() if v and param_suffix in k}\n\n    def download(self, collection, req_files, chuck_size=1024):\n        collection_obj = getattr(self.server, collection)\n        for File in req_files:\n            resource = getattr(collection_obj, File)\n            for ID in req_files[File]:\n                try:\n                    r = resource.get(ID)\n\n                    ext = guess_extension(r.headers['content-type'].partition(';')[0].strip())\n                    filename = os.path.join(self.output_dir, f'{ID}_{collection}_{File}{ext}')\n\n                    with io.open(filename, 'wb') as f:\n                        for chunk in r.iter_content(chunk_size=chuck_size):\n                            f.write(chunk)\n                    self.logger.info(f'Downloaded: {File} from {collection}_id={ID} \"{filename}\"')\n                except HTTPError as e:\n                    self.logger.error('Download failed: - {}'.format(e))\n\n    def run(self):\n        model_files = self.extract_args('model_')\n        portfolio_files = self.extract_args('portfolio_')\n        analyses_files = self.extract_args('analyses_')\n\n        # Check that at least one option is given\n        if not any([model_files, portfolio_files, analyses_files]):\n            raise OasisException('Select file for download e.g. \"--analyses_output <id_1> .. <id_n>\"')\n\n        if model_files:\n            self.download('models', model_files)\n        if portfolio_files:\n            self.download('portfolios', portfolio_files)\n        if analyses_files:\n            self.download('analyses', analyses_files)\n\n\"\"\"Docstring (excerpt)\"\"\"\nDownload file(s) from the api"
    },
    {
      "chunk_id": "oasislmf/computation/run/platform.py::load_credentials@42",
      "source_type": "code",
      "path": "oasislmf/computation/run/platform.py",
      "symbol_type": "function",
      "name": "load_credentials",
      "lineno": 42,
      "end_lineno": 72,
      "business_stage": "other",
      "docstring": "Load credentials from JSON file or Prompt for username / password\n\nOptions:\n    1.'--server-login ./APIcredentials.json'\n    2. Load credentials from default config file '-C oasislmf.json'",
      "content": "# File: oasislmf/computation/run/platform.py\n# function: load_credentials (lines 42-72)\n\n    def load_credentials(self, login_arg):\n        \"\"\"\n        Load credentials from JSON file or Prompt for username / password\n\n        Options:\n            1.'--server-login ./APIcredentials.json'\n            2. Load credentials from default config file '-C oasislmf.json'\n        \"\"\"\n        if isinstance(login_arg, str):\n            with io.open(login_arg, encoding='utf-8') as f:\n                return json.load(f)\n\n        while True:\n            user_response = input(\"Use simple JWT [Y/n]: \").lower().strip()\n            if user_response in [\"y\", \"yes\"]:\n                use_simple = True\n                break\n            elif user_response in [\"n\", \"no\"]:\n                use_simple = False\n                break\n\n        self.logger.info('API Login:')\n        api_login = {}\n        if use_simple:\n            api_login['username'] = input('Username: ')\n            api_login['password'] = getpass.getpass('Password: ')\n        else:\n            api_login['client_id'] = input('Client ID: ')\n            api_login['client_secret'] = getpass.getpass('Client Secret: ')\n\n        return api_login\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoad credentials from JSON file or Prompt for username / password\n\nOptions:\n    1.'--server-login ./APIcredentials.json'\n    2. Load credentials from default config file '-C oasislmf.json'"
    },
    {
      "chunk_id": "oasislmf/computation/run/platform.py::try_connection@74",
      "source_type": "code",
      "path": "oasislmf/computation/run/platform.py",
      "symbol_type": "function",
      "name": "try_connection",
      "lineno": 74,
      "end_lineno": 96,
      "business_stage": "other",
      "docstring": "Helper to safely try connecting and return None if unauthorized.",
      "content": "# File: oasislmf/computation/run/platform.py\n# function: try_connection (lines 74-96)\n\n    def try_connection(self, fail_safe=True, **kwargs):\n        \"\"\"Helper to safely try connecting and return None if unauthorized.\"\"\"\n        try:\n            return APIClient(\n                api_url=self.server_url,\n                api_ver=self.server_version,\n                **kwargs\n            )\n        except OasisException as e:\n            if not fail_safe:\n                raise e\n            orig_excep = e.original_exception\n            if (isinstance(orig_excep, HTTPError) and orig_excep.response.status_code == 401):\n                self.logger.debug(\"Login attempt failed, reason: Unauthorized (401) for credentials: %s\", list(kwargs.keys()))\n                return None\n            elif isinstance(orig_excep, HTTPError) and orig_excep.response.status_code == 400 and \"flow is disabled\" in orig_excep.response.text.lower():\n                self.logger.debug(\n                    \"Login attempt failed, reason: Validation Error (400) for credentials, invalid flow used, must be one of username/password or client_id/client_secret: %s\", list(\n                        kwargs.keys())\n                )\n                return None\n            else:\n                raise e  # Some other error  propagate\n\n\"\"\"Docstring (excerpt)\"\"\"\nHelper to safely try connecting and return None if unauthorized."
    },
    {
      "chunk_id": "oasislmf/computation/run/platform.py::open_connection@98",
      "source_type": "code",
      "path": "oasislmf/computation/run/platform.py",
      "symbol_type": "function",
      "name": "open_connection",
      "lineno": 98,
      "end_lineno": 146,
      "business_stage": "other",
      "docstring": "Attempts connection in this order:\n1. API_EXAMPLE_AUTH username/password\n2. API_EXAMPLE_AUTH client_id/client_secret\n3. Prompt or load credentials",
      "content": "# File: oasislmf/computation/run/platform.py\n# function: open_connection (lines 98-146)\n\n    def open_connection(self):\n        \"\"\"\n        Attempts connection in this order:\n        1. API_EXAMPLE_AUTH username/password\n        2. API_EXAMPLE_AUTH client_id/client_secret\n        3. Prompt or load credentials\n        \"\"\"\n        if not isinstance(self.server_login_json, str):\n            # 1. Try example username/password\n            if 'username' in API_EXAMPLE_AUTH and 'password' in API_EXAMPLE_AUTH:\n                conn = self.try_connection(\n                    auth_type=\"simple\",\n                    username=API_EXAMPLE_AUTH['username'],\n                    password=API_EXAMPLE_AUTH['password']\n                )\n                if conn:\n                    return conn\n\n            # 2. Try example client_id/client_secret\n            if 'client_id' in API_EXAMPLE_AUTH and 'client_secret' in API_EXAMPLE_AUTH:\n                conn = self.try_connection(\n                    auth_type=\"oidc\",\n                    client_id=API_EXAMPLE_AUTH['client_id'],\n                    client_secret=API_EXAMPLE_AUTH['client_secret']\n                )\n                if conn:\n                    return conn\n\n        # 3. Load credentials (file or prompt)\n        self.logger.info(\"-- Authentication Required --\")\n        credentials = self.load_credentials(self.server_login_json)\n        self.logger.info(f'Connecting to - {self.server_url}')\n        if 'username' in credentials and 'password' in credentials:\n            return self.try_connection(\n                fail_safe=False,\n                auth_type=\"simple\",\n                username=credentials['username'],\n                password=credentials['password']\n            )\n        if 'client_id' in credentials and 'client_secret' in credentials:\n            return self.try_connection(\n                fail_safe=False,\n                auth_type=\"oidc\",\n                client_id=credentials['client_id'],\n                client_secret=credentials['client_secret']\n            )\n        raise OasisException(\n            f\"Error: No valid credentials provided for platform, current credential keys [{credentials.keys()}], must be one of username/password or client_id/client_secret\"\n        )\n\n\"\"\"Docstring (excerpt)\"\"\"\nAttempts connection in this order:\n1. API_EXAMPLE_AUTH username/password\n2. API_EXAMPLE_AUTH client_id/client_secret\n3. Prompt or load credentials"
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::get_check_function@254",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "get_check_function",
      "lineno": 254,
      "end_lineno": 296,
      "business_stage": "other",
      "docstring": "Creates a bash function to check the logs to ensure same number of process started and finsished.\n\nArgs:\n    custom_gulcalc_log_start (str): Custom message printed to the logs when a process starts.\n    custom_gulcalc_log_finish (str): Custom message printed to the logs when a process ends.",
      "content": "# File: oasislmf/execution/bash.py\n# function: get_check_function (lines 254-296)\n\ndef get_check_function(custom_gulcalc_log_start=None, custom_gulcalc_log_finish=None):\n    \"\"\"Creates a bash function to check the logs to ensure same number of process started and finsished.\n\n    Args:\n        custom_gulcalc_log_start (str): Custom message printed to the logs when a process starts.\n        custom_gulcalc_log_finish (str): Custom message printed to the logs when a process ends.\n    \"\"\"\n    check_function = \"\"\"\ncheck_complete(){\n    set +e\n    proc_list=\"eve evepy getmodel gulcalc fmcalc summarycalc eltcalc aalcalc aalcalcmeanonly leccalc pltcalc ordleccalc modelpy gulpy fmpy gulmc summarypy eltpy pltpy aalpy lecpy\"\n    has_error=0\n    for p in $proc_list; do\n        started=$(find log -name \"${p}_[0-9]*.log\" | wc -l)\n        finished=$(find log -name \"${p}_[0-9]*.log\" -exec grep -l \"finish\" {} + | wc -l)\n        if [ \"$finished\" -lt \"$started\" ]; then\n            echo \"[ERROR] $p - $((started-finished)) processes lost\"\n            has_error=1\n        elif [ \"$started\" -gt 0 ]; then\n            echo \"[OK] $p\"\n        fi\n    done\n\"\"\"\n    # Add in check for custom gulcalc if settings are provided\n    if custom_gulcalc_log_start and custom_gulcalc_log_finish:\n        check_function += f\"\"\"\n    started=$( grep \"{custom_gulcalc_log_start}\" log/gul_stderror.err | wc -l)\n    finished=$( grep \"{custom_gulcalc_log_finish}\" log/gul_stderror.err | wc -l)\n    if [ \"$finished\" -lt \"$started\" ]; then\n        echo \"[ERROR] gulcalc - $((started-finished)) processes lost\"\n        has_error=1\n    elif [ \"$started\" -gt 0 ]; then\n        echo \"[OK] gulcalc\"\n    fi\n\"\"\"\n\n    check_function += \"\"\"    if [ \"$has_error\" -ne 0 ]; then\n        false # raise non-zero exit code\n    else\n        echo 'Run Completed'\n    fi\n}\"\"\"\n    return check_function\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreates a bash function to check the logs to ensure same number of process started and finsished.\n\nArgs:\n    custom_gulcalc_log_start (str): Custom message printed to the logs when a process starts.\n    custom_gulcalc_log_finish (str): Custom message printed to the logs when a process ends."
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::process_range@324",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "process_range",
      "lineno": 324,
      "end_lineno": 339,
      "business_stage": "other",
      "docstring": "Creates an iterable for all the process ids, if process number is set\nthen an iterable containing only that number is returned.\n\nThis allows for the loss generation to be ran in different processes\nrather than accross multiple cores.\n\n:param max_process_id: The largest process number\n:param process_number: If set iterable only containing this number is returned\n:return: iterable containing all the process numbers to process",
      "content": "# File: oasislmf/execution/bash.py\n# function: process_range (lines 324-339)\n\ndef process_range(max_process_id, process_number=None):\n    \"\"\"\n    Creates an iterable for all the process ids, if process number is set\n    then an iterable containing only that number is returned.\n\n    This allows for the loss generation to be ran in different processes\n    rather than accross multiple cores.\n\n    :param max_process_id: The largest process number\n    :param process_number: If set iterable only containing this number is returned\n    :return: iterable containing all the process numbers to process\n    \"\"\"\n    if process_number is not None:\n        return [process_number]\n    else:\n        return range(1, max_process_id + 1)\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreates an iterable for all the process ids, if process number is set\nthen an iterable containing only that number is returned.\n\nThis allows for the loss generation to be ran in different processes\nrather than accross multiple cores.\n\n:param max_process_id: The largest process number\n:param process_number: If set iterable only containing this number is returned\n:return: iterable containing all the process numbers to process"
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::get_modelcmd@342",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "get_modelcmd",
      "lineno": 342,
      "end_lineno": 365,
      "business_stage": "other",
      "docstring": "Gets the construct model command line argument for the bash script.\n\nArgs:\n    modelpy: (bool) if the getmodel Python setting is True or not\n    server: (bool) if set then enable 'TCP' ipc server/client mode\n    peril_filter: (list) list of perils to include (all included if empty)\n\nReturns: C++ getmodel if modelpy is False, Python getmodel if the modelpy if False",
      "content": "# File: oasislmf/execution/bash.py\n# function: get_modelcmd (lines 342-365)\n\ndef get_modelcmd(modelpy: bool, server=False, peril_filter=[]) -> str:\n    \"\"\"\n    Gets the construct model command line argument for the bash script.\n\n    Args:\n        modelpy: (bool) if the getmodel Python setting is True or not\n        server: (bool) if set then enable 'TCP' ipc server/client mode\n        peril_filter: (list) list of perils to include (all included if empty)\n\n    Returns: C++ getmodel if modelpy is False, Python getmodel if the modelpy if False\n    \"\"\"\n    py_cmd = 'modelpy'\n    cpp_cmd = 'getmodel'\n\n    if modelpy is True:\n        if server is True:\n            py_cmd = f'{py_cmd} --data-server'\n\n        if peril_filter:\n            py_cmd = f\"{py_cmd} --peril-filter {' '.join(peril_filter)}\"\n\n        return py_cmd\n    else:\n        return cpp_cmd\n\n\"\"\"Docstring (excerpt)\"\"\"\nGets the construct model command line argument for the bash script.\n\nArgs:\n    modelpy: (bool) if the getmodel Python setting is True or not\n    server: (bool) if set then enable 'TCP' ipc server/client mode\n    peril_filter: (list) list of perils to include (all included if empty)\n\nReturns: C++ getmodel if modelpy is False, Python getmodel if the modelpy if False"
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::get_gulcmd@368",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "get_gulcmd",
      "lineno": 368,
      "end_lineno": 399,
      "business_stage": "other",
      "docstring": "Get the ground-up loss calculation command.\n\nArgs:\n    gulpy (bool): if True, return the python command name, else the c++ one.\n\nReturns:\n    str: the ground-up loss calculation command",
      "content": "# File: oasislmf/execution/bash.py\n# function: get_gulcmd (lines 368-399)\n\ndef get_gulcmd(gulpy, gulpy_random_generator, gulmc, gulmc_random_generator, gulmc_effective_damageability, gulmc_vuln_cache_size, modelpy_server, peril_filter, model_df_engine='oasis_data_manager.df_reader.reader.OasisPandasReader', dynamic_footprint=False):\n    \"\"\"Get the ground-up loss calculation command.\n\n    Args:\n        gulpy (bool): if True, return the python command name, else the c++ one.\n\n    Returns:\n        str: the ground-up loss calculation command\n    \"\"\"\n    if gulpy and gulmc:\n        raise ValueError(\"Expect either gulpy or gulmc to be True, got both True.\")\n\n    if gulpy:\n        cmd = f'gulpy --random-generator={gulpy_random_generator}'\n    elif gulmc:\n        cmd = f\"gulmc --random-generator={gulmc_random_generator} {'--data-server'*modelpy_server} --model-df-engine=\\'{model_df_engine}\\'\"\n\n        if peril_filter:\n            cmd += f\" --peril-filter {' '.join(peril_filter)}\"\n\n        if gulmc_effective_damageability:\n            cmd += \" --effective-damageability\"\n\n        if gulmc_vuln_cache_size:\n            cmd += f\" --vuln-cache-size {gulmc_vuln_cache_size}\"\n\n        if dynamic_footprint:\n            cmd += \" --dynamic-footprint True\"\n    else:\n        cmd = 'gulcalc'\n\n    return cmd\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet the ground-up loss calculation command.\n\nArgs:\n    gulpy (bool): if True, return the python command name, else the c++ one.\n\nReturns:\n    str: the ground-up loss calculation command"
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::print_command@414",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "print_command",
      "lineno": 414,
      "end_lineno": 422,
      "business_stage": "other",
      "docstring": "Writes the supplied command to the end of the generated script\n\n:param command_file: File to append command to.\n:param cmd: The command to append",
      "content": "# File: oasislmf/execution/bash.py\n# function: print_command (lines 414-422)\n\ndef print_command(command_file, cmd):\n    \"\"\"\n    Writes the supplied command to the end of the generated script\n\n    :param command_file: File to append command to.\n    :param cmd: The command to append\n    \"\"\"\n    with io.open(command_file, \"a\", encoding='utf-8') as myfile:\n        myfile.writelines(cmd + \"\\n\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrites the supplied command to the end of the generated script\n\n:param command_file: File to append command to.\n:param cmd: The command to append"
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::leccalc_enabled@425",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "leccalc_enabled",
      "lineno": 425,
      "end_lineno": 462,
      "business_stage": "other",
      "docstring": "Checks if leccalc is enabled in a summaries section\n\n:param summary_options: Summaies section from an analysis_settings file\n:type summary_options: dict\n\nExample:\n{\n    \"aalcalc\": true,\n    \"eltcalc\": true,\n    \"id\": 1,\n    \"lec_output\": true,\n    \"leccalc\": {\n        \"full_uncertainty_aep\": true,\n        \"full_uncertainty_oep\": true,\n        \"return_period_file\": true\n    }\n}\n:return: True is leccalc is enables, False otherwise.",
      "content": "# File: oasislmf/execution/bash.py\n# function: leccalc_enabled (lines 425-462)\n\ndef leccalc_enabled(summary_options):\n    \"\"\"\n    Checks if leccalc is enabled in a summaries section\n\n    :param summary_options: Summaies section from an analysis_settings file\n    :type summary_options: dict\n\n    Example:\n    {\n        \"aalcalc\": true,\n        \"eltcalc\": true,\n        \"id\": 1,\n        \"lec_output\": true,\n        \"leccalc\": {\n            \"full_uncertainty_aep\": true,\n            \"full_uncertainty_oep\": true,\n            \"return_period_file\": true\n        }\n    }\n    :return: True is leccalc is enables, False otherwise.\n    \"\"\"\n\n    lec_options = summary_options.get('leccalc', {})\n    lec_boolean = summary_options.get('lec_output', False)\n\n    # Disabled if leccalc flag is missing or false\n    if not lec_boolean:\n        return False\n\n    # Backwards compatibility for nested \"outputs\" keys in lec_options\n    if \"outputs\" in lec_options:\n        lec_options = lec_options[\"outputs\"]\n\n    # Enabled if at least one option is selected\n    for ouput_opt in lec_options:\n        if ouput_opt in WAIT_PROCESSING_SWITCHES and lec_options[ouput_opt]:\n            return True\n    return False\n\n\"\"\"Docstring (excerpt)\"\"\"\nChecks if leccalc is enabled in a summaries section\n\n:param summary_options: Summaies section from an analysis_settings file\n:type summary_options: dict\n\nExample:\n{\n    \"aalcalc\": true,\n    \"eltcalc\": true,\n    \"id\": 1,\n    \"lec_output\": true,\n    \"leccalc\": {\n        \"full_uncertainty_aep\": true,\n        \"full_uncertainty_oep\": true,\n        \"return_period_file\": true\n    }\n}\n:return: True is leccalc is enables, False otherwise."
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::ord_enabled@465",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "ord_enabled",
      "lineno": 465,
      "end_lineno": 498,
      "business_stage": "other",
      "docstring": "Checks if ORD leccalc is enabled in a summaries section\n\n:param summary_options: Summaies section from an analysis_settings file\n:type summary_options: dict\n\n:param ORD_SWITCHES: Options from the analysis_settings 'Summaies' section to search\n:type  ORD_SWITCHES: dict\n\nExample:\n{\n    \"id\": 1,\n    \"ord_output\": {\n        \"ept_full_uncertainty_aep\": true,\n        \"ept_full_uncertainty_oep\": true,\n        \"ept_mean_sample_aep\": true,\n        \"ept_mean_sample_oep\": true,\n        \"ept_per_sample_mean_aep\": true,\n        \"ept_per_sample_mean_oep\": true,\n        \"psept_aep\": true,\n        \"psept_oep\": true,\n        \"return_period_file\": true\n    }\n}\n\n:return: True is leccalc is enables, False otherwise.",
      "content": "# File: oasislmf/execution/bash.py\n# function: ord_enabled (lines 465-498)\n\ndef ord_enabled(summary_options, ORD_SWITCHES):\n    \"\"\"\n    Checks if ORD leccalc is enabled in a summaries section\n\n    :param summary_options: Summaies section from an analysis_settings file\n    :type summary_options: dict\n\n    :param ORD_SWITCHES: Options from the analysis_settings 'Summaies' section to search\n    :type  ORD_SWITCHES: dict\n\n    Example:\n    {\n        \"id\": 1,\n        \"ord_output\": {\n            \"ept_full_uncertainty_aep\": true,\n            \"ept_full_uncertainty_oep\": true,\n            \"ept_mean_sample_aep\": true,\n            \"ept_mean_sample_oep\": true,\n            \"ept_per_sample_mean_aep\": true,\n            \"ept_per_sample_mean_oep\": true,\n            \"psept_aep\": true,\n            \"psept_oep\": true,\n            \"return_period_file\": true\n        }\n    }\n\n    :return: True is leccalc is enables, False otherwise.\n    \"\"\"\n\n    ord_options = summary_options.get('ord_output', {})\n    for ouput_opt in ord_options:\n        if ouput_opt in ORD_SWITCHES and ord_options[ouput_opt]:\n            return True\n    return False\n\n\"\"\"Docstring (excerpt)\"\"\"\nChecks if ORD leccalc is enabled in a summaries section\n\n:param summary_options: Summaies section from an analysis_settings file\n:type summary_options: dict\n\n:param ORD_SWITCHES: Options from the analysis_settings 'Summaies' section to search\n:type  ORD_SWITCHES: dict\n\nExample:\n{\n    \"id\": 1,\n    \"ord_output\": {\n        \"ept_full_uncertainty_aep\": true,\n        \"ept_full_uncertainty_oep\": true,\n        \"ept_mean_sample_aep\": true,\n        \"ept_mean_sample_oep\": true,\n        \"ept_per_sample_mean_aep\": true,\n        \"ept_per_sample_mean_oep\": true,\n        \"psept_aep\": true,\n        \"psept_oep\": true,\n        \"return_period_file\": true\n    }\n}\n\n:return: True is leccalc is enables, False otherwise."
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::get_fifo_name@748",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "get_fifo_name",
      "lineno": 748,
      "end_lineno": 753,
      "business_stage": "other",
      "docstring": "Standard name for FIFO",
      "content": "# File: oasislmf/execution/bash.py\n# function: get_fifo_name (lines 748-753)\n\ndef get_fifo_name(fifo_dir, producer, producer_id, consumer=''):\n    \"\"\"Standard name for FIFO\"\"\"\n    if consumer:\n        return f'{fifo_dir}{producer}_{consumer}_P{producer_id}'\n    else:\n        return f'{fifo_dir}{producer}_P{producer_id}'\n\n\"\"\"Docstring (excerpt)\"\"\"\nStandard name for FIFO"
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::do_waits@1512",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "do_waits",
      "lineno": 1512,
      "end_lineno": 1531,
      "business_stage": "other",
      "docstring": "Add waits to the script\n\n:param wait_variable: The type of wait\n:type wait_variable: str\n\n:param wait_count: The number of processes to wait for\n:type wait_count: int\n\n:param filename: Script to add waits to\n:type filename: str",
      "content": "# File: oasislmf/execution/bash.py\n# function: do_waits (lines 1512-1531)\n\ndef do_waits(wait_variable, wait_count, filename):\n    \"\"\"\n    Add waits to the script\n\n    :param wait_variable: The type of wait\n    :type wait_variable: str\n\n    :param wait_count: The number of processes to wait for\n    :type wait_count: int\n\n    :param filename: Script to add waits to\n    :type filename: str\n    \"\"\"\n    if wait_count > 0:\n        cmd = 'wait'\n        for pid in range(1, wait_count + 1):\n            cmd = '{} ${}{}'.format(cmd, wait_variable, pid)\n\n        print_command(filename, cmd)\n        print_command(filename, '')\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdd waits to the script\n\n:param wait_variable: The type of wait\n:type wait_variable: str\n\n:param wait_count: The number of processes to wait for\n:type wait_count: int\n\n:param filename: Script to add waits to\n:type filename: str"
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::do_pwaits@1534",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "do_pwaits",
      "lineno": 1534,
      "end_lineno": 1538,
      "business_stage": "other",
      "docstring": "Add pwaits to the script",
      "content": "# File: oasislmf/execution/bash.py\n# function: do_pwaits (lines 1534-1538)\n\ndef do_pwaits(filename, process_counter):\n    \"\"\"\n    Add pwaits to the script\n    \"\"\"\n    do_waits('pid', process_counter['pid_monitor_count'], filename)\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdd pwaits to the script"
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::do_awaits@1541",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "do_awaits",
      "lineno": 1541,
      "end_lineno": 1545,
      "business_stage": "other",
      "docstring": "Add awaits to the script",
      "content": "# File: oasislmf/execution/bash.py\n# function: do_awaits (lines 1541-1545)\n\ndef do_awaits(filename, process_counter):\n    \"\"\"\n    Add awaits to the script\n    \"\"\"\n    do_waits('apid', process_counter['apid_monitor_count'], filename)\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdd awaits to the script"
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::do_lwaits@1548",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "do_lwaits",
      "lineno": 1548,
      "end_lineno": 1552,
      "business_stage": "other",
      "docstring": "Add lwaits to the script",
      "content": "# File: oasislmf/execution/bash.py\n# function: do_lwaits (lines 1548-1552)\n\ndef do_lwaits(filename, process_counter):\n    \"\"\"\n    Add lwaits to the script\n    \"\"\"\n    do_waits('lpid', process_counter['lpid_monitor_count'], filename)\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdd lwaits to the script"
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::do_kwaits@1555",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "do_kwaits",
      "lineno": 1555,
      "end_lineno": 1559,
      "business_stage": "other",
      "docstring": "Add kwaits to the script",
      "content": "# File: oasislmf/execution/bash.py\n# function: do_kwaits (lines 1555-1559)\n\ndef do_kwaits(filename, process_counter):\n    \"\"\"\n    Add kwaits to the script\n    \"\"\"\n    do_waits('kpid', process_counter['kpid_monitor_count'], filename)\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdd kwaits to the script"
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::get_getmodel_itm_cmd@1562",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "get_getmodel_itm_cmd",
      "lineno": 1562,
      "end_lineno": 1639,
      "business_stage": "other",
      "docstring": "Gets the getmodel ktools command (3.1.0+) Gulcalc item stream\n:param number_of_samples: The number of samples to run\n:type number_of_samples: int\n:param gul_threshold: The GUL threshold to use\n:type gul_threshold: float\n:param use_random_number_file: flag to use the random number file\n:type use_random_number_file: bool\n:param gul_alloc_rule: back allocation rule for gulcalc\n:type gul_alloc_rule: int\n:param item_output: The item output\n:type item_output: str\n:param eve_shuffle_flag: The event shuffling rule\n:type eve_shuffle_flag: str\n:param evepy: enable evepy\n:type evepy: bool\n:param model_df_engine: The engine to use when loading dataframes\n:type  model_df_engine: str\n:return: The generated getmodel command",
      "content": "# File: oasislmf/execution/bash.py\n# function: get_getmodel_itm_cmd (lines 1562-1639)\n\ndef get_getmodel_itm_cmd(\n        number_of_samples,\n        gul_threshold,\n        use_random_number_file,\n        gul_alloc_rule,\n        item_output,\n        process_id,\n        max_process_id,\n        correlated_output,\n        eve_shuffle_flag,\n        modelpy=False,\n        modelpy_server=False,\n        peril_filter=[],\n        evepy=False,\n        gulpy=False,\n        gulpy_random_generator=1,\n        gulmc=False,\n        gulmc_random_generator=1,\n        gulmc_effective_damageability=False,\n        gulmc_vuln_cache_size=200,\n        model_df_engine='oasis_data_manager.df_reader.reader.OasisPandasReader',\n        dynamic_footprint=False,\n        **kwargs):\n    \"\"\"\n    Gets the getmodel ktools command (3.1.0+) Gulcalc item stream\n    :param number_of_samples: The number of samples to run\n    :type number_of_samples: int\n    :param gul_threshold: The GUL threshold to use\n    :type gul_threshold: float\n    :param use_random_number_file: flag to use the random number file\n    :type use_random_number_file: bool\n    :param gul_alloc_rule: back allocation rule for gulcalc\n    :type gul_alloc_rule: int\n    :param item_output: The item output\n    :type item_output: str\n    :param eve_shuffle_flag: The event shuffling rule\n    :type eve_shuffle_flag: str\n    :param evepy: enable evepy\n    :type evepy: bool\n    :param model_df_engine: The engine to use when loading dataframes\n    :type  model_df_engine: str\n    :return: The generated getmodel command\n    \"\"\"\n    if evepy:\n        cmd = f'evepy {eve_shuffle_flag}{process_id} {max_process_id} | '\n    else:\n        cmd = f'eve {eve_shuffle_flag}{process_id} {max_process_id} | '\n    if gulmc is True:\n        gulcmd = get_gulcmd(\n            gulpy, gulpy_random_generator, gulmc, gulmc_random_generator, gulmc_effective_damageability,\n            gulmc_vuln_cache_size, modelpy_server, peril_filter, model_df_engine=model_df_engine,\n            dynamic_footprint=dynamic_footprint\n        )\n        cmd += f'{gulcmd} -S{number_of_samples} -L{gul_threshold}'\n\n    else:\n        modelcmd = get_modelcmd(modelpy, modelpy_server, peril_filter)\n        gulcmd = get_gulcmd(gulpy, gulpy_random_generator, False, 0, False, 0, False, [], model_df_engine=model_df_engine)\n        cmd += f'{modelcmd} | {gulcmd} -S{number_of_samples} -L{gul_threshold}'\n\n    if use_random_number_file:\n        if not gulpy and not gulmc:\n            # append this arg only if gulcalc is used\n            cmd = '{} -r'.format(cmd)\n    if correlated_output != '':\n        if not gulpy and not gulmc:\n            # append this arg only if gulcalc is used\n            cmd = '{} -j {}'.format(cmd, correlated_output)\n\n    cmd = '{} -a{}'.format(cmd, gul_alloc_rule)\n\n    if not gulpy and not gulmc:\n        # append this arg only if gulcalc is used\n        cmd = '{} -i {}'.format(cmd, item_output)\n    else:\n        cmd = '{} {}'.format(cmd, item_output)\n\n    return cmd\n\n\"\"\"Docstring (excerpt)\"\"\"\nGets the getmodel ktools command (3.1.0+) Gulcalc item stream\n:param number_of_samples: The number of samples to run\n:type number_of_samples: int\n:param gul_threshold: The GUL threshold to use\n:type gul_threshold: float\n:param use_random_number_file: flag to use the random number file\n:type use_random_number_file: bool\n:param gul_alloc_rule: back allocation rule for gulcalc\n:type gul_alloc_rule: int\n:param item_output: The item output\n:type item_output: str\n:param eve_shuffle_flag: The event shuffling rule\n:type eve_shuffle_flag: str\n:param evepy: enable evepy\n:type evepy: bool\n:param model_df_engine: The engine to use when loading dataframes\n:type  model_df_engine: str\n:return: The generated getmodel command"
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::get_getmodel_cov_cmd@1642",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "get_getmodel_cov_cmd",
      "lineno": 1642,
      "end_lineno": 1716,
      "business_stage": "other",
      "docstring": "Gets the getmodel ktools command (version < 3.0.8) gulcalc coverage stream\n:param number_of_samples: The number of samples to run\n:type number_of_samples: int\n:param gul_threshold: The GUL threshold to use\n:type gul_threshold: float\n:param use_random_number_file: flag to use the random number file\n:type use_random_number_file: bool\n:param coverage_output: The coverage output\n:type coverage_output: str\n:param item_output: The item output\n:type item_output: str\n:param eve_shuffle_flag: The event shuffling rule\n:type  eve_shuffle_flag: str\n:param evepy: enable evepy\n:type evepy: bool\n:param df_engine: The engine to use when loading dataframes\n:type  df_engine: str\n:return: (str) The generated getmodel command",
      "content": "# File: oasislmf/execution/bash.py\n# function: get_getmodel_cov_cmd (lines 1642-1716)\n\ndef get_getmodel_cov_cmd(\n        number_of_samples,\n        gul_threshold,\n        use_random_number_file,\n        coverage_output,\n        item_output,\n        process_id,\n        max_process_id,\n        eve_shuffle_flag,\n        evepy=False,\n        modelpy=False,\n        modelpy_server=False,\n        peril_filter=[],\n        gulpy=False,\n        gulpy_random_generator=1,\n        gulmc=False,\n        gulmc_random_generator=1,\n        gulmc_effective_damageability=False,\n        gulmc_vuln_cache_size=200,\n        model_df_engine='oasis_data_manager.df_reader.reader.OasisPandasReader',\n        dynamic_footprint=False,\n        **kwargs) -> str:\n    \"\"\"\n    Gets the getmodel ktools command (version < 3.0.8) gulcalc coverage stream\n    :param number_of_samples: The number of samples to run\n    :type number_of_samples: int\n    :param gul_threshold: The GUL threshold to use\n    :type gul_threshold: float\n    :param use_random_number_file: flag to use the random number file\n    :type use_random_number_file: bool\n    :param coverage_output: The coverage output\n    :type coverage_output: str\n    :param item_output: The item output\n    :type item_output: str\n    :param eve_shuffle_flag: The event shuffling rule\n    :type  eve_shuffle_flag: str\n    :param evepy: enable evepy\n    :type evepy: bool\n    :param df_engine: The engine to use when loading dataframes\n    :type  df_engine: str\n    :return: (str) The generated getmodel command\n    \"\"\"\n    if evepy:\n        cmd = f'evepy {eve_shuffle_flag}{process_id} {max_process_id} | '\n    else:\n        cmd = f'eve {eve_shuffle_flag}{process_id} {max_process_id} | '\n    if gulmc is True:\n        gulcmd = get_gulcmd(\n            gulpy, gulpy_random_generator, gulmc, gulmc_random_generator, gulmc_effective_damageability,\n            gulmc_vuln_cache_size, modelpy_server, peril_filter, model_df_engine=model_df_engine,\n            dynamic_footprint=dynamic_footprint\n        )\n        cmd += f'{gulcmd} -S{number_of_samples} -L{gul_threshold}'\n\n    else:\n        modelcmd = get_modelcmd(modelpy, modelpy_server, peril_filter)\n        gulcmd = get_gulcmd(gulpy, gulpy_random_generator, False, 0, False, 0, False, [], model_df_engine=model_df_engine)\n        cmd += f'{modelcmd} | {gulcmd} -S{number_of_samples} -L{gul_threshold}'\n\n    if use_random_number_file:\n        if not gulpy and not gulmc:\n            # append this arg only if gulcalc is used\n            cmd = '{} -r'.format(cmd)\n    if coverage_output != '':\n        if not gulpy and not gulmc:\n            # append this arg only if gulcalc is used\n            cmd = '{} -c {}'.format(cmd, coverage_output)\n    if not gulpy and not gulmc:\n        # append this arg only if gulcalc is used\n        if item_output != '':\n            cmd = '{} -i {}'.format(cmd, item_output)\n    else:\n        cmd = '{} {}'.format(cmd, item_output)\n\n    return cmd\n\n\"\"\"Docstring (excerpt)\"\"\"\nGets the getmodel ktools command (version < 3.0.8) gulcalc coverage stream\n:param number_of_samples: The number of samples to run\n:type number_of_samples: int\n:param gul_threshold: The GUL threshold to use\n:type gul_threshold: float\n:param use_random_number_file: flag to use the random number file\n:type use_random_number_file: bool\n:param coverage_output: The coverage output\n:type coverage_output: str\n:param item_output: The item output\n:type item_output: str\n:param eve_shuffle_flag: The event shuffling rule\n:type  eve_shuffle_flag: str\n:param evepy: enable evepy\n:type evepy: bool\n:param df_engine: The engine to use when loading dataframes\n:type  df_engine: str\n:return: (str) The generated getmodel command"
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::add_pid_to_shell_command@1719",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "add_pid_to_shell_command",
      "lineno": 1719,
      "end_lineno": 1735,
      "business_stage": "other",
      "docstring": "Add a variable to the end of a command in order to track the ID of the process executing it.\nEach time this function is called, the counter `process_counter` is incremented.\n\nArgs:\n    cmd (str): the command whose process ID is to be stored in a variable.\n    process_counter (Counter or dict): the number of process IDs that are being tracked.\n\nReturns:\n    cmd (str): the updated command string.",
      "content": "# File: oasislmf/execution/bash.py\n# function: add_pid_to_shell_command (lines 1719-1735)\n\ndef add_pid_to_shell_command(cmd, process_counter):\n    \"\"\"\n    Add a variable to the end of a command in order to track the ID of the process executing it.\n    Each time this function is called, the counter `process_counter` is incremented.\n\n    Args:\n        cmd (str): the command whose process ID is to be stored in a variable.\n        process_counter (Counter or dict): the number of process IDs that are being tracked.\n\n    Returns:\n        cmd (str): the updated command string.\n    \"\"\"\n\n    process_counter[\"pid_monitor_count\"] += 1\n    cmd = f'{cmd} pid{process_counter[\"pid_monitor_count\"]}=$!'\n\n    return cmd\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdd a variable to the end of a command in order to track the ID of the process executing it.\nEach time this function is called, the counter `process_counter` is incremented.\n\nArgs:\n    cmd (str): the command whose process ID is to be stored in a variable.\n    process_counter (Counter or dict): the number of process IDs that are being tracked.\n\nReturns:\n    cmd (str): the updated command string."
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::get_main_cmd_ri_stream@1738",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "get_main_cmd_ri_stream",
      "lineno": 1738,
      "end_lineno": 1808,
      "business_stage": "other",
      "docstring": "Gets the fmcalc ktools command reinsurance stream\n:param cmd: either gulcalc command stream or correlated output file\n:type cmd: str\n:param process_id: ID corresponding to thread\n:type process_id: int\n:param il_output: If insured loss outputs required\n:type il_output: Boolean\n:param il_alloc_rule: insured loss allocation rule for fmcalc\n:type il_alloc_rule: int\n:param ri_alloc_rule: reinsurance allocation rule for fmcalc\n:type ri_alloc_rule: int\n:param num_reinsurance_iterations: number of reinsurance iterations\n:type num_reinsurance_iterations: int\n:param fifo_dir: path to fifo directory\n:type fifo_dir: str\n:param stderr_guard: send stderr output to log file\n:type stderr_guard: bool\n:param from_file: must be true if cmd is a file and false if it can be piped\n:type from_file: bool\n:param ri_inuring_priorities: Inuring priorities where net output has been requested\n:type ri_inuring_priorities: dict\n:param rl_inuring_priorities: Inuring priorities where gross output has been requested\n:type rl_inuring_priorities: dict",
      "content": "# File: oasislmf/execution/bash.py\n# function: get_main_cmd_ri_stream (lines 1738-1808)\n\ndef get_main_cmd_ri_stream(\n    cmd,\n    process_id,\n    il_output,\n    il_alloc_rule,\n    ri_alloc_rule,\n    num_reinsurance_iterations,\n    fifo_dir='fifo/',\n    stderr_guard=True,\n    from_file=False,\n    fmpy=True,\n    fmpy_low_memory=False,\n    fmpy_sort_output=False,\n    step_flag='',\n    process_counter=None,\n    ri_inuring_priorities=None,\n    rl_inuring_priorities=None\n):\n    \"\"\"\n    Gets the fmcalc ktools command reinsurance stream\n    :param cmd: either gulcalc command stream or correlated output file\n    :type cmd: str\n    :param process_id: ID corresponding to thread\n    :type process_id: int\n    :param il_output: If insured loss outputs required\n    :type il_output: Boolean\n    :param il_alloc_rule: insured loss allocation rule for fmcalc\n    :type il_alloc_rule: int\n    :param ri_alloc_rule: reinsurance allocation rule for fmcalc\n    :type ri_alloc_rule: int\n    :param num_reinsurance_iterations: number of reinsurance iterations\n    :type num_reinsurance_iterations: int\n    :param fifo_dir: path to fifo directory\n    :type fifo_dir: str\n    :param stderr_guard: send stderr output to log file\n    :type stderr_guard: bool\n    :param from_file: must be true if cmd is a file and false if it can be piped\n    :type from_file: bool\n    :param ri_inuring_priorities: Inuring priorities where net output has been requested\n    :type ri_inuring_priorities: dict\n    :param rl_inuring_priorities: Inuring priorities where gross output has been requested\n    :type rl_inuring_priorities: dict\n    \"\"\"\n    if from_file:\n        main_cmd = f'{get_fmcmd(fmpy, fmpy_low_memory, fmpy_sort_output)} -a{il_alloc_rule}{step_flag} < {cmd}'\n    else:\n        main_cmd = f'{cmd} | {get_fmcmd(fmpy, fmpy_low_memory, fmpy_sort_output)} -a{il_alloc_rule}{step_flag}'\n\n    if il_output:\n        main_cmd += f\" | tee {get_fifo_name(fifo_dir, RUNTYPE_INSURED_LOSS, process_id)}\"\n\n    for i in range(1, num_reinsurance_iterations + 1):\n        main_cmd += f\" | {get_fmcmd(fmpy, fmpy_low_memory, fmpy_sort_output)} -a{ri_alloc_rule} -p {os.path.join('input', 'RI_' + str(i))}\"\n        if rl_inuring_priorities:   # If rl output is requested then produce gross output at all inuring priorities\n            main_cmd += f\" -o {get_fifo_name(fifo_dir, RUNTYPE_REINSURANCE_GROSS_LOSS, process_id, consumer=rl_inuring_priorities[i].rstrip('_'))}\"\n        if i < num_reinsurance_iterations:   # Net output required to process next inuring priority\n            main_cmd += ' -n -'\n        if i in ri_inuring_priorities.keys():\n            if i == num_reinsurance_iterations:   # Final inuring priority always produces net output if ri output requested\n                ri_fifo_name = get_fifo_name(fifo_dir, RUNTYPE_REINSURANCE_LOSS, process_id)\n                main_cmd += f\" -n - > {ri_fifo_name}\"\n            else:\n                main_cmd += f\" | tee {get_fifo_name(fifo_dir, RUNTYPE_REINSURANCE_LOSS, process_id, consumer=ri_inuring_priorities[i].rstrip('_'))}\"\n\n    main_cmd = f'( {main_cmd} ) 2>> $LOG_DIR/stderror.err' if stderr_guard else f'{main_cmd}'\n    main_cmd = f'( {main_cmd} ) &'\n\n    if process_counter is not None:\n        main_cmd = add_pid_to_shell_command(main_cmd, process_counter)\n\n    return main_cmd\n\n\"\"\"Docstring (excerpt)\"\"\"\nGets the fmcalc ktools command reinsurance stream\n:param cmd: either gulcalc command stream or correlated output file\n:type cmd: str\n:param process_id: ID corresponding to thread\n:type process_id: int\n:param il_output: If insured loss outputs required\n:type il_output: Boolean\n:param il_alloc_rule: insured loss allocation rule for fmcalc\n:type il_alloc_rule: int\n:param ri_alloc_rule: reinsurance allocation rule for fmcalc\n:type ri_alloc_rule: int\n:param num_reinsurance_iterations: number of reinsurance iterations\n:type num_reinsurance_iterations: int\n:param fifo_dir: path to fifo directory\n:type fifo_dir: str\n:param stderr_guard: send stderr output to log file\n:type stderr_guard: bool\n:param from_file: must be true if cmd is a file and false if it can be piped\n:type from_file: bool\n:param ri_inuring_priorities: Inuring priorities where net output has been requested\n:type ri_inuring_priorities: dict\n:param rl_inuring_priorities: Inuring priorities where gross output has been requested\n:type rl_inuring_priorities: dict"
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::get_main_cmd_il_stream@1811",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "get_main_cmd_il_stream",
      "lineno": 1811,
      "end_lineno": 1855,
      "business_stage": "other",
      "docstring": "Gets the fmcalc ktools command insured losses stream\n:param cmd: either gulcalc command stream or correlated output file\n:type cmd: str\n:param process_id: ID corresponding to thread\n:type process_id: int\n:param il_alloc_rule: insured loss allocation rule for fmcalc\n:type il_alloc_rule: int\n:param fifo_dir: path to fifo directory\n:type fifo_dir: str\n:param stderr_guard: send stderr output to log file\n:type stderr_guard: bool\n:param from_file: must be true if cmd is a file and false if it can be piped\n:type from_file: bool\n:return: generated fmcalc command as str",
      "content": "# File: oasislmf/execution/bash.py\n# function: get_main_cmd_il_stream (lines 1811-1855)\n\ndef get_main_cmd_il_stream(\n    cmd,\n    process_id,\n    il_alloc_rule,\n    fifo_dir='fifo/',\n    stderr_guard=True,\n    from_file=False,\n    fmpy=True,\n    fmpy_low_memory=False,\n    fmpy_sort_output=False,\n    step_flag='',\n    process_counter=None,\n):\n    \"\"\"\n    Gets the fmcalc ktools command insured losses stream\n    :param cmd: either gulcalc command stream or correlated output file\n    :type cmd: str\n    :param process_id: ID corresponding to thread\n    :type process_id: int\n    :param il_alloc_rule: insured loss allocation rule for fmcalc\n    :type il_alloc_rule: int\n    :param fifo_dir: path to fifo directory\n    :type fifo_dir: str\n    :param stderr_guard: send stderr output to log file\n    :type stderr_guard: bool\n    :param from_file: must be true if cmd is a file and false if it can be piped\n    :type from_file: bool\n    :return: generated fmcalc command as str\n    \"\"\"\n\n    il_fifo_name = get_fifo_name(fifo_dir, RUNTYPE_INSURED_LOSS, process_id)\n\n    if from_file:\n        main_cmd = f'{get_fmcmd(fmpy, fmpy_low_memory, fmpy_sort_output)} -a{il_alloc_rule}{step_flag} < {cmd} > {il_fifo_name}'\n    else:\n        # need extra space at the end to pass test\n        main_cmd = f'{cmd} | {get_fmcmd(fmpy, fmpy_low_memory, fmpy_sort_output)} -a{il_alloc_rule}{step_flag} > {il_fifo_name} '\n\n    main_cmd = f'( {main_cmd} ) 2>> $LOG_DIR/stderror.err' if stderr_guard else f'{main_cmd}'\n    main_cmd = f'( {main_cmd} ) &'\n\n    if process_counter is not None:\n        main_cmd = add_pid_to_shell_command(main_cmd, process_counter)\n\n    return main_cmd\n\n\"\"\"Docstring (excerpt)\"\"\"\nGets the fmcalc ktools command insured losses stream\n:param cmd: either gulcalc command stream or correlated output file\n:type cmd: str\n:param process_id: ID corresponding to thread\n:type process_id: int\n:param il_alloc_rule: insured loss allocation rule for fmcalc\n:type il_alloc_rule: int\n:param fifo_dir: path to fifo directory\n:type fifo_dir: str\n:param stderr_guard: send stderr output to log file\n:type stderr_guard: bool\n:param from_file: must be true if cmd is a file and false if it can be piped\n:type from_file: bool\n:return: generated fmcalc command as str"
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::get_main_cmd_gul_stream@1858",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "get_main_cmd_gul_stream",
      "lineno": 1858,
      "end_lineno": 1888,
      "business_stage": "other",
      "docstring": "Gets the command to output ground up losses\n:param cmd: either gulcalc command stream or correlated output file\n:type cmd: str\n:param process_id: ID corresponding to thread\n:type process_id: int\n:param fifo_dir: path to fifo directory\n:type fifo_dir: str\n:param stderr_guard: send stderr output to log file\n:type stderr_guard: bool\n:param consumer: optional name of the consumer of the stream\n:type consumer: string\n:return: generated command as str",
      "content": "# File: oasislmf/execution/bash.py\n# function: get_main_cmd_gul_stream (lines 1858-1888)\n\ndef get_main_cmd_gul_stream(\n    cmd,\n    process_id,\n    fifo_dir='fifo/',\n    stderr_guard=True,\n    consumer='',\n    process_counter=None,\n):\n    \"\"\"\n    Gets the command to output ground up losses\n    :param cmd: either gulcalc command stream or correlated output file\n    :type cmd: str\n    :param process_id: ID corresponding to thread\n    :type process_id: int\n    :param fifo_dir: path to fifo directory\n    :type fifo_dir: str\n    :param stderr_guard: send stderr output to log file\n    :type stderr_guard: bool\n    :param consumer: optional name of the consumer of the stream\n    :type consumer: string\n    :return: generated command as str\n    \"\"\"\n    gul_fifo_name = get_fifo_name(fifo_dir, RUNTYPE_GROUNDUP_LOSS, process_id, consumer)\n    main_cmd = f'{cmd} > {gul_fifo_name} '\n    main_cmd = f'( {main_cmd} ) 2>> $LOG_DIR/stderror.err' if stderr_guard else f'{main_cmd}'\n    main_cmd = f'( {main_cmd} ) & '\n\n    if process_counter is not None:\n        main_cmd = add_pid_to_shell_command(main_cmd, process_counter)\n\n    return main_cmd\n\n\"\"\"Docstring (excerpt)\"\"\"\nGets the command to output ground up losses\n:param cmd: either gulcalc command stream or correlated output file\n:type cmd: str\n:param process_id: ID corresponding to thread\n:type process_id: int\n:param fifo_dir: path to fifo directory\n:type fifo_dir: str\n:param stderr_guard: send stderr output to log file\n:type stderr_guard: bool\n:param consumer: optional name of the consumer of the stream\n:type consumer: string\n:return: generated command as str"
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::get_pla_cmd@1979",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "get_pla_cmd",
      "lineno": 1979,
      "end_lineno": 1999,
      "business_stage": "other",
      "docstring": "Determine whether Post Loss Amplification should be implemented and issue\nplapy command.\n\nArgs:\n    pla (bool): flag to apply post loss amplification\n    secondary_factor (float): secondary factor to apply to post loss\n      amplification\n    uniform_factor (float): uniform factor to apply across all losses\n\nReturns:\n    pla_cmd (str): post loss amplification command",
      "content": "# File: oasislmf/execution/bash.py\n# function: get_pla_cmd (lines 1979-1999)\n\ndef get_pla_cmd(pla, secondary_factor, uniform_factor):\n    \"\"\"\n    Determine whether Post Loss Amplification should be implemented and issue\n    plapy command.\n\n    Args:\n        pla (bool): flag to apply post loss amplification\n        secondary_factor (float): secondary factor to apply to post loss\n          amplification\n        uniform_factor (float): uniform factor to apply across all losses\n\n    Returns:\n        pla_cmd (str): post loss amplification command\n    \"\"\"\n    pla_cmd = ' | plapy' * pla\n    if pla:\n        if uniform_factor > 0:\n            pla_cmd += f' -F {uniform_factor}'\n        elif secondary_factor != 1:\n            pla_cmd += f' -f {secondary_factor}'\n    return pla_cmd\n\n\"\"\"Docstring (excerpt)\"\"\"\nDetermine whether Post Loss Amplification should be implemented and issue\nplapy command.\n\nArgs:\n    pla (bool): flag to apply post loss amplification\n    secondary_factor (float): secondary factor to apply to post loss\n      amplification\n    uniform_factor (float): uniform factor to apply across all losses\n\nReturns:\n    pla_cmd (str): post loss amplification command"
    },
    {
      "chunk_id": "oasislmf/execution/bash.py::genbash@2983",
      "source_type": "code",
      "path": "oasislmf/execution/bash.py",
      "symbol_type": "function",
      "name": "genbash",
      "lineno": 2983,
      "end_lineno": 3130,
      "business_stage": "other",
      "docstring": "Generates a bash script containing ktools calculation instructions for an\nOasis model.\n\n:param max_process_id: The number of processes to create\n:type max_process_id: int\n\n:param analysis_settings: The analysis settings\n:type analysis_settings: dict\n\n:param filename: The output file name\n:type filename: string\n\n:param num_reinsurance_iterations: The number of reinsurance iterations\n:type num_reinsurance_iterations: int\n\n:param fifo_tmp_dir: When set to True, Create and use FIFO quese in `/tmp/[A-Z,0-9]/fifo`, if False run in './fifo'\n:type fifo_tmp_dir: boolean\n\n:param gul_alloc_rule: Allocation rule (None or 1) for gulcalc, if not set default to coverage stream\n:type gul_alloc_rule: Int\n\n:param il_alloc_rule: Allocation rule (0, 1 or 2) for fmcalc\n:type il_alloc_rule: Int\n\n:param ri_alloc_rule: Allocation rule (0, 1 or 2) for fmcalc\n:type ri_alloc_rule: Int\n\n:param num_gul_in_calc_block: number of gul in calc block\n:type num_gul_in_calc_block: Int\n\n:param num_fm_in_calc_block: number of gul in calc block\n:type num_fm_in_calc_block: Int\n\n:param get_getmodel_cmd: Method for getting the getmodel command, by default\n    ``GenerateLossesCmd.get_getmodel_cmd`` is used.\n:type get_getmodel_cmd: callable\n\n:param base_df_engine: The engine to use when loading dataframes.\n:type  base_df_engine: str\n\n:param model_df_engine: The engine to use when loading model dataframes.\n:type  model_df_engine: str",
      "content": "# File: oasislmf/execution/bash.py\n# function: genbash (lines 2983-3130)\n\ndef genbash(\n    max_process_id,\n    analysis_settings,\n    num_reinsurance_iterations=0,\n    fifo_tmp_dir=True,\n    gul_alloc_rule=None,\n    il_alloc_rule=None,\n    ri_alloc_rule=None,\n    num_gul_per_lb=None,\n    num_fm_per_lb=None,\n    stderr_guard=True,\n    gul_legacy_stream=False,\n    bash_trace=False,\n    filename='run_kools.sh',\n    _get_getmodel_cmd=None,\n    custom_gulcalc_log_start=None,\n    custom_gulcalc_log_finish=None,\n    custom_args={},\n    fmpy=True,\n    fmpy_low_memory=False,\n    fmpy_sort_output=False,\n    event_shuffle=None,\n    evepy=False,\n    modelpy=False,\n    gulpy=False,\n    gulpy_random_generator=1,\n    gulmc=False,\n    gulmc_random_generator=1,\n    gulmc_effective_damageability=False,\n    gulmc_vuln_cache_size=200,\n    model_py_server=False,\n    peril_filter=[],\n    summarypy=False,\n    join_summary_info=False,\n    eltpy=False,\n    pltpy=False,\n    aalpy=False,\n    lecpy=False,\n    base_df_engine='oasis_data_manager.df_reader.reader.OasisPandasReader',\n    model_df_engine=None,\n    dynamic_footprint=False,\n    analysis_pk=None,\n    socket_server=None\n):\n    \"\"\"\n    Generates a bash script containing ktools calculation instructions for an\n    Oasis model.\n\n    :param max_process_id: The number of processes to create\n    :type max_process_id: int\n\n    :param analysis_settings: The analysis settings\n    :type analysis_settings: dict\n\n    :param filename: The output file name\n    :type filename: string\n\n    :param num_reinsurance_iterations: The number of reinsurance iterations\n    :type num_reinsurance_iterations: int\n\n    :param fifo_tmp_dir: When set to True, Create and use FIFO quese in `/tmp/[A-Z,0-9]/fifo`, if False run in './fifo'\n    :type fifo_tmp_dir: boolean\n\n    :param gul_alloc_rule: Allocation rule (None or 1) for gulcalc, if not set default to coverage stream\n    :type gul_alloc_rule: Int\n\n    :param il_alloc_rule: Allocation rule (0, 1 or 2) for fmcalc\n    :type il_alloc_rule: Int\n\n    :param ri_alloc_rule: Allocation rule (0, 1 or 2) for fmcalc\n    :type ri_alloc_rule: Int\n\n    :param num_gul_in_calc_block: number of gul in calc block\n    :type num_gul_in_calc_block: Int\n\n    :param num_fm_in_calc_block: number of gul in calc block\n    :type num_fm_in_calc_block: Int\n\n    :param get_getmodel_cmd: Method for getting the getmodel command, by default\n        ``GenerateLossesCmd.get_getmodel_cmd`` is used.\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerates a bash script containing ktools calculation instructions for an\nOasis model.\n\n:param max_process_id: The number of processes to create\n:type max_process_id: int\n\n:param analysis_settings: The analysis settings\n:type analysis_settings: dict\n\n:param filename: The output file name\n:type filename: string\n\n:param num_reinsurance_iterations: The number of reinsurance iterations\n:type num_reinsurance_iterations: int\n\n:param fifo_tmp_dir: When set to True, Create and use FIFO quese in `/tmp/[A-Z,0-9]/fifo`, if False run in './fifo'\n:type fifo_tmp_dir: boolean\n\n:param gul_alloc_rule: Allocation rule (None or 1) for gulcalc, if not set default to coverage stream\n:type gul_alloc_rule: Int\n\n:param il_alloc_rule: Allocation rule (0, 1 or 2) for fmcalc\n:type il_alloc_rule: Int\n\n:param ri_alloc_rule: Allocation rule (0, 1 or 2) for fmcalc\n:type ri_alloc_rule: Int\n\n:param num_gul_in_calc_block: number of gul in calc block\n:type num_gul_in_calc_block: Int\n\n:param num_fm_in_calc_block: number of gul in calc block\n:type num_fm_in_calc_block: Int\n\n:param get_getmodel_cmd: Method for getting the getmodel command, by default\n    ``GenerateLossesCmd.get_getmodel_cmd`` is used.\n:type get_getmodel_cmd: callable\n\n:param base_df_engine: The engine to use when loading dataframes.\n:type  base_df_engine: str\n\n:param model_df_engine: The engine to use when loading model dataframes.\n:type  model_df_engine: str"
    },
    {
      "chunk_id": "oasislmf/execution/bin.py::prepare_run_directory@52",
      "source_type": "code",
      "path": "oasislmf/execution/bin.py",
      "symbol_type": "function",
      "name": "prepare_run_directory",
      "lineno": 52,
      "end_lineno": 236,
      "business_stage": "other",
      "docstring": "Ensures that the model run directory has the correct folder structure in\norder for the model run script (ktools) to be executed. Without the RI\nflag the model run directory will have the following structure\n\n::\n\n    <run_directory>\n    |-- fifo/\n    |-- input/\n    |   `-- csv/\n    |-- output/\n    |-- static/\n    |-- work/\n    |-- analysis_settings.json\n    `-- run_ktools.sh\n\n\nwhere the direct GUL and/or FM input files exist in the ``input/csv``\nsubfolder and the corresponding binaries exist in the ``input`` subfolder.\n\nWith the RI flag the model run directory has the following structure\n\n::\n    <run_directory>\n    |-- fifo\n    |-- input\n        |-- RI_1\n        |-- RI_2\n        |-- ri_layers.json\n    |-- ...\n    |-- output\n    |-- static\n    |-- work\n    |-- analysis_settings.json\n    `-- run_ktools.sh\n\nwhere the direct GUL and/or FM input files, and the corresponding binaries\nexist in the ``input`` subfolder, and the RI layer input files and binaries\nexist in the ``RI`` prefixed subfolders.\n\nIf any subfolders are missing they are created.\n\nOptionally, if the path to a set of Oasis files is provided then they\nare copied into the ``input/csv`` subfolder.\n\nOptionally, if the path to the analysis settings JSON file is provided\nthen it is copied to the base of the run directory.\n\nOptionally, if the path to model data is provided then the files are\nsymlinked into the ``static`` subfolder provided the OS is of type\nDarwin or Linux, otherwise the source folder tree is recursively\ncopied into the ``static`` subfolder.\n\n:param run_dir: the model run directory\n:type run_dir: str\n\n:param oasis_src_fp: path to a set of Oasis files\n:type oasis_src_fp: str\n\n:param ri: Boolean flag for RI mode\n:type ri: bool\n\n:param analysis_settings_fp: analysis settings JSON file path\n:type analysis_settings_fp: str\n\n:param model_data_fp: model data source path, if this is a file it will be loaded as a\n    custom storage class\n:type model_data_fp: str\n\n:param inputs_archive: path to a tar file containing input files\n:type inputs_archive: str\n\n:param user_data_dir: path to a directory containing additional user-supplied model data\n:type user_data_dir: str\n\n:param model_storage_config_fp: path to the model storage configuration, if not present\n    the model data will be copied to the static directory\n:type model_storage_config_fp: str",
      "content": "# File: oasislmf/execution/bin.py\n# function: prepare_run_directory (lines 52-236)\n\ndef prepare_run_directory(\n    run_dir,\n    oasis_src_fp,\n    model_data_fp,\n    analysis_settings_fp,\n    inputs_archive=None,\n    user_data_dir=None,\n    ri=False,\n    copy_model_data=False,\n    model_storage_config_fp=None\n):\n    \"\"\"\n    Ensures that the model run directory has the correct folder structure in\n    order for the model run script (ktools) to be executed. Without the RI\n    flag the model run directory will have the following structure\n\n    ::\n\n        <run_directory>\n        |-- fifo/\n        |-- input/\n        |   `-- csv/\n        |-- output/\n        |-- static/\n        |-- work/\n        |-- analysis_settings.json\n        `-- run_ktools.sh\n\n\n    where the direct GUL and/or FM input files exist in the ``input/csv``\n    subfolder and the corresponding binaries exist in the ``input`` subfolder.\n\n    With the RI flag the model run directory has the following structure\n\n    ::\n        <run_directory>\n        |-- fifo\n        |-- input\n            |-- RI_1\n            |-- RI_2\n            |-- ri_layers.json\n        |-- ...\n        |-- output\n        |-- static\n        |-- work\n        |-- analysis_settings.json\n        `-- run_ktools.sh\n\n    where the direct GUL and/or FM input files, and the corresponding binaries\n    exist in the ``input`` subfolder, and the RI layer input files and binaries\n    exist in the ``RI`` prefixed subfolders.\n\n    If any subfolders are missing they are created.\n\n    Optionally, if the path to a set of Oasis files is provided then they\n    are copied into the ``input/csv`` subfolder.\n\n    Optionally, if the path to the analysis settings JSON file is provided\n    then it is copied to the base of the run directory.\n\n    Optionally, if the path to model data is provided then the files are\n    symlinked into the ``static`` subfolder provided the OS is of type\n    Darwin or Linux, otherwise the source folder tree is recursively\n    copied into the ``static`` subfolder.\n\n    :param run_dir: the model run directory\n    :type run_dir: str\n\n    :param oasis_src_fp: path to a set of Oasis files\n    :type oasis_src_fp: str\n\n    :param ri: Boolean flag for RI mode\n    :type ri: bool\n\n    :param analysis_settings_fp: analysis settings JSON file path\n    :type analysis_settings_fp: str\n\n    :param model_data_fp: model data source path, if this is a file it will be loaded as a\n        custom storage class\n    :type model_data_fp: str\n\n\"\"\"Docstring (excerpt)\"\"\"\nEnsures that the model run directory has the correct folder structure in\norder for the model run script (ktools) to be executed. Without the RI\nflag the model run directory will have the following structure\n\n::\n\n    <run_directory>\n    |-- fifo/\n    |-- input/\n    |   `-- csv/\n    |-- output/\n    |-- static/\n    |-- work/\n    |-- analysis_settings.json\n    `-- run_ktools.sh\n\n\nwhere the direct GUL and/or FM input files exist in the ``input/csv``\nsubfolder and the corresponding binaries exist in the ``input`` subfolder.\n\nWith the RI flag the model run directory has the following structure\n\n::\n    <run_directory>\n    |-- fifo\n    |-- input\n        |-- RI_1\n        |-- RI_2\n        |-- ri_layers.json\n    |-- ...\n    |-- output\n    |-- static\n    |-- work\n    |-- analysis_settings.json\n    `-- run_ktools.sh\n\nwhere the direct GUL and/or FM input files, and the corresponding binaries\nexist in the ``input`` subfolder, and the RI layer input files and binaries\nexist in the ``RI`` prefixed subfolders.\n\nIf any subfolders are missing they are created.\n\nOptionally, if the path to a set of Oasis files is provided then they\nare copied into the ``input/csv`` subfolder.\n\nOptionally, if the path to the analysis settings JSON file is provided\nthen it is copied to the base of the run directory.\n\nOptionally, if the path to model data is provided then the files are\nsymlinked into the ``static`` subfolder provided the OS is of type\nDarwin or Linux, otherwise the source folder tree is recursively\ncopied into the ``static`` subfolder.\n\n:param run_dir: the model run directory\n:type run_dir: str\n\n:param oasis_src_fp: path to a set of Oasis files\n:type oasis_src_fp: str\n\n:param ri: Boolean flag for RI mode\n:type ri: bool\n\n:param analysis_settings_fp: analysis settings JSON file path\n:type analysis_settings_fp: str\n\n:param model_data_fp: model data source path, if this is a file it will be loaded as a\n    custom storage class\n:type model_data_fp: str\n\n:param inputs_archive: path to a tar file containing input files\n:type inputs_archive: str\n\n:param user_data_dir: path to a directory containing additional user-supplied model data\n:type user_data_dir: str\n\n:param model_storage_config_fp: path to the model storage configuration, if not present\n    the model data will be copied to the static directory\n:type model_storage_config_fp: str"
    },
    {
      "chunk_id": "oasislmf/execution/bin.py::get_event_range@359",
      "source_type": "code",
      "path": "oasislmf/execution/bin.py",
      "symbol_type": "function",
      "name": "get_event_range",
      "lineno": 359,
      "end_lineno": 379,
      "business_stage": "other",
      "docstring": "Parses event range string and returns a list of event ids.\n\n:param event_range: string representation of event range in format '1-5,10,89-100'\n:type event_range: str",
      "content": "# File: oasislmf/execution/bin.py\n# function: get_event_range (lines 359-379)\n\ndef get_event_range(event_range):\n    \"\"\"\n    Parses event range string and returns a list of event ids.\n\n    :param event_range: string representation of event range in format '1-5,10,89-100'\n    :type event_range: str\n    \"\"\"\n    lst_event_range = event_range.split(\",\")\n    lst_events = []\n\n    for er in lst_event_range:\n        if '-' in er:\n            rng = er.split('-')\n            e_from = int(rng[0])\n            e_to = int(rng[1])\n            lst_events.extend(range(e_from, e_to + 1))\n        else:\n            e = int(er)\n            lst_events.append(e)\n\n    return (lst_events)\n\n\"\"\"Docstring (excerpt)\"\"\"\nParses event range string and returns a list of event ids.\n\n:param event_range: string representation of event range in format '1-5,10,89-100'\n:type event_range: str"
    },
    {
      "chunk_id": "oasislmf/execution/bin.py::prepare_run_inputs@383",
      "source_type": "code",
      "path": "oasislmf/execution/bin.py",
      "symbol_type": "function",
      "name": "prepare_run_inputs",
      "lineno": 383,
      "end_lineno": 447,
      "business_stage": "other",
      "docstring": "Sets up binary files in the model inputs directory.\n\n:param analysis_settings: model analysis settings dict\n:type analysis_settings: dict\n\n:param run_dir: model run directory\n:type run_dir: str",
      "content": "# File: oasislmf/execution/bin.py\n# function: prepare_run_inputs (lines 383-447)\n\ndef prepare_run_inputs(analysis_settings, run_dir, model_storage: BaseStorage, ri=False):\n    \"\"\"\n    Sets up binary files in the model inputs directory.\n\n    :param analysis_settings: model analysis settings dict\n    :type analysis_settings: dict\n\n    :param run_dir: model run directory\n    :type run_dir: str\n    \"\"\"\n    try:\n        model_settings = analysis_settings.get('model_settings', {})\n        # Prepare events.bin\n        if analysis_settings.get('event_ids') or analysis_settings.get('event_ranges'):\n            # Create events file from user input\n            event_ids = []\n            if analysis_settings.get('event_ids'):\n                event_ids.extend(analysis_settings.get('event_ids'))\n            if analysis_settings.get('event_ranges'):\n                event_range = get_event_range(analysis_settings.get('event_ranges'))\n                event_ids.extend(event_range)\n            event_ids = list(set(event_ids))\n            _create_events_bin(run_dir, event_ids)\n        else:\n            # copy selected event set from static\n            _prepare_input_bin(run_dir, 'events', model_settings, model_storage, setting_key='event_set', ri=ri)\n\n        # Prepare event_rates.csv\n        if model_storage.exists('event_rates.csv') or model_settings.get('event_rates_set'):\n            _prepare_input_bin(run_dir, 'event_rates', model_settings, model_storage, setting_key='event_rates_set', ri=ri, extension='csv')\n\n        # Prepare quantile.bin\n        if analysis_settings.get('quantiles'):\n            # 1. Create quantile file from user input\n            _create_quantile_bin(run_dir, analysis_settings.get('quantiles'))\n        elif _calc_selected(analysis_settings, ['plt_quantile', 'elt_quantile']):\n            # 2. copy quantile file from model data\n            if model_storage.exists('quantile.bin'):\n                _prepare_input_bin(run_dir, 'quantile', model_settings, model_storage, ri=ri)\n            else:\n                # 3. Create quantile file from package `_data/quantile.csv`\n                _load_default_quantile_bin(run_dir)\n\n        # Prepare occurrence / returnperiod depending on output calcs selected\n        if _leccalc_selected(analysis_settings):\n            if analysis_settings.get('return_periods'):\n                # Create return periods from user input\n                _create_return_period_bin(run_dir, analysis_settings.get('return_periods'))\n            else:\n                # copy return periods from static\n                _prepare_input_bin(run_dir, 'returnperiods', model_settings, model_storage)\n\n            _prepare_input_bin(run_dir, 'occurrence', model_settings, model_storage, setting_key='event_occurrence_id', ri=ri)\n        elif _calc_selected(analysis_settings, [\n            'pltcalc', 'aalcalc', 'aalcalcmeanonly', 'alt_period', 'alt_meanonly', 'elt_moment', 'elt_quantile',\n            'elt_sample', 'plt_moment', 'plt_quantile', 'plt_sample'\n        ]):\n            _prepare_input_bin(run_dir, 'occurrence', model_settings, model_storage, setting_key='event_occurrence_id', ri=ri)\n\n        # Prepare periods.bin\n        if model_storage.exists('periods.bin'):\n            _prepare_input_bin(run_dir, 'periods', model_settings, model_storage, ri=ri)\n\n    except (OSError, IOError) as e:\n        raise OasisException(\"Error preparing the model 'inputs' directory: {}\".format(e))\n\n\"\"\"Docstring (excerpt)\"\"\"\nSets up binary files in the model inputs directory.\n\n:param analysis_settings: model analysis settings dict\n:type analysis_settings: dict\n\n:param run_dir: model run directory\n:type run_dir: str"
    },
    {
      "chunk_id": "oasislmf/execution/bin.py::set_footprint_set@451",
      "source_type": "code",
      "path": "oasislmf/execution/bin.py",
      "symbol_type": "function",
      "name": "set_footprint_set",
      "lineno": 451,
      "end_lineno": 486,
      "business_stage": "other",
      "docstring": "Create symbolic link to footprint file set that will be used for output\ncalculation.\n\n:param setting_val: identifier for footprint set\n:type setting_val: string\n\n:param run_dir: model run directory\n:type run_dir: string",
      "content": "# File: oasislmf/execution/bin.py\n# function: set_footprint_set (lines 451-486)\n\ndef set_footprint_set(setting_val, run_dir):\n    \"\"\"\n    Create symbolic link to footprint file set that will be used for output\n    calculation.\n\n    :param setting_val: identifier for footprint set\n    :type setting_val: string\n\n    :param run_dir: model run directory\n    :type run_dir: string\n    \"\"\"\n    priorities = Footprint.get_footprint_fmt_priorities()\n    setting_val = str(setting_val)\n\n    for footprint_class in priorities:\n        for filename in footprint_class.footprint_filenames:\n            if '.' in filename:\n                stem, extension = filename.split('.', 1)\n                extension = '.' + extension\n            else:\n                stem = filename\n                extension = ''\n            footprint_fp = os.path.join(run_dir, 'static', f'{stem}_{setting_val}{extension}')\n            footprint_target_fp = os.path.join(run_dir, 'static', filename)\n            if not os.path.exists(footprint_fp):\n                # 'compatibility' - Fallback name formatting to keep existing conversion\n                setting_val_old = setting_val.replace(' ', '_').lower()\n                footprint_fp = os.path.join(run_dir, 'static', f'{stem}_{setting_val_old}{extension}')\n                if not os.path.exists(footprint_fp):\n                    logger.debug(f'{footprint_fp} not found, moving on to next footprint class')\n                    break\n            os.symlink(footprint_fp, footprint_target_fp)\n        else:\n            return\n\n    raise OasisException(f'Could not find footprint data files with identifier \"{setting_val}\"')\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreate symbolic link to footprint file set that will be used for output\ncalculation.\n\n:param setting_val: identifier for footprint set\n:type setting_val: string\n\n:param run_dir: model run directory\n:type run_dir: string"
    },
    {
      "chunk_id": "oasislmf/execution/bin.py::set_vulnerability_set@490",
      "source_type": "code",
      "path": "oasislmf/execution/bin.py",
      "symbol_type": "function",
      "name": "set_vulnerability_set",
      "lineno": 490,
      "end_lineno": 531,
      "business_stage": "other",
      "docstring": "Create symbolic link to vulnerability file set that will be used for output\ncalculation.\n\n:param setting_val: identifier for vulnerability set\n:type setting_val: string\n\n:param run_dir: model run directory\n:type run_dir: string",
      "content": "# File: oasislmf/execution/bin.py\n# function: set_vulnerability_set (lines 490-531)\n\ndef set_vulnerability_set(setting_val, run_dir):\n    \"\"\"\n    Create symbolic link to vulnerability file set that will be used for output\n    calculation.\n\n    :param setting_val: identifier for vulnerability set\n    :type setting_val: string\n\n    :param run_dir: model run directory\n    :type run_dir: string\n    \"\"\"\n\n    vulnerability_formats = ['bin', 'parquet', 'csv']\n    setting_val = str(setting_val)\n\n    for file_format in vulnerability_formats:\n\n        if file_format == 'parquet':\n            base_name, format_extension = vulnerability_dataset.split('.', 1)\n            base_meta_name, _ = parquetvulnerability_meta_filename.split('.', 1)\n            # For Parquet, check if it's a directory\n            vulnerability_fp = os.path.join(run_dir, 'static', f'{base_name}_{setting_val}.{format_extension}')\n            vulnerability_target_fp = os.path.join(run_dir, 'static', f'{vulnerability_dataset}')\n            vulnerability_meta = os.path.join(run_dir, 'static', f'{base_meta_name}_{setting_val}.json')\n            vulnerability_meta_target = os.path.join(run_dir, 'static', f'{parquetvulnerability_meta_filename}')\n            if os.path.isdir(vulnerability_fp) and os.path.isfile(vulnerability_meta):\n                os.symlink(vulnerability_fp, vulnerability_target_fp)\n                os.symlink(vulnerability_meta, vulnerability_meta_target)\n                return\n            else:\n                logger.debug(f'{vulnerability_fp} and/or {vulnerability_meta} not found, trying next format')\n        else:\n            # For other file formats, check if it's a file\n            vulnerability_fp = os.path.join(run_dir, 'static', f'vulnerability_{setting_val}.{file_format}')\n            vulnerability_target_fp = os.path.join(run_dir, 'static', f'vulnerability.{file_format}')\n            if os.path.isfile(vulnerability_fp):\n                os.symlink(vulnerability_fp, vulnerability_target_fp)\n                return\n            else:\n                logger.debug(f'{vulnerability_fp} not found, trying next format')\n\n    raise OasisException(f'Could not find vulnerability data files with identifier \"{setting_val}\"')\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreate symbolic link to vulnerability file set that will be used for output\ncalculation.\n\n:param setting_val: identifier for vulnerability set\n:type setting_val: string\n\n:param run_dir: model run directory\n:type run_dir: string"
    },
    {
      "chunk_id": "oasislmf/execution/bin.py::check_inputs_directory@549",
      "source_type": "code",
      "path": "oasislmf/execution/bin.py",
      "symbol_type": "function",
      "name": "check_inputs_directory",
      "lineno": 549,
      "end_lineno": 570,
      "business_stage": "other",
      "docstring": "Check that all the required files are present in the directory.\n\n:param directory_to_check: directory containing the CSV files\n:type directory_to_check: string\n\n:param il: check insuured loss files\n:type il: bool\n\n:param il: check resinsurance sub-folders\n:type il: bool\n\n:param check_binaries: check binary files are not present\n:type check_binaries: bool",
      "content": "# File: oasislmf/execution/bin.py\n# function: check_inputs_directory (lines 549-570)\n\ndef check_inputs_directory(directory_to_check, il=False, ri=False, check_binaries=True):\n    \"\"\"\n    Check that all the required files are present in the directory.\n\n    :param directory_to_check: directory containing the CSV files\n    :type directory_to_check: string\n\n    :param il: check insuured loss files\n    :type il: bool\n\n    :param il: check resinsurance sub-folders\n    :type il: bool\n\n    :param check_binaries: check binary files are not present\n    :type check_binaries: bool\n    \"\"\"\n    # Check the top level directory, that containes the core files and any direct FM files\n    _check_each_inputs_directory(directory_to_check, il=il, check_binaries=check_binaries)\n\n    if ri:\n        for ri_directory_to_check in glob.glob('{}{}RI_\\d+$'.format(directory_to_check, os.path.sep)):\n            _check_each_inputs_directory(ri_directory_to_check, il=True, check_binaries=check_binaries)\n\n\"\"\"Docstring (excerpt)\"\"\"\nCheck that all the required files are present in the directory.\n\n:param directory_to_check: directory containing the CSV files\n:type directory_to_check: string\n\n:param il: check insuured loss files\n:type il: bool\n\n:param il: check resinsurance sub-folders\n:type il: bool\n\n:param check_binaries: check binary files are not present\n:type check_binaries: bool"
    },
    {
      "chunk_id": "oasislmf/execution/bin.py::csv_to_bin@595",
      "source_type": "code",
      "path": "oasislmf/execution/bin.py",
      "symbol_type": "function",
      "name": "csv_to_bin",
      "lineno": 595,
      "end_lineno": 623,
      "business_stage": "other",
      "docstring": "Create the binary files.\n\n:param csv_directory: the directory containing the CSV files\n:type csv_directory: str\n\n:param bin_directory: the directory to write the binary files\n:type bin_directory: str\n\n:param il: whether to create the binaries required for insured loss calculations\n:type il: bool\n\n:param ri: whether to create the binaries required for reinsurance calculations\n:type ri: bool\n\n:raises OasisException: If one of the conversions fails",
      "content": "# File: oasislmf/execution/bin.py\n# function: csv_to_bin (lines 595-623)\n\ndef csv_to_bin(csv_directory, bin_directory, il=False, ri=False):\n    \"\"\"\n    Create the binary files.\n\n    :param csv_directory: the directory containing the CSV files\n    :type csv_directory: str\n\n    :param bin_directory: the directory to write the binary files\n    :type bin_directory: str\n\n    :param il: whether to create the binaries required for insured loss calculations\n    :type il: bool\n\n    :param ri: whether to create the binaries required for reinsurance calculations\n    :type ri: bool\n\n    :raises OasisException: If one of the conversions fails\n    \"\"\"\n    csvdir = os.path.abspath(csv_directory)\n    bindir = os.path.abspath(bin_directory)\n\n    il = il or ri\n\n    _csv_to_bin(csvdir, bindir, il)\n\n    if ri:\n        for ri_csvdir in glob.glob('{}{}RI_[0-9]*'.format(csvdir, os.sep)):\n            _csv_to_bin(\n                ri_csvdir, os.path.join(bindir, os.path.basename(ri_csvdir)), il=True)\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreate the binary files.\n\n:param csv_directory: the directory containing the CSV files\n:type csv_directory: str\n\n:param bin_directory: the directory to write the binary files\n:type bin_directory: str\n\n:param il: whether to create the binaries required for insured loss calculations\n:type il: bool\n\n:param ri: whether to create the binaries required for reinsurance calculations\n:type ri: bool\n\n:raises OasisException: If one of the conversions fails"
    },
    {
      "chunk_id": "oasislmf/execution/bin.py::check_binary_tar_file@667",
      "source_type": "code",
      "path": "oasislmf/execution/bin.py",
      "symbol_type": "function",
      "name": "check_binary_tar_file",
      "lineno": 667,
      "end_lineno": 694,
      "business_stage": "other",
      "docstring": "Checks that all required files are present\n\n:param tar_file_path: Path to the tar file to check\n:type tar_file_path: str\n\n:param check_il: Flag whether to check insured loss files\n:type check_il: bool\n\n:raises OasisException: If a required file is missing\n\n:return: True if all required files are present, False otherwise\n:rtype: bool",
      "content": "# File: oasislmf/execution/bin.py\n# function: check_binary_tar_file (lines 667-694)\n\ndef check_binary_tar_file(tar_file_path, check_il=False):\n    \"\"\"\n    Checks that all required files are present\n\n    :param tar_file_path: Path to the tar file to check\n    :type tar_file_path: str\n\n    :param check_il: Flag whether to check insured loss files\n    :type check_il: bool\n\n    :raises OasisException: If a required file is missing\n\n    :return: True if all required files are present, False otherwise\n    :rtype: bool\n    \"\"\"\n    expected_members = ('{}.bin'.format(f['name']) for f in GUL_INPUT_FILES.values())\n\n    if check_il:\n        expected_members = chain(expected_members, ('{}.bin'.format(f['name']) for f in IL_INPUT_FILES.values()))\n\n    with tarfile.open(tar_file_path) as tar:\n        for member in expected_members:\n            try:\n                tar.getmember(member)\n            except KeyError:\n                raise OasisException('{} is missing from the tar file {}.'.format(member, tar_file_path))\n\n    return True\n\n\"\"\"Docstring (excerpt)\"\"\"\nChecks that all required files are present\n\n:param tar_file_path: Path to the tar file to check\n:type tar_file_path: str\n\n:param check_il: Flag whether to check insured loss files\n:type check_il: bool\n\n:raises OasisException: If a required file is missing\n\n:return: True if all required files are present, False otherwise\n:rtype: bool"
    },
    {
      "chunk_id": "oasislmf/execution/bin.py::create_binary_tar_file@698",
      "source_type": "code",
      "path": "oasislmf/execution/bin.py",
      "symbol_type": "function",
      "name": "create_binary_tar_file",
      "lineno": 698,
      "end_lineno": 713,
      "business_stage": "other",
      "docstring": "Package the binaries in a gzipped tar.\n\n:param directory: Path containing the binaries\n:type tar_file_path: str",
      "content": "# File: oasislmf/execution/bin.py\n# function: create_binary_tar_file (lines 698-713)\n\ndef create_binary_tar_file(directory):\n    \"\"\"\n    Package the binaries in a gzipped tar.\n\n    :param directory: Path containing the binaries\n    :type tar_file_path: str\n    \"\"\"\n    with tarfile.open(os.path.join(directory, TAR_FILE), \"w:gz\") as tar:\n        for f in glob.glob('{}*{}*.bin'.format(directory, os.sep)):\n            logging.info(\"Adding {} {}\".format(f, os.path.relpath(f, directory)))\n            relpath = os.path.relpath(f, directory)\n            tar.add(f, arcname=relpath)\n\n        for f in glob.glob('{}*{}*{}*.bin'.format(directory, os.sep, os.sep)):\n            relpath = os.path.relpath(f, directory)\n            tar.add(f, arcname=relpath)\n\n\"\"\"Docstring (excerpt)\"\"\"\nPackage the binaries in a gzipped tar.\n\n:param directory: Path containing the binaries\n:type tar_file_path: str"
    },
    {
      "chunk_id": "oasislmf/execution/bin.py::check_conversion_tools@717",
      "source_type": "code",
      "path": "oasislmf/execution/bin.py",
      "symbol_type": "function",
      "name": "check_conversion_tools",
      "lineno": 717,
      "end_lineno": 739,
      "business_stage": "other",
      "docstring": "Check that the conversion tools are available\n\n:param il: Flag whether to check insured loss tools\n:type il: bool\n\n:return: True if all required tools are present, False otherwise\n:rtype: bool",
      "content": "# File: oasislmf/execution/bin.py\n# function: check_conversion_tools (lines 717-739)\n\ndef check_conversion_tools(il=False):\n    \"\"\"\n    Check that the conversion tools are available\n\n    :param il: Flag whether to check insured loss tools\n    :type il: bool\n\n    :return: True if all required tools are present, False otherwise\n    :rtype: bool\n    \"\"\"\n    if il:\n        input_files = INPUT_FILES.values()\n    else:\n        input_files = (f for f in INPUT_FILES.values() if f['type'] != 'il')\n\n    for input_file in input_files:\n        tool = input_file['conversion_tool']\n        if shutilwhich.which(tool) is None:\n            error_message = \"Failed to find conversion tool: {}\".format(tool)\n            logging.error(error_message)\n            raise OasisException(error_message)\n\n    return True\n\n\"\"\"Docstring (excerpt)\"\"\"\nCheck that the conversion tools are available\n\n:param il: Flag whether to check insured loss tools\n:type il: bool\n\n:return: True if all required tools are present, False otherwise\n:rtype: bool"
    },
    {
      "chunk_id": "oasislmf/execution/bin.py::cleanup_bin_directory@743",
      "source_type": "code",
      "path": "oasislmf/execution/bin.py",
      "symbol_type": "function",
      "name": "cleanup_bin_directory",
      "lineno": 743,
      "end_lineno": 750,
      "business_stage": "other",
      "docstring": "Clean the tar and binary files.",
      "content": "# File: oasislmf/execution/bin.py\n# function: cleanup_bin_directory (lines 743-750)\n\ndef cleanup_bin_directory(directory):\n    \"\"\"\n    Clean the tar and binary files.\n    \"\"\"\n    for file in chain([TAR_FILE], (f + '.bin' for f in INPUT_FILES.keys())):\n        file_path = os.path.join(directory, file)\n        if os.path.exists(file_path):\n            os.remove(file_path)\n\n\"\"\"Docstring (excerpt)\"\"\"\nClean the tar and binary files."
    },
    {
      "chunk_id": "oasislmf/execution/conf.py::create_analysis_settings_json@39",
      "source_type": "code",
      "path": "oasislmf/execution/conf.py",
      "symbol_type": "function",
      "name": "create_analysis_settings_json",
      "lineno": 39,
      "end_lineno": 86,
      "business_stage": "other",
      "docstring": "Generate an analysis settings JSON from a set of\nCSV files in a specified directory.\nArgs:\n    ``directory`` (string): the directory containing the CSV files.\nReturns:\n    The analysis settings JSON.",
      "content": "# File: oasislmf/execution/conf.py\n# function: create_analysis_settings_json (lines 39-86)\n\ndef create_analysis_settings_json(directory):\n    \"\"\"\n    Generate an analysis settings JSON from a set of\n    CSV files in a specified directory.\n    Args:\n        ``directory`` (string): the directory containing the CSV files.\n    Returns:\n        The analysis settings JSON.\n    \"\"\"\n    if not os.path.exists(directory):\n        error_message = \"Directory does not exist: {}\".format(directory)\n        logging.getLogger(__name__).error(error_message)\n        raise OasisException(error_message)\n\n    general_settings_file = os.path.join(directory, GENERAL_SETTINGS_FILE)\n    model_settings_file = os.path.join(directory, MODEL_SETTINGS_FILE)\n    gul_summaries_file = os.path.join(directory, GUL_SUMMARIES_FILE)\n    il_summaries_file = os.path.join(directory, IL_SUMMARIES_FILE)\n\n    for file in [general_settings_file, model_settings_file, gul_summaries_file, il_summaries_file]:\n        if not os.path.exists(file):\n            error_message = \"File does not exist: {}\".format(directory)\n            logging.getLogger(__name__).error(error_message)\n            raise OasisException(error_message)\n\n    general_settings = dict()\n    with io.open(general_settings_file, 'r', encoding='utf-8') as csvfile:\n        reader = csv.reader(csvfile)\n        for row in reader:\n            general_settings[row[0]] = eval(\"{}('{}')\".format(row[2], row[1]))\n\n    model_settings = dict()\n    with io.open(model_settings_file, 'r', encoding='utf-8') as csvfile:\n        reader = csv.reader(csvfile)\n        for row in reader:\n            model_settings[row[0]] = eval(\"{}('{}')\".format(row[2], row[1]))\n\n    gul_summaries = _get_summaries(gul_summaries_file)\n    il_summaries = _get_summaries(il_summaries_file)\n\n    analysis_settings = general_settings\n    analysis_settings['model_settings'] = model_settings\n    analysis_settings['gul_summaries'] = gul_summaries\n    analysis_settings['il_summaries'] = il_summaries\n    output_json = json.dumps(analysis_settings)\n    logging.getLogger(__name__).info(\"Analysis settings json: {}\".format(output_json))\n\n    return output_json\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate an analysis settings JSON from a set of\nCSV files in a specified directory.\nArgs:\n    ``directory`` (string): the directory containing the CSV files.\nReturns:\n    The analysis settings JSON."
    },
    {
      "chunk_id": "oasislmf/execution/load_balancer.py::get_next_event_index@36",
      "source_type": "code",
      "path": "oasislmf/execution/load_balancer.py",
      "symbol_type": "function",
      "name": "get_next_event_index",
      "lineno": 36,
      "end_lineno": 63,
      "business_stage": "other",
      "docstring": "try to get the index of the end of the event\nif found return the index and 0 to indicate it is found\nif not found return the index of the last item parsed and the last event id\n\n:param sub: byte array to parse\n:param last_item_index: last index parsed\n:param last_event_id: last event idea parsed (0 means no event)\n:return: last index parsed, last event idea parsed (0 means the chunk sub[:last_item_index] is a full event",
      "content": "# File: oasislmf/execution/load_balancer.py\n# function: get_next_event_index (lines 36-63)\n\ndef get_next_event_index(read_buffer, last_event_index, last_event_id, max_cursor):\n    \"\"\"\n    try to get the index of the end of the event\n    if found return the index and 0 to indicate it is found\n    if not found return the index of the last item parsed and the last event id\n\n    :param sub: byte array to parse\n    :param last_item_index: last index parsed\n    :param last_event_id: last event idea parsed (0 means no event)\n    :return: last index parsed, last event idea parsed (0 means the chunk sub[:last_item_index] is a full event\n    \"\"\"\n    cursor = last_event_index\n    while cursor < max_cursor - 4:\n        cur_event_id = read_buffer[cursor]\n        if last_event_id != cur_event_id:\n            if last_event_id == 0:\n                last_event_id = read_buffer[cursor]\n            else:\n                return cursor, last_event_id, 1\n\n        cursor += 2\n        while cursor < max_cursor - 2:\n            sidx = read_buffer[cursor]\n            cursor += 2\n            if sidx == 0:\n                last_event_index = cursor\n                break\n    return last_event_index, last_event_id, 0\n\n\"\"\"Docstring (excerpt)\"\"\"\ntry to get the index of the end of the event\nif found return the index and 0 to indicate it is found\nif not found return the index of the last item parsed and the last event id\n\n:param sub: byte array to parse\n:param last_item_index: last index parsed\n:param last_event_id: last event idea parsed (0 means no event)\n:return: last index parsed, last event idea parsed (0 means the chunk sub[:last_item_index] is a full event"
    },
    {
      "chunk_id": "oasislmf/execution/load_balancer.py::balance@161",
      "source_type": "code",
      "path": "oasislmf/execution/load_balancer.py",
      "symbol_type": "function",
      "name": "balance",
      "lineno": 161,
      "end_lineno": 224,
      "business_stage": "other",
      "docstring": "Load balance events for a list of input fil_path to a list of output fil_path\n\n:param pipe_in: list of fil_path\n    fil_path to take as input\n:param pipe_out: list of fil_path\n    fil_path to take as input\n:param read_size: int\n    size of the maximum amount of Byte read from one input at a time\n:param queue_size: int\n    maximum size ofthe buffer queue",
      "content": "# File: oasislmf/execution/load_balancer.py\n# function: balance (lines 161-224)\n\ndef balance(pipe_in, pipe_out, read_size, write_size, queue_size):\n    \"\"\"\n    Load balance events for a list of input fil_path to a list of output fil_path\n\n    :param pipe_in: list of fil_path\n        fil_path to take as input\n    :param pipe_out: list of fil_path\n        fil_path to take as input\n    :param read_size: int\n        size of the maximum amount of Byte read from one input at a time\n    :param queue_size: int\n        maximum size ofthe buffer queue\n\n    \"\"\"\n    inputs = [open(p, 'rb') for p in pipe_in]\n    outputs = [open(p, 'wb') for p in pipe_out]\n\n    pipeline = queue.Queue(maxsize=queue_size)\n    sentinel = object()\n    stopper = np.zeros(1, dtype=np.bool)\n    try:\n        # check stream input header and write it to the stream output\n        headers = set([s.read(8) for s in inputs])\n        if len(headers) != 1:\n            raise Exception('input streams have different header type')\n        header = headers.pop()\n        [s.write(header) for s in outputs]\n        with concurrent.futures.ThreadPoolExecutor(max_workers=len(inputs) + len(outputs)) as executor:\n            producer_task = [executor.submit(producer, s, pipeline, read_size, stopper) for s in inputs]\n            consumer_task = [executor.submit(consumer, s, pipeline, write_size, sentinel, stopper) for s in outputs]\n            try:\n                prod_t = [t.result() for t in producer_task]\n            finally:\n                for _ in pipe_out:\n                    pipeline.put(sentinel)\n\n            wait_read_time, read_input_time, parse_input_time, buffer_management_time = 0, 0, 0, 0\n            for t in prod_t:\n                wait_read_time += t[0]\n                read_input_time += t[1]\n                parse_input_time += t[2]\n                buffer_management_time += t[3]\n\n            cons_t = [t.result() for t in consumer_task]\n\n            wait_write_time, write_output_time, wait_pipeline = 0, 0, 0\n            for t in cons_t:\n                wait_write_time += t[0]\n                write_output_time += t[1]\n                wait_pipeline += t[2]\n\n            logger.info(f\"\"\"\n    wait_read_time = {wait_read_time}, {wait_read_time / len(inputs)}\n    read_input_time = {read_input_time}, {read_input_time / len(inputs)}\n    parse_input_time = {parse_input_time}, {parse_input_time / len(inputs)}\n    buffer_management_time = {buffer_management_time}, {buffer_management_time / len(inputs)}\n    wait_write_time = {wait_write_time}, {wait_write_time / len(outputs)}\n    write_output_time = {write_output_time}, {write_output_time / len(outputs)}\n    wait_pipeline = {wait_pipeline}, {wait_pipeline / len(outputs)}\n    \"\"\")\n\n    finally:\n        [s.close() for s in inputs]\n        [s.close() for s in outputs]\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoad balance events for a list of input fil_path to a list of output fil_path\n\n:param pipe_in: list of fil_path\n    fil_path to take as input\n:param pipe_out: list of fil_path\n    fil_path to take as input\n:param read_size: int\n    size of the maximum amount of Byte read from one input at a time\n:param queue_size: int\n    maximum size ofthe buffer queue"
    },
    {
      "chunk_id": "oasislmf/execution/runner.py::rerun@114",
      "source_type": "code",
      "path": "oasislmf/execution/runner.py",
      "symbol_type": "function",
      "name": "rerun",
      "lineno": 114,
      "end_lineno": 158,
      "business_stage": "other",
      "docstring": "A function to find where an error was made and to rerun that part of the script without\nNumBa to give better error messages",
      "content": "# File: oasislmf/execution/runner.py\n# function: rerun (lines 114-158)\n\ndef rerun():\n    \"\"\"\n    A function to find where an error was made and to rerun that part of the script without\n    NumBa to give better error messages\n    \"\"\"\n    try:\n        with open(\"event_error.json\", \"r\") as f:\n            event_error = json.load(f).get(\"event_id\")\n    except FileNotFoundError:\n        return\n\n    env = os.environ.copy()\n    env['NUMBA_DISABLE_JIT'] = \"1\"\n    eve_cmd = f\"printf 'event_id\\n {event_error}\\n' | evetobin\"\n    ktools_pipeline = ''\n\n    with open(\"run_ktools.sh\", \"r\") as bash_script:\n        for line in bash_script:\n            if \"( ( eve\" in line:\n                ktools_pipeline = re.split(r'\\||\\)', line)\n                break\n\n    gul_cmd = [cmd.strip() for cmd in ktools_pipeline if cmd.strip().startswith(('gul'))].pop(0)\n    fm_cmds = [cmd.strip() for cmd in ktools_pipeline if cmd.strip().startswith(('fm'))]\n\n    pipe_output = \"/tmp/il_P1\"\n    summary_output = \"/tmp/il_S1_summary_P1\"\n    gul_output = f\"{event_error}_gul.bin\"\n\n    gul_pipe = f\"{eve_cmd} | {gul_cmd} -o {gul_output}\"\n    with open(\"gul_errors.log\", \"w\") as error_log:\n        subprocess.run(gul_pipe, shell=True, env=env, stderr=error_log)\n\n    fm_input = gul_output\n    for i in range(len(fm_cmds)):\n        fm_cmd = re.sub(r\"-\\s*>\\s*\\S+\", f\"-o 64_ri{i+1}.bin\", fm_cmds[i])\n        fm_output = f\"{event_error}_fm{i+1}.bin\"\n        fm_pipe = f\"{fm_cmd} -o {fm_output} -i {fm_input}\"\n        with open(\"fm_errors.log\", \"a\") as error_log:\n            subprocess.run(fm_pipe, shell=True, env=env, stderr=error_log)\n        fm_input = fm_output\n\n    summary_pipe = f\"summarypy -t il -m -1 {summary_output} < {fm_input}\"\n    with open(\"summary_errors.log\", \"w\") as error_log:\n        subprocess.run(summary_pipe, shell=True, env=env, stderr=error_log)\n\n\"\"\"Docstring (excerpt)\"\"\"\nA function to find where an error was made and to rerun that part of the script without\nNumBa to give better error messages"
    },
    {
      "chunk_id": "oasislmf/lookup/base.py::AbstractBasicKeyLookup@30",
      "source_type": "code",
      "path": "oasislmf/lookup/base.py",
      "symbol_type": "class",
      "name": "AbstractBasicKeyLookup",
      "lineno": 30,
      "end_lineno": 56,
      "business_stage": "other",
      "docstring": "Basic abstract class for KeyLookup",
      "content": "# File: oasislmf/lookup/base.py\n# class: AbstractBasicKeyLookup (lines 30-56)\n\nclass AbstractBasicKeyLookup:\n    \"\"\"Basic abstract class for KeyLookup\"\"\"\n\n    interface_version = \"1\"\n\n    def __init__(self, config, config_dir=None, user_data_dir=None, output_dir=None):\n        self.config = config\n        self.config_dir = config_dir or '.'\n        self.user_data_dir = user_data_dir\n        self.output_dir = output_dir\n\n        keys_data_path = config.get('keys_data_path')\n        keys_data_path = os.path.join(config_dir, keys_data_path) if keys_data_path else ''\n        config['keys_data_path'] = as_path(keys_data_path, 'keys_data_path', preexists=(True if keys_data_path else False))\n\n        if config.get(\"keys_data_storage\"):\n            self.storage = get_storage_from_config(config[\"keys_data_storage\"])\n        else:\n            self.storage = LocalStorage(config['keys_data_path'])\n\n    @abc.abstractmethod\n    def process_locations(self, locations):\n        \"\"\"\n        Process location rows - passed in as a pandas dataframe.\n        Results can be list, tuple, generator or a pandas dataframe.\n        \"\"\"\n        raise NotImplementedError\n\n\"\"\"Docstring (excerpt)\"\"\"\nBasic abstract class for KeyLookup"
    },
    {
      "chunk_id": "oasislmf/lookup/base.py::MultiprocLookupMixin@59",
      "source_type": "code",
      "path": "oasislmf/lookup/base.py",
      "symbol_type": "class",
      "name": "MultiprocLookupMixin",
      "lineno": 59,
      "end_lineno": 75,
      "business_stage": "other",
      "docstring": "Simple mixin class for multiprocessing\n\nimplement the process_locations_multiproc by transforming the result of process_locations into a pandas DataFrame",
      "content": "# File: oasislmf/lookup/base.py\n# class: MultiprocLookupMixin (lines 59-75)\n\nclass MultiprocLookupMixin:\n    \"\"\"\n    Simple mixin class for multiprocessing\n\n    implement the process_locations_multiproc by transforming the result of process_locations into a pandas DataFrame\n    \"\"\"\n\n    def process_locations_multiproc(self, loc_df_part):\n        result = self.process_locations(loc_df_part)\n        if isinstance(result, list) or isinstance(result, tuple):\n            return pd.DataFrame(result)\n        elif isinstance(result, types.GeneratorType):\n            return pd.DataFrame.from_records(result)\n        elif isinstance(result, pd.DataFrame):\n            return result\n        else:\n            raise OasisException(\"Unrecognised type for results: {type(results)}. expected \")\n\n\"\"\"Docstring (excerpt)\"\"\"\nSimple mixin class for multiprocessing\n\nimplement the process_locations_multiproc by transforming the result of process_locations into a pandas DataFrame"
    },
    {
      "chunk_id": "oasislmf/lookup/base.py::OasisBaseLookup@78",
      "source_type": "code",
      "path": "oasislmf/lookup/base.py",
      "symbol_type": "class",
      "name": "OasisBaseLookup",
      "lineno": 78,
      "end_lineno": 163,
      "business_stage": "other",
      "docstring": "Abstract class that help with the implementation of the KeyServerInterface.\nrequire lookup method to be implemented.\nLookup will be call to create a key for each peril id and coverage type",
      "content": "# File: oasislmf/lookup/base.py\n# class: OasisBaseLookup (lines 78-163)\n\nclass OasisBaseLookup(AbstractBasicKeyLookup, MultiprocLookupMixin):\n    \"\"\"\n    Abstract class that help with the implementation of the KeyServerInterface.\n    require lookup method to be implemented.\n    Lookup will be call to create a key for each peril id and coverage type\n    \"\"\"\n    multiproc_enabled = True\n\n    def __init__(self, config=None, config_json=None, config_fp=None, config_dir=None, user_data_dir=None, output_dir=None):\n        if config:\n            config_dir = config_dir or '.'\n        elif config_json:\n            config = json.loads(config_json)\n            config_dir = config_dir or '.'\n        elif config_fp:\n            config_dir = config_dir or os.path.dirname(config_fp)\n            _config_fp = as_path(config_fp, 'config_fp')\n            with open(_config_fp, 'r', encoding='utf-8') as f:\n                config = json.load(f)\n\n        super().__init__(config, config_dir=config_dir, user_data_dir=user_data_dir, output_dir=output_dir)\n\n        peril_config = self.config.get('peril') or {}\n\n        self.peril_ids = tuple(peril_config.get('peril_ids') or ())\n\n        self.peril_id_col = peril_config.get('peril_id_col') or 'peril_id'\n\n        coverage_config = self.config.get('coverage') or {}\n\n        self.coverage_types = tuple(coverage_config.get('coverage_types') or ())\n\n        self.coverage_type_col = peril_config.get('coverage_type_col') or 'coverage_type'\n\n        self.config.setdefault('exposure', self.config.get('exposure') or self.config.get('locations') or {})\n\n    def __tweak_config_data__(self):\n        for section in ('exposure', 'peril', 'vulnerability',):\n            section_config = self._config.get(section) or {}\n            for k, v in section_config.items():\n                if isinstance(v, str) and '%%KEYS_DATA_PATH%%' in v:\n                    self._config[section][k] = v.replace('%%KEYS_DATA_PATH%%', self._config['keys_data_path'])\n                elif type(v) == list:\n                    self._config[section][k] = tuple(v)\n                elif isinstance(v, dict):\n                    for _k, _v in v.items():\n                        if isinstance(_v, str) and '%%KEYS_DATA_PATH%%' in _v:\n                            self._config[section][k][_k] = _v.replace('%%KEYS_DATA_PATH%%', self._config['keys_data_path'])\n\n    @property\n    def config(self):\n        return self._config\n\n    @config.setter\n    def config(self, c):\n        self._config = c\n        self.__tweak_config_data__()\n\n    def lookup(self, loc, peril_id, coverage_type, **kwargs):\n        \"\"\"\n        Lookup for an individual location item, which could be a dict or a\n        Pandas series object.\n        \"\"\"\n        raise NotImplementedError\n\n    def process_locations(self, locs):\n        \"\"\"\n        Bulk vulnerability lookup for a list, tuple, generator, pandas data\n        frame or dict of location items, which can be dicts or Pandas series\n        objects or any object which has as a dict-like interface.\n\n        Generates results using ``yield``.\n        \"\"\"\n        locs_seq = None\n\n        if isinstance(locs, list) or isinstance(locs, tuple):\n            locs_seq = (loc for loc in locs)\n        elif isinstance(locs, types.GeneratorType):\n            locs_seq = locs\n        elif isinstance(locs, dict):\n\n\"\"\"Docstring (excerpt)\"\"\"\nAbstract class that help with the implementation of the KeyServerInterface.\nrequire lookup method to be implemented.\nLookup will be call to create a key for each peril id and coverage type"
    },
    {
      "chunk_id": "oasislmf/lookup/base.py::process_locations@51",
      "source_type": "code",
      "path": "oasislmf/lookup/base.py",
      "symbol_type": "function",
      "name": "process_locations",
      "lineno": 51,
      "end_lineno": 56,
      "business_stage": "other",
      "docstring": "Process location rows - passed in as a pandas dataframe.\nResults can be list, tuple, generator or a pandas dataframe.",
      "content": "# File: oasislmf/lookup/base.py\n# function: process_locations (lines 51-56)\n\n    def process_locations(self, locations):\n        \"\"\"\n        Process location rows - passed in as a pandas dataframe.\n        Results can be list, tuple, generator or a pandas dataframe.\n        \"\"\"\n        raise NotImplementedError\n\n\"\"\"Docstring (excerpt)\"\"\"\nProcess location rows - passed in as a pandas dataframe.\nResults can be list, tuple, generator or a pandas dataframe."
    },
    {
      "chunk_id": "oasislmf/lookup/base.py::lookup@136",
      "source_type": "code",
      "path": "oasislmf/lookup/base.py",
      "symbol_type": "function",
      "name": "lookup",
      "lineno": 136,
      "end_lineno": 141,
      "business_stage": "other",
      "docstring": "Lookup for an individual location item, which could be a dict or a\nPandas series object.",
      "content": "# File: oasislmf/lookup/base.py\n# function: lookup (lines 136-141)\n\n    def lookup(self, loc, peril_id, coverage_type, **kwargs):\n        \"\"\"\n        Lookup for an individual location item, which could be a dict or a\n        Pandas series object.\n        \"\"\"\n        raise NotImplementedError\n\n\"\"\"Docstring (excerpt)\"\"\"\nLookup for an individual location item, which could be a dict or a\nPandas series object."
    },
    {
      "chunk_id": "oasislmf/lookup/base.py::process_locations@143",
      "source_type": "code",
      "path": "oasislmf/lookup/base.py",
      "symbol_type": "function",
      "name": "process_locations",
      "lineno": 143,
      "end_lineno": 163,
      "business_stage": "other",
      "docstring": "Bulk vulnerability lookup for a list, tuple, generator, pandas data\nframe or dict of location items, which can be dicts or Pandas series\nobjects or any object which has as a dict-like interface.\n\nGenerates results using ``yield``.",
      "content": "# File: oasislmf/lookup/base.py\n# function: process_locations (lines 143-163)\n\n    def process_locations(self, locs):\n        \"\"\"\n        Bulk vulnerability lookup for a list, tuple, generator, pandas data\n        frame or dict of location items, which can be dicts or Pandas series\n        objects or any object which has as a dict-like interface.\n\n        Generates results using ``yield``.\n        \"\"\"\n        locs_seq = None\n\n        if isinstance(locs, list) or isinstance(locs, tuple):\n            locs_seq = (loc for loc in locs)\n        elif isinstance(locs, types.GeneratorType):\n            locs_seq = locs\n        elif isinstance(locs, dict):\n            locs_seq = locs.values()\n        elif isinstance(locs, pd.DataFrame):\n            locs_seq = (loc for _, loc in locs.iterrows())\n\n        for loc, peril_id, coverage_type in itertools.product(locs_seq, self.peril_ids, self.coverage_types):\n            yield self.lookup(loc, peril_id, coverage_type)\n\n\"\"\"Docstring (excerpt)\"\"\"\nBulk vulnerability lookup for a list, tuple, generator, pandas data\nframe or dict of location items, which can be dicts or Pandas series\nobjects or any object which has as a dict-like interface.\n\nGenerates results using ``yield``."
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::get_nearest@52",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "get_nearest",
      "lineno": 52,
      "end_lineno": 71,
      "business_stage": "other",
      "docstring": "Find nearest neighbors for all source points from a set of candidate points",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: get_nearest (lines 52-71)\n\ndef get_nearest(src_points, candidates, k_neighbors=1):\n    \"\"\"Find nearest neighbors for all source points from a set of candidate points\"\"\"\n\n    # Create tree from the candidate points\n    tree = BallTree(candidates, leaf_size=15, metric='haversine')\n\n    # Find closest points and distances\n    distances, indices = tree.query(src_points, k=k_neighbors)\n\n    # Transpose to get distances and indices into arrays\n    distances = distances.transpose()\n    indices = indices.transpose()\n\n    # Get closest indices and distances (i.e. array at index 0)\n    # note: for the second closest points, you would take index 1, etc.\n    closest = indices[0]\n    closest_dist = distances[0]\n\n    # Return indices and distances\n    return (closest, closest_dist)\n\n\"\"\"Docstring (excerpt)\"\"\"\nFind nearest neighbors for all source points from a set of candidate points"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::nearest_neighbor@74",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "nearest_neighbor",
      "lineno": 74,
      "end_lineno": 110,
      "business_stage": "other",
      "docstring": "For each point in left_gdf, find closest point in right GeoDataFrame and return them.\n\nNOTICE: Assumes that the input Points are in WGS84 projection (lat/lon).",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: nearest_neighbor (lines 74-110)\n\ndef nearest_neighbor(left_gdf, right_gdf, return_dist=False):\n    \"\"\"\n    For each point in left_gdf, find closest point in right GeoDataFrame and return them.\n\n    NOTICE: Assumes that the input Points are in WGS84 projection (lat/lon).\n    \"\"\"\n\n    left_geom_col = left_gdf.geometry.name\n    right_geom_col = right_gdf.geometry.name\n\n    # Ensure that index in right gdf is formed of sequential numbers\n    right = right_gdf.copy().reset_index(drop=True)\n\n    # Parse coordinates from points and insert them into a numpy array as RADIANS\n    left_radians = np.array(left_gdf[left_geom_col].apply(lambda geom: (geom.x * np.pi / 180, geom.y * np.pi / 180)).to_list())\n    right_radians = np.array(right[right_geom_col].apply(lambda geom: (geom.x * np.pi / 180, geom.y * np.pi / 180)).to_list())\n\n    # Find the nearest points\n    # -----------------------\n    # closest ==> index in right_gdf that corresponds to the closest point\n    # dist ==> distance between the nearest neighbors (in meters)\n\n    closest, dist = get_nearest(src_points=left_radians, candidates=right_radians)\n\n    # Return points from right GeoDataFrame that are closest to points in left GeoDataFrame\n    closest_points = right.loc[closest]\n\n    # Ensure that the index corresponds the one in left_gdf\n    closest_points = closest_points.set_index(left_gdf.index)\n\n    # Add distance if requested\n    if return_dist:\n        # Convert to meters from radians\n        earth_radius = 6371000  # meters\n        closest_points['distance'] = dist * earth_radius\n\n    return closest_points\n\n\"\"\"Docstring (excerpt)\"\"\"\nFor each point in left_gdf, find closest point in right GeoDataFrame and return them.\n\nNOTICE: Assumes that the input Points are in WGS84 projection (lat/lon)."
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::z_index@137",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "z_index",
      "lineno": 137,
      "end_lineno": 144,
      "business_stage": "other",
      "docstring": "Returns the Z-order index of cell (x,y) in a grid",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: z_index (lines 137-144)\n\ndef z_index(x, y):\n    \"\"\"Returns the Z-order index of cell (x,y) in a grid\"\"\"\n    bits = int(max(np.floor(np.log2(x + 1)) + 1, np.floor(np.log2(y + 1)) + 1))\n    index = 0\n    for i in range(bits):\n        index |= ((x >> i) & 1) << (2 * i)\n        index |= ((y >> i) & 1) << (2 * i + 1)\n    return index\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturns the Z-order index of cell (x,y) in a grid"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::undo_z_index@148",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "undo_z_index",
      "lineno": 148,
      "end_lineno": 154,
      "business_stage": "other",
      "docstring": "Returns the (x,y) coordinates of a z-order index in a grid",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: undo_z_index (lines 148-154)\n\ndef undo_z_index(z):\n    \"\"\"Returns the (x,y) coordinates of a z-order index in a grid\"\"\"\n    x = y = 0\n    for i in range(32):\n        x |= ((z >> (2 * i)) & 1) << i\n        y |= ((z >> (2 * i + 1)) & 1) << i\n    return x, y\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturns the (x,y) coordinates of a z-order index in a grid"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::z_index_to_normal@158",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "z_index_to_normal",
      "lineno": 158,
      "end_lineno": 164,
      "business_stage": "other",
      "docstring": "Converts from z-indexing to linear ordering",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: z_index_to_normal (lines 158-164)\n\ndef z_index_to_normal(index, size_across):\n    \"\"\"Converts from z-indexing to linear ordering\"\"\"\n    if index == OASIS_UNKNOWN_ID:\n        return index\n    index -= 1\n    lat, long = undo_z_index(index)\n    return (lat + long * size_across + 1)\n\n\"\"\"Docstring (excerpt)\"\"\"\nConverts from z-indexing to linear ordering"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::normal_to_z_index@168",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "normal_to_z_index",
      "lineno": 168,
      "end_lineno": 173,
      "business_stage": "other",
      "docstring": "Converts from linear ordering to z-indexing",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: normal_to_z_index (lines 168-173)\n\ndef normal_to_z_index(index, size_across):\n    \"\"\"Converts from linear ordering to z-indexing\"\"\"\n    if index == OASIS_UNKNOWN_ID:\n        return index\n    index -= 1\n    return z_index(index % size_across, index // size_across) + 1\n\n\"\"\"Docstring (excerpt)\"\"\"\nConverts from linear ordering to z-indexing"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::create_lat_lon_id_functions@176",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "create_lat_lon_id_functions",
      "lineno": 176,
      "end_lineno": 201,
      "business_stage": "other",
      "docstring": "Returns a function to give grid co-ordinates of a location",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: create_lat_lon_id_functions (lines 176-201)\n\ndef create_lat_lon_id_functions(\n    lat_min, lat_max, lon_min, lon_max, arc_size, lat_reverse, lon_reverse\n):\n    \"\"\"Returns a function to give grid co-ordinates of a location\"\"\"\n    lat_cell_size = arc_size\n    lon_cell_size = arc_size\n\n    if lat_reverse:\n        @nb.njit()\n        def lat_id(lat):\n            return math.floor((lat_max - lat) / lat_cell_size)\n    else:\n        @nb.njit()\n        def lat_id(lat):\n            return math.floor((lat - lat_min) / lat_cell_size)\n\n    if lon_reverse:\n        @nb.njit()\n        def lon_id(lon):\n            return math.floor((lon_max - lon) / lon_cell_size)\n    else:\n        @nb.njit()\n        def lon_id(lon):\n            return math.floor((lon - lon_min) / lon_cell_size)\n\n    return lat_id, lon_id\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturns a function to give grid co-ordinates of a location"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::jit_geo_grid_lookup@205",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "jit_geo_grid_lookup",
      "lineno": 205,
      "end_lineno": 216,
      "business_stage": "other",
      "docstring": "Returns an array of area peril IDs for all lats given",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: jit_geo_grid_lookup (lines 205-216)\n\ndef jit_geo_grid_lookup(\n    lat, lon, lat_min, lat_max, lon_min, lon_max, compute_id,\n    lat_id, lon_id\n):\n    \"\"\"Returns an array of area peril IDs for all lats given\"\"\"\n    area_peril_id = np.empty_like(lat, dtype=np.int64)\n    for i in range(lat.shape[0]):\n        if lat_min < lat[i] < lat_max and lon_min < lon[i] < lon_max:\n            area_peril_id[i] = compute_id(lat[i], lon[i], lat_id, lon_id)\n        else:\n            area_peril_id[i] = OASIS_UNKNOWN_ID\n    return area_peril_id\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturns an array of area peril IDs for all lats given"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::get_step@219",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "get_step",
      "lineno": 219,
      "end_lineno": 225,
      "business_stage": "other",
      "docstring": "Returns the grid size using the max and min long and latitude and arc size",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: get_step (lines 219-225)\n\ndef get_step(grid):\n    \"\"\"\n    Returns the grid size using the max and min long and latitude and arc size\n    \"\"\"\n    length = round((grid[\"lon_max\"] - grid[\"lon_min\"]) / grid[\"arc_size\"])\n    width = round((grid[\"lat_max\"] - grid[\"lat_min\"]) / grid[\"arc_size\"])\n    return length * width\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturns the grid size using the max and min long and latitude and arc size"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::Lookup@259",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "class",
      "name": "Lookup",
      "lineno": 259,
      "end_lineno": 1015,
      "business_stage": "other",
      "docstring": "Built-in Lookup class that implement the OasisLookupInterface\nThe aim of this class is to provide a data driven lookup capability that will be both flexible and efficient.\n\nit provide several generic function factory that can be define in the config under the \"step_definition\" key (ex:)\n\"step_definition\": {\n    \"split_loc_perils_covered\":{\n        \"type\": \"split_loc_perils_covered\" ,\n        \"columns\": [\"locperilscovered\"],\n        \"parameters\": {\n            \"model_perils_covered\": [\"WTC\", \"WSS\"]\n        }\n    },\n    \"vulnerability\": {\n        \"type\": \"merge\",\n        \"columns\": [\"peril_id\", \"coverage_type\", \"occupancycode\"],\n        \"parameters\": {\"file_path\": \"%%KEYS_DATA_PATH%%/vulnerability_dict.csv\",\n                       \"id_columns\": [\"vulnerability_id\"]\n                      }\n    }\n}\nmapper key: is called the step_name,\n    it will be added the the lookup object method once the function has been built\n    it can take any value but make sure it doesn't collide with already existing method\ntype: define the function factory to call.\n    in the class for type <fct_type> the function factory called will be build_<fct_type>\n    ex: \"type\": \"merge\" => build_merge\ncolumns: are the column required to be able to apply the step.\n    those are quite important as any column (except 'loc_id')\n    from the original Locations Dataframe that is not in any step will be drop to reduce memory consumption\nparameters: the parameter passed the the function factory.\n\nOnce all the functions have been defined, the order in which they must be applied is defined in the config\nunder the \"strategy\" key (ex:)\n    \"strategy\": [\"split_loc_perils_covered\", \"vulnerability\"]\n\nIt is totally possible to subclass Lookup in order to create your custom step or function factory\nfor custom step:\n    add your function definition to the \"mapper\"with no parameters\n\"my_custom_step\": {\n        \"type\": \"custom_type\" ,\n        \"columns\": [...],\n}\nsimply add it to your \"strategy\": [\"split_loc_perils_covered\", \"vulnerability\", \"my_custom_step\"]\nand code the function in your subclass\nclass MyLookup(Lookup):\n    @staticmethod\n    def my_custom_step(locations):\n        <do something on locations>\n        return modified_locations\n\nfor function factory:\nadd your function definition to the \"step_definition\" with the required parameters\n\"my_custom_step\": {\n        \"type\": \"custom_type\" ,\n        \"columns\": [...],\n        \"parameters\": {\n            \"param1\": \"value1\"\n        }\n}\nadd your step to \"strategy\": [\"split_loc_perils_covered\", \"vulnerability\", \"my_custom_step\"]\nand code the function factory in your subclass\nclass MyLookup(Lookup):\n    def build_custom_type(self, param1):\n        def fct(locations):\n            <do something on locations that depend on param1>\n            return modified_locations\n\n        return fct",
      "content": "# File: oasislmf/lookup/builtin.py\n# class: Lookup (lines 259-1015)\n\nclass Lookup(AbstractBasicKeyLookup, MultiprocLookupMixin):\n    \"\"\"\n    Built-in Lookup class that implement the OasisLookupInterface\n    The aim of this class is to provide a data driven lookup capability that will be both flexible and efficient.\n\n    it provide several generic function factory that can be define in the config under the \"step_definition\" key (ex:)\n    \"step_definition\": {\n        \"split_loc_perils_covered\":{\n            \"type\": \"split_loc_perils_covered\" ,\n            \"columns\": [\"locperilscovered\"],\n            \"parameters\": {\n                \"model_perils_covered\": [\"WTC\", \"WSS\"]\n            }\n        },\n        \"vulnerability\": {\n            \"type\": \"merge\",\n            \"columns\": [\"peril_id\", \"coverage_type\", \"occupancycode\"],\n            \"parameters\": {\"file_path\": \"%%KEYS_DATA_PATH%%/vulnerability_dict.csv\",\n                           \"id_columns\": [\"vulnerability_id\"]\n                          }\n        }\n    }\n    mapper key: is called the step_name,\n        it will be added the the lookup object method once the function has been built\n        it can take any value but make sure it doesn't collide with already existing method\n    type: define the function factory to call.\n        in the class for type <fct_type> the function factory called will be build_<fct_type>\n        ex: \"type\": \"merge\" => build_merge\n    columns: are the column required to be able to apply the step.\n        those are quite important as any column (except 'loc_id')\n        from the original Locations Dataframe that is not in any step will be drop to reduce memory consumption\n    parameters: the parameter passed the the function factory.\n\n    Once all the functions have been defined, the order in which they must be applied is defined in the config\n    under the \"strategy\" key (ex:)\n        \"strategy\": [\"split_loc_perils_covered\", \"vulnerability\"]\n\n    It is totally possible to subclass Lookup in order to create your custom step or function factory\n    for custom step:\n        add your function definition to the \"mapper\"with no parameters\n    \"my_custom_step\": {\n            \"type\": \"custom_type\" ,\n            \"columns\": [...],\n    }\n    simply add it to your \"strategy\": [\"split_loc_perils_covered\", \"vulnerability\", \"my_custom_step\"]\n    and code the function in your subclass\n    class MyLookup(Lookup):\n        @staticmethod\n        def my_custom_step(locations):\n            <do something on locations>\n            return modified_locations\n\n    for function factory:\n    add your function definition to the \"step_definition\" with the required parameters\n    \"my_custom_step\": {\n            \"type\": \"custom_type\" ,\n            \"columns\": [...],\n            \"parameters\": {\n                \"param1\": \"value1\"\n            }\n    }\n    add your step to \"strategy\": [\"split_loc_perils_covered\", \"vulnerability\", \"my_custom_step\"]\n    and code the function factory in your subclass\n    class MyLookup(Lookup):\n        def build_custom_type(self, param1):\n            def fct(locations):\n                <do something on locations that depend on param1>\n                return modified_locations\n\n            return fct\n\n    \"\"\"\n    interface_version = \"1\"\n\n    def set_step_function(self, step_name, step_config, function_being_set=None):\n        \"\"\"\n        set the step as a function of the lookup object if it's not already done and return it.\n        if the step is composed of several child steps, it will set the child steps recursively.\n\n        Args:\n\n\"\"\"Docstring (excerpt)\"\"\"\nBuilt-in Lookup class that implement the OasisLookupInterface\nThe aim of this class is to provide a data driven lookup capability that will be both flexible and efficient.\n\nit provide several generic function factory that can be define in the config under the \"step_definition\" key (ex:)\n\"step_definition\": {\n    \"split_loc_perils_covered\":{\n        \"type\": \"split_loc_perils_covered\" ,\n        \"columns\": [\"locperilscovered\"],\n        \"parameters\": {\n            \"model_perils_covered\": [\"WTC\", \"WSS\"]\n        }\n    },\n    \"vulnerability\": {\n        \"type\": \"merge\",\n        \"columns\": [\"peril_id\", \"coverage_type\", \"occupancycode\"],\n        \"parameters\": {\"file_path\": \"%%KEYS_DATA_PATH%%/vulnerability_dict.csv\",\n                       \"id_columns\": [\"vulnerability_id\"]\n                      }\n    }\n}\nmapper key: is called the step_name,\n    it will be added the the lookup object method once the function has been built\n    it can take any value but make sure it doesn't collide with already existing method\ntype: define the function factory to call.\n    in the class for type <fct_type> the function factory called will be build_<fct_type>\n    ex: \"type\": \"merge\" => build_merge\ncolumns: are the column required to be able to apply the step.\n    those are quite important as any column (except 'loc_id')\n    from the original Locations Dataframe that is not in any step will be drop to reduce memory consumption\nparameters: the parameter passed the the function factory.\n\nOnce all the functions have been defined, the order in which they must be applied is defined in the config\nunder the \"strategy\" key (ex:)\n    \"strategy\": [\"split_loc_perils_covered\", \"vulnerability\"]\n\nIt is totally possible to subclass Lookup in order to create your custom step or function factory\nfor custom step:\n    add your function definition to the \"mapper\"with no parameters\n\"my_custom_step\": {\n        \"type\": \"custom_type\" ,\n        \"columns\": [...],\n}\nsimply add it to your \"strategy\": [\"split_loc_perils_covered\", \"vulnerability\", \"my_custom_step\"]\nand code the function in your subclass\nclass MyLookup(Lookup):\n    @staticmethod\n    def my_custom_step(locations):\n        <do something on locations>\n        return modified_locations\n\nfor function factory:\nadd your function definition to the \"step_definition\" with the required parameters\n\"my_custom_step\": {\n        \"type\": \"custom_type\" ,\n        \"columns\": [...],\n        \"parameters\": {\n            \"param1\": \"value1\"\n        }\n}\nadd your step to \"strategy\": [\"split_loc_perils_covered\", \"vulnerability\", \"my_custom_step\"]\nand code the function factory in your subclass\nclass MyLookup(Lookup):\n    def build_custom_type(self, param1):\n        def fct(locations):\n            <do something on locations that depend on param1>\n            return modified_locations\n\n        return fct"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::set_step_function@333",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "set_step_function",
      "lineno": 333,
      "end_lineno": 368,
      "business_stage": "other",
      "docstring": "set the step as a function of the lookup object if it's not already done and return it.\nif the step is composed of several child steps, it will set the child steps recursively.\n\nArgs:\n    step_name (str): name of the strategy for this step\n    step_config (dict): config of the strategy for this step\n    function_being_set (set, None): set of all the strategy that are parent of this step\n\nReturns:\n    function: function corresponding this step",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: set_step_function (lines 333-368)\n\n    def set_step_function(self, step_name, step_config, function_being_set=None):\n        \"\"\"\n        set the step as a function of the lookup object if it's not already done and return it.\n        if the step is composed of several child steps, it will set the child steps recursively.\n\n        Args:\n            step_name (str): name of the strategy for this step\n            step_config (dict): config of the strategy for this step\n            function_being_set (set, None): set of all the strategy that are parent of this step\n\n        Returns:\n            function: function corresponding this step\n        \"\"\"\n        if hasattr(self, step_name):\n            step_function = getattr(self, step_name)\n        else:\n            if step_config['type'] == 'combine':  # we need to build the child function\n                if function_being_set is None:  # make sure we catch cyclic strategy definition\n                    function_being_set = {step_name}\n                elif step_name in function_being_set:\n                    raise OasisException(f\"lookup config has a cyclic strategy definition {function_being_set} then {step_name} again\")\n                else:\n                    function_being_set.add(step_name)\n\n                functions = []\n                for child_step_name in step_config[\"parameters\"]['strategy']:\n                    child_fct = self.set_step_function(\n                        step_name=child_step_name,\n                        step_config=self.config['step_definition'][child_step_name],\n                        function_being_set=function_being_set)\n                    functions.append({'function': child_fct, 'columns': set(step_config.get(\"columns\", []))})\n                step_config['parameters']['strategy'] = functions\n\n            step_function = getattr(self, f\"build_{step_config['type']}\")(**step_config['parameters'])\n            setattr(self, step_name, step_function)\n        return step_function\n\n\"\"\"Docstring (excerpt)\"\"\"\nset the step as a function of the lookup object if it's not already done and return it.\nif the step is composed of several child steps, it will set the child steps recursively.\n\nArgs:\n    step_name (str): name of the strategy for this step\n    step_config (dict): config of the strategy for this step\n    function_being_set (set, None): set of all the strategy that are parent of this step\n\nReturns:\n    function: function corresponding this step"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::to_abs_filepath@422",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "to_abs_filepath",
      "lineno": 422,
      "end_lineno": 437,
      "business_stage": "other",
      "docstring": "replace placeholder r'%%(.+?)%%' (ex: %%KEYS_DATA_PATH%%) with the path set in self.config\nArgs:\n    filepath (str): filepath with potentially a placeholder\n\nReturns:\n    str: filepath where placeholder are replace their actual value.",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: to_abs_filepath (lines 422-437)\n\n    def to_abs_filepath(self, filepath):\n        \"\"\"\n        replace placeholder r'%%(.+?)%%' (ex: %%KEYS_DATA_PATH%%) with the path set in self.config\n        Args:\n            filepath (str): filepath with potentially a placeholder\n\n        Returns:\n            str: filepath where placeholder are replace their actual value.\n        \"\"\"\n        placeholder_keys = set(re.findall(r'%%(.+?)%%', filepath))\n        for placeholder_key in placeholder_keys:\n            filepath = filepath.replace(f'%%{placeholder_key}%%', self.config[placeholder_key.lower()])\n        if \"keys_data_path\" in [key.lower() for key in placeholder_keys]:\n            return filepath\n        else:\n            return self.storage.get_storage_url(filepath, encode_params=False)[1].replace('file://', '')\n\n\"\"\"Docstring (excerpt)\"\"\"\nreplace placeholder r'%%(.+?)%%' (ex: %%KEYS_DATA_PATH%%) with the path set in self.config\nArgs:\n    filepath (str): filepath with potentially a placeholder\n\nReturns:\n    str: filepath where placeholder are replace their actual value."
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::set_id_columns@440",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "set_id_columns",
      "lineno": 440,
      "end_lineno": 452,
      "business_stage": "other",
      "docstring": "in Dataframes, only float column can have nan values. So after a left join for example if you have nan values\nthat will change the type of the original column into float.\nthis function replace the nan value with the OASIS_UNKNOWN_ID and reset the column type to int",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: set_id_columns (lines 440-452)\n\n    def set_id_columns(df, id_columns):\n        \"\"\"\n        in Dataframes, only float column can have nan values. So after a left join for example if you have nan values\n        that will change the type of the original column into float.\n        this function replace the nan value with the OASIS_UNKNOWN_ID and reset the column type to int\n        \"\"\"\n        for col in id_columns:\n            if col not in df:\n                df[col] = OASIS_UNKNOWN_ID\n            else:\n                df[col] = df[col].astype('Int64')\n                df.loc[(df[col].isna()) | (df[col] <= 0), col] = OASIS_UNKNOWN_ID\n        return df\n\n\"\"\"Docstring (excerpt)\"\"\"\nin Dataframes, only float column can have nan values. So after a left join for example if you have nan values\nthat will change the type of the original column into float.\nthis function replace the nan value with the OASIS_UNKNOWN_ID and reset the column type to int"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::build_interval_to_index@454",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "build_interval_to_index",
      "lineno": 454,
      "end_lineno": 483,
      "business_stage": "other",
      "docstring": "Allow to map a value column to an index according to it's index in the interval defined by sorted_array.\nnan value are kept as nan\nArgs:\n    value_column_name: name of the column to map\n    sorted_array: sorted value that define the interval to map to\n    index_column_name: name of the output column\n    side: define what index is returned (left or right) in case of equality with one of the interval boundary\n\nReturns:\n    function: return the mapping function",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: build_interval_to_index (lines 454-483)\n\n    def build_interval_to_index(self, value_column_name, sorted_array, index_column_name=None, side='left'):\n        \"\"\"\n        Allow to map a value column to an index according to it's index in the interval defined by sorted_array.\n        nan value are kept as nan\n        Args:\n            value_column_name: name of the column to map\n            sorted_array: sorted value that define the interval to map to\n            index_column_name: name of the output column\n            side: define what index is returned (left or right) in case of equality with one of the interval boundary\n\n        Returns:\n            function: return the mapping function\n        \"\"\"\n        if isinstance(sorted_array, list):\n            pass\n        elif isinstance(sorted_array, str):\n            sorted_array = [float(val) for val in open(self.to_abs_filepath(sorted_array)) if val.strip()]\n        else:\n            raise OasisException(\"sorted_array must be a list of the interval sorted or a path to a csv file containing those interval\")\n\n        if index_column_name is None:\n            index_column_name = value_column_name + '_idx'\n\n        def fct(locations):\n            locations[index_column_name] = np.searchsorted(sorted_array, locations[value_column_name], side=side)\n            empty_values = is_empty(locations, value_column_name)\n            locations.loc[empty_values, index_column_name] = locations.loc[empty_values, value_column_name]\n            return locations\n\n        return fct\n\n\"\"\"Docstring (excerpt)\"\"\"\nAllow to map a value column to an index according to it's index in the interval defined by sorted_array.\nnan value are kept as nan\nArgs:\n    value_column_name: name of the column to map\n    sorted_array: sorted value that define the interval to map to\n    index_column_name: name of the output column\n    side: define what index is returned (left or right) in case of equality with one of the interval boundary\n\nReturns:\n    function: return the mapping function"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::build_combine@486",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "build_combine",
      "lineno": 486,
      "end_lineno": 562,
      "business_stage": "other",
      "docstring": "build a function that will combine several strategy trying to achieve the same purpose by different mean into one.\nfor example, finding the correct area_peril_id for a location with one method using (latitude, longitude)\nand one using postcode.\neach strategy will be applied sequentially on the location that steal have OASIS_UNKNOWN_ID in their id_columns after the precedent strategy\n\n'or' example: (note: \"id_columns\" is a list)\n    \"vulnerability\":{\n        \"type\": \"combine\",\n        \"parameters\": {\n            \"id_columns\": [\"vulnerability_id\"],\n            \"strategy\": [\"vuln_cov_Building_Content\", \"vuln_cov_car\"]\n            \"logical_type\": \"or\"\n        }\n    }\n\n'and' example: (note: that \"id_columns\" is a list of list)\n    \"vuln_cov_car\":{\n        \"type\": \"combine\",\n        \"columns\": [\"autocode\"],\n        \"parameters\": {\n            \"id_columns\": [[\"vuln_id_car\"], [\"vulnerability_id\"]],\n            \"strategy\": [\"vulnerability_car\", \"coverage_type_car\"],\n            \"logical_type\": \"and\"\n        }\n    },\n\nArgs:\n    id_columns (list): columns that will be checked to determine if a strategy has succeeded\n    strategy (list): list of strategy to apply\n    logical_type: if 'or' apply the next strategy only on invalid id_columns\n                  if 'and' apply the next strategy only on valid id_columns\n                           id_columns needs to be a list of list of columns that each sublist is checked sequentially\n\n\nReturns:\n    function: function combining all strategies",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: build_combine (lines 486-562)\n\n    def build_combine(id_columns, strategy, logical_type='or'):\n        \"\"\"\n        build a function that will combine several strategy trying to achieve the same purpose by different mean into one.\n        for example, finding the correct area_peril_id for a location with one method using (latitude, longitude)\n        and one using postcode.\n        each strategy will be applied sequentially on the location that steal have OASIS_UNKNOWN_ID in their id_columns after the precedent strategy\n\n        'or' example: (note: \"id_columns\" is a list)\n            \"vulnerability\":{\n                \"type\": \"combine\",\n                \"parameters\": {\n                    \"id_columns\": [\"vulnerability_id\"],\n                    \"strategy\": [\"vuln_cov_Building_Content\", \"vuln_cov_car\"]\n                    \"logical_type\": \"or\"\n                }\n            }\n\n        'and' example: (note: that \"id_columns\" is a list of list)\n            \"vuln_cov_car\":{\n                \"type\": \"combine\",\n                \"columns\": [\"autocode\"],\n                \"parameters\": {\n                    \"id_columns\": [[\"vuln_id_car\"], [\"vulnerability_id\"]],\n                    \"strategy\": [\"vulnerability_car\", \"coverage_type_car\"],\n                    \"logical_type\": \"and\"\n                }\n            },\n\n        Args:\n            id_columns (list): columns that will be checked to determine if a strategy has succeeded\n            strategy (list): list of strategy to apply\n            logical_type: if 'or' apply the next strategy only on invalid id_columns\n                          if 'and' apply the next strategy only on valid id_columns\n                                   id_columns needs to be a list of list of columns that each sublist is checked sequentially\n\n\n        Returns:\n            function: function combining all strategies\n        \"\"\"\n        if logical_type.lower() == 'or':\n            def fct(locations):\n                initial_columns = locations.columns\n                result = []\n                for child_strategy in strategy:\n                    if not child_strategy['columns'].issubset(locations.columns):  # needed column not present to run this strategy\n                        continue\n                    locations = child_strategy['function'](locations)\n                    locations = Lookup.set_id_columns(locations, id_columns)\n                    is_valid = (locations[id_columns] != OASIS_UNKNOWN_ID).any(axis=1)\n                    result.append(locations[is_valid])\n                    locations = locations[~is_valid][initial_columns].copy()\n                    if locations.empty:\n                        break\n                result.append(locations)\n                return Lookup.set_id_columns(pd.concat(result, ignore_index=True), id_columns)\n\n        elif logical_type.lower() == 'and':\n            def fct(locations):\n                initial_columns = locations.columns\n                result = []\n                for i, child_strategy in enumerate(strategy):\n                    if not child_strategy['columns'].issubset(locations.columns):  # needed column not present to run this strategy\n                        continue\n                    locations = child_strategy['function'](locations)\n                    locations = Lookup.set_id_columns(locations, id_columns[i])\n                    is_valid = (locations[id_columns[i]] != OASIS_UNKNOWN_ID).any(axis=1)\n                    result.append(locations[~is_valid][initial_columns])\n                    locations = locations[is_valid]\n\n                    if locations.empty:\n                        break\n                result.append(locations)\n                return Lookup.set_id_columns(pd.concat(result, ignore_index=True), id_columns[-1])\n\n        else:\n            raise OasisException(f\"Unsupported logical_type {logical_type}\")\n        return fct\n\n\"\"\"Docstring (excerpt)\"\"\"\nbuild a function that will combine several strategy trying to achieve the same purpose by different mean into one.\nfor example, finding the correct area_peril_id for a location with one method using (latitude, longitude)\nand one using postcode.\neach strategy will be applied sequentially on the location that steal have OASIS_UNKNOWN_ID in their id_columns after the precedent strategy\n\n'or' example: (note: \"id_columns\" is a list)\n    \"vulnerability\":{\n        \"type\": \"combine\",\n        \"parameters\": {\n            \"id_columns\": [\"vulnerability_id\"],\n            \"strategy\": [\"vuln_cov_Building_Content\", \"vuln_cov_car\"]\n            \"logical_type\": \"or\"\n        }\n    }\n\n'and' example: (note: that \"id_columns\" is a list of list)\n    \"vuln_cov_car\":{\n        \"type\": \"combine\",\n        \"columns\": [\"autocode\"],\n        \"parameters\": {\n            \"id_columns\": [[\"vuln_id_car\"], [\"vulnerability_id\"]],\n            \"strategy\": [\"vulnerability_car\", \"coverage_type_car\"],\n            \"logical_type\": \"and\"\n        }\n    },\n\nArgs:\n    id_columns (list): columns that will be checked to determine if a strategy has succeeded\n    strategy (list): list of strategy to apply\n    logical_type: if 'or' apply the next strategy only on invalid id_columns\n                  if 'and' apply the next strategy only on valid id_columns\n                           id_columns needs to be a list of list of columns that each sublist is checked sequentially\n\n\nReturns:\n    function: function combining all strategies"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::build_split_loc_perils_covered@565",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "build_split_loc_perils_covered",
      "lineno": 565,
      "end_lineno": 603,
      "business_stage": "other",
      "docstring": "split the value of LocPerilsCovered into multiple line, taking peril group into account\ndrop all line that are not in the list model_perils_covered\n\nusefull inspirational code:\nhttps://stackoverflow.com/questions/17116814/pandas-how-do-i-split-text-in-a-column-into-multiple-rows",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: build_split_loc_perils_covered (lines 565-603)\n\n    def build_split_loc_perils_covered(model_perils_covered=None):\n        \"\"\"\n        split the value of LocPerilsCovered into multiple line, taking peril group into account\n        drop all line that are not in the list model_perils_covered\n\n        usefull inspirational code:\n        https://stackoverflow.com/questions/17116814/pandas-how-do-i-split-text-in-a-column-into-multiple-rows\n\n        \"\"\"\n        peril_groups_df = get_peril_groups_df()\n\n        def fct(locations):\n            for col in locations.columns:\n                if col.lower() == 'locperilscovered':\n                    perils_covered_column = col\n                    break\n                elif col.lower() == 'polperilscovered':\n                    perils_covered_column = col\n                    break\n            else:\n                raise OasisException('missing PerilsCovered column in location')\n\n            locations['peril_group_id'] = locations[perils_covered_column].str.split(';')\n            peril_locations = locations.explode('peril_group_id').drop_duplicates().merge(peril_groups_df)\n            locations.drop(columns='peril_group_id')\n\n            if model_perils_covered:\n                df_model_perils_covered = pd.Series(model_perils_covered)\n                df_model_perils_covered.name = 'model_perils_covered'\n                peril_locations = peril_locations.merge(df_model_perils_covered,\n                                                        left_on='peril_id', right_on='model_perils_covered',\n                                                        sort=True)\n            not_covered_location = locations[~locations['loc_id'].isin(peril_locations['loc_id'])]\n            if not not_covered_location.empty:\n                not_covered_location['status'] = OASIS_KEYS_STATUS['notatrisk']\n                not_covered_location['message'] = not_covered_location[perils_covered_column].astype(str) + \" have no perils modelled\"\n                peril_locations = pd.concat([peril_locations, not_covered_location], ignore_index=True)\n            return peril_locations\n        return fct\n\n\"\"\"Docstring (excerpt)\"\"\"\nsplit the value of LocPerilsCovered into multiple line, taking peril group into account\ndrop all line that are not in the list model_perils_covered\n\nusefull inspirational code:\nhttps://stackoverflow.com/questions/17116814/pandas-how-do-i-split-text-in-a-column-into-multiple-rows"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::build_prepare@606",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "build_prepare",
      "lineno": 606,
      "end_lineno": 631,
      "business_stage": "other",
      "docstring": "Prepare the dataframe by setting default, min and max values and type\nsupport several simple DataFrame preparation:\n    default: create the column if missing and replace the nan value with the default value\n    max: truncate the values in a column to the specified max\n    min: truncate the values in a column to the specified min\n    type: convert the type of the column to the specified numpy dtype\n        Note that we use the string representation of numpy dtype available at\n        https://numpy.org/doc/stable/reference/arrays.dtypes.html#arrays-dtypes-constructing",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: build_prepare (lines 606-631)\n\n    def build_prepare(**kwargs):\n        \"\"\"\n        Prepare the dataframe by setting default, min and max values and type\n        support several simple DataFrame preparation:\n            default: create the column if missing and replace the nan value with the default value\n            max: truncate the values in a column to the specified max\n            min: truncate the values in a column to the specified min\n            type: convert the type of the column to the specified numpy dtype\n                Note that we use the string representation of numpy dtype available at\n                https://numpy.org/doc/stable/reference/arrays.dtypes.html#arrays-dtypes-constructing\n        \"\"\"\n        def prepare(locations):\n            for column_name, preparations in kwargs.items():\n                if \"default\" in preparations:\n                    if column_name not in locations.columns:\n                        locations[column_name] = preparations[\"default\"]\n                    else:\n                        fill_empty(locations, column_name, preparations[\"default\"])\n                if 'max' in preparations:\n                    locations.loc[locations[column_name] > preparations['max'], column_name] = preparations['max']\n                if 'min' in preparations:\n                    locations.loc[locations[column_name] > preparations['min'], column_name] = preparations['min']\n                if 'type' in preparations:\n                    locations[column_name] = locations[column_name].astype(preparations['type'])\n            return locations\n        return prepare\n\n\"\"\"Docstring (excerpt)\"\"\"\nPrepare the dataframe by setting default, min and max values and type\nsupport several simple DataFrame preparation:\n    default: create the column if missing and replace the nan value with the default value\n    max: truncate the values in a column to the specified max\n    min: truncate the values in a column to the specified min\n    type: convert the type of the column to the specified numpy dtype\n        Note that we use the string representation of numpy dtype available at\n        https://numpy.org/doc/stable/reference/arrays.dtypes.html#arrays-dtypes-constructing"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::build_rtree@633",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "build_rtree",
      "lineno": 633,
      "end_lineno": 725,
      "business_stage": "other",
      "docstring": "Function Factory to associate location to area_peril based on the rtree method\n\n!!!\nplease note that this method is quite time consuming (specialy if you use the nearest point option\nif your peril_area are square you should use area_peril function fixed_size_geo_grid\n!!!\n\nfile_path: is the path to the file containing the area_peril_dictionary.\n    this file must be a geopandas Dataframe with a valid geometry.\n    an example on how to create such dataframe is available in PiWind\n    if you are new to geo data (in python) and want to learn more, you may have a look at this excellent course:\n    https://automating-gis-processes.github.io/site/index.html\n\nfile_type: can be any format readable by geopandas ('file', 'parquet', ...)\n    see: https://geopandas.readthedocs.io/en/latest/docs/reference/io.html\n    you may have to install additional library such as pyarrow for parquet\n\nid_columns: column to transform to an 'id_column' (type int32 with nan replace by -1)\n\nnearest_neighbor_min_distance: option to compute the nearest point if intersection method fails\n    we use:\n    https://automating-gis-processes.github.io/site/notebooks/L3/nearest-neighbor-faster.html\n    but alternatives can be found here:\n    https://gis.stackexchange.com/questions/222315/geopandas-find-nearest-point-in-other-dataframe",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: build_rtree (lines 633-725)\n\n    def build_rtree(self, file_path, file_type, id_columns, area_peril_read_params=None, nearest_neighbor_min_distance=-1):\n        \"\"\"\n        Function Factory to associate location to area_peril based on the rtree method\n\n        !!!\n        please note that this method is quite time consuming (specialy if you use the nearest point option\n        if your peril_area are square you should use area_peril function fixed_size_geo_grid\n        !!!\n\n        file_path: is the path to the file containing the area_peril_dictionary.\n            this file must be a geopandas Dataframe with a valid geometry.\n            an example on how to create such dataframe is available in PiWind\n            if you are new to geo data (in python) and want to learn more, you may have a look at this excellent course:\n            https://automating-gis-processes.github.io/site/index.html\n\n        file_type: can be any format readable by geopandas ('file', 'parquet', ...)\n            see: https://geopandas.readthedocs.io/en/latest/docs/reference/io.html\n            you may have to install additional library such as pyarrow for parquet\n\n        id_columns: column to transform to an 'id_column' (type int32 with nan replace by -1)\n\n        nearest_neighbor_min_distance: option to compute the nearest point if intersection method fails\n            we use:\n            https://automating-gis-processes.github.io/site/notebooks/L3/nearest-neighbor-faster.html\n            but alternatives can be found here:\n            https://gis.stackexchange.com/questions/222315/geopandas-find-nearest-point-in-other-dataframe\n\n        \"\"\"\n        if Point is None:\n            raise OasisException(f\"shapely and geopandas modules are needed for rtree, {OPT_INSTALL_MESSAGE}\")\n\n        if hasattr(gpd, f\"read_{file_type}\"):\n            if area_peril_read_params is None:\n                area_peril_read_params = {}\n            gdf_area_peril = getattr(gpd, f\"read_{file_type}\")(self.to_abs_filepath(file_path), **area_peril_read_params)\n        else:\n            raise OasisException(f\"Unregognised Geopandas read type {file_type}\")\n\n        if nearest_neighbor_min_distance > 0:\n            if BallTree is None:\n                raise OasisException(f\"scikit-learn modules are needed for rtree with nearest_neighbor_min_distance, {OPT_INSTALL_MESSAGE}\")\n            gdf_area_peril['center'] = gdf_area_peril.centroid\n            base_geometry_name = gdf_area_peril.geometry.name\n\n        def get_area(locations, gdf_area_peril):\n            # this conversion could be done in a separate step allowing more posibilities for the geometry\n            null_gdf = locations[\"longitude\"].isna() | locations[\"latitude\"].isna()\n            null_gdf_loc = locations[null_gdf]\n            if not null_gdf_loc.empty:\n                gdf_loc = gpd.GeoDataFrame(locations[~null_gdf], columns=locations.columns)\n            else:\n                gdf_loc = gpd.GeoDataFrame(locations, columns=locations.columns)\n\n            if not gdf_loc.empty:\n                gdf_loc[\"loc_geometry\"] = gdf_loc.apply(lambda row: Point(row[\"longitude\"], row[\"latitude\"]),\n                                                        axis=1,\n                                                        result_type='reduce')\n                gdf_loc = gdf_loc.set_geometry('loc_geometry')\n\n                gdf_loc = gpd.sjoin(gdf_loc, gdf_area_peril, 'left')\n\n                if nearest_neighbor_min_distance > 0:\n                    gdf_loc_na = gdf_loc.loc[gdf_loc['index_right'].isna()]\n\n                    if gdf_loc_na.shape[0]:\n                        gdf_area_peril.set_geometry('center', inplace=True)\n                        nearest_neighbor_df = nearest_neighbor(gdf_loc_na, gdf_area_peril, return_dist=True)\n\n                        gdf_area_peril.set_geometry(base_geometry_name, inplace=True)\n                        valid_nearest_neighbor = nearest_neighbor_df['distance'] <= nearest_neighbor_min_distance\n                        common_col = list(set(gdf_loc_na.columns) & set(nearest_neighbor_df.columns))\n                        gdf_loc.loc[valid_nearest_neighbor.index, common_col] = nearest_neighbor_df.loc[valid_nearest_neighbor, common_col]\n            if not null_gdf_loc.empty:\n                gdf_loc = pd.concat([gdf_loc, null_gdf_loc])\n            self.set_id_columns(gdf_loc, id_columns)\n\n            # index column are created during the sjoin, we can drop them\n            gdf_loc = gdf_loc.drop(columns=['index_right', 'index_left'], errors='ignore')\n\n            return gdf_loc\n\n\"\"\"Docstring (excerpt)\"\"\"\nFunction Factory to associate location to area_peril based on the rtree method\n\n!!!\nplease note that this method is quite time consuming (specialy if you use the nearest point option\nif your peril_area are square you should use area_peril function fixed_size_geo_grid\n!!!\n\nfile_path: is the path to the file containing the area_peril_dictionary.\n    this file must be a geopandas Dataframe with a valid geometry.\n    an example on how to create such dataframe is available in PiWind\n    if you are new to geo data (in python) and want to learn more, you may have a look at this excellent course:\n    https://automating-gis-processes.github.io/site/index.html\n\nfile_type: can be any format readable by geopandas ('file', 'parquet', ...)\n    see: https://geopandas.readthedocs.io/en/latest/docs/reference/io.html\n    you may have to install additional library such as pyarrow for parquet\n\nid_columns: column to transform to an 'id_column' (type int32 with nan replace by -1)\n\nnearest_neighbor_min_distance: option to compute the nearest point if intersection method fails\n    we use:\n    https://automating-gis-processes.github.io/site/notebooks/L3/nearest-neighbor-faster.html\n    but alternatives can be found here:\n    https://gis.stackexchange.com/questions/222315/geopandas-find-nearest-point-in-other-dataframe"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::build_fixed_size_geo_grid_multi_peril@728",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "build_fixed_size_geo_grid_multi_peril",
      "lineno": 728,
      "end_lineno": 759,
      "business_stage": "other",
      "docstring": "Create multiple grids of varying resolution, one per peril, and\nassociate an id to each square of the grid using the\n`fixed_size_geo_grid` method.\n\nParameters\n----------\nperils_dict: dict\n             Dictionary with `peril_id` as key and `fixed_size_geo_grid` parameter dict as\n             value. i.e `{'peril_id' : {fixed_size_geo_grid parameters}}`",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: build_fixed_size_geo_grid_multi_peril (lines 728-759)\n\n    def build_fixed_size_geo_grid_multi_peril(perils_dict):\n        \"\"\"\n        Create multiple grids of varying resolution, one per peril, and\n        associate an id to each square of the grid using the\n        `fixed_size_geo_grid` method.\n\n        Parameters\n        ----------\n        perils_dict: dict\n                     Dictionary with `peril_id` as key and `fixed_size_geo_grid` parameter dict as\n                     value. i.e `{'peril_id' : {fixed_size_geo_grid parameters}}`\n        \"\"\"\n        def fct(locs_peril):\n            start_index = 0\n            step = get_step(next(iter(perils_dict.values())))\n\n            locs_peril[\"area_peril_id\"] = OASIS_UNKNOWN_ID  # if `peril_id` not in `perils_dict`\n            for peril_id, fixed_geo_grid_params in perils_dict.items():\n                curr_grid_fct = Lookup.build_fixed_size_geo_grid(**fixed_geo_grid_params)\n\n                curr_locs_peril = locs_peril[locs_peril['peril_id'] == peril_id]\n                curr_locs_peril = curr_grid_fct(curr_locs_peril)\n                curr_locs_peril.loc[\n                    curr_locs_peril[\"area_peril_id\"] != OASIS_UNKNOWN_ID,\n                    \"area_peril_id\"\n                ] = curr_locs_peril[\"area_peril_id\"] + start_index\n\n                start_index += step\n\n                locs_peril[locs_peril[\"peril_id\"] == peril_id] = curr_locs_peril\n            return locs_peril\n        return fct\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreate multiple grids of varying resolution, one per peril, and\nassociate an id to each square of the grid using the\n`fixed_size_geo_grid` method.\n\nParameters\n----------\nperils_dict: dict\n             Dictionary with `peril_id` as key and `fixed_size_geo_grid` parameter dict as\n             value. i.e `{'peril_id' : {fixed_size_geo_grid parameters}}`"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::build_fixed_size_geo_grid@762",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "build_fixed_size_geo_grid",
      "lineno": 762,
      "end_lineno": 793,
      "business_stage": "other",
      "docstring": "associate an id to each square of the grid define by the limit of lat and lon\nreverse allow to change the ordering of id from (min to max) to (max to min)",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: build_fixed_size_geo_grid (lines 762-793)\n\n    def build_fixed_size_geo_grid(lat_min, lat_max, lon_min, lon_max, arc_size, lat_reverse=False, lon_reverse=False, lon_first=False):\n        \"\"\"\n        associate an id to each square of the grid define by the limit of lat and lon\n        reverse allow to change the ordering of id from (min to max) to (max to min)\n        \"\"\"\n\n        lat_id, lon_id = create_lat_lon_id_functions(\n            lat_min, lat_max, lon_min, lon_max, arc_size,\n            lat_reverse, lon_reverse\n        )\n\n        if lon_first:\n            grid_size = round((lon_max - lon_min) / arc_size)\n        else:\n            grid_size = round((lat_max - lat_min) / arc_size)\n\n        @nb.jit(cache=True)\n        def get_id(lat, lon, lat_id, lon_id):\n            if lon_first:\n                return lon_id(lon) + lat_id(lat) * grid_size + 1\n            return lon_id(lon) * grid_size + lat_id(lat) + 1\n\n        def geo_grid_lookup(locations):\n            locations['area_peril_id'] = jit_geo_grid_lookup(\n                locations['latitude'].to_numpy(),\n                locations['longitude'].to_numpy(),\n                lat_min, lat_max, lon_min, lon_max,\n                get_id, lat_id, lon_id\n            )\n            return locations\n\n        return geo_grid_lookup\n\n\"\"\"Docstring (excerpt)\"\"\"\nassociate an id to each square of the grid define by the limit of lat and lon\nreverse allow to change the ordering of id from (min to max) to (max to min)"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::build_fixed_size_z_index_geo_grid_multi_peril@796",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "build_fixed_size_z_index_geo_grid_multi_peril",
      "lineno": 796,
      "end_lineno": 824,
      "business_stage": "other",
      "docstring": "Create multiple grids of varying resolution, one per peril, and associate an id to each square of the grid using the\n`fixed_size_z_index_geo_grid` method.\n\nParameters\n----------\nperils_dict: dict\n             Dictionary with `peril_id` as key and `fixed_size_geo_grid` parameter dict as\n             value. i.e `{'peril_id' : {fixed_size_geo_grid parameters}}`",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: build_fixed_size_z_index_geo_grid_multi_peril (lines 796-824)\n\n    def build_fixed_size_z_index_geo_grid_multi_peril(perils_dict):\n        \"\"\"\n        Create multiple grids of varying resolution, one per peril, and associate an id to each square of the grid using the\n        `fixed_size_z_index_geo_grid` method.\n\n        Parameters\n        ----------\n        perils_dict: dict\n                     Dictionary with `peril_id` as key and `fixed_size_geo_grid` parameter dict as\n                     value. i.e `{'peril_id' : {fixed_size_geo_grid parameters}}`\n        \"\"\"\n        def fct(locs_peril):\n            locs_peril[\"area_peril_id\"] = OASIS_UNKNOWN_ID\n            # if `peril_id` not in `perils_dict`\n            shift = len(perils_dict.items())\n            for index, (peril_id, fixed_geo_grid_params) in enumerate(perils_dict.items()):\n                curr_grid_fct = Lookup.build_fixed_size_z_index_geo_grid(**fixed_geo_grid_params)\n\n                curr_locs_peril = locs_peril[locs_peril['peril_id'] == peril_id]\n                curr_locs_peril = curr_grid_fct(curr_locs_peril)\n                curr_locs_peril.loc[\n                    curr_locs_peril[\"area_peril_id\"] != OASIS_UNKNOWN_ID,\n                    \"area_peril_id\"\n                ] = curr_locs_peril[\"area_peril_id\"] * shift + index\n\n                locs_peril[locs_peril[\"peril_id\"] == peril_id] = curr_locs_peril\n\n            return locs_peril\n        return fct\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreate multiple grids of varying resolution, one per peril, and associate an id to each square of the grid using the\n`fixed_size_z_index_geo_grid` method.\n\nParameters\n----------\nperils_dict: dict\n             Dictionary with `peril_id` as key and `fixed_size_geo_grid` parameter dict as\n             value. i.e `{'peril_id' : {fixed_size_geo_grid parameters}}`"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::build_fixed_size_z_index_geo_grid@827",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "build_fixed_size_z_index_geo_grid",
      "lineno": 827,
      "end_lineno": 857,
      "business_stage": "other",
      "docstring": "associate an id to each square of the grid defined by z-order indexing.\nreverse allow to change the ordering of id from (min to max) to\n(max to min)",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: build_fixed_size_z_index_geo_grid (lines 827-857)\n\n    def build_fixed_size_z_index_geo_grid(\n        lat_min, lat_max, lon_min, lon_max, arc_size,\n        lat_reverse=False, lon_reverse=False, lon_first=False\n    ):\n        \"\"\"\n        associate an id to each square of the grid defined by z-order indexing.\n        reverse allow to change the ordering of id from (min to max) to\n        (max to min)\n        \"\"\"\n\n        lat_id, lon_id = create_lat_lon_id_functions(\n            lat_min, lat_max, lon_min, lon_max, arc_size,\n            lat_reverse, lon_reverse\n        )\n\n        @nb.jit(cache=True)\n        def get_id(lat, lon, lat_id, lon_id):\n            if lon_first:\n                return z_index(lon_id(lon), lat_id(lat)) + 1\n            return z_index(lat_id(lat), lon_id(lon)) + 1\n\n        def geo_grid_lookup(locations):\n            locations['area_peril_id'] = jit_geo_grid_lookup(\n                locations['latitude'].to_numpy(),\n                locations['longitude'].to_numpy(),\n                lat_min, lat_max, lon_min, lon_max,\n                get_id, lat_id, lon_id\n            )\n            return locations\n\n        return geo_grid_lookup\n\n\"\"\"Docstring (excerpt)\"\"\"\nassociate an id to each square of the grid defined by z-order indexing.\nreverse allow to change the ordering of id from (min to max) to\n(max to min)"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::build_geotiff@859",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "build_geotiff",
      "lineno": 859,
      "end_lineno": 907,
      "business_stage": "other",
      "docstring": "Args:\n    file_path: path to the geotiff file\n    band_info: a dict where keys are assigned column name, and values are dicts with\n        id is the id of the band in the tiff file\n        default is the(value for outside of range location\nReturns:\n    function to assign band value to each corresponding lat lon",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: build_geotiff (lines 859-907)\n\n    def build_geotiff(self, file_path, band_info):\n        \"\"\"\n\n        Args:\n            file_path: path to the geotiff file\n            band_info: a dict where keys are assigned column name, and values are dicts with\n                id is the id of the band in the tiff file\n                default is the(value for outside of range location\n        Returns:\n            function to assign band value to each corresponding lat lon\n        \"\"\"\n        if gdal is None:\n            raise OasisException(\n                \"##### gdal need to be installed to use geotiff !!!#####\\n\"\n                \"on ubuntu, first install gdal then run pip based on the installed version\\n\"\n                \"-> apt-get update && apt-get install -y gdal-bin\\n\"\n                \"-> gdalinfo --version\\n\"\n                \"-> pip install gdal==<version>\"\n            )\n\n        tiff_dataset = gdal.Open(self.to_abs_filepath(file_path), gdal.GA_ReadOnly)\n        inv_gt = gdal.InvGeoTransform(tiff_dataset.GetGeoTransform())\n\n        defaults = np.empty(len(band_info), dtype=tiff_dataset.GetVirtualMemArray().dtype)\n        usefull_array_idx = np.empty(len(band_info), dtype='int')\n        for i, (col_name, info) in enumerate(band_info.items()):\n            if not 1 <= info['id'] <= tiff_dataset.RasterCount:\n                raise OasisException(f\"band {col_name}, {info} has id outside of [1-{tiff_dataset.RasterCount}]\")\n            idx = info['id'] - 1\n            usefull_array_idx[i] = idx\n            defaults[idx] = info['default']\n\n        def geotiff_lookup(locations):\n            tiff_array = tiff_dataset.GetVirtualMemArray()\n            if len(tiff_array.shape) == 2:\n                tiff_array = tiff_array.reshape((tiff_array.shape[0], tiff_array.shape[1], 1))\n            res = np.empty((len(locations), len(band_info)), dtype=tiff_array.dtype)\n            jit_gda_loc_to_val(tiff_array,\n                               inv_gt,\n                               locations['longitude'].to_numpy(),\n                               locations['latitude'].to_numpy(),\n                               usefull_array_idx,\n                               defaults,\n                               res)\n            for col_i, col_name in enumerate(band_info.keys()):\n                locations[col_name] = res[:, col_i]\n            return locations\n\n        return geotiff_lookup\n\n\"\"\"Docstring (excerpt)\"\"\"\nArgs:\n    file_path: path to the geotiff file\n    band_info: a dict where keys are assigned column name, and values are dicts with\n        id is the id of the band in the tiff file\n        default is the(value for outside of range location\nReturns:\n    function to assign band value to each corresponding lat lon"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::build_merge@909",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "build_merge",
      "lineno": 909,
      "end_lineno": 928,
      "business_stage": "other",
      "docstring": "this method will merge the locations Dataframe with the Dataframe present in file_path\nAll non match column present in id_columns will be set to -1\n\nthis is an efficient way to map a combination of column that have a finite scope to an idea.",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: build_merge (lines 909-928)\n\n    def build_merge(self, file_path, id_columns=[], file_type='csv', **kwargs):\n        \"\"\"\n        this method will merge the locations Dataframe with the Dataframe present in file_path\n        All non match column present in id_columns will be set to -1\n\n        this is an efficient way to map a combination of column that have a finite scope to an idea.\n        \"\"\"\n\n        read_func = getattr(pd, f\"read_{file_type}\", None)\n        if callable(read_func):\n            df_to_merge = read_func(self.to_abs_filepath(file_path), **kwargs)\n        else:\n            df_to_merge = pd.read_csv(self.to_abs_filepath(file_path), **kwargs)\n        df_to_merge.rename(columns={column: column.lower() for column in df_to_merge.columns}, inplace=True)\n\n        def merge(locations: pd.DataFrame):\n            rename_map = {col.lower(): col for col in locations.columns if col.lower() in df_to_merge.columns}\n            locations = locations.merge(df_to_merge.rename(columns=rename_map), how='left')\n            return self.set_id_columns(locations, id_columns)\n        return merge\n\n\"\"\"Docstring (excerpt)\"\"\"\nthis method will merge the locations Dataframe with the Dataframe present in file_path\nAll non match column present in id_columns will be set to -1\n\nthis is an efficient way to map a combination of column that have a finite scope to an idea."
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::build_simple_pivot@931",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "build_simple_pivot",
      "lineno": 931,
      "end_lineno": 972,
      "business_stage": "other",
      "docstring": "allow to pivot columns of the locations dataframe into multiple rows\neach pivot in the pivot list may define:\n    \"on\": to rename a column into a new one\n    \"new_cols\": to create a new column with a certain values\nex:\n\"pivots\": [{\"on\": {\"vuln_str\": \"vulnerability_id\"},\n         \"new_cols\": {\"coverage_type\": 1}},\n        {\"on\": {\"vuln_con\": \"vulnerability_id\"},\n         \"new_cols\": {\"coverage_type\": 3}},\n       ],\nloc_id  vuln_str    vuln_con\n1       3           2\n2       18          4\n\n=>\nloc_id  vuln_str    vuln_con    vulnerability_id    coverage_type\n1       3           2           3                   1\n2       18          4           18                  1\n1       3           2           2                   3\n2       18          4           4                   3",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: build_simple_pivot (lines 931-972)\n\n    def build_simple_pivot(pivots, remove_pivoted_col=True):\n        \"\"\"\n        allow to pivot columns of the locations dataframe into multiple rows\n        each pivot in the pivot list may define:\n            \"on\": to rename a column into a new one\n            \"new_cols\": to create a new column with a certain values\n        ex:\n        \"pivots\": [{\"on\": {\"vuln_str\": \"vulnerability_id\"},\n                 \"new_cols\": {\"coverage_type\": 1}},\n                {\"on\": {\"vuln_con\": \"vulnerability_id\"},\n                 \"new_cols\": {\"coverage_type\": 3}},\n               ],\n        loc_id  vuln_str    vuln_con\n        1       3           2\n        2       18          4\n\n        =>\n        loc_id  vuln_str    vuln_con    vulnerability_id    coverage_type\n        1       3           2           3                   1\n        2       18          4           18                  1\n        1       3           2           2                   3\n        2       18          4           4                   3\n\n\n        \"\"\"\n        def simple_pivot(locations):\n            pivoted_dfs = []\n            pivoted_cols = set()\n            for pivot in pivots:\n                pivot_df = locations.copy()\n                for old_name, new_name in pivot.get(\"on\", {}).items():\n                    pivot_df[new_name] = pivot_df[old_name]\n                    pivoted_cols.add(old_name)\n                for col_name, value in pivot.get(\"new_cols\", {}).items():\n                    pivot_df[col_name] = value\n                pivoted_dfs.append(pivot_df)\n            locations = pd.concat(pivoted_dfs, ignore_index=True)\n            if remove_pivoted_col:\n                locations.drop(columns=pivoted_cols, inplace=True)\n            return locations\n\n        return simple_pivot\n\n\"\"\"Docstring (excerpt)\"\"\"\nallow to pivot columns of the locations dataframe into multiple rows\neach pivot in the pivot list may define:\n    \"on\": to rename a column into a new one\n    \"new_cols\": to create a new column with a certain values\nex:\n\"pivots\": [{\"on\": {\"vuln_str\": \"vulnerability_id\"},\n         \"new_cols\": {\"coverage_type\": 1}},\n        {\"on\": {\"vuln_con\": \"vulnerability_id\"},\n         \"new_cols\": {\"coverage_type\": 3}},\n       ],\nloc_id  vuln_str    vuln_con\n1       3           2\n2       18          4\n\n=>\nloc_id  vuln_str    vuln_con    vulnerability_id    coverage_type\n1       3           2           3                   1\n2       18          4           18                  1\n1       3           2           2                   3\n2       18          4           4                   3"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::build_model_data@975",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "build_model_data",
      "lineno": 975,
      "end_lineno": 993,
      "business_stage": "other",
      "docstring": "Serialises specified columns from the OED file into a model_data dict",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: build_model_data (lines 975-993)\n\n    def build_model_data(columns):\n        \"\"\"\n        Serialises specified columns from the OED file into a model_data dict\n        \"\"\"\n        lst_model_data = []\n\n        def model_data(locations):\n            # could improve with apply lambda\n            for index, i in locations.iterrows():\n                tmp_dict = {}\n                for col in columns:\n                    tmp_dict[col] = i[col]\n                lst_model_data.append(tmp_dict)\n\n            locations['model_data'] = lst_model_data\n\n            return locations\n\n        return model_data\n\n\"\"\"Docstring (excerpt)\"\"\"\nSerialises specified columns from the OED file into a model_data dict"
    },
    {
      "chunk_id": "oasislmf/lookup/builtin.py::build_dynamic_model_adjustment@996",
      "source_type": "code",
      "path": "oasislmf/lookup/builtin.py",
      "symbol_type": "function",
      "name": "build_dynamic_model_adjustment",
      "lineno": 996,
      "end_lineno": 1015,
      "business_stage": "other",
      "docstring": "Converts specified columns from the OED file into intensity adjustments and\nreturn period protection.",
      "content": "# File: oasislmf/lookup/builtin.py\n# function: build_dynamic_model_adjustment (lines 996-1015)\n\n    def build_dynamic_model_adjustment(intensity_adjustment_col, return_period_col):\n        \"\"\"\n        Converts specified columns from the OED file into intensity adjustments and\n        return period protection.\n        \"\"\"\n        lst_intensity_adjustment = []\n        lst_return_period = []\n\n        def adjustments(locations):\n            for index, row in locations.iterrows():\n                intensity_adjustment = row[intensity_adjustment_col]\n                return_period = row[return_period_col]\n                lst_intensity_adjustment.append(intensity_adjustment)\n                lst_return_period.append(return_period)\n\n            locations['intensity_adjustment'] = lst_intensity_adjustment\n            locations['return_period'] = lst_return_period\n            return locations\n\n        return adjustments\n\n\"\"\"Docstring (excerpt)\"\"\"\nConverts specified columns from the OED file into intensity adjustments and\nreturn period protection."
    },
    {
      "chunk_id": "oasislmf/lookup/factory.py::KeyServerFactory@99",
      "source_type": "code",
      "path": "oasislmf/lookup/factory.py",
      "symbol_type": "class",
      "name": "KeyServerFactory",
      "lineno": 99,
      "end_lineno": 217,
      "business_stage": "other",
      "docstring": "A factory class to create the Keys Server that will be use to generate the keys files\nAll Key Server must implement the interface defined in lookup.interface.KeyServerInterface\n\nOasis provides a built-in Key Server that manage the generation of the key files from the key provided by\na built-in or a custom Key Lookup.\n\nThe factory now return a KeyServer object and not a KeyLookup.\nThe parameter to pass has also been simplified\nusage of all the below parameter are now deprecated\n  - complex_lookup_config_fp => pass the path to your complex lookup config directly in lookup_config_fg\n  - lookup_module_path => set as key 'lookup_module_path' in the lookup config\n  - model_keys_data_path => set as key 'keys_data_path' in the lookup config\n  - model_version_file_path => set the model information ('supplier_id', 'model_id', 'model_version') directly\n    into the config",
      "content": "# File: oasislmf/lookup/factory.py\n# class: KeyServerFactory (lines 99-217)\n\nclass KeyServerFactory(object):\n    \"\"\"\n    A factory class to create the Keys Server that will be use to generate the keys files\n    All Key Server must implement the interface defined in lookup.interface.KeyServerInterface\n\n    Oasis provides a built-in Key Server that manage the generation of the key files from the key provided by\n    a built-in or a custom Key Lookup.\n\n    The factory now return a KeyServer object and not a KeyLookup.\n    The parameter to pass has also been simplified\n    usage of all the below parameter are now deprecated\n      - complex_lookup_config_fp => pass the path to your complex lookup config directly in lookup_config_fg\n      - lookup_module_path => set as key 'lookup_module_path' in the lookup config\n      - model_keys_data_path => set as key 'keys_data_path' in the lookup config\n      - model_version_file_path => set the model information ('supplier_id', 'model_id', 'model_version') directly\n        into the config\n    \"\"\"\n\n    @classmethod\n    def get_config(cls, config_fp):\n        return as_path(os.path.dirname(config_fp), 'config_fp'), get_json(config_fp)\n\n    @classmethod\n    def get_model_info(cls, model_version_file_path):\n        \"\"\"\n        Get model information from the model version file.\n        \"\"\"\n        model_version_file_path = as_path(model_version_file_path, 'model_version_file_path', preexists=True, null_is_valid=False)\n\n        with open(model_version_file_path, 'r', encoding='utf-8') as f:\n            return next(csv.DictReader(\n                f, fieldnames=['supplier_id', 'model_id', 'model_version']\n            ))\n\n    @classmethod\n    def update_deprecated_args(cls, config_dir, config,\n                               complex_lookup_config_fp, model_keys_data_path, model_version_file_path, lookup_module_path):\n        if (complex_lookup_config_fp or model_keys_data_path or model_version_file_path or lookup_module_path):\n            warnings.warn('usage of complex_lookup_config_fp, model_keys_data_path, '\n                          'model_version_file_path and lookup_module_path is now deprecated'\n                          'those variables now need to be set in lookup config see (key server documentation)')\n\n        if complex_lookup_config_fp and not isinstance(complex_lookup_config_fp, dict):\n            config_dir, config = cls.get_config(complex_lookup_config_fp)\n\n        if model_keys_data_path:\n            config['keys_data_path'] = as_path(model_keys_data_path, 'model_keys_data_path', preexists=True)\n\n        if model_version_file_path:\n            config['model'] = cls.get_model_info(model_version_file_path)\n\n        if lookup_module_path:\n            config['lookup_module_path'] = lookup_module_path\n\n        return config_dir, config\n\n    @classmethod\n    def create(\n            cls,\n            model_keys_data_path=None,\n            model_version_file_path=None,\n            lookup_module_path=None,\n            lookup_config=None,\n            lookup_config_json=None,\n            lookup_config_fp=None,\n            complex_lookup_config_fp=None,  # this is now use to pass the run settings if there is no complex_lookup_config_fp\n            user_data_dir=None,\n            output_directory=None,\n    ):\n        \"\"\"\n        Creates a keys lookup class instance for the given model and supplier -\n        local file paths are required for the model keys data folder, the model\n        version file and the Git repository for the model keys server. Returns a\n        pair ``(model_info, klc)``, where ``model_info`` is a dictionary holding\n        model information from the model version file and `klc` is the lookup\n        service class instance for the model.\n        \"\"\"\n        if lookup_config:\n            config_dir = '.'\n            config = lookup_config\n\n\"\"\"Docstring (excerpt)\"\"\"\nA factory class to create the Keys Server that will be use to generate the keys files\nAll Key Server must implement the interface defined in lookup.interface.KeyServerInterface\n\nOasis provides a built-in Key Server that manage the generation of the key files from the key provided by\na built-in or a custom Key Lookup.\n\nThe factory now return a KeyServer object and not a KeyLookup.\nThe parameter to pass has also been simplified\nusage of all the below parameter are now deprecated\n  - complex_lookup_config_fp => pass the path to your complex lookup config directly in lookup_config_fg\n  - lookup_module_path => set as key 'lookup_module_path' in the lookup config\n  - model_keys_data_path => set as key 'keys_data_path' in the lookup config\n  - model_version_file_path => set the model information ('supplier_id', 'model_id', 'model_version') directly\n    into the config"
    },
    {
      "chunk_id": "oasislmf/lookup/factory.py::BasicKeyServer@220",
      "source_type": "code",
      "path": "oasislmf/lookup/factory.py",
      "symbol_type": "class",
      "name": "BasicKeyServer",
      "lineno": 220,
      "end_lineno": 624,
      "business_stage": "other",
      "docstring": "A basic implementation of the KeyServerInterface\nwill load the KeyLookup class from config['lookup_module_path'] if present or used the built-in KeyLookup\nKeyLookup must implement the KeyLookupInterface\n\nwill provide a multiprocess solution if KeyLoopup implement the process_locations_multiproc method\n\nboth single and multiprocess solutions will use low amount of memory\nas they process the key by chunk of limited size.\n\nThis class implement all the file writing method that were previously handled by the lookup factory",
      "content": "# File: oasislmf/lookup/factory.py\n# class: BasicKeyServer (lines 220-624)\n\nclass BasicKeyServer:\n    \"\"\"\n    A basic implementation of the KeyServerInterface\n    will load the KeyLookup class from config['lookup_module_path'] if present or used the built-in KeyLookup\n    KeyLookup must implement the KeyLookupInterface\n\n    will provide a multiprocess solution if KeyLoopup implement the process_locations_multiproc method\n\n    both single and multiprocess solutions will use low amount of memory\n    as they process the key by chunk of limited size.\n\n    This class implement all the file writing method that were previously handled by the lookup factory\n    \"\"\"\n    interface_version = \"1\"\n\n    valid_format = ['oasis', 'json']\n\n    error_heading_row = OrderedDict([\n        ('loc_id', 'LocID'),\n        ('peril_id', 'PerilID'),\n        ('coverage_type', 'CoverageTypeID'),\n        ('status', 'Status'),\n        ('message', 'Message'),\n    ])\n\n    model_data_heading_row = OrderedDict([\n        ('loc_id', 'LocID'),\n        ('peril_id', 'PerilID'),\n        ('coverage_type', 'CoverageTypeID'),\n        ('model_data', 'ModelData'),\n    ])\n\n    dynamic_model_data_heading_row = OrderedDict([\n        ('loc_id', 'LocID'),\n        ('peril_id', 'PerilID'),\n        ('coverage_type', 'CoverageTypeID'),\n        ('area_peril_id', 'AreaPerilID'),\n        ('vulnerability_id', 'VulnerabilityID'),\n        ('intensity_adjustment', 'IntensityAdjustment'),\n        ('return_period', 'ReturnPeriod'),\n        ('section_id', 'section_id'),\n    ])\n\n    model_data_with_amplification_heading_row = OrderedDict([\n        ('loc_id', 'LocID'),\n        ('peril_id', 'PerilID'),\n        ('coverage_type', 'CoverageTypeID'),\n        ('model_data', 'ModelData'),\n        ('amplification_id', 'AmplificationID')\n    ])\n\n    dynamic_model_with_amplification_data_heading_row = OrderedDict([\n        ('loc_id', 'LocID'),\n        ('peril_id', 'PerilID'),\n        ('coverage_type', 'CoverageTypeID'),\n        ('area_peril_id', 'AreaPerilID'),\n        ('vulnerability_id', 'VulnerabilityID'),\n        ('intensity_adjustment', 'IntensityAdjustment'),\n        ('return_period', 'ReturnPeriod'),\n        ('section_id', 'section_id'),\n        ('amplification_id', 'AmplificationID')\n    ])\n\n    key_success_heading_row = OrderedDict([\n        ('loc_id', 'LocID'),\n        ('peril_id', 'PerilID'),\n        ('coverage_type', 'CoverageTypeID'),\n        ('area_peril_id', 'AreaPerilID'),\n        ('vulnerability_id', 'VulnerabilityID'),\n    ])\n\n    key_success_with_amplification_heading_row = OrderedDict([\n        ('loc_id', 'LocID'),\n        ('peril_id', 'PerilID'),\n        ('coverage_type', 'CoverageTypeID'),\n        ('area_peril_id', 'AreaPerilID'),\n        ('vulnerability_id', 'VulnerabilityID'),\n        ('amplification_id', 'AmplificationID')\n    ])\n\n\"\"\"Docstring (excerpt)\"\"\"\nA basic implementation of the KeyServerInterface\nwill load the KeyLookup class from config['lookup_module_path'] if present or used the built-in KeyLookup\nKeyLookup must implement the KeyLookupInterface\n\nwill provide a multiprocess solution if KeyLoopup implement the process_locations_multiproc method\n\nboth single and multiprocess solutions will use low amount of memory\nas they process the key by chunk of limited size.\n\nThis class implement all the file writing method that were previously handled by the lookup factory"
    },
    {
      "chunk_id": "oasislmf/lookup/factory.py::get_model_info@122",
      "source_type": "code",
      "path": "oasislmf/lookup/factory.py",
      "symbol_type": "function",
      "name": "get_model_info",
      "lineno": 122,
      "end_lineno": 131,
      "business_stage": "other",
      "docstring": "Get model information from the model version file.",
      "content": "# File: oasislmf/lookup/factory.py\n# function: get_model_info (lines 122-131)\n\n    def get_model_info(cls, model_version_file_path):\n        \"\"\"\n        Get model information from the model version file.\n        \"\"\"\n        model_version_file_path = as_path(model_version_file_path, 'model_version_file_path', preexists=True, null_is_valid=False)\n\n        with open(model_version_file_path, 'r', encoding='utf-8') as f:\n            return next(csv.DictReader(\n                f, fieldnames=['supplier_id', 'model_id', 'model_version']\n            ))\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet model information from the model version file."
    },
    {
      "chunk_id": "oasislmf/lookup/factory.py::create@156",
      "source_type": "code",
      "path": "oasislmf/lookup/factory.py",
      "symbol_type": "function",
      "name": "create",
      "lineno": 156,
      "end_lineno": 217,
      "business_stage": "other",
      "docstring": "Creates a keys lookup class instance for the given model and supplier -\nlocal file paths are required for the model keys data folder, the model\nversion file and the Git repository for the model keys server. Returns a\npair ``(model_info, klc)``, where ``model_info`` is a dictionary holding\nmodel information from the model version file and `klc` is the lookup\nservice class instance for the model.",
      "content": "# File: oasislmf/lookup/factory.py\n# function: create (lines 156-217)\n\n    def create(\n            cls,\n            model_keys_data_path=None,\n            model_version_file_path=None,\n            lookup_module_path=None,\n            lookup_config=None,\n            lookup_config_json=None,\n            lookup_config_fp=None,\n            complex_lookup_config_fp=None,  # this is now use to pass the run settings if there is no complex_lookup_config_fp\n            user_data_dir=None,\n            output_directory=None,\n    ):\n        \"\"\"\n        Creates a keys lookup class instance for the given model and supplier -\n        local file paths are required for the model keys data folder, the model\n        version file and the Git repository for the model keys server. Returns a\n        pair ``(model_info, klc)``, where ``model_info`` is a dictionary holding\n        model information from the model version file and `klc` is the lookup\n        service class instance for the model.\n        \"\"\"\n        if lookup_config:\n            config_dir = '.'\n            config = lookup_config\n        elif lookup_config_json:\n            config_dir = '.'\n            config = json.loads(lookup_config_json)\n        elif lookup_config_fp:\n            config_dir, config = cls.get_config(lookup_config_fp)\n        else:  # no config\n            config_dir, config = '.', {}\n\n        if not config:\n            config_dir, config = cls.update_deprecated_args(config_dir, config,\n                                                            complex_lookup_config_fp, model_keys_data_path,\n                                                            model_version_file_path, lookup_module_path)\n        else:  # reproduce lookup_config overwrite complex_lookup_config_fp\n            if isinstance(complex_lookup_config_fp, dict):\n                config['settings'] = complex_lookup_config_fp\n            elif complex_lookup_config_fp:\n                complex_config_dir, complex_config = cls.get_config(complex_lookup_config_fp)\n                config['complex_config_dir'] = complex_config_dir\n                config['complex_config'] = complex_config\n            complex_lookup_config_fp = None\n\n        if config.get('key_server_module_path'):\n            _KeyServer_module = get_custom_module(config.get('key_server_module_path'), 'key_server_module_path')\n            _KeyServer = getattr(_KeyServer_module, '{}KeysServer'.format(config['model']['model_id']))\n        else:\n            _KeyServer = BasicKeyServer\n\n        if _KeyServer.interface_version == '1':\n            key_server = _KeyServer(config,\n                                    config_dir=config_dir,\n                                    user_data_dir=user_data_dir,\n                                    output_dir=output_directory)\n        else:\n            raise OasisException(f\"KeyServer interface version {_KeyServer.interface_version} not implemented\")\n\n        if complex_lookup_config_fp:\n            key_server.complex_lookup_config_fp = complex_lookup_config_fp\n\n        return config['model'], key_server\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreates a keys lookup class instance for the given model and supplier -\nlocal file paths are required for the model keys data folder, the model\nversion file and the Git repository for the model keys server. Returns a\npair ``(model_info, klc)``, where ``model_info`` is a dictionary holding\nmodel information from the model version file and `klc` is the lookup\nservice class instance for the model."
    },
    {
      "chunk_id": "oasislmf/lookup/factory.py::get_locations@397",
      "source_type": "code",
      "path": "oasislmf/lookup/factory.py",
      "symbol_type": "function",
      "name": "get_locations",
      "lineno": 397,
      "end_lineno": 401,
      "business_stage": "other",
      "docstring": "load exposure data from location_fp and return the exposure dataframe",
      "content": "# File: oasislmf/lookup/factory.py\n# function: get_locations (lines 397-401)\n\n    def get_locations(self, location_fp):\n        \"\"\"load exposure data from location_fp and return the exposure dataframe\"\"\"\n        raise NotImplementedError('oasislmf now use ods_tools to pass location to the KeyServer. '\n                                  'this method need to be implemented'\n                                  'if you want to provide you own loader from filepath')\n\n\"\"\"Docstring (excerpt)\"\"\"\nload exposure data from location_fp and return the exposure dataframe"
    },
    {
      "chunk_id": "oasislmf/lookup/factory.py::generate_key_files_multiproc@528",
      "source_type": "code",
      "path": "oasislmf/lookup/factory.py",
      "symbol_type": "function",
      "name": "generate_key_files_multiproc",
      "lineno": 528,
      "end_lineno": 580,
      "business_stage": "other",
      "docstring": "Process and return the lookup results a location row\nUsed in multiprocessing based query\n\nlocation_row is of type <class 'pandas.core.series.Series'>",
      "content": "# File: oasislmf/lookup/factory.py\n# function: generate_key_files_multiproc (lines 528-580)\n\n    def generate_key_files_multiproc(self, loc_df, successes_fp, errors_fp, output_format, keys_success_msg,\n                                     num_cores, num_partitions, **kwargs):\n        \"\"\"\n        Process and return the lookup results a location row\n        Used in multiprocessing based query\n\n        location_row is of type <class 'pandas.core.series.Series'>\n\n        \"\"\"\n        pool_count = num_cores if num_cores > 0 else multiprocessing.cpu_count()\n        if num_partitions > 0:\n            part_count = num_partitions\n        else:\n            bloc_size = min(max(math.ceil(loc_df.shape[0] / pool_count), self.min_bloc_size), self.max_bloc_size)\n            part_count = math.ceil(loc_df.shape[0] / bloc_size)\n            pool_count = min(pool_count, part_count)\n        if pool_count <= 1:\n            return self.generate_key_files_singleproc(loc_df, successes_fp, errors_fp, output_format, keys_success_msg)\n\n        ct = multiprocessing.get_context(\"fork\")\n        loc_queue = ct.Queue(maxsize=pool_count)\n        key_queue = ct.Queue(maxsize=pool_count)\n        error_queue = ct.Queue()\n\n        this_location_producer = ct.Process(target=location_producer, args=(error_queue, loc_df, part_count, loc_queue))\n\n        workers = [ct.Process(target=lookup_multiproc_worker,\n                              args=(error_queue, self.lookup_cls, self.config, self.config_dir,\n                                    self.user_data_dir, self.output_dir,\n                                    lookup_id, loc_queue, key_queue))\n                   for lookup_id in range(pool_count)]\n\n        this_location_producer.start()\n        [worker.start() for worker in workers]\n\n        try:\n            return self.write_keys_file(self.key_producer(key_queue, error_queue, worker_count=pool_count),\n                                        successes_fp=successes_fp,\n                                        errors_fp=errors_fp,\n                                        output_format=output_format,\n                                        keys_success_msg=keys_success_msg, )\n        except Exception:\n            error_queue.put(sys.exc_info())\n        finally:\n            for process in [this_location_producer] + workers:\n                if process.is_alive():\n                    process.terminate()\n                    process.join()\n            loc_queue.close()\n            key_queue.close()\n            if not error_queue.empty():\n                exc_info = error_queue.get()\n                raise exc_info[0].with_traceback(exc_info[1], exc_info[2])\n\n\"\"\"Docstring (excerpt)\"\"\"\nProcess and return the lookup results a location row\nUsed in multiprocessing based query\n\nlocation_row is of type <class 'pandas.core.series.Series'>"
    },
    {
      "chunk_id": "oasislmf/lookup/factory.py::generate_key_files@583",
      "source_type": "code",
      "path": "oasislmf/lookup/factory.py",
      "symbol_type": "function",
      "name": "generate_key_files",
      "lineno": 583,
      "end_lineno": 624,
      "business_stage": "other",
      "docstring": "generate key files by calling:\n1. get_locations to get a location object from the location_fp\n2. process_locations or process_locations_multiproc to get results object from the locations object\n3. write_keys_file to writes the relevant files from the results object",
      "content": "# File: oasislmf/lookup/factory.py\n# function: generate_key_files (lines 583-624)\n\n    def generate_key_files(\n            self,\n            location_fp=None,\n            successes_fp=None,\n            errors_fp=None,\n            output_format='oasis',\n            keys_success_msg=False,\n            multiproc_enabled=True,\n            multiproc_num_cores=-1,\n            multiproc_num_partitions=-1,\n            location_df=None,\n            **kwargs\n    ):\n        \"\"\"\n        generate key files by calling:\n        1. get_locations to get a location object from the location_fp\n        2. process_locations or process_locations_multiproc to get results object from the locations object\n        3. write_keys_file to writes the relevant files from the results object\n        \"\"\"\n        successes_fp = as_path(successes_fp, 'successes_fp', preexists=False)\n        errors_fp = as_path(errors_fp, 'errors_fp', preexists=False)\n\n        if location_df is not None:\n            locations = location_df\n        else:\n            locations = self.get_locations(location_fp)  # need overwrite as not supported anymore we pass the df\n\n        if multiproc_enabled and hasattr(self.lookup_cls, 'process_locations_multiproc'):\n            return self.generate_key_files_multiproc(locations,\n                                                     successes_fp=successes_fp,\n                                                     errors_fp=errors_fp,\n                                                     output_format=output_format,\n                                                     keys_success_msg=keys_success_msg,\n                                                     num_cores=multiproc_num_cores,\n                                                     num_partitions=multiproc_num_partitions)\n        else:\n            return self.generate_key_files_singleproc(locations,\n                                                      successes_fp=successes_fp,\n                                                      errors_fp=errors_fp,\n                                                      output_format=output_format,\n                                                      keys_success_msg=keys_success_msg,\n                                                      )\n\n\"\"\"Docstring (excerpt)\"\"\"\ngenerate key files by calling:\n1. get_locations to get a location object from the location_fp\n2. process_locations or process_locations_multiproc to get results object from the locations object\n3. write_keys_file to writes the relevant files from the results object"
    },
    {
      "chunk_id": "oasislmf/lookup/interface.py::KeyServerInterface@20",
      "source_type": "code",
      "path": "oasislmf/lookup/interface.py",
      "symbol_type": "class",
      "name": "KeyServerInterface",
      "lineno": 20,
      "end_lineno": 101,
      "business_stage": "other",
      "docstring": "Interface to implement to create a KeyServer\nIt define the method to be implemented to be used correctly in lookup.factory.KeyServerFactory\nall classes must:\n - specify the version of the interface they use\n - implement the init method\n - implement the generate_key_files method",
      "content": "# File: oasislmf/lookup/interface.py\n# class: KeyServerInterface (lines 20-101)\n\nclass KeyServerInterface(metaclass=abc.ABCMeta):\n    \"\"\"\n    Interface to implement to create a KeyServer\n    It define the method to be implemented to be used correctly in lookup.factory.KeyServerFactory\n    all classes must:\n     - specify the version of the interface they use\n     - implement the init method\n     - implement the generate_key_files method\n\n    \"\"\"\n    interface_version = \"1\"\n\n    @abc.abstractmethod\n    def __init__(self, config, config_dir, user_data_dir, output_dir):\n        \"\"\"\n        During the key generation step, the generic factory will call the constructor of the lookup class with the\n        following parameters.\n\n        :param config: contains all the information necessary to run the model\n        :type config: dict\n\n        :param config_dir: path to the model directory, can be used to locate relative path to all the files\n                           that serve as base for the model\n        :type config_dir: str\n\n        :param user_data_dir: Path to additional data necessary for the model that can vary from analysis to analysis\n        :type user_data_dir: str\n\n        :param output_dir: Path to the analysis output directory, can be use to write additional files that are produce\n                           during the keys file generation\n\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def generate_key_files(self,\n                           location_fp,\n                           successes_fp,\n                           errors_fp=None,\n                           output_format='oasis',\n                           keys_success_msg=False,\n                           multiproc_enabled=True,\n                           multiproc_num_cores=-1,\n                           multiproc_num_partitions=-1,\n                           **kwargs):\n        \"\"\"\n        Writes a keys file, and optionally a keys error file.\n\n        :param location_fp: path to the locations file\n        :type location_fp: str\n\n        :param successes_fp: path to the success keys file\n        :type successes_fp: str\n\n        :param errors_fp: path to the error keys file (optional)\n        :type errors_fp: str\n\n        :param output_format: format of the keys files (oasis or json)\n        :type output_format: str\n\n        :param keys_success_msg: option to write msg for success key\n        :type keys_success_msg: bool\n\n        :param multiproc_enabled: option to run with multiple processor\n        :type multiproc_enabled: bool\n\n        :param multiproc_num_cores: number of cores to use in multiproc mode\n        :type multiproc_num_cores: int\n\n        :param multiproc_num_partitions: number of partition to create in multiproc mode\n        :type multiproc_num_partitions: int\n\n        If ``keys_errors_file_path`` is not present then the method returns a\n        pair ``(p, n)`` where ``p`` is the keys file path and ``n`` is the\n        number of \"successful\" keys records written to the keys file, otherwise\n        it returns a quadruple ``(p1, n1, p2, n2)`` where ``p1`` is the keys\n        file path, ``n1`` is the number of \"successful\" keys records written to\n        the keys file, ``p2`` is the keys errors file path and ``n2`` is the\n        number of \"unsuccessful\" keys records written to keys errors file.\n\n\"\"\"Docstring (excerpt)\"\"\"\nInterface to implement to create a KeyServer\nIt define the method to be implemented to be used correctly in lookup.factory.KeyServerFactory\nall classes must:\n - specify the version of the interface they use\n - implement the init method\n - implement the generate_key_files method"
    },
    {
      "chunk_id": "oasislmf/lookup/interface.py::KeyLookupInterface@104",
      "source_type": "code",
      "path": "oasislmf/lookup/interface.py",
      "symbol_type": "class",
      "name": "KeyLookupInterface",
      "lineno": 104,
      "end_lineno": 124,
      "business_stage": "other",
      "docstring": "Interface for KeyLookup\nit define the interface to be used correctly by lookup.factory.BasicKeyServer\nall classes must:\n - specify the version of the interface they use\n - implement the init method\n - implement the process_location method",
      "content": "# File: oasislmf/lookup/interface.py\n# class: KeyLookupInterface (lines 104-124)\n\nclass KeyLookupInterface(metaclass=abc.ABCMeta):\n    \"\"\"Interface for KeyLookup\n    it define the interface to be used correctly by lookup.factory.BasicKeyServer\n    all classes must:\n     - specify the version of the interface they use\n     - implement the init method\n     - implement the process_location method\n    \"\"\"\n\n    interface_version = \"1\"\n\n    @abc.abstractmethod\n    def __init__(self, config, config_dir, user_data_dir, output_dir):\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def process_locations(self, loc_df):\n        \"\"\"\n        Process location rows - passed in as a pandas dataframe.\n        \"\"\"\n        raise NotImplementedError\n\n\"\"\"Docstring (excerpt)\"\"\"\nInterface for KeyLookup\nit define the interface to be used correctly by lookup.factory.BasicKeyServer\nall classes must:\n - specify the version of the interface they use\n - implement the init method\n - implement the process_location method"
    },
    {
      "chunk_id": "oasislmf/lookup/interface.py::OasisLookupInterface@127",
      "source_type": "code",
      "path": "oasislmf/lookup/interface.py",
      "symbol_type": "class",
      "name": "OasisLookupInterface",
      "lineno": 127,
      "end_lineno": 205,
      "business_stage": "other",
      "docstring": "Old Oasis base class -deprecated\nIf you were using this interface, you can make you class inherit from the new abstract class AbstractBasicKeyServer\nor implement the KeyServerInterface interface",
      "content": "# File: oasislmf/lookup/interface.py\n# class: OasisLookupInterface (lines 127-205)\n\nclass OasisLookupInterface:  # pragma: no cover\n    \"\"\"\n    Old Oasis base class -deprecated\n    If you were using this interface, you can make you class inherit from the new abstract class AbstractBasicKeyServer\n    or implement the KeyServerInterface interface\n    \"\"\"\n    interface_version = \"0\"\n\n    @oasis_log()\n    def __init__(\n        self,\n        keys_data_directory=None,\n        supplier=None,\n        model_name=None,\n        model_version=None,\n        complex_lookup_config_fp=None,\n        output_directory=None\n    ):\n        \"\"\"\n        Class constructor\n        \"\"\"\n        if keys_data_directory is not None:\n            self.keys_data_directory = keys_data_directory\n        else:\n            self.keys_data_directory = os.path.join(os.sep, 'var', 'oasis', 'keys_data')\n\n        self.supplier = supplier\n        self.model_name = model_name\n        self.model_version = model_version\n        self.complex_lookup_config_fp = complex_lookup_config_fp\n        self.output_directory = output_directory\n        self.UNKNOWN_ID = -1\n\n    @oasis_log()\n    def process_locations(self, loc_df):\n        \"\"\"\n        Process location rows - passed in as a pandas dataframe.\n        \"\"\"\n        pass\n\n    def _get_area_peril_id(self, record):\n        \"\"\"\n        Get the area peril ID for a particular location record.\n        \"\"\"\n        return self.UNKNOWN_ID, \"Not implemented\"\n\n    def _get_vulnerability_id(self, record):\n        \"\"\"\n        Get the vulnerability ID for a particular location record.\n        \"\"\"\n        return self.UNKNOWN_ID, \"Not implemented\"\n\n    @oasis_log()\n    def _get_area_peril_ids(self, loc_data, include_context=True):\n        \"\"\"\n        Generates area peril IDs in two modes - if include_context is\n        True (default) it will generate location records/rows including\n        the area peril IDs, otherwise it will generate pairs of location\n        IDs and the corresponding area peril IDs.\n        \"\"\"\n        pass\n\n    @oasis_log()\n    def _get_vulnerability_ids(self, loc_data, include_context=True):\n        \"\"\"\n        Generates vulnerability IDs in two modes - if include_context is\n        True (default) it will generate location records/rows including\n        the area peril IDs, otherwise it will generate pairs of location\n        IDs and the corresponding vulnerability IDs.\n        \"\"\"\n        pass\n\n    def _get_custom_lookup_success(self, ap_id, vul_id):\n        \"\"\"\n        Determine the status of the keys lookup.\n        \"\"\"\n        if ap_id == self.UNKNOWN_ID or vul_id == self.UNKNOWN_ID:\n            return OASIS_KEYS_STATUS['nomatch']['id']\n        return OASIS_KEYS_STATUS['success']['id']\n\n\"\"\"Docstring (excerpt)\"\"\"\nOld Oasis base class -deprecated\nIf you were using this interface, you can make you class inherit from the new abstract class AbstractBasicKeyServer\nor implement the KeyServerInterface interface"
    },
    {
      "chunk_id": "oasislmf/lookup/interface.py::generate_key_files@56",
      "source_type": "code",
      "path": "oasislmf/lookup/interface.py",
      "symbol_type": "function",
      "name": "generate_key_files",
      "lineno": 56,
      "end_lineno": 101,
      "business_stage": "other",
      "docstring": "Writes a keys file, and optionally a keys error file.\n\n:param location_fp: path to the locations file\n:type location_fp: str\n\n:param successes_fp: path to the success keys file\n:type successes_fp: str\n\n:param errors_fp: path to the error keys file (optional)\n:type errors_fp: str\n\n:param output_format: format of the keys files (oasis or json)\n:type output_format: str\n\n:param keys_success_msg: option to write msg for success key\n:type keys_success_msg: bool\n\n:param multiproc_enabled: option to run with multiple processor\n:type multiproc_enabled: bool\n\n:param multiproc_num_cores: number of cores to use in multiproc mode\n:type multiproc_num_cores: int\n\n:param multiproc_num_partitions: number of partition to create in multiproc mode\n:type multiproc_num_partitions: int\n\nIf ``keys_errors_file_path`` is not present then the method returns a\npair ``(p, n)`` where ``p`` is the keys file path and ``n`` is the\nnumber of \"successful\" keys records written to the keys file, otherwise\nit returns a quadruple ``(p1, n1, p2, n2)`` where ``p1`` is the keys\nfile path, ``n1`` is the number of \"successful\" keys records written to\nthe keys file, ``p2`` is the keys errors file path and ``n2`` is the\nnumber of \"unsuccessful\" keys records written to keys errors file.",
      "content": "# File: oasislmf/lookup/interface.py\n# function: generate_key_files (lines 56-101)\n\n    def generate_key_files(self,\n                           location_fp,\n                           successes_fp,\n                           errors_fp=None,\n                           output_format='oasis',\n                           keys_success_msg=False,\n                           multiproc_enabled=True,\n                           multiproc_num_cores=-1,\n                           multiproc_num_partitions=-1,\n                           **kwargs):\n        \"\"\"\n        Writes a keys file, and optionally a keys error file.\n\n        :param location_fp: path to the locations file\n        :type location_fp: str\n\n        :param successes_fp: path to the success keys file\n        :type successes_fp: str\n\n        :param errors_fp: path to the error keys file (optional)\n        :type errors_fp: str\n\n        :param output_format: format of the keys files (oasis or json)\n        :type output_format: str\n\n        :param keys_success_msg: option to write msg for success key\n        :type keys_success_msg: bool\n\n        :param multiproc_enabled: option to run with multiple processor\n        :type multiproc_enabled: bool\n\n        :param multiproc_num_cores: number of cores to use in multiproc mode\n        :type multiproc_num_cores: int\n\n        :param multiproc_num_partitions: number of partition to create in multiproc mode\n        :type multiproc_num_partitions: int\n\n        If ``keys_errors_file_path`` is not present then the method returns a\n        pair ``(p, n)`` where ``p`` is the keys file path and ``n`` is the\n        number of \"successful\" keys records written to the keys file, otherwise\n        it returns a quadruple ``(p1, n1, p2, n2)`` where ``p1`` is the keys\n        file path, ``n1`` is the number of \"successful\" keys records written to\n        the keys file, ``p2`` is the keys errors file path and ``n2`` is the\n        number of \"unsuccessful\" keys records written to keys errors file.\n        \"\"\"\n        raise NotImplementedError\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrites a keys file, and optionally a keys error file.\n\n:param location_fp: path to the locations file\n:type location_fp: str\n\n:param successes_fp: path to the success keys file\n:type successes_fp: str\n\n:param errors_fp: path to the error keys file (optional)\n:type errors_fp: str\n\n:param output_format: format of the keys files (oasis or json)\n:type output_format: str\n\n:param keys_success_msg: option to write msg for success key\n:type keys_success_msg: bool\n\n:param multiproc_enabled: option to run with multiple processor\n:type multiproc_enabled: bool\n\n:param multiproc_num_cores: number of cores to use in multiproc mode\n:type multiproc_num_cores: int\n\n:param multiproc_num_partitions: number of partition to create in multiproc mode\n:type multiproc_num_partitions: int\n\nIf ``keys_errors_file_path`` is not present then the method returns a\npair ``(p, n)`` where ``p`` is the keys file path and ``n`` is the\nnumber of \"successful\" keys records written to the keys file, otherwise\nit returns a quadruple ``(p1, n1, p2, n2)`` where ``p1`` is the keys\nfile path, ``n1`` is the number of \"successful\" keys records written to\nthe keys file, ``p2`` is the keys errors file path and ``n2`` is the\nnumber of \"unsuccessful\" keys records written to keys errors file."
    },
    {
      "chunk_id": "oasislmf/lookup/interface.py::process_locations@120",
      "source_type": "code",
      "path": "oasislmf/lookup/interface.py",
      "symbol_type": "function",
      "name": "process_locations",
      "lineno": 120,
      "end_lineno": 124,
      "business_stage": "other",
      "docstring": "Process location rows - passed in as a pandas dataframe.",
      "content": "# File: oasislmf/lookup/interface.py\n# function: process_locations (lines 120-124)\n\n    def process_locations(self, loc_df):\n        \"\"\"\n        Process location rows - passed in as a pandas dataframe.\n        \"\"\"\n        raise NotImplementedError\n\n\"\"\"Docstring (excerpt)\"\"\"\nProcess location rows - passed in as a pandas dataframe."
    },
    {
      "chunk_id": "oasislmf/lookup/interface.py::process_locations@161",
      "source_type": "code",
      "path": "oasislmf/lookup/interface.py",
      "symbol_type": "function",
      "name": "process_locations",
      "lineno": 161,
      "end_lineno": 165,
      "business_stage": "other",
      "docstring": "Process location rows - passed in as a pandas dataframe.",
      "content": "# File: oasislmf/lookup/interface.py\n# function: process_locations (lines 161-165)\n\n    def process_locations(self, loc_df):\n        \"\"\"\n        Process location rows - passed in as a pandas dataframe.\n        \"\"\"\n        pass\n\n\"\"\"Docstring (excerpt)\"\"\"\nProcess location rows - passed in as a pandas dataframe."
    },
    {
      "chunk_id": "oasislmf/platform_api/client.py::ApiEndpoint@32",
      "source_type": "code",
      "path": "oasislmf/platform_api/client.py",
      "symbol_type": "class",
      "name": "ApiEndpoint",
      "lineno": 32,
      "end_lineno": 61,
      "business_stage": "other",
      "docstring": "Used to Implement the default requests common to all Oasis API\nEnd points.",
      "content": "# File: oasislmf/platform_api/client.py\n# class: ApiEndpoint (lines 32-61)\n\nclass ApiEndpoint(object):\n    \"\"\"\n    Used to Implement the default requests common to all Oasis API\n    End points.\n    \"\"\"\n\n    def __init__(self, session, url_endpoint, logger=None):\n        self.logger = logger or logging.getLogger(__name__)\n        self.session = session\n        self.url_endpoint = str(url_endpoint)\n\n    def create(self, data):\n        return self.session.post(self.url_endpoint, json=data)\n\n    def get(self, ID=None):\n        if ID:\n            return self.session.get(urljoin(self.url_endpoint, f'{ID}/'))\n        return self.session.get(self.url_endpoint)\n\n    def delete(self, ID):\n        return self.session.delete(urljoin(self.url_endpoint, f'{ID}/'))\n\n    def search(self, metadata={}):\n        search_string = \"\"\n        for key in metadata:\n            if not search_string:\n                search_string = f'?{key}={metadata[key]}'\n            else:\n                search_string += f'&{key}={metadata[key]}'\n        return self.session.get(f'{self.url_endpoint}{search_string}')\n\n\"\"\"Docstring (excerpt)\"\"\"\nUsed to Implement the default requests common to all Oasis API\nEnd points."
    },
    {
      "chunk_id": "oasislmf/platform_api/client.py::JsonEndpoint@64",
      "source_type": "code",
      "path": "oasislmf/platform_api/client.py",
      "symbol_type": "class",
      "name": "JsonEndpoint",
      "lineno": 64,
      "end_lineno": 103,
      "business_stage": "other",
      "docstring": "Used for JSON data End points.",
      "content": "# File: oasislmf/platform_api/client.py\n# class: JsonEndpoint (lines 64-103)\n\nclass JsonEndpoint(object):\n    \"\"\"\n    Used for JSON data End points.\n    \"\"\"\n\n    def __init__(self, session, url_endpoint, url_resource, logger=None):\n        self.logger = logger or logging.getLogger(__name__)\n        self.session = session\n        self.url_endpoint = str(url_endpoint)\n        self.url_resource = str(url_resource)\n\n    def _build_url(self, ID):\n        return urljoin(self.url_endpoint, str(ID), self.url_resource)\n\n    def get(self, ID):\n        return self.session.get(self._build_url(ID))\n\n    def post(self, ID, data):\n        return self.session.post(self._build_url(ID), json=data)\n\n    def delete(self, ID):\n        return self.session.delete(self._build_url(ID))\n\n    def download(self, ID, file_path, overwrite=True):\n        abs_fp = os.path.realpath(os.path.expanduser(file_path))\n        dir_fp = os.path.dirname(abs_fp)\n\n        # Check and create base dir\n        if not os.path.exists(dir_fp):\n            os.makedirs(dir_fp)\n\n        # Check if file exists\n        if os.path.exists(abs_fp) and not overwrite:\n            error_message = 'Local file alreday exists: {}'.format(abs_fp)\n            raise IOError(error_message)\n\n        with io.open(abs_fp, 'w', encoding='utf-8') as f:\n            r = self.get(ID)\n            f.write(json.dumps(r.json(), ensure_ascii=False, indent=4))\n        return r\n\n\"\"\"Docstring (excerpt)\"\"\"\nUsed for JSON data End points."
    },
    {
      "chunk_id": "oasislmf/platform_api/client.py::FileEndpoint@106",
      "source_type": "code",
      "path": "oasislmf/platform_api/client.py",
      "symbol_type": "class",
      "name": "FileEndpoint",
      "lineno": 106,
      "end_lineno": 215,
      "business_stage": "other",
      "docstring": "File Resources Endpoint for Upload / Downloading",
      "content": "# File: oasislmf/platform_api/client.py\n# class: FileEndpoint (lines 106-215)\n\nclass FileEndpoint(object):\n    \"\"\"\n    File Resources Endpoint for Upload / Downloading\n    \"\"\"\n\n    def __init__(self, session, url_endpoint, url_resource, logger=None):\n        self.logger = logger or logging.getLogger(__name__)\n        self.session = session\n        self.url_endpoint = str(url_endpoint)\n        self.url_resource = str(url_resource)\n\n    def _build_url(self, ID):\n        return urljoin(self.url_endpoint, str(ID), self.url_resource)\n\n    def _set_content_type(self, file_path):\n        content_type_map = {\n            'parquet': 'application/octet-stream',\n            'pq': 'application/octet-stream',\n            'csv': 'text/csv',\n            'gz': 'application/gzip',\n            'zip': 'application/zip',\n            'bz2': 'application/x-bzip2',\n        }\n        file_ext = pathlib.Path(file_path).suffix[1:].lower()\n        return content_type_map[file_ext] if file_ext in content_type_map else 'text/csv'\n\n    def upload(self, ID, file_path, content_type=None):\n        if not content_type:\n            content_type = self._set_content_type(file_path)\n        return self.session.upload(self._build_url(ID), file_path, content_type)\n\n    def upload_byte(self, ID, file_bytes, filename, content_type=None):\n        if not content_type:\n            content_type = self._set_content_type(filename)\n        return self.session.upload_byte(self._build_url(ID), file_bytes, filename, content_type)\n\n    def download(self, ID, file_path, overwrite=True, chuck_size=1024):\n        abs_fp = os.path.realpath(os.path.expanduser(file_path))\n        dir_fp = os.path.dirname(abs_fp)\n\n        # Check and create base dir\n        if not os.path.exists(dir_fp):\n            os.makedirs(dir_fp)\n\n        # Check if file exists\n        if os.path.exists(abs_fp) and not overwrite:\n            error_message = 'Local file alreday exists: {}'.format(abs_fp)\n            raise IOError(error_message)\n\n        with io.open(abs_fp, 'wb') as f:\n            r = self.session.get(self._build_url(ID), stream=True)\n            for chunk in r.iter_content(chunk_size=chuck_size):\n                f.write(chunk)\n            return r\n\n    def get(self, ID):\n        return self.session.get(self._build_url(ID))\n\n    def get_dataframe(self, ID):\n        '''\n        Return file endpoint as dict of pandas Dataframes:\n\n        either 'application/gzip': search and extract all csv\n        or 'text/csv': return as dataframe\n        '''\n        supported_content = [\n            'text/csv',\n            'application/gzip',\n            'application/octet-stream',\n        ]\n        r = self.get(ID)\n        file_type = r.headers['Content-Type']\n\n        if file_type not in supported_content:\n            raise OasisException(f'Unsupported filetype for Dataframe conversion: {file_type}')\n\n        if file_type == 'text/csv':\n            return pd.read_csv(io.StringIO(r.content.decode('utf-8')))\n\n        if file_type == 'application/octet-stream':\n\n\"\"\"Docstring (excerpt)\"\"\"\nFile Resources Endpoint for Upload / Downloading"
    },
    {
      "chunk_id": "oasislmf/platform_api/client.py::SettingTemplatesEndpoint@251",
      "source_type": "code",
      "path": "oasislmf/platform_api/client.py",
      "symbol_type": "class",
      "name": "SettingTemplatesEndpoint",
      "lineno": 251,
      "end_lineno": 258,
      "business_stage": "other",
      "docstring": "Settings Template Endpoint for interacting with analysis settings templates for a given model.",
      "content": "# File: oasislmf/platform_api/client.py\n# class: SettingTemplatesEndpoint (lines 251-258)\n\nclass SettingTemplatesEndpoint(SettingTemplatesBaseEndpoint):\n    \"\"\"\n    Settings Template Endpoint for interacting with analysis settings templates for a given model.\n    \"\"\"\n\n    def __init__(self, session, url_endpoint, logger=None):\n        super().__init__(session, url_endpoint)\n        self.content = SettingTemplatesBaseEndpoint(session, url_endpoint, 'content/', logger=logger)\n\n\"\"\"Docstring (excerpt)\"\"\"\nSettings Template Endpoint for interacting with analysis settings templates for a given model."
    },
    {
      "chunk_id": "oasislmf/platform_api/client.py::get_dataframe@164",
      "source_type": "code",
      "path": "oasislmf/platform_api/client.py",
      "symbol_type": "function",
      "name": "get_dataframe",
      "lineno": 164,
      "end_lineno": 199,
      "business_stage": "other",
      "docstring": "Return file endpoint as dict of pandas Dataframes:\n\neither 'application/gzip': search and extract all csv\nor 'text/csv': return as dataframe",
      "content": "# File: oasislmf/platform_api/client.py\n# function: get_dataframe (lines 164-199)\n\n    def get_dataframe(self, ID):\n        '''\n        Return file endpoint as dict of pandas Dataframes:\n\n        either 'application/gzip': search and extract all csv\n        or 'text/csv': return as dataframe\n        '''\n        supported_content = [\n            'text/csv',\n            'application/gzip',\n            'application/octet-stream',\n        ]\n        r = self.get(ID)\n        file_type = r.headers['Content-Type']\n\n        if file_type not in supported_content:\n            raise OasisException(f'Unsupported filetype for Dataframe conversion: {file_type}')\n\n        if file_type == 'text/csv':\n            return pd.read_csv(io.StringIO(r.content.decode('utf-8')))\n\n        if file_type == 'application/octet-stream':\n            return pd.read_parquet(io.BytesIO(r.content))\n\n        if file_type == 'application/gzip':\n            dataframes_list = {}\n            tar = tarfile.open(fileobj=io.BytesIO(r.content))\n\n            for member in [f for f in tar.getmembers() if '.csv' in f.name]:\n                csv = tar.extractfile(member)\n                dataframes_list[os.path.basename(member.name)] = pd.read_csv(csv)\n\n            for member in [f for f in tar.getmembers() if '.parquet' in f.name]:\n                pq = tar.extractfile(member)\n                dataframes_list[os.path.basename(member.name)] = pd.read_parquet(pq)\n            return dataframes_list\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturn file endpoint as dict of pandas Dataframes:\n\neither 'application/gzip': search and extract all csv\nor 'text/csv': return as dataframe"
    },
    {
      "chunk_id": "oasislmf/platform_api/client.py::create_analyses@310",
      "source_type": "code",
      "path": "oasislmf/platform_api/client.py",
      "symbol_type": "function",
      "name": "create_analyses",
      "lineno": 310,
      "end_lineno": 315,
      "business_stage": "other",
      "docstring": "Create new analyses from Exisiting portfolio",
      "content": "# File: oasislmf/platform_api/client.py\n# function: create_analyses (lines 310-315)\n\n    def create_analyses(self, ID, name, model_id):\n        \"\"\" Create new analyses from Exisiting portfolio\n        \"\"\"\n        data = {\"name\": name,\n                \"model\": model_id}\n        return self.session.post('{}{}/create_analysis/'.format(self.url_endpoint, ID), json=data)\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreate new analyses from Exisiting portfolio"
    },
    {
      "chunk_id": "oasislmf/platform_api/client.py::upload_portfolio_file@458",
      "source_type": "code",
      "path": "oasislmf/platform_api/client.py",
      "symbol_type": "function",
      "name": "upload_portfolio_file",
      "lineno": 458,
      "end_lineno": 495,
      "business_stage": "other",
      "docstring": "Upload a portfolio file using the API. Supports absolute filepaths or\nbytestreams.\n\nIf uploading a byte stream `upload_data` is a dict with the keys `name`\nwhich is a `str` containing the filename and `bytes` containing the\nbyte stream of the file data. For example:\n\n```python\nupload_portfolio_file(<portfolio_id>, \"location_file\",\n                      {'bytes': <byte_stream>, 'name': <filename>})\n```\n\nParameters\n:param portfolio_id: Portfolio {id} from\n:type portfolio_id: int\n\n:param portfolio_file: The name of the portfolio file to update. One of\n    the following options `location_file`, `accounts_file`,\n    `reinsurance_info_file` or `reinsurance_scope_file`.\n:type settings: str\n\n:param upload_data: The file to upload through the api. This should be\na `str` if it is a filepath or a `dict` if it is a byte stream with the\nkeys `name` and `bytes` corresponding to the filename and bytestream\nrespectively.\n:type upload_data: [str, dict]\n----------",
      "content": "# File: oasislmf/platform_api/client.py\n# function: upload_portfolio_file (lines 458-495)\n\n    def upload_portfolio_file(self, portfolio_id, portfolio_file, upload_data):\n        \"\"\"\n        Upload a portfolio file using the API. Supports absolute filepaths or\n        bytestreams.\n\n        If uploading a byte stream `upload_data` is a dict with the keys `name`\n        which is a `str` containing the filename and `bytes` containing the\n        byte stream of the file data. For example:\n\n        ```python\n        upload_portfolio_file(<portfolio_id>, \"location_file\",\n                              {'bytes': <byte_stream>, 'name': <filename>})\n        ```\n\n        Parameters\n        :param portfolio_id: Portfolio {id} from\n        :type portfolio_id: int\n\n        :param portfolio_file: The name of the portfolio file to update. One of\n            the following options `location_file`, `accounts_file`,\n            `reinsurance_info_file` or `reinsurance_scope_file`.\n        :type settings: str\n\n        :param upload_data: The file to upload through the api. This should be\n        a `str` if it is a filepath or a `dict` if it is a byte stream with the\n        keys `name` and `bytes` corresponding to the filename and bytestream\n        respectively.\n        :type upload_data: [str, dict]\n        ----------\n        \"\"\"\n        if isinstance(upload_data, dict):\n            getattr(self.portfolios, portfolio_file).upload_byte(portfolio_id,\n                                                                 upload_data['bytes'],\n                                                                 upload_data['name'])\n            self.logger.info(\"File uploaded: {}\".format(upload_data['name']))\n        else:\n            getattr(self.portfolios, portfolio_file).upload(portfolio_id, upload_data)\n            self.logger.info(\"File uploaded: {}\".format(upload_data))\n\n\"\"\"Docstring (excerpt)\"\"\"\nUpload a portfolio file using the API. Supports absolute filepaths or\nbytestreams.\n\nIf uploading a byte stream `upload_data` is a dict with the keys `name`\nwhich is a `str` containing the filename and `bytes` containing the\nbyte stream of the file data. For example:\n\n```python\nupload_portfolio_file(<portfolio_id>, \"location_file\",\n                      {'bytes': <byte_stream>, 'name': <filename>})\n```\n\nParameters\n:param portfolio_id: Portfolio {id} from\n:type portfolio_id: int\n\n:param portfolio_file: The name of the portfolio file to update. One of\n    the following options `location_file`, `accounts_file`,\n    `reinsurance_info_file` or `reinsurance_scope_file`.\n:type settings: str\n\n:param upload_data: The file to upload through the api. This should be\na `str` if it is a filepath or a `dict` if it is a byte stream with the\nkeys `name` and `bytes` corresponding to the filename and bytestream\nrespectively.\n:type upload_data: [str, dict]\n----------"
    },
    {
      "chunk_id": "oasislmf/platform_api/client.py::upload_settings@524",
      "source_type": "code",
      "path": "oasislmf/platform_api/client.py",
      "symbol_type": "function",
      "name": "upload_settings",
      "lineno": 524,
      "end_lineno": 551,
      "business_stage": "other",
      "docstring": "Upload an analyses run settings to an API\n\nMethod to post JSON data or upload a settings file containing JSON data\n\nParameters\n----------\n:param analyses_id: Analyses settings {id} from, `v1/analyses/{id}/settings`\n:type analyses_id: int\n\n:param settings: Either a valid filepath or dictionary holding the settings\n:type settings: [str, dict]\n\n:return:\n:rtype None",
      "content": "# File: oasislmf/platform_api/client.py\n# function: upload_settings (lines 524-551)\n\n    def upload_settings(self, analyses_id, settings):\n        \"\"\"\n        Upload an analyses run settings to an API\n\n        Method to post JSON data or upload a settings file containing JSON data\n\n        Parameters\n        ----------\n        :param analyses_id: Analyses settings {id} from, `v1/analyses/{id}/settings`\n        :type analyses_id: int\n\n        :param settings: Either a valid filepath or dictionary holding the settings\n        :type settings: [str, dict]\n\n        :return:\n        :rtype None\n        \"\"\"\n        if isinstance(settings, dict):\n            self.analyses.settings.post(analyses_id, settings)\n            self.logger.info(\"Settings JSON uploaded: {}\".format(settings))\n\n        elif os.path.isfile(str(settings)):\n            with io.open(settings, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n            self.analyses.settings.post(analyses_id, data)\n            self.logger.info(\"Settings JSON uploaded: {}\".format(settings))\n        else:\n            raise TypeError(\"'settings': not a valid filepath or dictionary\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nUpload an analyses run settings to an API\n\nMethod to post JSON data or upload a settings file containing JSON data\n\nParameters\n----------\n:param analyses_id: Analyses settings {id} from, `v1/analyses/{id}/settings`\n:type analyses_id: int\n\n:param settings: Either a valid filepath or dictionary holding the settings\n:type settings: [str, dict]\n\n:return:\n:rtype None"
    },
    {
      "chunk_id": "oasislmf/platform_api/client.py::run_generate@566",
      "source_type": "code",
      "path": "oasislmf/platform_api/client.py",
      "symbol_type": "function",
      "name": "run_generate",
      "lineno": 566,
      "end_lineno": 643,
      "business_stage": "other",
      "docstring": "Generates the inputs for the analysis based on the portfolio.\nThe analysis must have one of the following statuses, `NEW`, `INPUTS_GENERATION_ERROR`,\n`INPUTS_GENERATION_CANCELLED`, `READY`, `RUN_COMPLETED`, `RUN_CANCELLED` or\n`RUN_ERROR`.",
      "content": "# File: oasislmf/platform_api/client.py\n# function: run_generate (lines 566-643)\n\n    def run_generate(self, analysis_id, poll_interval=5):\n        \"\"\"\n        Generates the inputs for the analysis based on the portfolio.\n        The analysis must have one of the following statuses, `NEW`, `INPUTS_GENERATION_ERROR`,\n        `INPUTS_GENERATION_CANCELLED`, `READY`, `RUN_COMPLETED`, `RUN_CANCELLED` or\n        `RUN_ERROR`.\n        \"\"\"\n\n        try:\n            r = self.analyses.generate(analysis_id)\n            analysis = r.json()\n            self.logger.info('Inputs Generation: Starting (id={})'.format(analysis_id))\n            logged_queued = None\n            logged_running = None\n\n            while True:\n                if analysis['status'] in ['READY']:\n                    self.logger.info('Inputs Generation: Complete (id={})'.format(analysis_id))\n                    return True\n\n                elif analysis['status'] in ['INPUTS_GENERATION_CANCELLED']:\n                    self.logger.info('Input Generation: Cancelled (id={})'.format(analysis_id))\n                    return False\n\n                elif analysis['status'] in ['INPUTS_GENERATION_ERROR']:\n                    self.logger.info('Input Generation: Failed (id={})'.format(analysis_id))\n                    error_trace = self.analyses.input_generation_traceback_file.get(analysis_id).text\n                    self.logger.error(\"\\nServer logs:\")\n                    self.logger.error(error_trace)\n                    return False\n\n                elif analysis['status'] in ['INPUTS_GENERATION_QUEUED']:\n                    if not logged_queued:\n                        logged_queued = True\n                        self.logger.info('Input Generation: Queued (id={})'.format(analysis_id))\n\n                    time.sleep(poll_interval)\n                    r = self.analyses.get(analysis_id)\n                    analysis = r.json()\n                    continue\n\n                elif analysis['status'] in ['INPUTS_GENERATION_STARTED']:\n                    if not logged_running:\n                        logged_running = True\n                        self.logger.info('Input Generation: Executing (id={})'.format(analysis_id))\n\n                    if analysis.get('run_mode', '') == 'V2':\n                        sub_tasks_list = self.analyses.sub_task_list(analysis_id).json()\n                        with tqdm(total=len(sub_tasks_list),\n                                  unit=' sub_task',\n                                  desc='Input Generation') as pbar:\n\n                            completed = []\n                            while len(completed) < len(sub_tasks_list):\n                                sub_tasks_list = self.analyses.sub_task_list(analysis_id).json()\n                                analysis = self.analyses.get(analysis_id).json()\n                                completed = [tsk for tsk in sub_tasks_list if tsk['status'] == 'COMPLETED']\n                                pbar.update(len(completed) - pbar.n)\n                                time.sleep(poll_interval)\n\n                                # Exit conditions\n                                if ('_CANCELLED' in analysis['status']) or ('_ERROR' in analysis['status']):\n                                    break\n                                elif 'READY' in analysis['status']:\n                                    pbar.update(pbar.total - pbar.n)\n                                    break\n\n                    else:\n                        time.sleep(poll_interval)\n                        analysis = self.analyses.get(analysis_id).json()\n\n                    continue\n\n                else:\n                    err_msg = \"Input Generation: Unknown State'{}'\".format(analysis['status'])\n                    raise OasisException(err_msg)\n        except HTTPError as e:\n            self.api.unrecoverable_error(e, 'run_generate: failed')\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerates the inputs for the analysis based on the portfolio.\nThe analysis must have one of the following statuses, `NEW`, `INPUTS_GENERATION_ERROR`,\n`INPUTS_GENERATION_CANCELLED`, `READY`, `RUN_COMPLETED`, `RUN_CANCELLED` or\n`RUN_ERROR`."
    },
    {
      "chunk_id": "oasislmf/platform_api/client.py::run_analysis@645",
      "source_type": "code",
      "path": "oasislmf/platform_api/client.py",
      "symbol_type": "function",
      "name": "run_analysis",
      "lineno": 645,
      "end_lineno": 660,
      "business_stage": "other",
      "docstring": "Runs all the analysis. The analysis must have one of the following\nstatuses, `NEW`, `RUN_COMPLETED`, `RUN_CANCELLED` or\n`RUN_ERROR`",
      "content": "# File: oasislmf/platform_api/client.py\n# function: run_analysis (lines 645-660)\n\n    def run_analysis(self, analysis_id, analysis_settings_fp=None, poll_interval=5):\n        \"\"\"\n        Runs all the analysis. The analysis must have one of the following\n        statuses, `NEW`, `RUN_COMPLETED`, `RUN_CANCELLED` or\n        `RUN_ERROR`\n        \"\"\"\n        try:\n            if analysis_settings_fp:\n                self.upload_settings(analysis_id, analysis_settings_fp)\n\n            self.analyses.run(analysis_id)\n            self.logger.info(f'Analysis Run: Starting (id={analysis_id})')\n            return self._poll_analysis_until_complete(analysis_id, poll_interval)\n\n        except HTTPError as e:\n            self.api.unrecoverable_error(e, 'run_analysis: failed')\n\n\"\"\"Docstring (excerpt)\"\"\"\nRuns all the analysis. The analysis must have one of the following\nstatuses, `NEW`, `RUN_COMPLETED`, `RUN_CANCELLED` or\n`RUN_ERROR`"
    },
    {
      "chunk_id": "oasislmf/platform_api/client.py::cancel_generate@725",
      "source_type": "code",
      "path": "oasislmf/platform_api/client.py",
      "symbol_type": "function",
      "name": "cancel_generate",
      "lineno": 725,
      "end_lineno": 733,
      "business_stage": "other",
      "docstring": "Cancels a currently inputs generation. The analysis status must be `GENERATING_INPUTS`",
      "content": "# File: oasislmf/platform_api/client.py\n# function: cancel_generate (lines 725-733)\n\n    def cancel_generate(self, analysis_id):\n        \"\"\"\n        Cancels a currently inputs generation. The analysis status must be `GENERATING_INPUTS`\n        \"\"\"\n        try:\n            self.analyses.cancel_generate_inputs(analysis_id)\n            self.logger.info('Cancelled Input generation: (Id={})'.format(analysis_id))\n        except HTTPError as e:\n            self.api.unrecoverable_error(e, 'cancel_generate: Failed')\n\n\"\"\"Docstring (excerpt)\"\"\"\nCancels a currently inputs generation. The analysis status must be `GENERATING_INPUTS`"
    },
    {
      "chunk_id": "oasislmf/platform_api/client.py::cancel_analysis@735",
      "source_type": "code",
      "path": "oasislmf/platform_api/client.py",
      "symbol_type": "function",
      "name": "cancel_analysis",
      "lineno": 735,
      "end_lineno": 744,
      "business_stage": "other",
      "docstring": "Cancels a currently running analysis. The analysis must have one of the following\nstatuses, `PENDING` or `STARTED`",
      "content": "# File: oasislmf/platform_api/client.py\n# function: cancel_analysis (lines 735-744)\n\n    def cancel_analysis(self, analysis_id):\n        \"\"\"\n        Cancels a currently running analysis. The analysis must have one of the following\n        statuses, `PENDING` or `STARTED`\n        \"\"\"\n        try:\n            self.analyses.cancel_analysis_run(analysis_id)\n            self.logger.info('Cancelled analysis run: (Id={})'.format(analysis_id))\n        except HTTPError as e:\n            self.api.unrecoverable_error(e, 'cancel_analysis: Failed')\n\n\"\"\"Docstring (excerpt)\"\"\"\nCancels a currently running analysis. The analysis must have one of the following\nstatuses, `PENDING` or `STARTED`"
    },
    {
      "chunk_id": "oasislmf/platform_api/session.py::health_check@149",
      "source_type": "code",
      "path": "oasislmf/platform_api/session.py",
      "symbol_type": "function",
      "name": "health_check",
      "lineno": 149,
      "end_lineno": 161,
      "business_stage": "other",
      "docstring": "Checks the health of the server.",
      "content": "# File: oasislmf/platform_api/session.py\n# function: health_check (lines 149-161)\n\n    def health_check(self):\n        \"\"\"\n        Checks the health of the server.\n\n        \"\"\"\n        try:\n            url = urljoin(self.url_base, 'healthcheck/')\n            r = super(APISession, self).get(url)\n            r.raise_for_status()\n            return r\n        except (TypeError, AttributeError, BytesWarning, HTTPError, ConnectionError, ReadTimeout) as e:\n            err_msg = 'Health check failed: Unable to connect to {}'.format(self.url_base)\n            raise OasisException(err_msg, e)\n\n\"\"\"Docstring (excerpt)\"\"\"\nChecks the health of the server."
    },
    {
      "chunk_id": "oasislmf/preparation/correlations.py::map_data@10",
      "source_type": "code",
      "path": "oasislmf/preparation/correlations.py",
      "symbol_type": "function",
      "name": "map_data",
      "lineno": 10,
      "end_lineno": 44,
      "business_stage": "other",
      "docstring": "Maps data from the model settings to to have Peril ID, peril_correlation_group, and damage_correlation_value.\n\nArgs:\n    data: (dict) the data loaded from the model settings\n\nReturns: (pd.DataFrame) the mapped data",
      "content": "# File: oasislmf/preparation/correlations.py\n# function: map_data (lines 10-44)\n\ndef map_data(data: Optional[dict], logger) -> Optional[pd.DataFrame]:\n    \"\"\"\n    Maps data from the model settings to to have Peril ID, peril_correlation_group, and damage_correlation_value.\n\n    Args:\n        data: (dict) the data loaded from the model settings\n\n    Returns: (pd.DataFrame) the mapped data\n    \"\"\"\n    if data is not None:\n        supported_perils = data.get(\"lookup_settings\", {}).get(\"supported_perils\", [])\n        correlations_legacy = data.get(\"correlation_settings\", [])\n        correlation_settings = data.get(\"model_settings\", {}).get(\"correlation_settings\", correlations_legacy)\n\n        for supported_peril in supported_perils:  # supported_perils is expected to be a list of dict\n            supported_peril[\"peril_correlation_group\"] = supported_peril.get(\"peril_correlation_group\", 0)\n\n        supported_perils_df = pd.DataFrame(supported_perils)\n        correlation_settings_df = pd.DataFrame(correlation_settings)\n\n        if len(correlation_settings_df) > 0:\n            # correlations_settings are defined\n            if \"damage_correlation_value\" not in correlation_settings_df.columns:\n                logger.info(\"Correlation settings: No `damage_correlation_value` found\")\n                correlation_settings_df[\"damage_correlation_value\"] = 0\n\n            if \"hazard_correlation_value\" not in correlation_settings_df.columns:\n                logger.info(\"Correlation settings: No `hazard_correlation_value` found\")\n                correlation_settings_df[\"hazard_correlation_value\"] = 0\n\n        # merge allows duplicates of the \"peril_correlation_group\" in the supported perils\n        # merge does not allow duplicates of the \"peril_correlation_group\" in the correlation settings\n        if len(supported_perils_df) > 0 and len(correlation_settings_df) > 0:\n            mapped_data = pd.merge(supported_perils_df, correlation_settings_df, on=\"peril_correlation_group\")\n            return mapped_data\n\n\"\"\"Docstring (excerpt)\"\"\"\nMaps data from the model settings to to have Peril ID, peril_correlation_group, and damage_correlation_value.\n\nArgs:\n    data: (dict) the data loaded from the model settings\n\nReturns: (pd.DataFrame) the mapped data"
    },
    {
      "chunk_id": "oasislmf/preparation/gul_inputs.py::process_group_id_cols@49",
      "source_type": "code",
      "path": "oasislmf/preparation/gul_inputs.py",
      "symbol_type": "function",
      "name": "process_group_id_cols",
      "lineno": 49,
      "end_lineno": 73,
      "business_stage": "gul",
      "docstring": "cleans out columns that are not valid oasis group columns.\n\nValid group id columns can be either\n1. exist in the location file\n2. be listed as a useful internal col\n\nArgs:\n    group_id_cols: (List[str]) the ID columns that are going to be filtered\n    exposure_df_columns: (List[str]) the columns in the exposure dataframe\n    has_correlation_groups: (bool) if set to True means that we are hashing with correlations in mind therefore the\n                         \"peril_correlation_group\" column is added\n\nReturns: (List[str]) the filtered columns",
      "content": "# File: oasislmf/preparation/gul_inputs.py\n# function: process_group_id_cols (lines 49-73)\n\ndef process_group_id_cols(group_id_cols, exposure_df_columns, has_correlation_groups):\n    \"\"\"\n    cleans out columns that are not valid oasis group columns.\n\n    Valid group id columns can be either\n    1. exist in the location file\n    2. be listed as a useful internal col\n\n    Args:\n        group_id_cols: (List[str]) the ID columns that are going to be filtered\n        exposure_df_columns: (List[str]) the columns in the exposure dataframe\n        has_correlation_groups: (bool) if set to True means that we are hashing with correlations in mind therefore the\n                             \"peril_correlation_group\" column is added\n\n    Returns: (List[str]) the filtered columns\n    \"\"\"\n    for col in group_id_cols:\n        if col not in list(exposure_df_columns) + VALID_OASIS_GROUP_COLS:\n            warnings.warn('Column {} not found in loc file, or a valid internal oasis column'.format(col))\n            group_id_cols.remove(col)\n\n    if PERIL_CORRELATION_GROUP_COL not in group_id_cols and has_correlation_groups is True:\n        group_id_cols.append(PERIL_CORRELATION_GROUP_COL)\n\n    return group_id_cols\n\n\"\"\"Docstring (excerpt)\"\"\"\ncleans out columns that are not valid oasis group columns.\n\nValid group id columns can be either\n1. exist in the location file\n2. be listed as a useful internal col\n\nArgs:\n    group_id_cols: (List[str]) the ID columns that are going to be filtered\n    exposure_df_columns: (List[str]) the columns in the exposure dataframe\n    has_correlation_groups: (bool) if set to True means that we are hashing with correlations in mind therefore the\n                         \"peril_correlation_group\" column is added\n\nReturns: (List[str]) the filtered columns"
    },
    {
      "chunk_id": "oasislmf/preparation/gul_inputs.py::get_gul_input_items@77",
      "source_type": "code",
      "path": "oasislmf/preparation/gul_inputs.py",
      "symbol_type": "function",
      "name": "get_gul_input_items",
      "lineno": 77,
      "end_lineno": 381,
      "business_stage": "gul",
      "docstring": "Generates and returns a Pandas dataframe of GUL input items.\n\n:param exposure_df: Exposure dataframe\n:type exposure_df: pandas.DataFrame\n\n:param keys_df: Keys dataframe\n:type keys_df: pandas.DataFrame\n\n:param output_dir: the output directory where input files are stored\n:type output_dir: str\n\n:param exposure_profile: Exposure profile\n:type exposure_profile: dict\n\n:param damage_group_id_cols: Columns to be used to generate a hashed damage group id.\n:type damage_group_id_cols: list[str]\n\n:param hazard_group_id_cols: Columns to be used to generate a hashed hazard group id.\n:type hazard_group_id_cols: list[str]\n\n:param do_disaggregation: If True, disaggregates by the number of buildings\n:type do_disaggregation: bool\n\n:return: GUL inputs dataframe\n:rtype: pandas.DataFrame",
      "content": "# File: oasislmf/preparation/gul_inputs.py\n# function: get_gul_input_items (lines 77-381)\n\ndef get_gul_input_items(\n    location_df,\n    keys_df,\n    correlations=False,\n    peril_correlation_group_df=None,\n    exposure_profile=get_default_exposure_profile(),\n    damage_group_id_cols=None,\n    hazard_group_id_cols=None,\n    do_disaggregation=True\n):\n    \"\"\"\n    Generates and returns a Pandas dataframe of GUL input items.\n\n    :param exposure_df: Exposure dataframe\n    :type exposure_df: pandas.DataFrame\n\n    :param keys_df: Keys dataframe\n    :type keys_df: pandas.DataFrame\n\n    :param output_dir: the output directory where input files are stored\n    :type output_dir: str\n\n    :param exposure_profile: Exposure profile\n    :type exposure_profile: dict\n\n    :param damage_group_id_cols: Columns to be used to generate a hashed damage group id.\n    :type damage_group_id_cols: list[str]\n\n    :param hazard_group_id_cols: Columns to be used to generate a hashed hazard group id.\n    :type hazard_group_id_cols: list[str]\n\n    :param do_disaggregation: If True, disaggregates by the number of buildings\n    :type do_disaggregation: bool\n\n    :return: GUL inputs dataframe\n    :rtype: pandas.DataFrame\n\n    \"\"\"\n    # Get the grouped exposure profile - this describes the financial terms to\n    # to be found in the source exposure file, which are for the following\n    # FM levels: site coverage (# 1), site pd (# 2), site all (# 3). It also\n    # describes the OED hierarchy terms present in the exposure file, namely\n    # portfolio num., acc. num., loc. num., and cond. num.\n\n    profile = get_grouped_fm_profile_by_level_and_term_group(exposure_profile=exposure_profile)\n\n    if not profile:\n        raise OasisException(\n            'Source exposure profile is possibly missing FM term information: '\n            'FM term definitions for TIV, limit, deductible, attachment and/or share.'\n        )\n\n    # Get the OED hierarchy terms profile - this defines the column names for loc.\n    # ID, acc. ID, policy no. and portfolio no., as used in the source exposure\n    # and accounts files. This is to ensure that the method never makes hard\n    # coded references to the corresponding columns in the source files, as\n    # that would mean that changes to these column names in the source files\n    # may break the method\n    oed_hierarchy = get_oed_hierarchy(exposure_profile=exposure_profile)\n    loc_num = oed_hierarchy['locnum']['ProfileElementName']\n    acc_num = oed_hierarchy['accnum']['ProfileElementName']\n    portfolio_num = oed_hierarchy['portnum']['ProfileElementName']\n\n    # The (site) coverage FM level ID (# 1 in the OED FM levels hierarchy)\n    cov_level_id = SUPPORTED_FM_LEVELS['site coverage']['id']\n\n    # Get the TIV column names and corresponding coverage types\n    tiv_terms = OrderedDict({v['tiv']['CoverageTypeID']: v['tiv']['ProfileElementName'] for k, v in profile[cov_level_id].items()})\n    tiv_cols = list(set(tiv_col for tiv_col in tiv_terms.values() if tiv_col in location_df.columns))\n\n    # Create the basic GUL inputs dataframe from merging the exposure and\n    # keys dataframes on loc. number/loc. ID; filter out any rows with\n    # zeros for TIVs for all coverage types, and replace any nulls in the\n    # cond.num. and TIV columns with zeros\n\n    # add default values if missing\n    if 'IsAggregate' not in location_df.columns:\n        location_df['IsAggregate'] = 0\n    else:\n        location_df['IsAggregate'] = location_df['IsAggregate'].fillna(0)\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerates and returns a Pandas dataframe of GUL input items.\n\n:param exposure_df: Exposure dataframe\n:type exposure_df: pandas.DataFrame\n\n:param keys_df: Keys dataframe\n:type keys_df: pandas.DataFrame\n\n:param output_dir: the output directory where input files are stored\n:type output_dir: str\n\n:param exposure_profile: Exposure profile\n:type exposure_profile: dict\n\n:param damage_group_id_cols: Columns to be used to generate a hashed damage group id.\n:type damage_group_id_cols: list[str]\n\n:param hazard_group_id_cols: Columns to be used to generate a hashed hazard group id.\n:type hazard_group_id_cols: list[str]\n\n:param do_disaggregation: If True, disaggregates by the number of buildings\n:type do_disaggregation: bool\n\n:return: GUL inputs dataframe\n:rtype: pandas.DataFrame"
    },
    {
      "chunk_id": "oasislmf/preparation/gul_inputs.py::write_complex_items_file@385",
      "source_type": "code",
      "path": "oasislmf/preparation/gul_inputs.py",
      "symbol_type": "function",
      "name": "write_complex_items_file",
      "lineno": 385,
      "end_lineno": 407,
      "business_stage": "gul",
      "docstring": "Writes a complex model items file.\n\n:param gul_inputs_df: GUL inputs dataframe\n:type gul_inputs_df: pandas.DataFrame\n\n:param complex_items_fp: Complex/custom model items file path\n:type complex_items_fp: str\n\n:return: Complex/custom model items file path\n:rtype: str",
      "content": "# File: oasislmf/preparation/gul_inputs.py\n# function: write_complex_items_file (lines 385-407)\n\ndef write_complex_items_file(gul_inputs_df, complex_items_fp, chunksize=100000):\n    \"\"\"\n    Writes a complex model items file.\n\n    :param gul_inputs_df: GUL inputs dataframe\n    :type gul_inputs_df: pandas.DataFrame\n\n    :param complex_items_fp: Complex/custom model items file path\n    :type complex_items_fp: str\n\n    :return: Complex/custom model items file path\n    :rtype: str\n    \"\"\"\n    try:\n        gul_inputs_df.loc[:, ['item_id', 'coverage_id', 'model_data', 'group_id']].drop_duplicates().to_csv(\n            path_or_buf=complex_items_fp,\n            encoding='utf-8',\n            mode=('w' if os.path.exists(complex_items_fp) else 'a'),\n            chunksize=chunksize,\n            index=False\n        )\n    except (IOError, OSError) as e:\n        raise OasisException(\"Exception raised in 'write_complex_items_file'\", e)\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrites a complex model items file.\n\n:param gul_inputs_df: GUL inputs dataframe\n:type gul_inputs_df: pandas.DataFrame\n\n:param complex_items_fp: Complex/custom model items file path\n:type complex_items_fp: str\n\n:return: Complex/custom model items file path\n:rtype: str"
    },
    {
      "chunk_id": "oasislmf/preparation/gul_inputs.py::write_sections_file@411",
      "source_type": "code",
      "path": "oasislmf/preparation/gul_inputs.py",
      "symbol_type": "function",
      "name": "write_sections_file",
      "lineno": 411,
      "end_lineno": 435,
      "business_stage": "gul",
      "docstring": "Writes a section id file based on the input location area perils.\n\n:param gul_inputs_df: GUL inputs dataframe\n:type gul_inputs_df: pandas.DataFrame\n\n:param dynamic_events_fp: events file path to output\n:type sections_fp: str",
      "content": "# File: oasislmf/preparation/gul_inputs.py\n# function: write_sections_file (lines 411-435)\n\ndef write_sections_file(gul_inputs_df, sections_fp, chunksize=100000):\n    \"\"\"\n    Writes a section id file based on the input location area perils.\n\n    :param gul_inputs_df: GUL inputs dataframe\n    :type gul_inputs_df: pandas.DataFrame\n\n    :param dynamic_events_fp: events file path to output\n    :type sections_fp: str\n    \"\"\"\n    try:\n        sections = gul_inputs_df.loc[:, ['section_id']].drop_duplicates()\n        sections['section_id'] = sections['section_id'].astype(str)\n        sections['section_id'] = sections['section_id'].str.split(';')\n        sections = sections.explode('section_id').drop_duplicates()\n        sections['section_id'] = sections['section_id'].astype(int)\n        sections.to_csv(\n            path_or_buf=sections_fp,\n            encoding='utf-8',\n            mode=('w' if os.path.exists(sections_fp) else 'a'),\n            chunksize=chunksize,\n            index=False\n        )\n    except (IOError, OSError) as e:\n        raise OasisException(\"Exception raised in 'write_sections_file'\", e)\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrites a section id file based on the input location area perils.\n\n:param gul_inputs_df: GUL inputs dataframe\n:type gul_inputs_df: pandas.DataFrame\n\n:param dynamic_events_fp: events file path to output\n:type sections_fp: str"
    },
    {
      "chunk_id": "oasislmf/preparation/gul_inputs.py::write_item_adjustments_file@439",
      "source_type": "code",
      "path": "oasislmf/preparation/gul_inputs.py",
      "symbol_type": "function",
      "name": "write_item_adjustments_file",
      "lineno": 439,
      "end_lineno": 458,
      "business_stage": "gul",
      "docstring": "Writes a item_adjustments id file based on the gul inputs.\n\n:param gul_inputs_df: GUL inputs dataframe\n:type gul_inputs_df: pandas.DataFrame\n\n:param item_adjustments_fp: item_adjustments file path to output\n:type sections_fp: str",
      "content": "# File: oasislmf/preparation/gul_inputs.py\n# function: write_item_adjustments_file (lines 439-458)\n\ndef write_item_adjustments_file(gul_inputs_df, item_adjustments_fp, chunksize=100000):\n    \"\"\"\n    Writes a item_adjustments id file based on the gul inputs.\n\n    :param gul_inputs_df: GUL inputs dataframe\n    :type gul_inputs_df: pandas.DataFrame\n\n    :param item_adjustments_fp: item_adjustments file path to output\n    :type sections_fp: str\n    \"\"\"\n    try:\n        gul_inputs_df.loc[:, ['item_id', 'intensity_adjustment', 'return_period']].drop_duplicates().to_csv(\n            path_or_buf=item_adjustments_fp,\n            encoding='utf-8',\n            mode=('w' if os.path.exists(item_adjustments_fp) else 'a'),\n            chunksize=chunksize,\n            index=False\n        )\n    except (IOError, OSError) as e:\n        raise OasisException(\"Exception raised in 'write_item_adjustments_file'\", e)\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrites a item_adjustments id file based on the gul inputs.\n\n:param gul_inputs_df: GUL inputs dataframe\n:type gul_inputs_df: pandas.DataFrame\n\n:param item_adjustments_fp: item_adjustments file path to output\n:type sections_fp: str"
    },
    {
      "chunk_id": "oasislmf/preparation/gul_inputs.py::write_amplifications_file@462",
      "source_type": "code",
      "path": "oasislmf/preparation/gul_inputs.py",
      "symbol_type": "function",
      "name": "write_amplifications_file",
      "lineno": 462,
      "end_lineno": 487,
      "business_stage": "gul",
      "docstring": "Writes an amplifications file. This is the mapping between item IDs and\namplifications IDs.\n\n:param gul_inputs_df: GUL inputs dataframe\n:type gul_inputs_df: pandas.DataFrame\n\n:param amplifications_fp: amplifications file path\n:type amplifications_fp: str\n\n:return: amplifications file path\n:rtype: str",
      "content": "# File: oasislmf/preparation/gul_inputs.py\n# function: write_amplifications_file (lines 462-487)\n\ndef write_amplifications_file(gul_inputs_df, amplifications_fp, chunksize=100000):\n    \"\"\"\n    Writes an amplifications file. This is the mapping between item IDs and\n    amplifications IDs.\n\n    :param gul_inputs_df: GUL inputs dataframe\n    :type gul_inputs_df: pandas.DataFrame\n\n    :param amplifications_fp: amplifications file path\n    :type amplifications_fp: str\n\n    :return: amplifications file path\n    :rtype: str\n    \"\"\"\n    try:\n        gul_inputs_df.loc[:, ['item_id', 'amplification_id']].drop_duplicates().to_csv(\n            path_or_buf=amplifications_fp,\n            encoding='utf-8',\n            mode=('w' if os.path.exists(amplifications_fp) else 'a'),\n            chunksize=chunksize,\n            index=False\n        )\n    except (IOError, OSError) as e:\n        raise OasisException(\"Exception raise in 'write_amplifications_file'\", e)\n\n    return amplifications_fp\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrites an amplifications file. This is the mapping between item IDs and\namplifications IDs.\n\n:param gul_inputs_df: GUL inputs dataframe\n:type gul_inputs_df: pandas.DataFrame\n\n:param amplifications_fp: amplifications file path\n:type amplifications_fp: str\n\n:return: amplifications file path\n:rtype: str"
    },
    {
      "chunk_id": "oasislmf/preparation/gul_inputs.py::write_items_file@491",
      "source_type": "code",
      "path": "oasislmf/preparation/gul_inputs.py",
      "symbol_type": "function",
      "name": "write_items_file",
      "lineno": 491,
      "end_lineno": 515,
      "business_stage": "gul",
      "docstring": "Writes an items file.\n\n:param gul_inputs_df: GUL inputs dataframe\n:type gul_inputs_df: pandas.DataFrame\n\n:param items_fp: Items file path\n:type items_fp: str\n\n:return: Items file path\n:rtype: str",
      "content": "# File: oasislmf/preparation/gul_inputs.py\n# function: write_items_file (lines 491-515)\n\ndef write_items_file(gul_inputs_df, items_fp, chunksize=100000):\n    \"\"\"\n    Writes an items file.\n\n    :param gul_inputs_df: GUL inputs dataframe\n    :type gul_inputs_df: pandas.DataFrame\n\n    :param items_fp: Items file path\n    :type items_fp: str\n\n    :return: Items file path\n    :rtype: str\n    \"\"\"\n    try:\n        gul_inputs_df.loc[:, ['item_id', 'coverage_id', 'areaperil_id', 'vulnerability_id', 'group_id']].drop_duplicates().to_csv(\n            path_or_buf=items_fp,\n            encoding='utf-8',\n            mode=('w' if os.path.exists(items_fp) else 'a'),\n            chunksize=chunksize,\n            index=False\n        )\n    except (IOError, OSError) as e:\n        raise OasisException(\"Exception raised in 'write_items_file'\", e)\n\n    return items_fp\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrites an items file.\n\n:param gul_inputs_df: GUL inputs dataframe\n:type gul_inputs_df: pandas.DataFrame\n\n:param items_fp: Items file path\n:type items_fp: str\n\n:return: Items file path\n:rtype: str"
    },
    {
      "chunk_id": "oasislmf/preparation/gul_inputs.py::write_coverages_file@519",
      "source_type": "code",
      "path": "oasislmf/preparation/gul_inputs.py",
      "symbol_type": "function",
      "name": "write_coverages_file",
      "lineno": 519,
      "end_lineno": 543,
      "business_stage": "gul",
      "docstring": "Writes a coverages file.\n\n:param gul_inputs_df: GUL inputs dataframe\n:type gul_inputs_df: pandas.DataFrame\n\n:param coverages_fp: Coverages file path\n:type coverages_fp: str\n\n:return: Coverages file path\n:rtype: str",
      "content": "# File: oasislmf/preparation/gul_inputs.py\n# function: write_coverages_file (lines 519-543)\n\ndef write_coverages_file(gul_inputs_df, coverages_fp, chunksize=100000):\n    \"\"\"\n    Writes a coverages file.\n\n    :param gul_inputs_df: GUL inputs dataframe\n    :type gul_inputs_df: pandas.DataFrame\n\n    :param coverages_fp: Coverages file path\n    :type coverages_fp: str\n\n    :return: Coverages file path\n    :rtype: str\n    \"\"\"\n    try:\n        gul_inputs_df.loc[:, ['coverage_id', 'tiv']].drop_duplicates().to_csv(\n            path_or_buf=coverages_fp,\n            encoding='utf-8',\n            mode=('w' if os.path.exists(coverages_fp) else 'a'),\n            chunksize=chunksize,\n            index=False\n        )\n    except (IOError, OSError) as e:\n        raise OasisException(\"Exception raised in 'write_coverages_file'\", e)\n\n    return coverages_fp\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrites a coverages file.\n\n:param gul_inputs_df: GUL inputs dataframe\n:type gul_inputs_df: pandas.DataFrame\n\n:param coverages_fp: Coverages file path\n:type coverages_fp: str\n\n:return: Coverages file path\n:rtype: str"
    },
    {
      "chunk_id": "oasislmf/preparation/gul_inputs.py::write_gul_input_files@547",
      "source_type": "code",
      "path": "oasislmf/preparation/gul_inputs.py",
      "symbol_type": "function",
      "name": "write_gul_input_files",
      "lineno": 547,
      "end_lineno": 627,
      "business_stage": "gul",
      "docstring": "Writes the standard Oasis GUL input files to a target directory, using a\npre-generated dataframe of GUL input items. The files written are\n::\n\n    items.csv\n    coverages.csv\n\nand optionally a complex items file in case of a complex/custom model.\n\n:param gul_inputs_df: GUL inputs dataframe\n:type gul_inputs_df: pandas.DataFrame\n\n:param target_dir: Target directory in which to write the files\n:type target_dir: str\n\n:param oasis_files_prefixes: Oasis GUL input file name prefixes\n:param oasis_files_prefixes: dict\n\n:param chunksize: The chunk size to use when writing out the\n                  input files\n:type chunksize: int\n\n:return: GUL input files dict\n:rtype: dict",
      "content": "# File: oasislmf/preparation/gul_inputs.py\n# function: write_gul_input_files (lines 547-627)\n\ndef write_gul_input_files(\n    gul_inputs_df,\n    target_dir,\n    correlations_df,\n    output_dir,\n    oasis_files_prefixes=OASIS_FILES_PREFIXES['gul'],\n    chunksize=(2 * 10 ** 5),\n):\n    \"\"\"\n    Writes the standard Oasis GUL input files to a target directory, using a\n    pre-generated dataframe of GUL input items. The files written are\n    ::\n\n        items.csv\n        coverages.csv\n\n    and optionally a complex items file in case of a complex/custom model.\n\n    :param gul_inputs_df: GUL inputs dataframe\n    :type gul_inputs_df: pandas.DataFrame\n\n    :param target_dir: Target directory in which to write the files\n    :type target_dir: str\n\n    :param oasis_files_prefixes: Oasis GUL input file name prefixes\n    :param oasis_files_prefixes: dict\n\n    :param chunksize: The chunk size to use when writing out the\n                      input files\n    :type chunksize: int\n\n    :return: GUL input files dict\n    :rtype: dict\n    \"\"\"\n    # Clean the target directory path\n    target_dir = as_path(target_dir, 'Target IL input files directory', is_dir=True, preexists=False)\n    oasis_files_prefixes = copy.deepcopy(oasis_files_prefixes)\n\n    if correlations_df is None:\n        correlations_df = pd.DataFrame(columns=correlations_headers)\n\n    # write the correlations to a binary file\n    correlations_df.to_csv(f\"{output_dir}/correlations.csv\", index=False)\n    correlations_df_np_data = np.array([r for r in correlations_df.itertuples(index=False)], dtype=correlations_dtype)\n    correlations_df_np_data.tofile(f\"{output_dir}/correlations.bin\")\n\n    # Set chunk size for writing the CSV files - default is the minimum of 100K\n    # or the GUL inputs frame size\n    chunksize = chunksize or min(chunksize, len(gul_inputs_df))\n    # If no complex model data present then remove the corresponding file\n    # name from the files prefixes dict, which is used for writing the\n    # GUl input files\n    if 'model_data' not in gul_inputs_df:\n        oasis_files_prefixes.pop('complex_items', None)\n\n    # If no amplification IDs then remove corresponding file name from files\n    # prefixes dict\n    if 'amplification_id' not in gul_inputs_df:\n        oasis_files_prefixes.pop('amplifications', None)\n\n    # If no section IDs then remove corresponding file name from files\n    # prefixes dict\n    if 'section_id' not in gul_inputs_df:\n        oasis_files_prefixes.pop('sections', None)\n\n    # If no adjustments data then remove corresponding file name from files\n    # prefixes dict\n    if 'intensity_adjustment' not in gul_inputs_df:\n        oasis_files_prefixes.pop('item_adjustments', None)\n\n    # A dict of GUL input file names and file paths\n    gul_input_files = {\n        fn: os.path.join(target_dir, '{}.csv'.format(oasis_files_prefixes[fn]))\n        for fn in oasis_files_prefixes\n    }\n    this_module = sys.modules[__name__]\n    # Write the files serially\n    for fn in gul_input_files:\n        getattr(this_module, 'write_{}_file'.format(fn))(gul_inputs_df.copy(deep=True), gul_input_files[fn], chunksize)\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrites the standard Oasis GUL input files to a target directory, using a\npre-generated dataframe of GUL input items. The files written are\n::\n\n    items.csv\n    coverages.csv\n\nand optionally a complex items file in case of a complex/custom model.\n\n:param gul_inputs_df: GUL inputs dataframe\n:type gul_inputs_df: pandas.DataFrame\n\n:param target_dir: Target directory in which to write the files\n:type target_dir: str\n\n:param oasis_files_prefixes: Oasis GUL input file name prefixes\n:param oasis_files_prefixes: dict\n\n:param chunksize: The chunk size to use when writing out the\n                  input files\n:type chunksize: int\n\n:return: GUL input files dict\n:rtype: dict"
    },
    {
      "chunk_id": "oasislmf/preparation/il_inputs.py::get_calc_rule_ids@95",
      "source_type": "code",
      "path": "oasislmf/preparation/il_inputs.py",
      "symbol_type": "function",
      "name": "get_calc_rule_ids",
      "lineno": 95,
      "end_lineno": 154,
      "business_stage": "other",
      "docstring": "merge selected il_inputs with the correct calc_rule table and  return a pandas Series of calc. rule IDs\n\nArgs:\n il_inputs_calc_rules_df (DataFrame):  IL input items dataframe\n calc_rule_type (str): type of calc_rule to look for\n\nReturns:\n    pandas Series of calc. rule IDs",
      "content": "# File: oasislmf/preparation/il_inputs.py\n# function: get_calc_rule_ids (lines 95-154)\n\ndef get_calc_rule_ids(il_inputs_calc_rules_df, calc_rule_type):\n    \"\"\"\n    merge selected il_inputs with the correct calc_rule table and  return a pandas Series of calc. rule IDs\n\n    Args:\n     il_inputs_calc_rules_df (DataFrame):  IL input items dataframe\n     calc_rule_type (str): type of calc_rule to look for\n\n    Returns:\n        pandas Series of calc. rule IDs\n    \"\"\"\n    il_inputs_calc_rules_df = il_inputs_calc_rules_df.copy()\n    calc_rules_df, calc_rule_term_info = get_calc_rules(calc_rule_type)\n    calc_rules_df = calc_rules_df.drop(columns=['desc', 'id_key'], axis=1, errors='ignore')\n\n    terms = []\n    terms_indicators = []\n    no_terms = {\n        'deductible': ['ded_type', 'ded_code'],\n        'limit': ['lim_type', 'lim_code']\n    }\n    for term in calc_rule_term_info['terms']:\n        if term in il_inputs_calc_rules_df.columns:\n            terms.append(term)\n            terms_indicators.append('{}_gt_0'.format(term))\n        else:\n            calc_rules_df = calc_rules_df[calc_rules_df['{}_gt_0'.format(term)] == 0].drop(columns=['{}_gt_0'.format(term)])\n\n    for term in calc_rule_term_info['types_and_codes']:\n        if term in il_inputs_calc_rules_df.columns:\n            il_inputs_calc_rules_df[term] = il_inputs_calc_rules_df[term].fillna(0).astype('uint8')\n        else:\n            calc_rules_df = calc_rules_df[calc_rules_df[term] == 0].drop(columns=[term])\n\n    for term, type_and_code in no_terms.items():\n        if f'{term}_gt_0' in calc_rules_df.columns:\n            for elm in type_and_code:\n                if elm in il_inputs_calc_rules_df.columns:\n                    il_inputs_calc_rules_df.loc[il_inputs_calc_rules_df[term] == 0, elm] = 0\n        else:\n            for elm in type_and_code:\n                if elm in calc_rules_df.columns:\n                    calc_rules_df = calc_rules_df[calc_rules_df[elm] == 0].drop(columns=[elm])\n\n    il_inputs_calc_rules_df.loc[:, terms_indicators] = np.where(il_inputs_calc_rules_df[terms] > 0, 1, 0)\n\n    merge_col = list(set(il_inputs_calc_rules_df.columns).intersection(calc_rules_df.columns).difference({'calcrule_id'}))\n    if len(merge_col):\n        calcrule_ids = (\n            il_inputs_calc_rules_df[merge_col].reset_index()\n            .merge(calc_rules_df[merge_col + ['calcrule_id']].drop_duplicates(), how='left', on=merge_col)\n        ).set_index('index')['calcrule_id'].fillna(0)\n    else:\n        return 100  # no term we return pass through\n    if 0 in calcrule_ids.unique():\n        _cols = list(set(['PortNumber', 'AccNumber', 'LocNumber'] + merge_col).intersection(il_inputs_calc_rules_df.columns))\n        no_match_keys = il_inputs_calc_rules_df.loc[calcrule_ids == 0, _cols].drop_duplicates()\n        err_msg = 'Calculation Rule mapping error, non-matching keys:\\n{}'.format(no_match_keys)\n        raise OasisException(err_msg)\n    return calcrule_ids\n\n\"\"\"Docstring (excerpt)\"\"\"\nmerge selected il_inputs with the correct calc_rule table and  return a pandas Series of calc. rule IDs\n\nArgs:\n il_inputs_calc_rules_df (DataFrame):  IL input items dataframe\n calc_rule_type (str): type of calc_rule to look for\n\nReturns:\n    pandas Series of calc. rule IDs"
    },
    {
      "chunk_id": "oasislmf/preparation/il_inputs.py::get_profile_ids@157",
      "source_type": "code",
      "path": "oasislmf/preparation/il_inputs.py",
      "symbol_type": "function",
      "name": "get_profile_ids",
      "lineno": 157,
      "end_lineno": 168,
      "business_stage": "other",
      "docstring": "Returns a Numpy array of policy TC IDs from a table of IL input items\n\n:param il_inputs_df: IL input items dataframe\n:type il_inputs_df: pandas.DataFrame\n\n:return: Numpy array of policy TC IDs\n:rtype: numpy.ndarray",
      "content": "# File: oasislmf/preparation/il_inputs.py\n# function: get_profile_ids (lines 157-168)\n\ndef get_profile_ids(il_inputs_df):\n    \"\"\"\n    Returns a Numpy array of policy TC IDs from a table of IL input items\n\n    :param il_inputs_df: IL input items dataframe\n    :type il_inputs_df: pandas.DataFrame\n\n    :return: Numpy array of policy TC IDs\n    :rtype: numpy.ndarray\n    \"\"\"\n    factor_col = list(set(il_inputs_df.columns).intersection(policytc_cols).difference({'profile_id', }))\n    return factorize_ndarray(il_inputs_df.loc[:, factor_col].values, col_idxs=range(len(factor_col)))[0]\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturns a Numpy array of policy TC IDs from a table of IL input items\n\n:param il_inputs_df: IL input items dataframe\n:type il_inputs_df: pandas.DataFrame\n\n:return: Numpy array of policy TC IDs\n:rtype: numpy.ndarray"
    },
    {
      "chunk_id": "oasislmf/preparation/il_inputs.py::associate_items_peril_to_policy_peril@417",
      "source_type": "code",
      "path": "oasislmf/preparation/il_inputs.py",
      "symbol_type": "function",
      "name": "associate_items_peril_to_policy_peril",
      "lineno": 417,
      "end_lineno": 433,
      "business_stage": "other",
      "docstring": "for each peril_id in item_peril_list we map it to each string representing policy perils\nwe then merge this mapping to the initial policies so that each line will have a peril_id\nthat can be use directly as key when merging with the gul_input_df\nArgs:\n    item_perils: all peril id from gul_input_df\n    policy_df: the df with the policies\n    fm_peril_col: the name of the column to use that define the policy peril filter\n    oed_schema: the schema object we use to map peril and subperils\nReturns:",
      "content": "# File: oasislmf/preparation/il_inputs.py\n# function: associate_items_peril_to_policy_peril (lines 417-433)\n\ndef associate_items_peril_to_policy_peril(item_perils, policy_df, fm_peril_col, oed_schema):\n    \"\"\"\n    for each peril_id in item_peril_list we map it to each string representing policy perils\n    we then merge this mapping to the initial policies so that each line will have a peril_id\n    that can be use directly as key when merging with the gul_input_df\n    Args:\n        item_perils: all peril id from gul_input_df\n        policy_df: the df with the policies\n        fm_peril_col: the name of the column to use that define the policy peril filter\n        oed_schema: the schema object we use to map peril and subperils\n    Returns:\n\n    \"\"\"\n    fm_perils = policy_df[[fm_peril_col]].drop_duplicates()\n    peril_map_df = pd.merge(fm_perils, item_perils, how='cross')\n    peril_map_df = peril_map_df[oed_schema.peril_filtering(peril_map_df['peril_id'], peril_map_df[fm_peril_col])]\n    return policy_df.merge(peril_map_df)\n\n\"\"\"Docstring (excerpt)\"\"\"\nfor each peril_id in item_peril_list we map it to each string representing policy perils\nwe then merge this mapping to the initial policies so that each line will have a peril_id\nthat can be use directly as key when merging with the gul_input_df\nArgs:\n    item_perils: all peril id from gul_input_df\n    policy_df: the df with the policies\n    fm_peril_col: the name of the column to use that define the policy peril filter\n    oed_schema: the schema object we use to map peril and subperils\nReturns:"
    },
    {
      "chunk_id": "oasislmf/preparation/il_inputs.py::get_il_input_items@437",
      "source_type": "code",
      "path": "oasislmf/preparation/il_inputs.py",
      "symbol_type": "function",
      "name": "get_il_input_items",
      "lineno": 437,
      "end_lineno": 937,
      "business_stage": "other",
      "docstring": "Generates and returns a Pandas dataframe of IL input items.\n\n:param gul_inputs_df: GUL input items\n:type gul_inputs_df: pandas.DataFrame\n\n:param exposure_data: object containing all information about the insurance policies\n\n:param target_dir: path to the directory used to write the fm files\n\n:param logger: logger object to trace progress\n\n:param exposure_profile: Source exposure profile (optional)\n:type exposure_profile: dict\n\n:param accounts_profile: Source accounts profile (optional)\n:type accounts_profile: dict\n\n:param fm_aggregation_profile: FM aggregation profile (optional)\n:param fm_aggregation_profile: dict\n\n:param do_disaggregation: whether to split terms and conditions for aggregate exposure (optional)\n:param do_disaggregation: bool\n\n:return: IL inputs dataframe\n:rtype: pandas.DataFrame\n\n:return Accounts dataframe\n:rtype: pandas.DataFrame",
      "content": "# File: oasislmf/preparation/il_inputs.py\n# function: get_il_input_items (lines 437-937)\n\ndef get_il_input_items(\n        gul_inputs_df,\n        exposure_data,\n        target_dir,\n        logger,\n        exposure_profile=get_default_exposure_profile(),\n        accounts_profile=get_default_accounts_profile(),\n        fm_aggregation_profile=get_default_fm_aggregation_profile(),\n        do_disaggregation=True,\n        oasis_files_prefixes=OASIS_FILES_PREFIXES['il'],\n        chunksize=(2 * 10 ** 5),\n):\n    \"\"\"\n    Generates and returns a Pandas dataframe of IL input items.\n\n    :param gul_inputs_df: GUL input items\n    :type gul_inputs_df: pandas.DataFrame\n\n    :param exposure_data: object containing all information about the insurance policies\n\n    :param target_dir: path to the directory used to write the fm files\n\n    :param logger: logger object to trace progress\n\n    :param exposure_profile: Source exposure profile (optional)\n    :type exposure_profile: dict\n\n    :param accounts_profile: Source accounts profile (optional)\n    :type accounts_profile: dict\n\n    :param fm_aggregation_profile: FM aggregation profile (optional)\n    :param fm_aggregation_profile: dict\n\n    :param do_disaggregation: whether to split terms and conditions for aggregate exposure (optional)\n    :param do_disaggregation: bool\n\n    :return: IL inputs dataframe\n    :rtype: pandas.DataFrame\n\n    :return Accounts dataframe\n    :rtype: pandas.DataFrame\n    \"\"\"\n    target_dir = as_path(target_dir, 'Target IL input files directory', is_dir=True, preexists=False)\n    with contextlib.ExitStack() as stack:\n        fm_policytc_file = stack.enter_context(open(os.path.join(target_dir, f\"{oasis_files_prefixes['fm_policytc']}.csv\"), 'w'))\n        fm_policytc_file.write(f\"{','.join(fm_policytc_headers)}{os.linesep}\")\n\n        fm_profile_file = stack.enter_context(open(os.path.join(target_dir, f\"{oasis_files_prefixes['fm_profile']}.csv\"), 'w'))\n\n        fm_programme_file = stack.enter_context(open(os.path.join(target_dir, f\"{oasis_files_prefixes['fm_programme']}.csv\"), 'w'))\n        fm_programme_file.write(f\"{','.join(fm_programme_headers)}{os.linesep}\")\n\n        fm_xref_file = stack.enter_context(open(os.path.join(target_dir, f\"{oasis_files_prefixes['fm_xref']}.csv\"), 'w'))\n\n        if exposure_data.location is not None:\n            locations_df = exposure_data.location.dataframe\n            accounts_df = exposure_data.account.dataframe\n            if 'acc_id' not in accounts_df:\n                accounts_df['acc_id'] = get_ids(exposure_data.account.dataframe, ['PortNumber', 'AccNumber'])\n            acc_id_map = accounts_df[['PortNumber', 'AccNumber', 'acc_id']].drop_duplicates()\n            gul_inputs_df = gul_inputs_df.merge(acc_id_map, how='left')\n            locations_df = locations_df.merge(acc_id_map, how='left')\n            locations_df = locations_df.drop(columns=['PortNumber', 'AccNumber', 'LocNumber'])\n            accounts_df = accounts_df[accounts_df['acc_id'].isin(locations_df['acc_id'].unique())].drop(columns=['PortNumber', 'AccNumber'])\n\n        else:  # no location, case for cyber, marine ...\n            locations_df = None\n            gul_inputs_df['acc_id'] = gul_inputs_df['loc_id']\n            accounts_df = exposure_data.account.dataframe\n            accounts_df['acc_id'] = accounts_df['loc_id']\n            accounts_df = accounts_df.drop(columns=['PortNumber', 'AccNumber'])\n\n        oed_schema = exposure_data.oed_schema\n\n        profile = get_grouped_fm_profile_by_level_and_term_group(exposure_profile, accounts_profile)\n\n        # Get the FM aggregation profile - this describes how the IL input\n        # items are to be aggregated in the various FM levels\n        fm_aggregation_profile = copy.deepcopy(fm_aggregation_profile)\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerates and returns a Pandas dataframe of IL input items.\n\n:param gul_inputs_df: GUL input items\n:type gul_inputs_df: pandas.DataFrame\n\n:param exposure_data: object containing all information about the insurance policies\n\n:param target_dir: path to the directory used to write the fm files\n\n:param logger: logger object to trace progress\n\n:param exposure_profile: Source exposure profile (optional)\n:type exposure_profile: dict\n\n:param accounts_profile: Source accounts profile (optional)\n:type accounts_profile: dict\n\n:param fm_aggregation_profile: FM aggregation profile (optional)\n:param fm_aggregation_profile: dict\n\n:param do_disaggregation: whether to split terms and conditions for aggregate exposure (optional)\n:param do_disaggregation: bool\n\n:return: IL inputs dataframe\n:rtype: pandas.DataFrame\n\n:return Accounts dataframe\n:rtype: pandas.DataFrame"
    },
    {
      "chunk_id": "oasislmf/preparation/il_inputs.py::write_fm_profile_level@961",
      "source_type": "code",
      "path": "oasislmf/preparation/il_inputs.py",
      "symbol_type": "function",
      "name": "write_fm_profile_level",
      "lineno": 961,
      "end_lineno": 1029,
      "business_stage": "other",
      "docstring": "Writes an FM profile file.\n\n:param level_df: level_df (fm terms) dataframe\n:type level_df: pandas.DataFrame\n\n:param fm_profile_file: open file to write to\n:type fm_profile_file: fileObj\n\n:param step_policies_present: flag to know which type of file to write\n:type step_policies_present: bool\n\n:param chunksize: chunksize\n:type chunksize: int\n\n:return: FM profile file path\n:rtype: str",
      "content": "# File: oasislmf/preparation/il_inputs.py\n# function: write_fm_profile_level (lines 961-1029)\n\ndef write_fm_profile_level(level_df, fm_profile_file, step_policies_present, chunksize=100000):\n    \"\"\"\n    Writes an FM profile file.\n\n    :param level_df: level_df (fm terms) dataframe\n    :type level_df: pandas.DataFrame\n\n    :param fm_profile_file: open file to write to\n    :type fm_profile_file: fileObj\n\n    :param step_policies_present: flag to know which type of file to write\n    :type step_policies_present: bool\n\n    :param chunksize: chunksize\n    :type chunksize: int\n\n    :return: FM profile file path\n    :rtype: str\n    \"\"\"\n    level_df = level_df.astype({'calcrule_id': 'i4', 'profile_id': 'i4'})\n    # Step policies exist\n    if step_policies_present:\n        fm_profile_df = level_df[list(set(level_df.columns).intersection(set(fm_profile_step_headers + ['steptriggertype'])))].copy()\n        for col in fm_profile_step_headers + ['steptriggertype']:\n            if col not in fm_profile_df.columns:\n                fm_profile_df[col] = 0\n        for non_step_name, step_name in profile_cols_map.items():\n            if step_name not in fm_profile_df.columns:\n                fm_profile_df[step_name] = 0\n            fm_profile_df[step_name] = fm_profile_df[step_name].astype(object)\n            if non_step_name in level_df.columns:\n                fm_profile_df.loc[\n                    ~(fm_profile_df['steptriggertype'] > 0), step_name\n                ] = level_df.loc[\n                    ~(fm_profile_df['steptriggertype'] > 0),\n                    non_step_name\n                ]\n        fm_profile_df.fillna(0, inplace=True)\n        fm_profile_df = fm_profile_df.drop_duplicates()\n\n        # Ensure step_id is of int data type and set default value to 1\n        fm_profile_df = fm_profile_df.astype(fm_profile_step_dtype)\n        fm_profile_df.loc[fm_profile_df['step_id'] == 0, 'step_id'] = 1\n\n        fm_profile_df = fm_profile_df[fm_profile_step_headers].sort_values(by=[\"profile_id\", 'step_id']).drop_duplicates()\n    # No step policies\n    else:\n        # make sure there is no step file in the folder\n        fm_profile_df = level_df[list(set(level_df.columns).intersection(set(policytc_cols)))].copy()\n        for col in policytc_cols[2:]:\n            if col not in fm_profile_df.columns:\n                fm_profile_df[col] = 0.\n\n        fm_profile_df = (\n            fm_profile_df\n            .rename(columns=profile_cols_map)\n            .drop_duplicates()\n            .assign(share2=0.0, share3=0.0)\n            .astype(fm_profile_dtype)[fm_profile_headers]\n        )\n    try:\n        fm_profile_df.to_csv(\n            fm_profile_file,\n            index=False,\n            header=False,\n            chunksize=chunksize,\n        )\n    except (IOError, OSError) as e:\n        raise OasisException(\"Exception raised in 'write_fm_profile_file'\", e)\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrites an FM profile file.\n\n:param level_df: level_df (fm terms) dataframe\n:type level_df: pandas.DataFrame\n\n:param fm_profile_file: open file to write to\n:type fm_profile_file: fileObj\n\n:param step_policies_present: flag to know which type of file to write\n:type step_policies_present: bool\n\n:param chunksize: chunksize\n:type chunksize: int\n\n:return: FM profile file path\n:rtype: str"
    },
    {
      "chunk_id": "oasislmf/preparation/reinsurance_layer.py::create_risk_level_profile_id@176",
      "source_type": "code",
      "path": "oasislmf/preparation/reinsurance_layer.py",
      "symbol_type": "function",
      "name": "create_risk_level_profile_id",
      "lineno": 176,
      "end_lineno": 290,
      "business_stage": "fm",
      "docstring": "Create new profile id from reinsurance in ri_df corresponding to reins_type.\nAdd them to fm_profile_df and match the profile_ids in ri_df and profile_map_df\nArgs:\n    ri_df: ri info and scope\n    profile_map_df: tree structure df representing each ri fm levels\n    fm_profile_df: df containing all the profiles\n    reins_type: type of reinsurance (one of oed.REINS_TYPES)\n    risk_level: level of the reinsurance terms (one of oed.REINS_RISK_LEVELS)\n    fm_level_id: fm level in profile_map_df\n\nReturns:\n    fm_profile_df: updated version of fm_profile_df",
      "content": "# File: oasislmf/preparation/reinsurance_layer.py\n# function: create_risk_level_profile_id (lines 176-290)\n\ndef create_risk_level_profile_id(ri_df, profile_map_df, fm_profile_df, reins_type, risk_level, fm_level_id, logger):\n    \"\"\"\n    Create new profile id from reinsurance in ri_df corresponding to reins_type.\n    Add them to fm_profile_df and match the profile_ids in ri_df and profile_map_df\n    Args:\n        ri_df: ri info and scope\n        profile_map_df: tree structure df representing each ri fm levels\n        fm_profile_df: df containing all the profiles\n        reins_type: type of reinsurance (one of oed.REINS_TYPES)\n        risk_level: level of the reinsurance terms (one of oed.REINS_RISK_LEVELS)\n        fm_level_id: fm level in profile_map_df\n\n    Returns:\n        fm_profile_df: updated version of fm_profile_df\n    \"\"\"\n    reins_type_filter = ri_df['ReinsType'] == reins_type\n    if not reins_type_filter.any():\n        return fm_profile_df\n\n    # create new fm profile from ri terms corresponding to the reins_type\n    ri_term_map = {term_info['oed_col']: term for term, term_info in FM_TERMS_PER_REINS_TYPE[reins_type].get(fm_level_id, {}).items()\n                   if 'oed_col' in term_info}\n\n    if ri_term_map:\n        # create a profile_id for each unique term combination\n        ri_df.loc[reins_type_filter, 'profile_id'] = pd.factorize(ri_df[reins_type_filter][list(ri_term_map)].apply(tuple, axis=1)\n                                                                  )[0] + 1 + fm_profile_df['profile_id'].max()\n\n        # create complete profile from terms and add it to fm_profile_df\n        cur_fm_profiles = ri_df[reins_type_filter][['profile_id'] + list(ri_term_map)].rename(columns=ri_term_map)\n        for term, term_info in FM_TERMS_PER_REINS_TYPE[reins_type][fm_level_id].items():\n            if term not in cur_fm_profiles:\n                cur_fm_profiles[term] = term_info['default']\n            else:\n                fill_empty(cur_fm_profiles, [term], term_info['default'])\n                cur_fm_profiles.loc[cur_fm_profiles[term].isin(term_info.get('to_default', [])), term] = term_info['default']\n        cur_fm_profiles.drop_duplicates(inplace=True)\n        if not cur_fm_profiles.empty:\n            fm_profile_df = pd.concat([fm_profile_df, cur_fm_profiles], ignore_index=True)\n\n    else:  # No terms at risk level\n        ri_df.loc[reins_type_filter, 'profile_id'] = PASSTHROUGH_PROFILE_ID\n\n    # update profile_map profile_id for filter and risk level\n    layer_id_set = set(ri_df[reins_type_filter]['layer_id'])\n    these_profile_map_layers = profile_map_df[profile_map_df['layer_id'].isin(layer_id_set)]\n\n    # filter level\n    if fm_level_id == RISK_LEVEL_ID:\n        ri_filter_fields = RISK_LEVEL_ALL_FIELDS + [field for field in FILTER_LEVEL_EXTRA_FIELDS if field in ri_df]\n        ri_filter_valid_fields = [field + '_valid' for field in ri_filter_fields]\n        ri_date_fields = ['ReinsInceptionDate', 'ReinsExpiryDate', 'AttachmentBasis']\n        merge_on = ['layer_id']\n        if reins_type in REINS_TYPE_EXACT_MATCH:\n            merge_on += RISK_LEVEL_FIELD_MAP[risk_level]\n        filter_df = (\n            these_profile_map_layers[these_profile_map_layers['level_id'] == FILTER_LEVEL_ID]\n            .reset_index()\n            .merge(\n                ri_df[reins_type_filter][ri_filter_fields + ri_filter_valid_fields + ['layer_id'] + ri_date_fields],\n                how='inner',\n                on=merge_on\n            )\n        )\n\n        def _match(row):\n            for field in ri_filter_fields:\n                if (field not in merge_on\n                        and row[f'{field}_valid'] and row[f'{field}_x'] != row[f'{field}_y']):\n                    return False\n\n            # Risk Attaching filter for reinsurance\n            if \"AttachmentBasis\" in row and row[\"AttachmentBasis\"] == \"RA\":\n                if row[\"ReinsInceptionDate\"] == \"\" or row[\"ReinsExpiryDate\"] == \"\":\n                    error_msg = \"Error: ReinsInceptionDate/ReinsExpiryDate missing, cannot use AttachmentBasis [RA]. Please check the ri_info file\"\n                    raise OasisException(error_msg)\n                elif row[\"PolInceptionDate\"] == \"\":\n                    acc_info = {\n                        field: row[f'{field}_x'] if f'{field}_x' in row else row[f'{field}']\n                        for field in RISK_LEVEL_FIELD_MAP[oed.REINS_RISK_LEVEL_ACCOUNT]\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreate new profile id from reinsurance in ri_df corresponding to reins_type.\nAdd them to fm_profile_df and match the profile_ids in ri_df and profile_map_df\nArgs:\n    ri_df: ri info and scope\n    profile_map_df: tree structure df representing each ri fm levels\n    fm_profile_df: df containing all the profiles\n    reins_type: type of reinsurance (one of oed.REINS_TYPES)\n    risk_level: level of the reinsurance terms (one of oed.REINS_RISK_LEVELS)\n    fm_level_id: fm level in profile_map_df\n\nReturns:\n    fm_profile_df: updated version of fm_profile_df"
    },
    {
      "chunk_id": "oasislmf/preparation/reinsurance_layer.py::get_xref_df@306",
      "source_type": "code",
      "path": "oasislmf/preparation/reinsurance_layer.py",
      "symbol_type": "function",
      "name": "get_xref_df",
      "lineno": 306,
      "end_lineno": 365,
      "business_stage": "fm",
      "docstring": "Build the cross-reference dataframe, which serves as a representation\nof the insurance programme depending on the reinsurance risk level.\nDataframes for programme, risk, filter and items levels are created.\nThe fields agg_id, level_id and to_agg_id (agg_id_to), which are used\nto construct the FM Programmes structure, are assigned. The\naforementioned dataframes are concatenated to form a single dataframe\ncalled xref_df, which is returned. The returned dataframe features the\nfields necessary for the assignment of profile IDs.\nArgs:\n    xref_descriptions_df: Fm summary mapping enhanced by relevant information from Loc and Account\n    risk_level: risk_level\n\nReturns:\n    df_levels: list of dataframes, one per fm level",
      "content": "# File: oasislmf/preparation/reinsurance_layer.py\n# function: get_xref_df (lines 306-365)\n\ndef get_xref_df(xref_descriptions_df, risk_level):\n    \"\"\"\n    Build the cross-reference dataframe, which serves as a representation\n    of the insurance programme depending on the reinsurance risk level.\n    Dataframes for programme, risk, filter and items levels are created.\n    The fields agg_id, level_id and to_agg_id (agg_id_to), which are used\n    to construct the FM Programmes structure, are assigned. The\n    aforementioned dataframes are concatenated to form a single dataframe\n    called xref_df, which is returned. The returned dataframe features the\n    fields necessary for the assignment of profile IDs.\n    Args:\n        xref_descriptions_df: Fm summary mapping enhanced by relevant information from Loc and Account\n        risk_level: risk_level\n\n    Returns:\n        df_levels: list of dataframes, one per fm level\n    \"\"\"\n\n    xref_descriptions = xref_descriptions_df.sort_values(by=REINS_RISK_LEVEL_XREF_COLUMN_MAP.get(risk_level, XREF_COLUMN_DEFAULT), kind='stable')\n    risk_level_fields = RISK_LEVEL_FIELD_MAP[risk_level]\n\n    df_levels = dict()\n    # Programme level\n    df_levels['programme_level'] = pd.DataFrame(\n        {'agg_id': 1, 'level_id': PROGRAM_LEVEL_ID, 'agg_id_to': 0},\n        index=[0]\n    )\n\n    # Risk level\n    risk_level_df = xref_descriptions.drop_duplicates(\n        subset=risk_level_fields, keep='first'\n    ).reset_index(drop=True)\n    risk_level_df['agg_id'] = risk_level_df.index + 1\n    risk_level_df['level_id'] = RISK_LEVEL_ID\n    risk_level_df['agg_id_to'] = 1\n    df_levels['risk_level'] = risk_level_df\n\n    # Filter level\n    filter_level_df = xref_descriptions.drop_duplicates(\n        subset=RISK_LEVEL_ALL_FIELDS\n    ).reset_index(drop=True)\n    filter_level_df['agg_id'] = filter_level_df.index + 1\n    filter_level_df['level_id'] = FILTER_LEVEL_ID\n    filter_level_df = filter_level_df.merge(\n        risk_level_df[risk_level_fields + ['agg_id']], how='left', on=risk_level_fields,\n        suffixes=['', '_to']\n    )\n    df_levels['filter_level'] = filter_level_df\n\n    # Item level\n    item_level_df = xref_descriptions.reset_index(drop=True)\n    item_level_df['agg_id'] = item_level_df['output_id']\n    item_level_df['level_id'] = ITEM_LEVEL_ID\n    item_level_df = item_level_df.merge(\n        filter_level_df[RISK_LEVEL_ALL_FIELDS + ['agg_id']], how='left',\n        on=RISK_LEVEL_ALL_FIELDS, suffixes=['', '_to']\n    )\n    df_levels['items_level'] = item_level_df\n\n    return df_levels\n\n\"\"\"Docstring (excerpt)\"\"\"\nBuild the cross-reference dataframe, which serves as a representation\nof the insurance programme depending on the reinsurance risk level.\nDataframes for programme, risk, filter and items levels are created.\nThe fields agg_id, level_id and to_agg_id (agg_id_to), which are used\nto construct the FM Programmes structure, are assigned. The\naforementioned dataframes are concatenated to form a single dataframe\ncalled xref_df, which is returned. The returned dataframe features the\nfields necessary for the assignment of profile IDs.\nArgs:\n    xref_descriptions_df: Fm summary mapping enhanced by relevant information from Loc and Account\n    risk_level: risk_level\n\nReturns:\n    df_levels: list of dataframes, one per fm level"
    },
    {
      "chunk_id": "oasislmf/preparation/reinsurance_layer.py::write_files_for_reinsurance@376",
      "source_type": "code",
      "path": "oasislmf/preparation/reinsurance_layer.py",
      "symbol_type": "function",
      "name": "write_files_for_reinsurance",
      "lineno": 376,
      "end_lineno": 520,
      "business_stage": "fm",
      "docstring": "Create the Oasis structures - FM Programmes, FM Profiles and FM Policy\nTCs - that represent the reinsurance structure.\n\nThe cross-reference dataframe, which serves as a representation of the\ninsurance programme depending on the reinsurance risk level, is built.\nExcept facultative contracts, each contract is a\nseparate layer. Profile IDs for the risk and filter levels are created\nusing the merged reinsurance scope and info dataframes. These profile\nIDs are assigned according to some combination of the fields\nPortNumber, AccNumber, PolNumber, LocGroup and LocNumber, dependent on\nreinsurance risk level. Individual programme level profile IDs are\nassigned for each row of the reinsurance info dataframe. Finally, the\nOasis structure is written out.",
      "content": "# File: oasislmf/preparation/reinsurance_layer.py\n# function: write_files_for_reinsurance (lines 376-520)\n\ndef write_files_for_reinsurance(ri_info_df, ri_scope_df, xref_descriptions_df, output_dir, fm_xref_fp, logger):\n    \"\"\"\n    Create the Oasis structures - FM Programmes, FM Profiles and FM Policy\n    TCs - that represent the reinsurance structure.\n\n    The cross-reference dataframe, which serves as a representation of the\n    insurance programme depending on the reinsurance risk level, is built.\n    Except facultative contracts, each contract is a\n    separate layer. Profile IDs for the risk and filter levels are created\n    using the merged reinsurance scope and info dataframes. These profile\n    IDs are assigned according to some combination of the fields\n    PortNumber, AccNumber, PolNumber, LocGroup and LocNumber, dependent on\n    reinsurance risk level. Individual programme level profile IDs are\n    assigned for each row of the reinsurance info dataframe. Finally, the\n    Oasis structure is written out.\n    \"\"\"\n    fm_xref_df = get_dataframe(fm_xref_fp)\n    fm_xref_df['agg_id'] = range(1, 1 + len(fm_xref_df))\n    fill_empty(ri_scope_df, RISK_LEVEL_ALL_FIELDS, '')\n\n    reinsurance_index = 1\n    inuring_metadata = {}\n    for inuring_priority in range(1, ri_info_df['InuringPriority'].max() + 1):\n        for risk_level in oed.REINS_RISK_LEVELS:\n            cur_ri_info_df = ri_info_df[(ri_info_df['InuringPriority'] == inuring_priority) & (ri_info_df['RiskLevel'] == risk_level)]\n            if cur_ri_info_df.empty:\n                continue\n\n            output_name = f\"ri_{inuring_priority}_{risk_level}\"\n            df_levels = get_xref_df(xref_descriptions_df, risk_level)\n\n            _log_dataframe(logger, df_levels, output_name)\n\n            no_loss_profile = {'profile_id': NO_LOSS_PROFILE_ID, **NO_LOSS_PROFILE}\n            pass_through_profile = {'profile_id': PASSTHROUGH_PROFILE_ID, **PASSTHROUGH_PROFILE}\n\n            fm_profile_df = pd.DataFrame([no_loss_profile, pass_through_profile])\n\n            xref_df = pd.concat(df_levels.values(), ignore_index=True)\n\n            # Assign default profile IDs\n            xref_df['profile_id'] = NO_LOSS_PROFILE_ID\n            xref_df['profile_id'] = xref_df['profile_id'].where(\n                xref_df['level_id'].isin([FILTER_LEVEL_ID, RISK_LEVEL_ID]),\n                PASSTHROUGH_PROFILE_ID\n            )\n\n            # Merge RI info and scope dataframes, and assign layers\n            # Use as few layers as possible for FAC\n            # Otherwise separate layers for each contract\n            logger.debug(\n                'Merging RI info and scope dataframes and assigning layers'\n            )\n            risk_level_fields = RISK_LEVEL_FIELD_MAP[risk_level]\n            ri_df = cur_ri_info_df.merge(ri_scope_df, on='ReinsNumber', suffixes=['', '_scope'])\n            valid_rows = check_ri_scope_filter(ri_df, risk_level)\n            if not valid_rows.all():\n                raise OasisException(\n                    f'Invalid combination of Risk Level and Reinsurance Type. Please check scope file:\\n{ri_df[~valid_rows]}'\n                )\n\n            ri_df['layer_id'] = 0\n            ri_df.loc[ri_df['ReinsType'] == oed.REINS_TYPE_FAC, 'layer_id'] = (ri_df.loc[ri_df['ReinsType'] == oed.REINS_TYPE_FAC]\n                                                                               .groupby(risk_level_fields, observed=True).cumcount() + 1)\n            ri_info_no_fac = cur_ri_info_df[cur_ri_info_df['ReinsType'] != oed.REINS_TYPE_FAC].reset_index(drop=True)\n            ri_info_no_fac['layer_id'] = ri_info_no_fac.index + 1 + ri_df['layer_id'].max()\n            ri_df = ri_df.merge(ri_info_no_fac, how='left', on=ri_info_no_fac.columns.to_list()[:-1], suffixes=['', '_y'])\n            ri_df.loc[~ri_df['layer_id_y'].isna(), 'layer_id'] = ri_df.loc[~ri_df['layer_id_y'].isna(), 'layer_id_y'].astype('int')\n            ri_df = ri_df.drop('layer_id_y', axis=1)\n\n            for field in RISK_LEVEL_ALL_FIELDS:\n                ri_df[field + '_valid'] = (ri_df[field] != '')\n            for field in FILTER_LEVEL_EXTRA_FIELDS:\n                if field in ri_df.columns:\n                    fill_empty(ri_df, [field], '')\n                    ri_df[field + '_valid'] = (ri_df[field] != '')\n\n            del ri_info_no_fac\n\n            profile_maps = [xref_df.copy() for i in range(ri_df['layer_id'].max())]\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreate the Oasis structures - FM Programmes, FM Profiles and FM Policy\nTCs - that represent the reinsurance structure.\n\nThe cross-reference dataframe, which serves as a representation of the\ninsurance programme depending on the reinsurance risk level, is built.\nExcept facultative contracts, each contract is a\nseparate layer. Profile IDs for the risk and filter levels are created\nusing the merged reinsurance scope and info dataframes. These profile\nIDs are assigned according to some combination of the fields\nPortNumber, AccNumber, PolNumber, LocGroup and LocNumber, dependent on\nreinsurance risk level. Individual programme level profile IDs are\nassigned for each row of the reinsurance info dataframe. Finally, the\nOasis structure is written out."
    },
    {
      "chunk_id": "oasislmf/preparation/summaries.py::get_summary_mapping@110",
      "source_type": "code",
      "path": "oasislmf/preparation/summaries.py",
      "symbol_type": "function",
      "name": "get_summary_mapping",
      "lineno": 110,
      "end_lineno": 152,
      "business_stage": "other",
      "docstring": "Create a DataFrame with linking information between Ktools `OasisFiles`\nAnd the Exposure data\n\n:param inputs_df: datafame from gul_inputs.get_gul_input_items(..)  / il_inputs.get_il_input_items(..)\n:type inputs_df: pandas.DataFrame\n\n:param is_fm_summary: Indicates whether an FM summary mapping is required\n:type is_fm_summary: bool\n\n:return: Subset of columns from gul_inputs_df / il_inputs_df\n:rtype: pandas.DataFrame",
      "content": "# File: oasislmf/preparation/summaries.py\n# function: get_summary_mapping (lines 110-152)\n\ndef get_summary_mapping(inputs_df, oed_hierarchy, is_fm_summary=False):\n    \"\"\"\n    Create a DataFrame with linking information between Ktools `OasisFiles`\n    And the Exposure data\n\n    :param inputs_df: datafame from gul_inputs.get_gul_input_items(..)  / il_inputs.get_il_input_items(..)\n    :type inputs_df: pandas.DataFrame\n\n    :param is_fm_summary: Indicates whether an FM summary mapping is required\n    :type is_fm_summary: bool\n\n    :return: Subset of columns from gul_inputs_df / il_inputs_df\n    :rtype: pandas.DataFrame\n    \"\"\"\n    # Case GUL+FM (based on il_inputs_df)\n    if is_fm_summary:\n        summary_mapping = inputs_df.rename(columns={'gul_input_id': 'agg_id'})\n\n    # GUL Only\n    else:\n        summary_mapping = inputs_df.copy(deep=True)\n        summary_mapping['layer_id'] = 1\n        summary_mapping['agg_id'] = summary_mapping['item_id']\n\n    summary_mapping.drop(\n        [c for c in summary_mapping.columns if c not in get_useful_summary_cols(oed_hierarchy)],\n        axis=1,\n        inplace=True\n    )\n    acc_num = oed_hierarchy['accnum']['ProfileElementName']\n    loc_num = oed_hierarchy['locnum']['ProfileElementName']\n    policy_num = oed_hierarchy['polnum']['ProfileElementName']\n    portfolio_num = oed_hierarchy['portnum']['ProfileElementName']\n\n    dtypes = {\n        **{t: 'str' for t in [portfolio_num, policy_num, acc_num, loc_num, 'peril_id']},\n        **{t: 'uint8' for t in ['coverage_type_id']},\n        **{t: 'uint32' for t in [SOURCE_IDX['loc'], SOURCE_IDX['acc'], 'loc_id', 'item_id', 'layer_id', 'coverage_id', 'agg_id', 'output_id',\n                                 'building_id', 'risk_id']},\n        **{t: 'float64' for t in ['tiv']}\n    }\n    summary_mapping = set_dataframe_column_dtypes(summary_mapping, dtypes)\n    return summary_mapping\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreate a DataFrame with linking information between Ktools `OasisFiles`\nAnd the Exposure data\n\n:param inputs_df: datafame from gul_inputs.get_gul_input_items(..)  / il_inputs.get_il_input_items(..)\n:type inputs_df: pandas.DataFrame\n\n:param is_fm_summary: Indicates whether an FM summary mapping is required\n:type is_fm_summary: bool\n\n:return: Subset of columns from gul_inputs_df / il_inputs_df\n:rtype: pandas.DataFrame"
    },
    {
      "chunk_id": "oasislmf/preparation/summaries.py::merge_oed_to_mapping@155",
      "source_type": "code",
      "path": "oasislmf/preparation/summaries.py",
      "symbol_type": "function",
      "name": "merge_oed_to_mapping",
      "lineno": 155,
      "end_lineno": 187,
      "business_stage": "other",
      "docstring": "Create a factorized col (summary ids) based on a list of oed column names\n\n:param :summary_map_df dataframe return from get_summary_mapping\n:type summary_map_df: pandas.DataFrame\n\n:param exposure_df: Summary map file path\n:type exposure_df: pandas.DataFrame\n\n:param oed_column_join: column to join on\n:type oed_column_join: list\n\n:param oed_column_info: Dictionary of columns to pick from exposure_df and their default value\n:type oed_column_info: dict\n\n{'Col_A': 0, 'Col_B': 1, 'Col_C': 2}\n\n:return: New DataFrame of summary_map_df + exposure_df merged on exposure index\n:rtype: pandas.DataFrame",
      "content": "# File: oasislmf/preparation/summaries.py\n# function: merge_oed_to_mapping (lines 155-187)\n\ndef merge_oed_to_mapping(summary_map_df, exposure_df, oed_column_join, oed_column_info):\n    \"\"\"\n    Create a factorized col (summary ids) based on a list of oed column names\n\n    :param :summary_map_df dataframe return from get_summary_mapping\n    :type summary_map_df: pandas.DataFrame\n\n    :param exposure_df: Summary map file path\n    :type exposure_df: pandas.DataFrame\n\n    :param oed_column_join: column to join on\n    :type oed_column_join: list\n\n    :param oed_column_info: Dictionary of columns to pick from exposure_df and their default value\n    :type oed_column_info: dict\n\n    {'Col_A': 0, 'Col_B': 1, 'Col_C': 2}\n\n    :return: New DataFrame of summary_map_df + exposure_df merged on exposure index\n    :rtype: pandas.DataFrame\n    \"\"\"\n\n    column_set = set(oed_column_info)\n    columns_found = [c for c in column_set if c in exposure_df.columns and c not in summary_map_df.columns]\n    columns_missing = list(set(column_set) - set(columns_found))\n\n    new_summary_map_df = merge_dataframes(summary_map_df, exposure_df.loc[:, columns_found + oed_column_join], join_on=oed_column_join, how='inner')\n    for col, default in oed_column_info.items():\n        if col in columns_missing:\n            new_summary_map_df[col] = default\n    fill_na_with_categoricals(new_summary_map_df, oed_column_info)\n\n    return new_summary_map_df\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreate a factorized col (summary ids) based on a list of oed column names\n\n:param :summary_map_df dataframe return from get_summary_mapping\n:type summary_map_df: pandas.DataFrame\n\n:param exposure_df: Summary map file path\n:type exposure_df: pandas.DataFrame\n\n:param oed_column_join: column to join on\n:type oed_column_join: list\n\n:param oed_column_info: Dictionary of columns to pick from exposure_df and their default value\n:type oed_column_info: dict\n\n{'Col_A': 0, 'Col_B': 1, 'Col_C': 2}\n\n:return: New DataFrame of summary_map_df + exposure_df merged on exposure index\n:rtype: pandas.DataFrame"
    },
    {
      "chunk_id": "oasislmf/preparation/summaries.py::group_by_oed@190",
      "source_type": "code",
      "path": "oasislmf/preparation/summaries.py",
      "symbol_type": "function",
      "name": "group_by_oed",
      "lineno": 190,
      "end_lineno": 244,
      "business_stage": "other",
      "docstring": "Adds list of OED fields from `column_set` to summary map file\n\n:param :summary_map_df dataframe return from get_summary_mapping\n:type summary_map_df: pandas.DataFrame\n\n:param exposure_df: DataFrame loaded from location.csv\n:type exposure_df: pandas.DataFrame\n\n:param accounts_df: DataFrame loaded from accounts.csv\n:type accounts_df: pandas.DataFrame\n\n:return: subset of columns from exposure_df to merge\n:rtype: list\n\n    summary_ids[0] is an int list 1..n  array([1, 2, 1, 2, 1, 2, 1, 2, 1, 2, ... ])\n    summary_ids[1] is an array of values used to factorize  `array(['Layer1', 'Layer2'], dtype=object)`",
      "content": "# File: oasislmf/preparation/summaries.py\n# function: group_by_oed (lines 190-244)\n\ndef group_by_oed(oed_col_group, summary_map_df, exposure_df, sort_by, accounts_df=None):\n    \"\"\"\n    Adds list of OED fields from `column_set` to summary map file\n\n    :param :summary_map_df dataframe return from get_summary_mapping\n    :type summary_map_df: pandas.DataFrame\n\n    :param exposure_df: DataFrame loaded from location.csv\n    :type exposure_df: pandas.DataFrame\n\n    :param accounts_df: DataFrame loaded from accounts.csv\n    :type accounts_df: pandas.DataFrame\n\n    :return: subset of columns from exposure_df to merge\n    :rtype: list\n\n        summary_ids[0] is an int list 1..n  array([1, 2, 1, 2, 1, 2, 1, 2, 1, 2, ... ])\n        summary_ids[1] is an array of values used to factorize  `array(['Layer1', 'Layer2'], dtype=object)`\n    \"\"\"\n    oed_cols = oed_col_group  # All required columns\n    exposure_cols = [c for c in oed_cols if c not in summary_map_df.columns\n                     and c not in calculated_summary_cols]  # columns which are in locations / Accounts file\n    mapped_cols = [c for c in oed_cols + [SOURCE_IDX['loc'], SOURCE_IDX['acc'], sort_by]\n                   if c in summary_map_df.columns]  # Columns already in summary_map_df\n    to_calculate_column = [c for c in oed_cols if c in calculated_summary_cols]\n\n    tiv_cols = ['tiv', 'loc_id', 'building_id', 'coverage_type_id']\n\n    # Extract mapped_cols from summary_map_df\n    summary_group_df = summary_map_df.loc[:, list(set(tiv_cols).union(mapped_cols))]\n\n    # Search Loc / Acc files and merge in remaing\n    if exposure_cols is not []:\n        # Location file columns\n        exposure_cols_loc = [c for c in exposure_cols if c in exposure_df.columns]\n        exposure_col_df = exposure_df.loc[:, exposure_cols_loc + [SOURCE_IDX['loc']]]\n        summary_group_df = merge_dataframes(summary_group_df, exposure_col_df, join_on=SOURCE_IDX['loc'], how='left')\n\n        # Account file columns\n        if isinstance(accounts_df, pd.DataFrame):\n            accounts_cols = [c for c in exposure_cols if c in set(accounts_df.columns) - set(exposure_df.columns)]\n            if accounts_cols:\n                accounts_col_df = accounts_df.loc[:, accounts_cols + [SOURCE_IDX['acc']]]\n                summary_group_df = merge_dataframes(summary_group_df, accounts_col_df, join_on=SOURCE_IDX['acc'], how='left')\n\n    for col in to_calculate_column:\n        summary_group_df = calculated_summary_cols[col](summary_group_df)\n\n    fill_na_with_categoricals(summary_group_df, 0)\n    summary_group_df.sort_values(by=[sort_by], inplace=True, kind='stable')\n    summary_ids = factorize_dataframe(summary_group_df, by_col_labels=oed_cols)\n    summary_tiv = summary_group_df.drop_duplicates(['loc_id', 'building_id', 'coverage_type_id'] + oed_col_group,\n                                                   keep='first').groupby(oed_col_group, observed=True).agg({'tiv': \"sum\"})\n\n    return summary_ids[0], summary_ids[1], summary_tiv\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdds list of OED fields from `column_set` to summary map file\n\n:param :summary_map_df dataframe return from get_summary_mapping\n:type summary_map_df: pandas.DataFrame\n\n:param exposure_df: DataFrame loaded from location.csv\n:type exposure_df: pandas.DataFrame\n\n:param accounts_df: DataFrame loaded from accounts.csv\n:type accounts_df: pandas.DataFrame\n\n:return: subset of columns from exposure_df to merge\n:rtype: list\n\n    summary_ids[0] is an int list 1..n  array([1, 2, 1, 2, 1, 2, 1, 2, 1, 2, ... ])\n    summary_ids[1] is an array of values used to factorize  `array(['Layer1', 'Layer2'], dtype=object)`"
    },
    {
      "chunk_id": "oasislmf/preparation/summaries.py::write_summary_levels@248",
      "source_type": "code",
      "path": "oasislmf/preparation/summaries.py",
      "symbol_type": "function",
      "name": "write_summary_levels",
      "lineno": 248,
      "end_lineno": 310,
      "business_stage": "other",
      "docstring": "Json file with list Available / Recommended columns for use in the summary reporting\n\nAvailable: Columns which exists in input files and has at least one non-zero / NaN value\nRecommended: Columns which are available + also in the list of `useful` groupings SUMMARY_LEVEL_LOC\n\n{\n    'GUL': {\n        'available': ['AccNumber',\n                     'LocNumber',\n                     'istenant',\n                     'buildingid',\n                     'countrycode',\n                     'latitude',\n                     'longitude',\n                     'streetaddress',\n                     'postalcode',\n                     'occupancycode',\n                     'constructioncode',\n                     'locperilscovered',\n                     'BuildingTIV',\n                     'ContentsTIV',\n                     'BITIV',\n                     'PortNumber'],\n\n    'IL': {\n            ... etc ...\n    }\n}",
      "content": "# File: oasislmf/preparation/summaries.py\n# function: write_summary_levels (lines 248-310)\n\ndef write_summary_levels(exposure_df, accounts_df, exposure_data, target_dir):\n    '''\n    Json file with list Available / Recommended columns for use in the summary reporting\n\n    Available: Columns which exists in input files and has at least one non-zero / NaN value\n    Recommended: Columns which are available + also in the list of `useful` groupings SUMMARY_LEVEL_LOC\n\n    {\n        'GUL': {\n            'available': ['AccNumber',\n                         'LocNumber',\n                         'istenant',\n                         'buildingid',\n                         'countrycode',\n                         'latitude',\n                         'longitude',\n                         'streetaddress',\n                         'postalcode',\n                         'occupancycode',\n                         'constructioncode',\n                         'locperilscovered',\n                         'BuildingTIV',\n                         'ContentsTIV',\n                         'BITIV',\n                         'PortNumber'],\n\n        'IL': {\n                ... etc ...\n        }\n    }\n    '''\n    # Manage internal columns, (Non-OED exposure input)\n    int_excluded_cols = ['loc_id', SOURCE_IDX['loc']]\n    desc_non_oed = 'Not an OED field'\n    int_oasis_cols = {\n        'coverage_type_id': 'Oasis coverage type',\n        'peril_id': 'OED peril code',\n        'coverage_id': 'Oasis coverage identifier',\n    }\n\n    # GUL perspective (loc columns only)\n    l_col_list = (exposure_df.drop(columns=['peril_group_id'], errors='ignore')\n                             .replace(0, np.nan)\n                             .dropna(how='any', axis=1)\n                             .columns.to_list())\n    l_col_info = exposure_data.get_input_fields('Loc')\n    gul_avail = {k: l_col_info[k.lower()][\"Type & Description\"] if k.lower() in l_col_info else desc_non_oed\n                 for k in set([c for c in l_col_list]).difference(int_excluded_cols)}\n\n    # IL perspective (join of acc + loc col with no dups)\n    il_avail = {}\n    if accounts_df is not None:\n        a_col_list = accounts_df.loc[:, ~accounts_df.isnull().all()].columns.to_list()\n        a_col_info = exposure_data.get_input_fields('Acc')\n        a_avail = set([c for c in a_col_list])\n        il_avail = {k: a_col_info[k.lower()][\"Type & Description\"] if k.lower() in a_col_info else desc_non_oed\n                    for k in a_avail.difference(gul_avail.keys())}\n\n    # Write JSON\n    gul_summary_lvl = {'GUL': {'available': {**gul_avail, **il_avail, **int_oasis_cols}}}\n    il_summary_lvl = {'IL': {'available': {**gul_avail, **il_avail, **int_oasis_cols}}} if il_avail else {}\n    with io.open(os.path.join(target_dir, 'exposure_summary_levels.json'), 'w', encoding='utf-8') as f:\n        f.write(json.dumps({**gul_summary_lvl, **il_summary_lvl}, sort_keys=True, ensure_ascii=False, indent=4))\n\n\"\"\"Docstring (excerpt)\"\"\"\nJson file with list Available / Recommended columns for use in the summary reporting\n\nAvailable: Columns which exists in input files and has at least one non-zero / NaN value\nRecommended: Columns which are available + also in the list of `useful` groupings SUMMARY_LEVEL_LOC\n\n{\n    'GUL': {\n        'available': ['AccNumber',\n                     'LocNumber',\n                     'istenant',\n                     'buildingid',\n                     'countrycode',\n                     'latitude',\n                     'longitude',\n                     'streetaddress',\n                     'postalcode',\n                     'occupancycode',\n                     'constructioncode',\n                     'locperilscovered',\n                     'BuildingTIV',\n                     'ContentsTIV',\n                     'BITIV',\n                     'PortNumber'],\n\n    'IL': {\n            ... etc ...\n    }\n}"
    },
    {
      "chunk_id": "oasislmf/preparation/summaries.py::write_mapping_file@314",
      "source_type": "code",
      "path": "oasislmf/preparation/summaries.py",
      "symbol_type": "function",
      "name": "write_mapping_file",
      "lineno": 314,
      "end_lineno": 355,
      "business_stage": "other",
      "docstring": "Writes a summary map file, used to build summarycalc xref files.\n\n:param summary_mapping: dataframe return from get_summary_mapping\n:type summary_mapping: pandas.DataFrame\n\n:param sum_mapping_fp: Summary map file path\n:type sum_mapping_fp: str\n\n:param is_fm_summary: Indicates whether an FM summary mapping is required\n:type is_fm_summary: bool\n\n:return: Summary xref file path\n:rtype: str",
      "content": "# File: oasislmf/preparation/summaries.py\n# function: write_mapping_file (lines 314-355)\n\ndef write_mapping_file(sum_inputs_df, target_dir, is_fm_summary=False):\n    \"\"\"\n    Writes a summary map file, used to build summarycalc xref files.\n\n    :param summary_mapping: dataframe return from get_summary_mapping\n    :type summary_mapping: pandas.DataFrame\n\n    :param sum_mapping_fp: Summary map file path\n    :type sum_mapping_fp: str\n\n    :param is_fm_summary: Indicates whether an FM summary mapping is required\n    :type is_fm_summary: bool\n\n    :return: Summary xref file path\n    :rtype: str\n    \"\"\"\n    target_dir = as_path(\n        target_dir,\n        'Target IL input files directory',\n        is_dir=True,\n        preexists=False\n    )\n\n    # Set chunk size for writing the CSV files - default is max 20K, min 1K\n    chunksize = min(2 * 10 ** 5, max(len(sum_inputs_df), 1000))\n\n    if is_fm_summary:\n        sum_mapping_fp = os.path.join(target_dir, SUMMARY_MAPPING['fm_map_fn'])\n    else:\n        sum_mapping_fp = os.path.join(target_dir, SUMMARY_MAPPING['gul_map_fn'])\n    try:\n        sum_inputs_df.to_csv(\n            path_or_buf=sum_mapping_fp,\n            encoding='utf-8',\n            mode=('w' if os.path.exists(sum_mapping_fp) else 'a'),\n            chunksize=chunksize,\n            index=False\n        )\n    except (IOError, OSError) as e:\n        raise OasisException(\"Exception raised in 'write_mapping_file'\", e)\n\n    return sum_mapping_fp\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrites a summary map file, used to build summarycalc xref files.\n\n:param summary_mapping: dataframe return from get_summary_mapping\n:type summary_mapping: pandas.DataFrame\n\n:param sum_mapping_fp: Summary map file path\n:type sum_mapping_fp: str\n\n:param is_fm_summary: Indicates whether an FM summary mapping is required\n:type is_fm_summary: bool\n\n:return: Summary xref file path\n:rtype: str"
    },
    {
      "chunk_id": "oasislmf/preparation/summaries.py::get_column_selection@358",
      "source_type": "code",
      "path": "oasislmf/preparation/summaries.py",
      "symbol_type": "function",
      "name": "get_column_selection",
      "lineno": 358,
      "end_lineno": 384,
      "business_stage": "other",
      "docstring": "Given a analysis_settings summary definition, return either\n    1. the set of OED columns requested to group by\n    2. If no information key 'oed_fields', then group all outputs into a single summary_set\n\n:param summary_set: summary group dictionary from the `analysis_settings.json`\n:type summary_set: dict\n\n:return: List of selected OED columns to create summary groups from\n:rtype: list",
      "content": "# File: oasislmf/preparation/summaries.py\n# function: get_column_selection (lines 358-384)\n\ndef get_column_selection(summary_set):\n    \"\"\"\n    Given a analysis_settings summary definition, return either\n        1. the set of OED columns requested to group by\n        2. If no information key 'oed_fields', then group all outputs into a single summary_set\n\n    :param summary_set: summary group dictionary from the `analysis_settings.json`\n    :type summary_set: dict\n\n    :return: List of selected OED columns to create summary groups from\n    :rtype: list\n    \"\"\"\n    if \"oed_fields\" not in summary_set:\n        return []\n    if not summary_set[\"oed_fields\"]:\n        return []\n\n    # Use OED column list set in analysis_settings file\n    elif isinstance(summary_set['oed_fields'], list) and len(summary_set['oed_fields']) > 0:\n        return [c for c in summary_set['oed_fields']]\n    elif isinstance(summary_set['oed_fields'], str) and len(summary_set['oed_fields']) > 0:\n        return [summary_set['oed_fields']]\n    else:\n        raise OasisException(\n            'Error processing settings file: \"oed_fields\" '\n            'is expected to be a list of strings, not {}'.format(type(summary_set['oed_fields']))\n        )\n\n\"\"\"Docstring (excerpt)\"\"\"\nGiven a analysis_settings summary definition, return either\n    1. the set of OED columns requested to group by\n    2. If no information key 'oed_fields', then group all outputs into a single summary_set\n\n:param summary_set: summary group dictionary from the `analysis_settings.json`\n:type summary_set: dict\n\n:return: List of selected OED columns to create summary groups from\n:rtype: list"
    },
    {
      "chunk_id": "oasislmf/preparation/summaries.py::get_ri_settings@387",
      "source_type": "code",
      "path": "oasislmf/preparation/summaries.py",
      "symbol_type": "function",
      "name": "get_ri_settings",
      "lineno": 387,
      "end_lineno": 406,
      "business_stage": "other",
      "docstring": "Return the contents of ri_layers.json\n\nExample:\n{\n    \"1\": {\n        \"inuring_priority\": 1,\n        \"risk_level\": \"LOC\",\n        \"directory\": \"  ... /runs/ProgOasis-20190501145127/RI_1\"\n    }\n}\n\n:param run_dir: The file path of the model run directory\n:type run_dir: str\n\n:return: metadata for the Reinsurance layers\n:rtype: dict",
      "content": "# File: oasislmf/preparation/summaries.py\n# function: get_ri_settings (lines 387-406)\n\ndef get_ri_settings(run_dir):\n    \"\"\"\n    Return the contents of ri_layers.json\n\n    Example:\n    {\n        \"1\": {\n            \"inuring_priority\": 1,\n            \"risk_level\": \"LOC\",\n            \"directory\": \"  ... /runs/ProgOasis-20190501145127/RI_1\"\n        }\n    }\n\n    :param run_dir: The file path of the model run directory\n    :type run_dir: str\n\n    :return: metadata for the Reinsurance layers\n    :rtype: dict\n    \"\"\"\n    return get_json(src_fp=os.path.join(run_dir, 'ri_layers.json'))\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturn the contents of ri_layers.json\n\nExample:\n{\n    \"1\": {\n        \"inuring_priority\": 1,\n        \"risk_level\": \"LOC\",\n        \"directory\": \"  ... /runs/ProgOasis-20190501145127/RI_1\"\n    }\n}\n\n:param run_dir: The file path of the model run directory\n:type run_dir: str\n\n:return: metadata for the Reinsurance layers\n:rtype: dict"
    },
    {
      "chunk_id": "oasislmf/preparation/summaries.py::write_df_to_csv_file@409",
      "source_type": "code",
      "path": "oasislmf/preparation/summaries.py",
      "symbol_type": "function",
      "name": "write_df_to_csv_file",
      "lineno": 409,
      "end_lineno": 437,
      "business_stage": "other",
      "docstring": "Write a generated summary xref dataframe to disk in csv format.\n\n:param df: The dataframe output of get_df( .. )\n:type df:  pandas.DataFrame\n\n:param target_dir: Abs directory to write a summary_xref file\n:type target_dir:  str\n\n:param filename: Name of output file\n:type filename:  str",
      "content": "# File: oasislmf/preparation/summaries.py\n# function: write_df_to_csv_file (lines 409-437)\n\ndef write_df_to_csv_file(df, target_dir, filename):\n    \"\"\"\n    Write a generated summary xref dataframe to disk in csv format.\n\n    :param df: The dataframe output of get_df( .. )\n    :type df:  pandas.DataFrame\n\n    :param target_dir: Abs directory to write a summary_xref file\n    :type target_dir:  str\n\n    :param filename: Name of output file\n    :type filename:  str\n    \"\"\"\n    target_dir = as_path(target_dir, 'Input files directory', is_dir=True, preexists=False)\n    pathlib.Path(target_dir).mkdir(parents=True, exist_ok=True)\n    chunksize = min(2 * 10 ** 5, max(len(df), 1000))\n    csv_fp = os.path.join(target_dir, filename)\n    try:\n        df.to_csv(\n            path_or_buf=csv_fp,\n            encoding='utf-8',\n            mode=('w'),\n            chunksize=chunksize,\n            index=False\n        )\n    except (IOError, OSError) as e:\n        raise OasisException(\"Exception raised in 'write_df_to_csv_file'\", e)\n\n    return csv_fp\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrite a generated summary xref dataframe to disk in csv format.\n\n:param df: The dataframe output of get_df( .. )\n:type df:  pandas.DataFrame\n\n:param target_dir: Abs directory to write a summary_xref file\n:type target_dir:  str\n\n:param filename: Name of output file\n:type filename:  str"
    },
    {
      "chunk_id": "oasislmf/preparation/summaries.py::write_df_to_parquet_file@440",
      "source_type": "code",
      "path": "oasislmf/preparation/summaries.py",
      "symbol_type": "function",
      "name": "write_df_to_parquet_file",
      "lineno": 440,
      "end_lineno": 464,
      "business_stage": "other",
      "docstring": "Write a generated summary xref dataframe to disk in parquet format.\n\n:param df: The dataframe output of get_df( .. )\n:type df: pandas.DataFrame\n\n:param target_dir: Abs directory to write a summary_xref file\n:type target_dir: str\n\n:param filename: Name of output file\n:type filename: str",
      "content": "# File: oasislmf/preparation/summaries.py\n# function: write_df_to_parquet_file (lines 440-464)\n\ndef write_df_to_parquet_file(df, target_dir, filename):\n    \"\"\"\n    Write a generated summary xref dataframe to disk in parquet format.\n\n    :param df: The dataframe output of get_df( .. )\n    :type df: pandas.DataFrame\n\n    :param target_dir: Abs directory to write a summary_xref file\n    :type target_dir: str\n\n    :param filename: Name of output file\n    :type filename: str\n    \"\"\"\n    target_dir = as_path(\n        target_dir, 'Output files directory', is_dir=True, preexists=False\n    )\n    parquet_fp = os.path.join(target_dir, filename)\n    try:\n        df.to_parquet(path=parquet_fp, engine='pyarrow')\n    except (IOError, OSError) as e:\n        raise OasisException(\n            \"Exception raised in 'write_df_to_parquet_file'\", e\n        )\n\n    return parquet_fp\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrite a generated summary xref dataframe to disk in parquet format.\n\n:param df: The dataframe output of get_df( .. )\n:type df: pandas.DataFrame\n\n:param target_dir: Abs directory to write a summary_xref file\n:type target_dir: str\n\n:param filename: Name of output file\n:type filename: str"
    },
    {
      "chunk_id": "oasislmf/preparation/summaries.py::get_summary_xref_df@467",
      "source_type": "code",
      "path": "oasislmf/preparation/summaries.py",
      "symbol_type": "function",
      "name": "get_summary_xref_df",
      "lineno": 467,
      "end_lineno": 576,
      "business_stage": "other",
      "docstring": "Create a Dataframe for either gul / il / ri  based on a section\nfrom the analysis settings\n\n\n:param map_df: Summary Map dataframe (GUL / IL)\n:type map_df:  pandas.DataFrame\n\n:param exposure_df: Location OED data\n:type exposure_df:  pandas.DataFrame\n\n:param accounts_df: Accounts OED data\n:type accounts_df:  pandas.DataFrame\n\n:param summaries_info_dict: list of dictionary definitionfor a summary group from the analysis_settings file\n:type summaries_info_dict:  list\n\n[{\n    \"summarycalc\": true,\n    \"eltcalc\": true,\n    \"aalcalc\": true,\n    \"pltcalc\": true,\n    \"id\": 1,\n    \"oed_fields\": [],\n    \"lec_output\": true,\n    \"leccalc\": {\n      \"return_period_file\": true,\n      \"outputs\": {\n        \"full_uncertainty_aep\": true,\n        \"full_uncertainty_oep\": true,\n        \"wheatsheaf_aep\": true,\n        \"wheatsheaf_oep\": true\n      }\n    }\n  },\n\n  ...\n ]\n\n:param summaries_type: Text label to use as key in summary description either ['gul', 'il', 'ri']\n:type summaries_type: String\n\n:return summaryxref_df: Dataframe containing abstracted summary data for ktools\n:rtypwrite_xref_filee: pandas.DataFrame\n\n:return summary_desc: dictionary of dataFrames listing what summary_ids map to\n:rtype: dictionary",
      "content": "# File: oasislmf/preparation/summaries.py\n# function: get_summary_xref_df (lines 467-576)\n\ndef get_summary_xref_df(\n    map_df, exposure_df, accounts_df, summaries_info_dict, summaries_type,\n    id_set_index='output_id'\n):\n    \"\"\"\n    Create a Dataframe for either gul / il / ri  based on a section\n    from the analysis settings\n\n\n    :param map_df: Summary Map dataframe (GUL / IL)\n    :type map_df:  pandas.DataFrame\n\n    :param exposure_df: Location OED data\n    :type exposure_df:  pandas.DataFrame\n\n    :param accounts_df: Accounts OED data\n    :type accounts_df:  pandas.DataFrame\n\n    :param summaries_info_dict: list of dictionary definitionfor a summary group from the analysis_settings file\n    :type summaries_info_dict:  list\n\n    [{\n        \"summarycalc\": true,\n        \"eltcalc\": true,\n        \"aalcalc\": true,\n        \"pltcalc\": true,\n        \"id\": 1,\n        \"oed_fields\": [],\n        \"lec_output\": true,\n        \"leccalc\": {\n          \"return_period_file\": true,\n          \"outputs\": {\n            \"full_uncertainty_aep\": true,\n            \"full_uncertainty_oep\": true,\n            \"wheatsheaf_aep\": true,\n            \"wheatsheaf_oep\": true\n          }\n        }\n      },\n\n      ...\n     ]\n\n    :param summaries_type: Text label to use as key in summary description either ['gul', 'il', 'ri']\n    :type summaries_type: String\n\n    :return summaryxref_df: Dataframe containing abstracted summary data for ktools\n    :rtypwrite_xref_filee: pandas.DataFrame\n\n    :return summary_desc: dictionary of dataFrames listing what summary_ids map to\n    :rtype: dictionary\n    \"\"\"\n    summaryxref_df = pd.DataFrame()\n    summary_desc = {}\n\n    all_cols = set(map_df.columns.to_list() + exposure_df.columns.to_list() + list(calculated_summary_cols.keys()))\n    if isinstance(accounts_df, pd.DataFrame):\n        all_cols.update(accounts_df.columns.to_list())\n\n    # Extract the summary id index column depending on id_set_index\n    map_df.sort_values(id_set_index, inplace=True, kind='stable')\n    ids_set_df = map_df.loc[:, [id_set_index]].rename(columns={'output_id': \"output\"})\n\n    # For each granularity build a set grouping\n    for summary_set in summaries_info_dict:\n        summary_set_df = ids_set_df\n        cols_group_by = get_column_selection(summary_set)\n        file_extension = 'csv'\n        if summary_set.get('ord_output', {}).get('parquet_format'):\n            file_extension = 'parquet'\n        desc_key = '{}_S{}_summary-info.{}'.format(\n            summaries_type, summary_set['id'], file_extension\n        )\n\n        # an empty intersection means no selected columns from the input data\n        if not set(cols_group_by).intersection(all_cols):\n\n            # is the intersection empty because the columns don't exist?\n            if set(cols_group_by).difference(all_cols):\n                err_msg = 'Input error: Summary set columns missing from the input files: {}'.format(\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreate a Dataframe for either gul / il / ri  based on a section\nfrom the analysis settings\n\n\n:param map_df: Summary Map dataframe (GUL / IL)\n:type map_df:  pandas.DataFrame\n\n:param exposure_df: Location OED data\n:type exposure_df:  pandas.DataFrame\n\n:param accounts_df: Accounts OED data\n:type accounts_df:  pandas.DataFrame\n\n:param summaries_info_dict: list of dictionary definitionfor a summary group from the analysis_settings file\n:type summaries_info_dict:  list\n\n[{\n    \"summarycalc\": true,\n    \"eltcalc\": true,\n    \"aalcalc\": true,\n    \"pltcalc\": true,\n    \"id\": 1,\n    \"oed_fields\": [],\n    \"lec_output\": true,\n    \"leccalc\": {\n      \"return_period_file\": true,\n      \"outputs\": {\n        \"full_uncertainty_aep\": true,\n        \"full_uncertainty_oep\": true,\n        \"wheatsheaf_aep\": true,\n        \"wheatsheaf_oep\": true\n      }\n    }\n  },\n\n  ...\n ]\n\n:param summaries_type: Text label to use as key in summary description either ['gul', 'il', 'ri']\n:type summaries_type: String\n\n:return summaryxref_df: Dataframe containing abstracted summary data for ktools\n:rtypwrite_xref_filee: pandas.DataFrame\n\n:return summary_desc: dictionary of dataFrames listing what summary_ids map to\n:rtype: dictionary"
    },
    {
      "chunk_id": "oasislmf/preparation/summaries.py::generate_summaryxref_files@580",
      "source_type": "code",
      "path": "oasislmf/preparation/summaries.py",
      "symbol_type": "function",
      "name": "generate_summaryxref_files",
      "lineno": 580,
      "end_lineno": 762,
      "business_stage": "other",
      "docstring": "Top level function for creating the summaryxref files from the manager.py\n\n:param model_run_fp: Model run directory file path\n:type model_run_fp:  str\n\n:param analysis_settings: Model analysis settings file\n:type analysis_settings:  dict\n\n:param il: Boolean to indicate the insured loss level mode - false if the\n           source accounts file path not provided to Oasis files gen.\n:type il: bool\n\n:param ri: Boolean to indicate the RI loss level mode - false if the\n           source accounts file path not provided to Oasis files gen.\n:type ri: bool\n\n:param rl: Boolean to indicate the RL loss level mode - false if the\n           source accounts file path not provided to Oasis files gen.\n:type rl: bool\n\n:param gul_items: Boolean to gul to use item_id instead of coverage_id\n:type gul_items: bool\n\n:param fmpy: Boolean to indicate whether fmpy python version will be used\n:type fmpy: bool",
      "content": "# File: oasislmf/preparation/summaries.py\n# function: generate_summaryxref_files (lines 580-762)\n\ndef generate_summaryxref_files(\n    location_df, account_df, model_run_fp, analysis_settings, il=False,\n    ri=False, rl=False, gul_item_stream=False, fmpy=False\n):\n    \"\"\"\n    Top level function for creating the summaryxref files from the manager.py\n\n    :param model_run_fp: Model run directory file path\n    :type model_run_fp:  str\n\n    :param analysis_settings: Model analysis settings file\n    :type analysis_settings:  dict\n\n    :param il: Boolean to indicate the insured loss level mode - false if the\n               source accounts file path not provided to Oasis files gen.\n    :type il: bool\n\n    :param ri: Boolean to indicate the RI loss level mode - false if the\n               source accounts file path not provided to Oasis files gen.\n    :type ri: bool\n\n    :param rl: Boolean to indicate the RL loss level mode - false if the\n               source accounts file path not provided to Oasis files gen.\n    :type rl: bool\n\n    :param gul_items: Boolean to gul to use item_id instead of coverage_id\n    :type gul_items: bool\n\n    :param fmpy: Boolean to indicate whether fmpy python version will be used\n    :type fmpy: bool\n    \"\"\"\n\n    # Boolean checks for summary generation types (gul / il / ri)\n    gul_summaries = all([\n        analysis_settings.get('gul_output'),\n        analysis_settings.get('gul_summaries'),\n    ])\n    il_summaries = all([\n        analysis_settings.get('il_output'),\n        analysis_settings.get('il_summaries'),\n        il,\n    ])\n    ri_summaries = all([\n        analysis_settings.get('ri_output'),\n        analysis_settings.get('ri_summaries'),\n        ri,\n    ])\n    rl_summaries = all([\n        analysis_settings.get('rl_output'),\n        analysis_settings.get('rl_summaries'),\n        rl,\n    ])\n\n    # Verify account file + il_map file\n    il_map_fp = os.path.join(model_run_fp, 'input', SUMMARY_MAPPING['fm_map_fn'])\n    if il_summaries or ri_summaries or rl_summaries:\n        if account_df is None:\n            raise OasisException('No account file found.')\n\n        if not os.path.exists(il_map_fp):\n            raise OasisException('No summary map file found.')\n\n    # Load il_map if present\n    if os.path.exists(il_map_fp):\n        il_map_df = get_dataframe(\n            src_fp=il_map_fp,\n            lowercase_cols=False,\n            col_dtypes=MAP_SUMMARY_DTYPES,\n            empty_data_error_msg='No summary map file found.',\n        )\n        il_map_df = il_map_df[list(set(il_map_df).intersection(MAP_SUMMARY_DTYPES))]\n\n        if gul_summaries:\n            gul_map_df = il_map_df\n            gul_map_df['item_id'] = gul_map_df['agg_id']\n\n    elif gul_summaries:\n        gul_map_fp = os.path.join(model_run_fp, 'input', SUMMARY_MAPPING['gul_map_fn'])\n        gul_map_df = get_dataframe(\n            src_fp=gul_map_fp,\n\n\"\"\"Docstring (excerpt)\"\"\"\nTop level function for creating the summaryxref files from the manager.py\n\n:param model_run_fp: Model run directory file path\n:type model_run_fp:  str\n\n:param analysis_settings: Model analysis settings file\n:type analysis_settings:  dict\n\n:param il: Boolean to indicate the insured loss level mode - false if the\n           source accounts file path not provided to Oasis files gen.\n:type il: bool\n\n:param ri: Boolean to indicate the RI loss level mode - false if the\n           source accounts file path not provided to Oasis files gen.\n:type ri: bool\n\n:param rl: Boolean to indicate the RL loss level mode - false if the\n           source accounts file path not provided to Oasis files gen.\n:type rl: bool\n\n:param gul_items: Boolean to gul to use item_id instead of coverage_id\n:type gul_items: bool\n\n:param fmpy: Boolean to indicate whether fmpy python version will be used\n:type fmpy: bool"
    },
    {
      "chunk_id": "oasislmf/preparation/summaries.py::get_exposure_summary_field@765",
      "source_type": "code",
      "path": "oasislmf/preparation/summaries.py",
      "symbol_type": "function",
      "name": "get_exposure_summary_field",
      "lineno": 765,
      "end_lineno": 813,
      "business_stage": "other",
      "docstring": "Populate exposure_summary dictionary with the values below grouped by field and status\n    - tiv\n    - number_of_locations\n    - number_of_buildings\n    - number_of_risks\n\n:param df: dataframe from gul_inputs.get_gul_input_items(..)\n:type df: pandas.DataFrame\n\n:param exposure_summary: input exposure_summary dictionary\n:type exposure_summary: dict\n\n:param field_name: Name of OED field to add to exposure_summary\n:type field_name: str\n\n:param field_value: OED field vlaue to add to exposure_summary\n:type field_value: str\n\n:param status: status returned by lookup ('all', 'success', 'fail' or 'nomatch')\n:type status: str\n\n:return: populated exposure_summary dictionary\n:rtype: dict",
      "content": "# File: oasislmf/preparation/summaries.py\n# function: get_exposure_summary_field (lines 765-813)\n\ndef get_exposure_summary_field(df, exposure_summary, field_name, field_value, status):\n    \"\"\"\n    Populate exposure_summary dictionary with the values below grouped by field and status\n        - tiv\n        - number_of_locations\n        - number_of_buildings\n        - number_of_risks\n\n    :param df: dataframe from gul_inputs.get_gul_input_items(..)\n    :type df: pandas.DataFrame\n\n    :param exposure_summary: input exposure_summary dictionary\n    :type exposure_summary: dict\n\n    :param field_name: Name of OED field to add to exposure_summary\n    :type field_name: str\n\n    :param field_value: OED field vlaue to add to exposure_summary\n    :type field_value: str\n\n    :param status: status returned by lookup ('all', 'success', 'fail' or 'nomatch')\n    :type status: str\n\n    :return: populated exposure_summary dictionary\n    :rtype: dict\n    \"\"\"\n    dedupe_cols_tiv = ['loc_id', 'peril_id']\n    useful_cols = ['tiv', 'loc_id', 'peril_id', 'coverage_type_id',\n                   'number_of_buildings', 'number_of_risks']\n    df_field = df.loc[df[field_name] == field_value, useful_cols]\n\n    for coverage_type in SUPPORTED_COVERAGE_TYPES:\n        df_cov = df_field.loc[df_field['coverage_type_id'] == SUPPORTED_COVERAGE_TYPES[coverage_type]['id']]\n        df_cov = df_cov.drop_duplicates(subset=dedupe_cols_tiv)\n        tiv_sum = float(df_cov['tiv'].sum())\n        exposure_summary[field_name][field_value][status]['tiv_by_coverage'][coverage_type] = tiv_sum\n        exposure_summary[field_name][field_value][status]['tiv'] += tiv_sum\n\n        df_num = df_cov.drop_duplicates(subset='loc_id')\n        exposure_summary[field_name][field_value][status]['number_of_locations_by_coverage'][coverage_type] = len(df_num)\n        exposure_summary[field_name][field_value][status]['number_of_buildings_by_coverage'][coverage_type] = int(df_num['number_of_buildings'].sum())\n        exposure_summary[field_name][field_value][status]['number_of_risks_by_coverage'][coverage_type] = int(df_num['number_of_risks'].sum())\n\n    num_df = df_field.drop_duplicates(subset='loc_id')\n    exposure_summary[field_name][field_value][status]['number_of_locations'] = len(num_df['loc_id'])\n    exposure_summary[field_name][field_value][status]['number_of_buildings'] = int(num_df['number_of_buildings'].sum())\n    exposure_summary[field_name][field_value][status]['number_of_risks'] = int(num_df['number_of_risks'].sum())\n\n    return exposure_summary\n\n\"\"\"Docstring (excerpt)\"\"\"\nPopulate exposure_summary dictionary with the values below grouped by field and status\n    - tiv\n    - number_of_locations\n    - number_of_buildings\n    - number_of_risks\n\n:param df: dataframe from gul_inputs.get_gul_input_items(..)\n:type df: pandas.DataFrame\n\n:param exposure_summary: input exposure_summary dictionary\n:type exposure_summary: dict\n\n:param field_name: Name of OED field to add to exposure_summary\n:type field_name: str\n\n:param field_value: OED field vlaue to add to exposure_summary\n:type field_value: str\n\n:param status: status returned by lookup ('all', 'success', 'fail' or 'nomatch')\n:type status: str\n\n:return: populated exposure_summary dictionary\n:rtype: dict"
    },
    {
      "chunk_id": "oasislmf/preparation/summaries.py::get_exposure_totals@817",
      "source_type": "code",
      "path": "oasislmf/preparation/summaries.py",
      "symbol_type": "function",
      "name": "get_exposure_totals",
      "lineno": 817,
      "end_lineno": 868,
      "business_stage": "other",
      "docstring": "Return dictionary with total TIVs and number of locations\n\n:param df: dataframe `df_summary_peril` from `get_exposure_summary`\n:type df: pandas.DataFrame\n\n:return: totals section for exposure_summary dictionary\n:rtype: dict",
      "content": "# File: oasislmf/preparation/summaries.py\n# function: get_exposure_totals (lines 817-868)\n\ndef get_exposure_totals(df):\n    \"\"\"\n    Return dictionary with total TIVs and number of locations\n\n    :param df: dataframe `df_summary_peril` from `get_exposure_summary`\n    :type df: pandas.DataFrame\n\n    :return: totals section for exposure_summary dictionary\n    :rtype: dict\n    \"\"\"\n\n    dedupe_cols = ['loc_id', 'coverage_type_id']\n\n    within_scope_tiv = df[df.status.isin(OASIS_KEYS_STATUS_MODELLED)].drop_duplicates(subset=dedupe_cols)['tiv'].sum()\n    within_scope_num = len(df[df.status.isin(OASIS_KEYS_STATUS_MODELLED)]['loc_id'].unique())\n\n    within_scope_num_buildings = int(df[df.status.isin(OASIS_KEYS_STATUS_MODELLED)].drop_duplicates(subset='loc_id')['number_of_buildings'].sum())\n\n    within_scope_num_risks = int(df[df.status.isin(OASIS_KEYS_STATUS_MODELLED)].drop_duplicates(subset='loc_id')['number_of_risks'].sum())\n\n    outside_scope_tiv = df[~df.status.isin(OASIS_KEYS_STATUS_MODELLED)].drop_duplicates(subset=dedupe_cols)['tiv'].sum()\n    outside_scope_num = len(df[~df.status.isin(OASIS_KEYS_STATUS_MODELLED)]['loc_id'].unique())\n\n    outside_scope_num_buildings = int(df[~df.status.isin(OASIS_KEYS_STATUS_MODELLED)].drop_duplicates(subset='loc_id')['number_of_buildings'].sum())\n\n    outside_scope_num_risks = int(df[~df.status.isin(OASIS_KEYS_STATUS_MODELLED)].drop_duplicates(subset='loc_id')['number_of_risks'].sum())\n\n    portfolio_tiv = df.drop_duplicates(subset=dedupe_cols)['tiv'].sum()\n    portfolio_num = len(df['loc_id'].unique())\n    portfolio_num_buildings = int(df.drop_duplicates(subset='loc_id')['number_of_buildings'].sum())\n    portfolio_num_risks = int(df.drop_duplicates(subset='loc_id')['number_of_risks'].sum())\n\n    return {\n        \"modelled\": {\n            \"tiv\": within_scope_tiv,\n            \"number_of_locations\": within_scope_num,\n            \"number_of_buildings\": within_scope_num_buildings,\n            \"number_of_risks\": within_scope_num_risks,\n        },\n        \"not-modelled\": {\n            \"tiv\": outside_scope_tiv,\n            \"number_of_locations\": outside_scope_num,\n            \"number_of_buildings\": outside_scope_num_buildings,\n            \"number_of_risks\": outside_scope_num_risks,\n        },\n        \"portfolio\": {\n            \"tiv\": portfolio_tiv,\n            \"number_of_locations\": portfolio_num,\n            \"number_of_buildings\": portfolio_num_buildings,\n            \"number_of_risks\": portfolio_num_risks,\n        }\n    }\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturn dictionary with total TIVs and number of locations\n\n:param df: dataframe `df_summary_peril` from `get_exposure_summary`\n:type df: pandas.DataFrame\n\n:return: totals section for exposure_summary dictionary\n:rtype: dict"
    },
    {
      "chunk_id": "oasislmf/preparation/summaries.py::convert_col_name@871",
      "source_type": "code",
      "path": "oasislmf/preparation/summaries.py",
      "symbol_type": "function",
      "name": "convert_col_name",
      "lineno": 871,
      "end_lineno": 887,
      "business_stage": "other",
      "docstring": "Convert a column from OED format to exposure summary report format. For\nexample `CountryCode` will be converted to `country_code`.\n\n:param col_name: original OED column name\n:type col_name: str\n\n:return: exposure summary report field name\n:rtype: str",
      "content": "# File: oasislmf/preparation/summaries.py\n# function: convert_col_name (lines 871-887)\n\ndef convert_col_name(col_name):\n    \"\"\"\n    Convert a column from OED format to exposure summary report format. For\n    example `CountryCode` will be converted to `country_code`.\n\n    :param col_name: original OED column name\n    :type col_name: str\n\n    :return: exposure summary report field name\n    :rtype: str\n    \"\"\"\n    col_list = [col_name[0].lower()]\n    for i, c in enumerate(col_name[1:]):\n        if c.isupper() and not col_name[i].isupper():\n            col_list += ['_']\n        col_list += c.lower()\n    return ''.join(col_list)\n\n\"\"\"Docstring (excerpt)\"\"\"\nConvert a column from OED format to exposure summary report format. For\nexample `CountryCode` will be converted to `country_code`.\n\n:param col_name: original OED column name\n:type col_name: str\n\n:return: exposure summary report field name\n:rtype: str"
    },
    {
      "chunk_id": "oasislmf/preparation/summaries.py::get_exposure_summary@891",
      "source_type": "code",
      "path": "oasislmf/preparation/summaries.py",
      "symbol_type": "function",
      "name": "get_exposure_summary",
      "lineno": 891,
      "end_lineno": 1013,
      "business_stage": "other",
      "docstring": "Create exposure summary as dictionary of TIVs and number of locations\ngrouped by peril and validity respectively. returns a python dict().\n\n:param exposure_df: source exposure dataframe\n:type exposure df: pandas.DataFrame\n\n:param keys_df: dataFrame holding keys data (success and errors)\n:type keys_errors_df: pandas.DataFrame\n\n:param exposure_profile: profile defining exposure file\n:type exposure_profile: dict\n\n:return: Exposure summary dictionary\n:rtype: dict",
      "content": "# File: oasislmf/preparation/summaries.py\n# function: get_exposure_summary (lines 891-1013)\n\ndef get_exposure_summary(\n        exposure_df,\n        keys_df,\n        exposure_profile=get_default_exposure_profile(),\n        additional_fields=[]\n):\n    \"\"\"\n    Create exposure summary as dictionary of TIVs and number of locations\n    grouped by peril and validity respectively. returns a python dict().\n\n    :param exposure_df: source exposure dataframe\n    :type exposure df: pandas.DataFrame\n\n    :param keys_df: dataFrame holding keys data (success and errors)\n    :type keys_errors_df: pandas.DataFrame\n\n    :param exposure_profile: profile defining exposure file\n    :type exposure_profile: dict\n\n    :return: Exposure summary dictionary\n    :rtype: dict\n    \"\"\"\n    # get location tivs by coveragetype\n    df_summary = []\n\n    exposure_fields = ['loc_id']\n    exposure_col_names = ['loc_id']\n\n    for field_name in additional_fields:\n        if field_name in exposure_df.columns:\n            exposure_fields += [field_name]\n            exposure_col_names += [convert_col_name(field_name)]\n        else:\n            logger.warn(f'exposure summary field not found: {field_name}')\n\n    for field_name in exposure_profile:\n        if 'FMTermType' in exposure_profile[field_name].keys():\n            if exposure_profile[field_name]['FMTermType'] == 'TIV' and exposure_profile[field_name]['ProfileElementName'] in exposure_df.columns:\n                cov_name = exposure_profile[field_name]['ProfileElementName']\n                coverage_type_id = exposure_profile[field_name]['CoverageTypeID']\n\n                fields = exposure_fields + [cov_name]\n                column_names = exposure_col_names + ['tiv']\n\n                tmp_df = exposure_df[fields].copy()\n                tmp_df.columns = column_names\n                tmp_df['coverage_type_id'] = coverage_type_id\n\n                # Add number_of_buildings column\n                if 'NumberOfBuildings' in exposure_df:\n                    tmp_df['number_of_buildings'] = exposure_df['NumberOfBuildings']\n                else:\n                    tmp_df['number_of_buildings'] = 1\n\n                # Add number_of_risks column\n                if 'IsAggregate' in exposure_df:\n                    tmp_df['number_of_risks'] = tmp_df['number_of_buildings']\n                    tmp_df.loc[exposure_df['IsAggregate'] == 0, 'number_of_risks'] = 1\n                else:\n                    tmp_df['number_of_risks'] = 1\n                df_summary.append(tmp_df)\n    df_summary = pd.concat(df_summary)\n\n    # fix 0 number_of_buildings and risks\n    df_summary = df_summary.replace({'number_of_buildings': 0, 'number_of_risks': 0}, 1)\n\n    # get all perils\n    peril_list = keys_df['peril_id'].drop_duplicates().to_list()\n\n    # Initialise sub categories\n    oed_categories = {'peril_id': peril_list}\n    for field_name in exposure_col_names[1:]:  # ignore 'loc_id'\n        field_list = df_summary[field_name].drop_duplicates().to_list()\n        oed_categories[field_name] = field_list\n\n    df_summary_peril = []\n    for peril_id in peril_list:\n        tmp_df = df_summary.copy()\n        tmp_df['peril_id'] = peril_id\n        df_summary_peril.append(tmp_df)\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreate exposure summary as dictionary of TIVs and number of locations\ngrouped by peril and validity respectively. returns a python dict().\n\n:param exposure_df: source exposure dataframe\n:type exposure df: pandas.DataFrame\n\n:param keys_df: dataFrame holding keys data (success and errors)\n:type keys_errors_df: pandas.DataFrame\n\n:param exposure_profile: profile defining exposure file\n:type exposure_profile: dict\n\n:return: Exposure summary dictionary\n:rtype: dict"
    },
    {
      "chunk_id": "oasislmf/preparation/summaries.py::write_gul_errors_map@1017",
      "source_type": "code",
      "path": "oasislmf/preparation/summaries.py",
      "symbol_type": "function",
      "name": "write_gul_errors_map",
      "lineno": 1017,
      "end_lineno": 1065,
      "business_stage": "other",
      "docstring": "Create csv file to help map keys errors back to original exposures.\n\n:param target_dir: directory on disk to write csv file\n:type target_dir: str\n\n:param exposure_df: source exposure dataframe\n:type exposure df: pandas.DataFrame\n\n:param keys_errors_df: keys errors dataframe\n:type keys_errors_df: pandas.DataFrame\n\n:param exposure_profile: profile defining exposure file\n:type exposure_profile: dict",
      "content": "# File: oasislmf/preparation/summaries.py\n# function: write_gul_errors_map (lines 1017-1065)\n\ndef write_gul_errors_map(\n        target_dir,\n        exposure_df,\n        keys_errors_df,\n        exposure_profile,\n):\n    \"\"\"\n    Create csv file to help map keys errors back to original exposures.\n\n    :param target_dir: directory on disk to write csv file\n    :type target_dir: str\n\n    :param exposure_df: source exposure dataframe\n    :type exposure df: pandas.DataFrame\n\n    :param keys_errors_df: keys errors dataframe\n    :type keys_errors_df: pandas.DataFrame\n\n    :param exposure_profile: profile defining exposure file\n    :type exposure_profile: dict\n    \"\"\"\n\n    cols = ['loc_id', 'PortNumber', 'AccNumber', 'LocNumber', 'peril_id', 'coverage_type_id', 'tiv', 'status', 'message']\n    gul_error_map_fp = os.path.join(target_dir, 'gul_errors_map.csv')\n\n    exposure_id_cols = ['loc_id', 'PortNumber', 'AccNumber', 'LocNumber']\n    keys_error_cols = ['loc_id', 'peril_id', 'coverage_type_id', 'status', 'message']\n    cov_level_id = SUPPORTED_FM_LEVELS['site coverage']['id']\n    tiv_maps = {term_info['CoverageTypeID']: term_info['ProfileElementName'] for term_info in exposure_profile.values()\n                if (term_info.get('FMTermType') == 'TIV'\n                    and term_info.get('FMLevel') == cov_level_id\n                    and term_info['ProfileElementName'] in exposure_df.columns)}\n\n    exposure_cols = list(set(exposure_id_cols + list(tiv_maps.values())).intersection(exposure_df.columns))\n\n    keys_errors_df.columns = keys_error_cols\n\n    gul_inputs_errors_df = exposure_df[exposure_cols].merge(keys_errors_df[keys_error_cols], on=['loc_id'])\n    gul_inputs_errors_df['tiv'] = 0.0\n    for cov_type in tiv_maps:\n        tiv_field = tiv_maps[cov_type]\n        gul_inputs_errors_df['tiv'] = np.where(\n            gul_inputs_errors_df['coverage_type_id'] == cov_type,\n            gul_inputs_errors_df[tiv_field],\n            gul_inputs_errors_df['tiv']\n        )\n    gul_inputs_errors_df['tiv'] = gul_inputs_errors_df['tiv'].fillna(0.0)\n\n    gul_inputs_errors_df[list(set(cols).intersection(gul_inputs_errors_df.columns))].to_csv(gul_error_map_fp, index=False)\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreate csv file to help map keys errors back to original exposures.\n\n:param target_dir: directory on disk to write csv file\n:type target_dir: str\n\n:param exposure_df: source exposure dataframe\n:type exposure df: pandas.DataFrame\n\n:param keys_errors_df: keys errors dataframe\n:type keys_errors_df: pandas.DataFrame\n\n:param exposure_profile: profile defining exposure file\n:type exposure_profile: dict"
    },
    {
      "chunk_id": "oasislmf/preparation/summaries.py::write_exposure_summary@1069",
      "source_type": "code",
      "path": "oasislmf/preparation/summaries.py",
      "symbol_type": "function",
      "name": "write_exposure_summary",
      "lineno": 1069,
      "end_lineno": 1127,
      "business_stage": "other",
      "docstring": "Create exposure summary as dictionary of TIVs and number of locations\ngrouped by peril and validity respectively. Writes dictionary as json file\nto disk.\n\n:param target_dir: directory on disk to write exposure summary file\n:type target_dir: str\n\n:param exposure_df: source exposure dataframe\n:type exposure df: pandas.DataFrame\n\n:param keys_fp: file path to keys file\n:type keys_fp: str\n\n:param keys_errors_fp: file path to keys errors file\n:type keys_errors_fp: str\n\n:param exposure_profile: profile defining exposure file\n:type exposure_profile: dict\n\n:param additional_fields: list of additional OED fields to add to exposure summary file\n:type additional_fields: list[str]\n\n:return: Exposure summary file path\n:rtype: str",
      "content": "# File: oasislmf/preparation/summaries.py\n# function: write_exposure_summary (lines 1069-1127)\n\ndef write_exposure_summary(\n        target_dir,\n        exposure_df,\n        keys_fp,\n        keys_errors_fp,\n        exposure_profile,\n        additional_fields=[]\n):\n    \"\"\"\n    Create exposure summary as dictionary of TIVs and number of locations\n    grouped by peril and validity respectively. Writes dictionary as json file\n    to disk.\n\n    :param target_dir: directory on disk to write exposure summary file\n    :type target_dir: str\n\n    :param exposure_df: source exposure dataframe\n    :type exposure df: pandas.DataFrame\n\n    :param keys_fp: file path to keys file\n    :type keys_fp: str\n\n    :param keys_errors_fp: file path to keys errors file\n    :type keys_errors_fp: str\n\n    :param exposure_profile: profile defining exposure file\n    :type exposure_profile: dict\n\n    :param additional_fields: list of additional OED fields to add to exposure summary file\n    :type additional_fields: list[str]\n\n    :return: Exposure summary file path\n    :rtype: str\n    \"\"\"\n    keys_success_df = keys_errors_df = None\n\n    # get keys success\n    if keys_fp:\n        keys_success_df = pd.read_csv(keys_fp)[['LocID', 'PerilID', 'CoverageTypeID']]\n        keys_success_df['status'] = OASIS_KEYS_STATUS['success']['id']\n        keys_success_df.columns = ['loc_id', 'peril_id', 'coverage_type_id', 'status']\n\n    # get keys errors\n    if keys_errors_fp:\n        keys_errors_df = pd.read_csv(keys_errors_fp)[['LocID', 'PerilID', 'CoverageTypeID', 'Status', 'Message']]\n        keys_errors_df.columns = ['loc_id', 'peril_id', 'coverage_type_id', 'status', 'message']\n        if not keys_errors_df.empty:\n            write_gul_errors_map(target_dir, exposure_df, keys_errors_df, exposure_profile)\n\n    # concatinate keys responses & run\n    df_keys = pd.concat([keys_success_df, keys_errors_df])\n    exposure_summary = get_exposure_summary(exposure_df, df_keys, exposure_profile, additional_fields)\n\n    # write exposure summary as json fileV\n    fp = os.path.join(target_dir, 'exposure_summary_report.json')\n    with io.open(fp, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(exposure_summary, ensure_ascii=False, indent=4))\n\n    return fp\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreate exposure summary as dictionary of TIVs and number of locations\ngrouped by peril and validity respectively. Writes dictionary as json file\nto disk.\n\n:param target_dir: directory on disk to write exposure summary file\n:type target_dir: str\n\n:param exposure_df: source exposure dataframe\n:type exposure df: pandas.DataFrame\n\n:param keys_fp: file path to keys file\n:type keys_fp: str\n\n:param keys_errors_fp: file path to keys errors file\n:type keys_errors_fp: str\n\n:param exposure_profile: profile defining exposure file\n:type exposure_profile: dict\n\n:param additional_fields: list of additional OED fields to add to exposure summary file\n:type additional_fields: list[str]\n\n:return: Exposure summary file path\n:rtype: str"
    },
    {
      "chunk_id": "oasislmf/pytools/indexconvert.py::change_footprint_apid@7",
      "source_type": "code",
      "path": "oasislmf/pytools/indexconvert.py",
      "symbol_type": "function",
      "name": "change_footprint_apid",
      "lineno": 7,
      "end_lineno": 9,
      "business_stage": "other",
      "docstring": "Reads CSV, applies Z-index conversion, and writes back.",
      "content": "# File: oasislmf/pytools/indexconvert.py\n# function: change_footprint_apid (lines 7-9)\n\ndef change_footprint_apid(path, size_lat):\n    \"\"\"Reads CSV, applies Z-index conversion, and writes back.\"\"\"\n    change_footprint_apid_multi_peril(path, size_lat, None, 1)\n\n\"\"\"Docstring (excerpt)\"\"\"\nReads CSV, applies Z-index conversion, and writes back."
    },
    {
      "chunk_id": "oasislmf/pytools/indexconvert.py::change_footprint_apid_multi_peril@12",
      "source_type": "code",
      "path": "oasislmf/pytools/indexconvert.py",
      "symbol_type": "function",
      "name": "change_footprint_apid_multi_peril",
      "lineno": 12,
      "end_lineno": 42,
      "business_stage": "other",
      "docstring": "Reads CSV, applies Z-index conversion for multiple perils and writes\nback to file",
      "content": "# File: oasislmf/pytools/indexconvert.py\n# function: change_footprint_apid_multi_peril (lines 12-42)\n\ndef change_footprint_apid_multi_peril(path, size_lat, size_lon, num_perils):\n    \"\"\"\n    Reads CSV, applies Z-index conversion for multiple perils and writes\n    back to file\n    \"\"\"\n    df = pd.read_csv(path)\n\n    areaperil_ids = df['areaperil_id'].values - 1\n\n    mask = df['areaperil_id'] != OASIS_UNKNOWN_ID\n    filtered_df = df[mask].copy()\n\n    areaperil_ids = filtered_df['areaperil_id'].values - 1\n\n    if size_lon is not None:\n        grid_size = size_lat * size_lon\n        peril_values = areaperil_ids // grid_size\n        index_values = areaperil_ids % grid_size + 1\n    else:\n        peril_values = areaperil_ids // 1\n        index_values = areaperil_ids + 1\n\n    z_indices = np.array(\n        [normal_to_z_index(id, size_lat) for id in index_values]\n    )\n\n    filtered_df['areaperil_id'] = z_indices * num_perils + peril_values\n    df.loc[mask, 'areaperil_id'] = filtered_df['areaperil_id']\n\n    df = df.sort_values(by=df.columns.tolist(), kind='stable')\n    df.to_csv(path, index=False)\n\n\"\"\"Docstring (excerpt)\"\"\"\nReads CSV, applies Z-index conversion for multiple perils and writes\nback to file"
    },
    {
      "chunk_id": "oasislmf/pytools/modelpy.py::main@23",
      "source_type": "code",
      "path": "oasislmf/pytools/modelpy.py",
      "symbol_type": "function",
      "name": "main",
      "lineno": 23,
      "end_lineno": 39,
      "business_stage": "other",
      "docstring": "Is the entry point for the modelpy command which loads data and constructs a model.\n\nReturns: None",
      "content": "# File: oasislmf/pytools/modelpy.py\n# function: main (lines 23-39)\n\ndef main() -> None:\n    \"\"\"\n    Is the entry point for the modelpy command which loads data and constructs a model.\n\n    Returns: None\n    \"\"\"\n    kwargs = vars(parser.parse_args())\n\n    # add handler to fm logger\n    ch = logging.StreamHandler()\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n    logging_level = kwargs.pop('logging_level')\n    logger.setLevel(logging_level)\n\n    manager.run(**kwargs)\n\n\"\"\"Docstring (excerpt)\"\"\"\nIs the entry point for the modelpy command which loads data and constructs a model.\n\nReturns: None"
    },
    {
      "chunk_id": "oasislmf/pytools/utils.py::redirect_logging@37",
      "source_type": "code",
      "path": "oasislmf/pytools/utils.py",
      "symbol_type": "function",
      "name": "redirect_logging",
      "lineno": 37,
      "end_lineno": 115,
      "business_stage": "other",
      "docstring": "Decorator that redirects logging output to a file.\n\nApply to the main run function of a python exec from the pytools directory.\nOnly errors will be send to STDERR, all other logging is stored in a file named:\n   \"<log_dir>/<exec_name>_<PID>.log\"\n\nEach log file is timestamped with start / finish times\n     cat log/fmpy_112820.log\n    2023-03-01 13:48:31,286 - oasislmf - INFO - starting process\n    2023-03-01 13:48:36,476 - oasislmf - INFO - finishing process\n\nArgs:\n    exec_name (str): The name of the script or function being executed. This will be used as part of the log file name.\n    log_dir (str, optional): The path to the directory where log files will be stored. Defaults to './log'.\n    log_level (int or str, optional): The logging level to use. Can be an integer or a string. Defaults to logging.INFO.\n\nReturns:\n    function: The decorated function.\n\nExample:\n    @redirect_logging(exec_name='my_script', log_dir='./logs', log_level=logging.DEBUG)\n    def my_run_function():\n        # code here",
      "content": "# File: oasislmf/pytools/utils.py\n# function: redirect_logging (lines 37-115)\n\ndef redirect_logging(exec_name, log_dir='./log', log_level=logging.WARNING):\n    \"\"\"\n    Decorator that redirects logging output to a file.\n\n    Apply to the main run function of a python exec from the pytools directory.\n    Only errors will be send to STDERR, all other logging is stored in a file named:\n       \"<log_dir>/<exec_name>_<PID>.log\"\n\n    Each log file is timestamped with start / finish times\n         cat log/fmpy_112820.log\n        2023-03-01 13:48:31,286 - oasislmf - INFO - starting process\n        2023-03-01 13:48:36,476 - oasislmf - INFO - finishing process\n\n    Args:\n        exec_name (str): The name of the script or function being executed. This will be used as part of the log file name.\n        log_dir (str, optional): The path to the directory where log files will be stored. Defaults to './log'.\n        log_level (int or str, optional): The logging level to use. Can be an integer or a string. Defaults to logging.INFO.\n\n    Returns:\n        function: The decorated function.\n\n    Example:\n        @redirect_logging(exec_name='my_script', log_dir='./logs', log_level=logging.DEBUG)\n        def my_run_function():\n            # code here\n\n    \"\"\"\n    def inner(func):\n        def wrapper(*args, **kwargs):\n            if not os.path.isdir(log_dir):\n                os.makedirs(log_dir)\n            logging_config = logging.root.manager.loggerDict.keys()\n            logging.captureWarnings(True)\n            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n            log_file = f'{exec_name}_{os.getpid()}_{uuid.uuid4()}.log'\n\n            childFileHandler = logging.FileHandler(os.path.join(log_dir, log_file))\n            childFileHandler.setLevel(log_level)\n            childFileHandler.setFormatter(formatter)\n\n            rootFileHandler = logging.FileHandler(os.path.join(log_dir, log_file))\n            rootFileHandler.setLevel(logging.INFO)\n            rootFileHandler.setFormatter(formatter)\n\n            # Set all logger handlers to level ERROR\n            for lg_name in logging_config:\n                logging_set_handlers(lg_name, childFileHandler, log_level)\n\n            # Set root oasislmf logger to INFO\n            logger = logging.getLogger('oasislmf')\n            logger.setLevel(logging.INFO)\n            logger.addHandler(rootFileHandler)\n\n            # Set warning log handler\n            warn_logger = logging.getLogger('py.warnings')\n            warn_logger.addHandler(rootFileHandler)\n\n            # # Debug: print logging tree\n            # import ipdb; ipdb.set_trace()\n            # import logging_tree; logging_tree.printout()\n            try:\n                logger.info(kwargs)\n                logger.info('starting process')\n\n                # Run the wrapped function\n                retval = func(*args, **kwargs)\n                logger.info('finishing process')\n                return retval\n            except Exception as err:\n                logger.exception(err)\n                raise err\n            finally:\n                for lg_name in logging_config:\n                    logging_reset_handlers(lg_name)\n                logger.removeHandler(rootFileHandler)\n                logging.shutdown()\n                logging.captureWarnings(False)\n        return wrapper\n    return inner\n\n\"\"\"Docstring (excerpt)\"\"\"\nDecorator that redirects logging output to a file.\n\nApply to the main run function of a python exec from the pytools directory.\nOnly errors will be send to STDERR, all other logging is stored in a file named:\n   \"<log_dir>/<exec_name>_<PID>.log\"\n\nEach log file is timestamped with start / finish times\n     cat log/fmpy_112820.log\n    2023-03-01 13:48:31,286 - oasislmf - INFO - starting process\n    2023-03-01 13:48:36,476 - oasislmf - INFO - finishing process\n\nArgs:\n    exec_name (str): The name of the script or function being executed. This will be used as part of the log file name.\n    log_dir (str, optional): The path to the directory where log files will be stored. Defaults to './log'.\n    log_level (int or str, optional): The logging level to use. Can be an integer or a string. Defaults to logging.INFO.\n\nReturns:\n    function: The decorated function.\n\nExample:\n    @redirect_logging(exec_name='my_script', log_dir='./logs', log_level=logging.DEBUG)\n    def my_run_function():\n        # code here"
    },
    {
      "chunk_id": "oasislmf/pytools/utils.py::assert_allclose@118",
      "source_type": "code",
      "path": "oasislmf/pytools/utils.py",
      "symbol_type": "function",
      "name": "assert_allclose",
      "lineno": 118,
      "end_lineno": 151,
      "business_stage": "other",
      "docstring": "Drop in replacement for `numpy.testing.assert_allclose` that also shows\nthe nonmatching elements in a nice human-readable format.\n\nArgs:\n    x (np.array or scalar): first input to compare.\n    y (np.array or scalar): second input to compare.\n    rtol (float, optional): relative tolreance. Defaults to 1e-10.\n    atol (float, optional): absolute tolerance. Defaults to 1e-8.\n    x_name (str, optional): header to print for x if x and y do not match. Defaults to \"x\".\n    y_name (str, optional): header to print for y if x and y do not match. Defaults to \"y\".\n\nRaises:\n    AssertionError: if x and y shapes do not match.\n    AssertionError: if x and y data do not match.",
      "content": "# File: oasislmf/pytools/utils.py\n# function: assert_allclose (lines 118-151)\n\ndef assert_allclose(x, y, rtol=1e-10, atol=1e-8, x_name=\"x\", y_name=\"y\"):\n    \"\"\"\n    Drop in replacement for `numpy.testing.assert_allclose` that also shows\n    the nonmatching elements in a nice human-readable format.\n\n    Args:\n        x (np.array or scalar): first input to compare.\n        y (np.array or scalar): second input to compare.\n        rtol (float, optional): relative tolreance. Defaults to 1e-10.\n        atol (float, optional): absolute tolerance. Defaults to 1e-8.\n        x_name (str, optional): header to print for x if x and y do not match. Defaults to \"x\".\n        y_name (str, optional): header to print for y if x and y do not match. Defaults to \"y\".\n\n    Raises:\n        AssertionError: if x and y shapes do not match.\n        AssertionError: if x and y data do not match.\n\n    \"\"\"\n    if np.isscalar(x) and np.isscalar(y) == 1:\n        return np.testing.assert_allclose(x, y, rtol=rtol, atol=atol)\n\n    if x.shape != y.shape:\n        raise AssertionError(\"Shape mismatch: %s vs %s\" % (str(x.shape), str(y.shape)))\n\n    d = ~np.isclose(x, y, rtol, atol)\n\n    if np.any(d):\n        miss = np.where(d)[0]\n        msg = f\"Mismatch of {len(miss):d} elements ({len(miss) / x.size * 100:g} %) at the level of rtol={rtol:g}, atol={atol:g},\\n\" \\\n            f\"{repr(miss)}\\n\" \\\n            f\"x: {x_name}\\n{str(x[d])}\\n\\n\" \\\n            f\"y: {y_name}\\n{str(y[d])}\"\\\n\n        raise AssertionError(msg)\n\n\"\"\"Docstring (excerpt)\"\"\"\nDrop in replacement for `numpy.testing.assert_allclose` that also shows\nthe nonmatching elements in a nice human-readable format.\n\nArgs:\n    x (np.array or scalar): first input to compare.\n    y (np.array or scalar): second input to compare.\n    rtol (float, optional): relative tolreance. Defaults to 1e-10.\n    atol (float, optional): absolute tolerance. Defaults to 1e-8.\n    x_name (str, optional): header to print for x if x and y do not match. Defaults to \"x\".\n    y_name (str, optional): header to print for y if x and y do not match. Defaults to \"y\".\n\nRaises:\n    AssertionError: if x and y shapes do not match.\n    AssertionError: if x and y data do not match."
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::process_bin_file@51",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "process_bin_file",
      "lineno": 51,
      "end_lineno": 110,
      "business_stage": "other",
      "docstring": "Reads summary<n>.bin file event_ids and summary_ids to populate summaries_data\nArgs:\n    fbin (np.memmap): summary binary memmap\n    offset (int): file offset to read from\n    occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file\n    summaries_data (ndarray[_SUMMARIES_DTYPE]): Index summary data (summaries.idx data)\n    summaries_idx (int): current index reached in summaries_data\n    file_index (int): Summary bin file index\nReturns:\n    summaries_idx (int): current index reached in summaries_data\n    resize_flag (bool): flag to indicate whether to resize summaries_data when full\n    offset (int): file offset to read from",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: process_bin_file (lines 51-110)\n\ndef process_bin_file(\n    fbin,\n    offset,\n    occ_map,\n    summaries_data,\n    summaries_idx,\n    file_index,\n):\n    \"\"\"Reads summary<n>.bin file event_ids and summary_ids to populate summaries_data\n    Args:\n        fbin (np.memmap): summary binary memmap\n        offset (int): file offset to read from\n        occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file\n        summaries_data (ndarray[_SUMMARIES_DTYPE]): Index summary data (summaries.idx data)\n        summaries_idx (int): current index reached in summaries_data\n        file_index (int): Summary bin file index\n    Returns:\n        summaries_idx (int): current index reached in summaries_data\n        resize_flag (bool): flag to indicate whether to resize summaries_data when full\n        offset (int): file offset to read from\n    \"\"\"\n    while offset < len(fbin):\n        cursor = offset\n        event_id, cursor = mv_read(fbin, cursor, oasis_int, oasis_int_size)\n        summary_id, cursor = mv_read(fbin, cursor, oasis_int, oasis_int_size)\n\n        if event_id not in occ_map:\n            # If the event_id doesn't exist in occ_map, continue with the next\n            offset = cursor\n            # Skip over Expval and losses\n            _, offset = mv_read(fbin, offset, oasis_float, oasis_float_size)\n            offset = skip_losses(fbin, offset)\n            continue\n\n        # Get the number of rows for the current event_id\n        n_rows = occ_map[event_id].shape[0]\n\n        if summaries_idx + n_rows >= len(summaries_data):\n            # Resize array if full\n            return summaries_idx, True, offset\n\n        # Now fill the summaries_data with the rows that match the current event_id\n        current_row = 0\n        for row in occ_map[event_id]:\n            summaries_data[summaries_idx][\"summary_id\"] = summary_id\n            summaries_data[summaries_idx][\"file_idx\"] = file_index\n            summaries_data[summaries_idx][\"period_no\"] = row[\"period_no\"]\n            summaries_data[summaries_idx][\"file_offset\"] = offset\n            summaries_idx += 1\n            current_row += 1\n            if current_row >= n_rows:\n                break\n\n        offset = cursor\n        # Read Expval\n        _, offset = mv_read(fbin, offset, oasis_float, oasis_float_size)\n        # Skip over losses\n        offset = skip_losses(fbin, offset)\n\n    return summaries_idx, False, offset\n\n\"\"\"Docstring (excerpt)\"\"\"\nReads summary<n>.bin file event_ids and summary_ids to populate summaries_data\nArgs:\n    fbin (np.memmap): summary binary memmap\n    offset (int): file offset to read from\n    occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file\n    summaries_data (ndarray[_SUMMARIES_DTYPE]): Index summary data (summaries.idx data)\n    summaries_idx (int): current index reached in summaries_data\n    file_index (int): Summary bin file index\nReturns:\n    summaries_idx (int): current index reached in summaries_data\n    resize_flag (bool): flag to indicate whether to resize summaries_data when full\n    offset (int): file offset to read from"
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::sort_and_save_chunk@113",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "sort_and_save_chunk",
      "lineno": 113,
      "end_lineno": 122,
      "business_stage": "other",
      "docstring": "Sort a chunk of summaries data and save it to a temporary file.\nArgs:\n    summaries_data (ndarray[_SUMMARIES_DTYPE]): Indexed summary data\n    temp_file_path (str | os.PathLike): Path to temporary file",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: sort_and_save_chunk (lines 113-122)\n\ndef sort_and_save_chunk(summaries_data, temp_file_path):\n    \"\"\"Sort a chunk of summaries data and save it to a temporary file.\n    Args:\n        summaries_data (ndarray[_SUMMARIES_DTYPE]): Indexed summary data\n        temp_file_path (str | os.PathLike): Path to temporary file\n    \"\"\"\n    sort_columns = [\"file_idx\", \"period_no\", \"summary_id\"]\n    sorted_indices = np.lexsort([summaries_data[col] for col in sort_columns])\n    sorted_chunk = summaries_data[sorted_indices]\n    sorted_chunk.tofile(temp_file_path)\n\n\"\"\"Docstring (excerpt)\"\"\"\nSort a chunk of summaries data and save it to a temporary file.\nArgs:\n    summaries_data (ndarray[_SUMMARIES_DTYPE]): Indexed summary data\n    temp_file_path (str | os.PathLike): Path to temporary file"
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::merge_sorted_chunks@126",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "merge_sorted_chunks",
      "lineno": 126,
      "end_lineno": 160,
      "business_stage": "other",
      "docstring": "Merge sorted chunks using a k-way merge algorithm and yield next smallest row\nArgs:\n    memmaps (List[np.memmap]): List of temporary file memmaps\nYields:\n    smallest_row (ndarray[_SUMMARIES_DTYPE]): yields the next smallest row from sorted summaries partial files",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: merge_sorted_chunks (lines 126-160)\n\ndef merge_sorted_chunks(memmaps):\n    \"\"\"\n    Merge sorted chunks using a k-way merge algorithm and yield next smallest row\n    Args:\n        memmaps (List[np.memmap]): List of temporary file memmaps\n    Yields:\n        smallest_row (ndarray[_SUMMARIES_DTYPE]): yields the next smallest row from sorted summaries partial files\n    \"\"\"\n    min_heap = init_heap(num_compare=3)\n    size = 0\n    # Initialize the min_heap with the first row of each memmap\n    for i, mmap in enumerate(memmaps):\n        if len(mmap) > 0:\n            first_row = mmap[0]\n            min_heap, size = heap_push(min_heap, size, np.array(\n                [first_row[\"summary_id\"], first_row[\"period_no\"], first_row[\"file_idx\"], i, 0],\n                dtype=np.int32\n            ))\n\n    # Perform the k-way merge\n    while size > 0:\n        # The min heap will store the smallest row at the top when popped\n        element, min_heap, size = heap_pop(min_heap, size)\n        file_idx = element[3]\n        row_num = element[4]\n        smallest_row = memmaps[file_idx][row_num]\n        yield smallest_row\n\n        # Push the next row from the same file into the heap if there are any more rows\n        if row_num + 1 < len(memmaps[file_idx]):\n            next_row = memmaps[file_idx][row_num + 1]\n            min_heap, size = heap_push(min_heap, size, np.array(\n                [next_row[\"summary_id\"], next_row[\"period_no\"], next_row[\"file_idx\"], file_idx, row_num + 1],\n                dtype=np.int32\n            ))\n\n\"\"\"Docstring (excerpt)\"\"\"\nMerge sorted chunks using a k-way merge algorithm and yield next smallest row\nArgs:\n    memmaps (List[np.memmap]): List of temporary file memmaps\nYields:\n    smallest_row (ndarray[_SUMMARIES_DTYPE]): yields the next smallest row from sorted summaries partial files"
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::get_summaries_data@163",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "get_summaries_data",
      "lineno": 163,
      "end_lineno": 226,
      "business_stage": "other",
      "docstring": "Gets the indexed summaries data, ordered with k-way merge if not enough memory\nArgs:\n    path (os.PathLike): Path to the workspace folder containing summary binaries\n    files_handles (List[np.memmap]): List of memmaps for summary files data\n    occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file\n    aal_max_memory (float): OASIS_AAL_MEMORY value (has to be passed in as numba won't update from environment variable)\nReturns:\n    memmaps (List[np.memmap]): List of temporary file memmaps\n    max_summary_id (int): Max summary ID",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: get_summaries_data (lines 163-226)\n\ndef get_summaries_data(\n    path,\n    files_handles,\n    occ_map,\n    aal_max_memory\n):\n    \"\"\"Gets the indexed summaries data, ordered with k-way merge if not enough memory\n    Args:\n        path (os.PathLike): Path to the workspace folder containing summary binaries\n        files_handles (List[np.memmap]): List of memmaps for summary files data\n        occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file\n        aal_max_memory (float): OASIS_AAL_MEMORY value (has to be passed in as numba won't update from environment variable)\n    Returns:\n        memmaps (List[np.memmap]): List of temporary file memmaps\n        max_summary_id (int): Max summary ID\n    \"\"\"\n    # Remove existing temp bdat files if exists\n    for temp_file in path.glob(\"indexed_summaries.part*.bdat\"):\n        os.remove(temp_file)\n\n    buffer_size = int(((aal_max_memory * (1024**3) // _SUMMARIES_DTYPE_size)))\n    temp_files = []\n    chunk_index = 0\n\n    summaries_data = np.empty(buffer_size, dtype=_SUMMARIES_DTYPE)\n    summaries_idx = 0\n    max_summary_id = 0\n    for file_index, fbin in enumerate(files_handles):\n        offset = oasis_int_size * 3  # Summary stream header size\n\n        while True:\n            summaries_idx, resize_flag, offset = process_bin_file(\n                fbin,\n                offset,\n                occ_map,\n                summaries_data,\n                summaries_idx,\n                file_index,\n            )\n\n            # Write new summaries partial file when buffer size or end of summary file reached\n            if resize_flag:\n                temp_file_path = Path(path, f\"indexed_summaries.part{chunk_index}.bdat\")\n                summaries_data = summaries_data[:summaries_idx]\n                sort_and_save_chunk(summaries_data, temp_file_path)\n                temp_files.append(temp_file_path)\n                chunk_index += 1\n                summaries_idx = 0\n                max_summary_id = max(max_summary_id, np.max(summaries_data[\"summary_id\"]))\n\n            # End of file, move to next file\n            if offset >= len(fbin):\n                break\n\n    # Write remaining summaries data to temporary file\n    temp_file_path = Path(path, f\"indexed_summaries.part{chunk_index}.bdat\")\n    summaries_data = summaries_data[:summaries_idx]\n    sort_and_save_chunk(summaries_data, temp_file_path)\n    max_summary_id = max(max_summary_id, np.max(summaries_data[\"summary_id\"]))\n    temp_files.append(temp_file_path)\n\n    memmaps = [np.memmap(temp_file, mode=\"r\", dtype=_SUMMARIES_DTYPE) for temp_file in temp_files]\n\n    return memmaps, max_summary_id\n\n\"\"\"Docstring (excerpt)\"\"\"\nGets the indexed summaries data, ordered with k-way merge if not enough memory\nArgs:\n    path (os.PathLike): Path to the workspace folder containing summary binaries\n    files_handles (List[np.memmap]): List of memmaps for summary files data\n    occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file\n    aal_max_memory (float): OASIS_AAL_MEMORY value (has to be passed in as numba won't update from environment variable)\nReturns:\n    memmaps (List[np.memmap]): List of temporary file memmaps\n    max_summary_id (int): Max summary ID"
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::summary_index@229",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "summary_index",
      "lineno": 229,
      "end_lineno": 259,
      "business_stage": "other",
      "docstring": "Index the summary binary outputs\nArgs:\n    path (os.PathLike): Path to the workspace folder containing summary binaries\n    occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file\n    stack (ExitStack): Exit stack\nReturns:\n    files_handles (List[np.memmap]): List of memmaps for summary files data\n    sample_size (int): Sample size\n    max_summary_id (int): Max summary ID\n    memmaps (List[np.memmap]): List of temporary file memmaps",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: summary_index (lines 229-259)\n\ndef summary_index(path, occ_map, stack):\n    \"\"\"Index the summary binary outputs\n    Args:\n        path (os.PathLike): Path to the workspace folder containing summary binaries\n        occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file\n        stack (ExitStack): Exit stack\n    Returns:\n        files_handles (List[np.memmap]): List of memmaps for summary files data\n        sample_size (int): Sample size\n        max_summary_id (int): Max summary ID\n        memmaps (List[np.memmap]): List of temporary file memmaps\n    \"\"\"\n    # work folder for aal files\n    aal_files_folder = Path(path, \"aal_files\")\n    aal_files_folder.mkdir(parents=False, exist_ok=True)\n\n    # Find summary binary files\n    files = [file for file in path.glob(\"*.bin\")]\n    files_handles = [np.memmap(file, mode=\"r\", dtype=\"u1\") for file in files]\n\n    streams_in, (stream_source_type, stream_agg_type, sample_size) = init_streams_in(files, stack)\n    if stream_source_type != SUMMARY_STREAM_ID:\n        raise RuntimeError(f\"Error: Not a summary stream type {stream_source_type}\")\n\n    memmaps, max_summary_id = get_summaries_data(\n        aal_files_folder,\n        files_handles,\n        occ_map,\n        OASIS_AAL_MEMORY\n    )\n    return files_handles, sample_size, max_summary_id, memmaps\n\n\"\"\"Docstring (excerpt)\"\"\"\nIndex the summary binary outputs\nArgs:\n    path (os.PathLike): Path to the workspace folder containing summary binaries\n    occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file\n    stack (ExitStack): Exit stack\nReturns:\n    files_handles (List[np.memmap]): List of memmaps for summary files data\n    sample_size (int): Sample size\n    max_summary_id (int): Max summary ID\n    memmaps (List[np.memmap]): List of temporary file memmaps"
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::read_input_files@262",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "read_input_files",
      "lineno": 262,
      "end_lineno": 279,
      "business_stage": "other",
      "docstring": "Reads all input files and returns a dict of relevant data\nArgs:\n    run_dir (str | os.PathLike): Path to directory containing required files structure\nReturns:\n    file_data (Dict[str, Any]): A dict of relevent data extracted from files",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: read_input_files (lines 262-279)\n\ndef read_input_files(run_dir):\n    \"\"\"Reads all input files and returns a dict of relevant data\n    Args:\n        run_dir (str | os.PathLike): Path to directory containing required files structure\n    Returns:\n        file_data (Dict[str, Any]): A dict of relevent data extracted from files\n    \"\"\"\n    occ_map, date_algorithm, granular_date, no_of_periods = read_occurrence(Path(run_dir, \"input\"))\n    period_weights = read_periods(no_of_periods, Path(run_dir, \"input\"))\n\n    file_data = {\n        \"occ_map\": occ_map,\n        \"date_algorithm\": date_algorithm,\n        \"granular_date\": granular_date,\n        \"no_of_periods\": no_of_periods,\n        \"period_weights\": period_weights,\n    }\n    return file_data\n\n\"\"\"Docstring (excerpt)\"\"\"\nReads all input files and returns a dict of relevant data\nArgs:\n    run_dir (str | os.PathLike): Path to directory containing required files structure\nReturns:\n    file_data (Dict[str, Any]): A dict of relevent data extracted from files"
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::get_num_subsets@282",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "get_num_subsets",
      "lineno": 282,
      "end_lineno": 314,
      "business_stage": "other",
      "docstring": "Gets the number of subsets required to generates the Sample AAL np map for subset sizes up to sample_size\nExample: sample_size[10], max_summary_id[2] generates following ndarray\n[\n    #   subset_size, mean,  mean_squared, mean_period\n    [0, 0, 0],  # subset_size = 1 , summary_id = 1\n    [0, 0, 0],  # subset_size = 1 , summary_id = 2\n    [0, 0, 0],  # subset_size = 2 , summary_id = 1\n    [0, 0, 0],  # subset_size = 2 , summary_id = 2\n    [0, 0, 0],  # subset_size = 4 , summary_id = 1\n    [0, 0, 0],  # subset_size = 4 , summary_id = 2\n    [0, 0, 0],  # subset_size = 10 , summary_id = 1, subset_size = sample_size\n    [0, 0, 0],  # subset_size = 10 , summary_id = 2, subset_size = sample_size\n]\nSubset_size is implicit based on position in array, grouped by max_summary_id\nSo first two arrays are subset_size 2^0 = 1\nThe next two arrays are subset_size 2^1 = 2\nThe next two arrays are subset_size 2^2 = 4\nThe last two arrays are subset_size = sample_size = 10\nDoesn't generate one with subset_size 8 as double that is larger than sample_size\nTherefore this function returns 4, and the sample aal array is 4 * 2\nArgs:\n    alct (bool): Boolean for ALCT output\n    sample_size (int): Sample size\n    max_summary_id (int): Max summary ID\nReturns:\n    num_subsets (int): Number of subsets",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: get_num_subsets (lines 282-314)\n\ndef get_num_subsets(alct, sample_size, max_summary_id):\n    \"\"\"Gets the number of subsets required to generates the Sample AAL np map for subset sizes up to sample_size\n    Example: sample_size[10], max_summary_id[2] generates following ndarray\n    [\n        #   subset_size, mean,  mean_squared, mean_period\n        [0, 0, 0],  # subset_size = 1 , summary_id = 1\n        [0, 0, 0],  # subset_size = 1 , summary_id = 2\n        [0, 0, 0],  # subset_size = 2 , summary_id = 1\n        [0, 0, 0],  # subset_size = 2 , summary_id = 2\n        [0, 0, 0],  # subset_size = 4 , summary_id = 1\n        [0, 0, 0],  # subset_size = 4 , summary_id = 2\n        [0, 0, 0],  # subset_size = 10 , summary_id = 1, subset_size = sample_size\n        [0, 0, 0],  # subset_size = 10 , summary_id = 2, subset_size = sample_size\n    ]\n    Subset_size is implicit based on position in array, grouped by max_summary_id\n    So first two arrays are subset_size 2^0 = 1\n    The next two arrays are subset_size 2^1 = 2\n    The next two arrays are subset_size 2^2 = 4\n    The last two arrays are subset_size = sample_size = 10\n    Doesn't generate one with subset_size 8 as double that is larger than sample_size\n    Therefore this function returns 4, and the sample aal array is 4 * 2\n    Args:\n        alct (bool): Boolean for ALCT output\n        sample_size (int): Sample size\n        max_summary_id (int): Max summary ID\n    Returns:\n        num_subsets (int): Number of subsets\n    \"\"\"\n    i = 0\n    if alct and sample_size > 1:\n        while ((1 << i) + ((1 << i) - 1)) <= sample_size:\n            i += 1\n    return i + 1\n\n\"\"\"Docstring (excerpt)\"\"\"\nGets the number of subsets required to generates the Sample AAL np map for subset sizes up to sample_size\nExample: sample_size[10], max_summary_id[2] generates following ndarray\n[\n    #   subset_size, mean,  mean_squared, mean_period\n    [0, 0, 0],  # subset_size = 1 , summary_id = 1\n    [0, 0, 0],  # subset_size = 1 , summary_id = 2\n    [0, 0, 0],  # subset_size = 2 , summary_id = 1\n    [0, 0, 0],  # subset_size = 2 , summary_id = 2\n    [0, 0, 0],  # subset_size = 4 , summary_id = 1\n    [0, 0, 0],  # subset_size = 4 , summary_id = 2\n    [0, 0, 0],  # subset_size = 10 , summary_id = 1, subset_size = sample_size\n    [0, 0, 0],  # subset_size = 10 , summary_id = 2, subset_size = sample_size\n]\nSubset_size is implicit based on position in array, grouped by max_summary_id\nSo first two arrays are subset_size 2^0 = 1\nThe next two arrays are subset_size 2^1 = 2\nThe next two arrays are subset_size 2^2 = 4\nThe last two arrays are subset_size = sample_size = 10\nDoesn't generate one with subset_size 8 as double that is larger than sample_size\nTherefore this function returns 4, and the sample aal array is 4 * 2\nArgs:\n    alct (bool): Boolean for ALCT output\n    sample_size (int): Sample size\n    max_summary_id (int): Max summary ID\nReturns:\n    num_subsets (int): Number of subsets"
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::get_weighted_means@318",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "get_weighted_means",
      "lineno": 318,
      "end_lineno": 341,
      "business_stage": "other",
      "docstring": "Get sum of weighted mean and weighted mean_squared\nArgs:\n    vec_sample_sum_loss (ndarray[_AAL_REC_DTYPE]): Vector for sample sum losses\n    weighting (float): Weighting value\n    sidx (int): start index\n    end_sidx (int): end index\nReturns:\n    weighted_mean (float): Sum weighted mean\n    weighted_mean_squared (float): Sum weighted mean squared",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: get_weighted_means (lines 318-341)\n\ndef get_weighted_means(\n    vec_sample_sum_loss,\n    weighting,\n    sidx,\n    end_sidx,\n):\n    \"\"\"Get sum of weighted mean and weighted mean_squared\n    Args:\n        vec_sample_sum_loss (ndarray[_AAL_REC_DTYPE]): Vector for sample sum losses\n        weighting (float): Weighting value\n        sidx (int): start index\n        end_sidx (int): end index\n    Returns:\n        weighted_mean (float): Sum weighted mean\n        weighted_mean_squared (float): Sum weighted mean squared\n    \"\"\"\n    weighted_mean = 0\n    weighted_mean_squared = 0\n    while sidx < end_sidx:\n        sumloss = vec_sample_sum_loss[sidx]\n        weighted_mean += sumloss * weighting\n        weighted_mean_squared += sumloss * sumloss * weighting\n        sidx += 1\n    return weighted_mean, weighted_mean_squared\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet sum of weighted mean and weighted mean_squared\nArgs:\n    vec_sample_sum_loss (ndarray[_AAL_REC_DTYPE]): Vector for sample sum losses\n    weighting (float): Weighting value\n    sidx (int): start index\n    end_sidx (int): end index\nReturns:\n    weighted_mean (float): Sum weighted mean\n    weighted_mean_squared (float): Sum weighted mean squared"
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::do_calc_end@345",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "do_calc_end",
      "lineno": 345,
      "end_lineno": 436,
      "business_stage": "other",
      "docstring": "Updates Analytical and Sample AAL vectors from sample sum losses\nArgs:\n    period_no (int): Period Number\n    no_of_periods (int): Number of periods\n    period_weights (ndarray[periods_dtype]): Period Weights\n    sample_size (int): Sample Size\n    curr_summary_id (int): Current summary_id\n    max_summary_id (int): Max summary_id\n    vec_analytical_aal (ndarray[_AAL_REC_DTYPE]): Vector for Analytical AAL\n    vecs_sample_aal (ndarray[_AAL_REC_PERIODS_DTYPE]): Vector for Sample AAL\n    vec_used_summary_id (ndarray[bool]): vector to store if summary_id is used\n    vec_sample_sum_loss (ndarray[_AAL_REC_DTYPE]): Vector for sample sum losses",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: do_calc_end (lines 345-436)\n\ndef do_calc_end(\n    period_no,\n    no_of_periods,\n    period_weights,\n    sample_size,\n    curr_summary_id,\n    max_summary_id,\n    vec_analytical_aal,\n    vecs_sample_aal,\n    vec_used_summary_id,\n    vec_sample_sum_loss,\n):\n    \"\"\"Updates Analytical and Sample AAL vectors from sample sum losses\n    Args:\n        period_no (int): Period Number\n        no_of_periods (int): Number of periods\n        period_weights (ndarray[periods_dtype]): Period Weights\n        sample_size (int): Sample Size\n        curr_summary_id (int): Current summary_id\n        max_summary_id (int): Max summary_id\n        vec_analytical_aal (ndarray[_AAL_REC_DTYPE]): Vector for Analytical AAL\n        vecs_sample_aal (ndarray[_AAL_REC_PERIODS_DTYPE]): Vector for Sample AAL\n        vec_used_summary_id (ndarray[bool]): vector to store if summary_id is used\n        vec_sample_sum_loss (ndarray[_AAL_REC_DTYPE]): Vector for sample sum losses\n    \"\"\"\n    # Get weighting\n    weighting = 1\n    if no_of_periods > 0:\n        # period_no in period_weights\n        if period_no > 0 and period_no <= no_of_periods:\n            weighting = period_weights[period_no - 1][1] * no_of_periods\n        else:\n            weighting = 0\n\n    # Update Analytical AAL\n    mean = vec_sample_sum_loss[0]  # 0 index is where the analytical mean is stored\n    vec_used_summary_id[curr_summary_id - 1] = True\n\n    vec_analytical_aal[curr_summary_id - 1][\"mean\"] += mean * weighting\n    vec_analytical_aal[curr_summary_id - 1][\"mean_squared\"] += mean * mean * weighting\n\n    # Update Sample AAL\n    # Get relevant indexes for curr_summary_id\n    len_sample_aal = len(vecs_sample_aal)\n    num_subsets = len_sample_aal // max_summary_id\n    idxs = [i * max_summary_id + (curr_summary_id - 1) for i in range(num_subsets)]\n\n    # Get sample aal idx for sample_size\n    last_sample_aal = vecs_sample_aal[idxs[-1]]\n\n    total_mean_by_period = 0\n    sidx = 1\n    aal_idx = 0\n    while sidx < sample_size + 1:\n        # Iterate through aal_idx except the last one which is subset_size == sample_size\n        while aal_idx < num_subsets - 1:\n            curr_sample_aal = vecs_sample_aal[idxs[aal_idx]]\n\n            # Calculate the subset_size and assign to sidx\n            sidx = 1 << aal_idx\n            end_sidx = sidx << 1\n\n            # Traverse sidx == subset_size to sidx == subset_size * 2\n            weighted_mean, weighted_mean_squared = get_weighted_means(\n                vec_sample_sum_loss,\n                weighting,\n                sidx,\n                end_sidx\n            )\n\n            # Update sample size Sample AAL\n            last_sample_aal[\"mean\"] += weighted_mean\n            last_sample_aal[\"mean_squared\"] += weighted_mean_squared\n            total_mean_by_period += weighted_mean\n\n            # Update current Sample AAL\n            curr_sample_aal[\"mean\"] += weighted_mean\n            curr_sample_aal[\"mean_squared\"] += weighted_mean_squared\n            # Update current Sample AAL mean_period\n            curr_sample_aal[\"mean_period\"] += weighted_mean * weighted_mean\n\n\"\"\"Docstring (excerpt)\"\"\"\nUpdates Analytical and Sample AAL vectors from sample sum losses\nArgs:\n    period_no (int): Period Number\n    no_of_periods (int): Number of periods\n    period_weights (ndarray[periods_dtype]): Period Weights\n    sample_size (int): Sample Size\n    curr_summary_id (int): Current summary_id\n    max_summary_id (int): Max summary_id\n    vec_analytical_aal (ndarray[_AAL_REC_DTYPE]): Vector for Analytical AAL\n    vecs_sample_aal (ndarray[_AAL_REC_PERIODS_DTYPE]): Vector for Sample AAL\n    vec_used_summary_id (ndarray[bool]): vector to store if summary_id is used\n    vec_sample_sum_loss (ndarray[_AAL_REC_DTYPE]): Vector for sample sum losses"
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::read_losses@440",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "read_losses",
      "lineno": 440,
      "end_lineno": 464,
      "business_stage": "other",
      "docstring": "Read losses from summary_fin starting at cursor, populate vec_sample_sum_loss\nArgs:\n    summary_fin (np.memmap): summary file memmap\n    cursor (int): data offset for reading binary files\n    (ndarray[_AAL_REC_DTYPE]): Vector for sample sum losses\nReturns:\n    cursor (int): data offset for reading binary files",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: read_losses (lines 440-464)\n\ndef read_losses(summary_fin, cursor, vec_sample_sum_loss):\n    \"\"\"Read losses from summary_fin starting at cursor, populate vec_sample_sum_loss\n    Args:\n        summary_fin (np.memmap): summary file memmap\n        cursor (int): data offset for reading binary files\n        (ndarray[_AAL_REC_DTYPE]): Vector for sample sum losses\n    Returns:\n        cursor (int): data offset for reading binary files\n    \"\"\"\n    # Max losses is sample_size + num special sidxs\n    valid_buff = len(summary_fin)\n    while True:\n        if valid_buff - cursor < oasis_int_size + oasis_float_size:\n            raise RuntimeError(\"Error: broken summary file, not enough data\")\n        sidx, cursor = mv_read(summary_fin, cursor, oasis_int, oasis_int_size)\n        loss, cursor = mv_read(summary_fin, cursor, oasis_float, oasis_float_size)\n\n        if sidx == 0:\n            break\n        if sidx == NUMBER_OF_AFFECTED_RISK_IDX or sidx == MAX_LOSS_IDX:\n            continue\n        if sidx == MEAN_IDX:\n            sidx = 0\n        vec_sample_sum_loss[sidx] += loss\n    return cursor\n\n\"\"\"Docstring (excerpt)\"\"\"\nRead losses from summary_fin starting at cursor, populate vec_sample_sum_loss\nArgs:\n    summary_fin (np.memmap): summary file memmap\n    cursor (int): data offset for reading binary files\n    (ndarray[_AAL_REC_DTYPE]): Vector for sample sum losses\nReturns:\n    cursor (int): data offset for reading binary files"
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::skip_losses@468",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "skip_losses",
      "lineno": 468,
      "end_lineno": 483,
      "business_stage": "other",
      "docstring": "Skip through losses in summary_fin starting at cursor\nArgs:\n    summary_fin (np.memmap): summary file memmap\n    cursor (int): data offset for reading binary files\nReturns:\n    cursor (int): data offset for reading binary files",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: skip_losses (lines 468-483)\n\ndef skip_losses(summary_fin, cursor):\n    \"\"\"Skip through losses in summary_fin starting at cursor\n    Args:\n        summary_fin (np.memmap): summary file memmap\n        cursor (int): data offset for reading binary files\n    Returns:\n        cursor (int): data offset for reading binary files\n    \"\"\"\n    valid_buff = len(summary_fin)\n    sidx = 1\n    while sidx:\n        if valid_buff - cursor < oasis_int_size + oasis_float_size:\n            raise RuntimeError(\"Error: broken summary file, not enough data\")\n        sidx, cursor = mv_read(summary_fin, cursor, oasis_int, oasis_int_size)\n        cursor += oasis_float_size\n    return cursor\n\n\"\"\"Docstring (excerpt)\"\"\"\nSkip through losses in summary_fin starting at cursor\nArgs:\n    summary_fin (np.memmap): summary file memmap\n    cursor (int): data offset for reading binary files\nReturns:\n    cursor (int): data offset for reading binary files"
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::run_aal@487",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "run_aal",
      "lineno": 487,
      "end_lineno": 574,
      "business_stage": "other",
      "docstring": "Run AAL calculation loop to populate vec data\nArgs:\n    memmaps (List[np.memmap]): List of temporary file memmaps\n    no_of_periods (int): Number of periods\n    period_weights (ndarray[periods_dtype]): Period Weights\n    sample_size (int): Sample Size\n    max_summary_id (int): Max summary_id\n    files_handles (List[np.memmap]): List of memmaps for summary files data\n    vec_analytical_aal (ndarray[_AAL_REC_DTYPE]): Vector for Analytical AAL\n    vecs_sample_aal (ndarray[_AAL_REC_PERIODS_DTYPE]): Vector for Sample AAL\n    vec_used_summary_id (ndarray[bool]): vector to store if summary_id is used",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: run_aal (lines 487-574)\n\ndef run_aal(\n    memmaps,\n    no_of_periods,\n    period_weights,\n    sample_size,\n    max_summary_id,\n    files_handles,\n    vec_analytical_aal,\n    vecs_sample_aal,\n    vec_used_summary_id,\n):\n    \"\"\"Run AAL calculation loop to populate vec data\n    Args:\n        memmaps (List[np.memmap]): List of temporary file memmaps\n        no_of_periods (int): Number of periods\n        period_weights (ndarray[periods_dtype]): Period Weights\n        sample_size (int): Sample Size\n        max_summary_id (int): Max summary_id\n        files_handles (List[np.memmap]): List of memmaps for summary files data\n        vec_analytical_aal (ndarray[_AAL_REC_DTYPE]): Vector for Analytical AAL\n        vecs_sample_aal (ndarray[_AAL_REC_PERIODS_DTYPE]): Vector for Sample AAL\n        vec_used_summary_id (ndarray[bool]): vector to store if summary_id is used\n    \"\"\"\n    if len(memmaps) == 0:\n        raise ValueError(\"File is empty or missing data\")\n\n    # Index 0 is mean\n    vec_sample_sum_loss = np.zeros(sample_size + 1, dtype=np.float64)\n    last_summary_id = -1\n    last_period_no = -1\n\n    for line in merge_sorted_chunks(memmaps):\n        summary_id = line[\"summary_id\"]\n        file_idx = line[\"file_idx\"]\n        period_no = line[\"period_no\"]\n        file_offset = line[\"file_offset\"]\n\n        if last_summary_id != summary_id:\n            if last_summary_id != -1:\n                do_calc_end(\n                    last_period_no,\n                    no_of_periods,\n                    period_weights,\n                    sample_size,\n                    last_summary_id,\n                    max_summary_id,\n                    vec_analytical_aal,\n                    vecs_sample_aal,\n                    vec_used_summary_id,\n                    vec_sample_sum_loss,\n                )\n            last_period_no = period_no\n            last_summary_id = summary_id\n        elif last_period_no != period_no:\n            if last_period_no != -1:\n                do_calc_end(\n                    last_period_no,\n                    no_of_periods,\n                    period_weights,\n                    sample_size,\n                    last_summary_id,\n                    max_summary_id,\n                    vec_analytical_aal,\n                    vecs_sample_aal,\n                    vec_used_summary_id,\n                    vec_sample_sum_loss,\n                )\n            last_period_no = period_no\n        summary_fin = files_handles[file_idx]\n\n        # Read summary header values (event_id, summary_id, expval)\n        cursor = file_offset + (2 * oasis_int_size) + oasis_float_size\n\n        read_losses(summary_fin, cursor, vec_sample_sum_loss)\n\n    if last_summary_id != -1:\n        do_calc_end(\n            last_period_no,\n            no_of_periods,\n            period_weights,\n\n\"\"\"Docstring (excerpt)\"\"\"\nRun AAL calculation loop to populate vec data\nArgs:\n    memmaps (List[np.memmap]): List of temporary file memmaps\n    no_of_periods (int): Number of periods\n    period_weights (ndarray[periods_dtype]): Period Weights\n    sample_size (int): Sample Size\n    max_summary_id (int): Max summary_id\n    files_handles (List[np.memmap]): List of memmaps for summary files data\n    vec_analytical_aal (ndarray[_AAL_REC_DTYPE]): Vector for Analytical AAL\n    vecs_sample_aal (ndarray[_AAL_REC_PERIODS_DTYPE]): Vector for Sample AAL\n    vec_used_summary_id (ndarray[bool]): vector to store if summary_id is used"
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::calculate_mean_stddev@578",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "calculate_mean_stddev",
      "lineno": 578,
      "end_lineno": 599,
      "business_stage": "other",
      "docstring": "Compute the mean and standard deviation from the sum and squared sum of an observable\nArgs:\n    observable_sum (ndarray[oasis_float]): Observable sum\n    observable_squared_sum (ndarray[oasis_float]): Observable squared sum\n    number_of_observations (int | ndarray[int]): number of observations\nReturns:\n    mean (ndarray[oasis_float]): Mean\n    std (ndarray[oasis_float]): Standard Deviation",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: calculate_mean_stddev (lines 578-599)\n\ndef calculate_mean_stddev(\n    observable_sum,\n    observable_squared_sum,\n    number_of_observations\n):\n    \"\"\"Compute the mean and standard deviation from the sum and squared sum of an observable\n    Args:\n        observable_sum (ndarray[oasis_float]): Observable sum\n        observable_squared_sum (ndarray[oasis_float]): Observable squared sum\n        number_of_observations (int | ndarray[int]): number of observations\n    Returns:\n        mean (ndarray[oasis_float]): Mean\n        std (ndarray[oasis_float]): Standard Deviation\n    \"\"\"\n    mean = observable_sum / number_of_observations\n    std = np.sqrt(\n        (\n            observable_squared_sum - (observable_sum * observable_sum)\n            / number_of_observations\n        ) / (number_of_observations - 1)\n    )\n    return mean, std\n\n\"\"\"Docstring (excerpt)\"\"\"\nCompute the mean and standard deviation from the sum and squared sum of an observable\nArgs:\n    observable_sum (ndarray[oasis_float]): Observable sum\n    observable_squared_sum (ndarray[oasis_float]): Observable squared sum\n    number_of_observations (int | ndarray[int]): number of observations\nReturns:\n    mean (ndarray[oasis_float]): Mean\n    std (ndarray[oasis_float]): Standard Deviation"
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::get_aal_data@603",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "get_aal_data",
      "lineno": 603,
      "end_lineno": 644,
      "business_stage": "other",
      "docstring": "Generate AAL csv data\nArgs:\n    vec_analytical_aal (ndarray[_AAL_REC_DTYPE]): Vector for Analytical AAL\n    vecs_sample_aal (ndarray[_AAL_REC_PERIODS_DTYPE]): Vector for Sample AAL\n    vec_used_summary_id (ndarray[bool]): vector to store if summary_id is used\n    sample_size (int): Sample Size\n    no_of_periods (int): Number of periods\nReturns:\n    aal_data (List[Tuple]): AAL csv data",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: get_aal_data (lines 603-644)\n\ndef get_aal_data(\n    vec_analytical_aal,\n    vecs_sample_aal,\n    vec_used_summary_id,\n    sample_size,\n    no_of_periods\n):\n    \"\"\"Generate AAL csv data\n    Args:\n        vec_analytical_aal (ndarray[_AAL_REC_DTYPE]): Vector for Analytical AAL\n        vecs_sample_aal (ndarray[_AAL_REC_PERIODS_DTYPE]): Vector for Sample AAL\n        vec_used_summary_id (ndarray[bool]): vector to store if summary_id is used\n        sample_size (int): Sample Size\n        no_of_periods (int): Number of periods\n    Returns:\n        aal_data (List[Tuple]): AAL csv data\n    \"\"\"\n    aal_data = []\n    assert len(vec_analytical_aal) == len(vecs_sample_aal), \\\n        f\"Lengths of analytical ({len(vec_analytical_aal)}) and sample ({len(vecs_sample_aal)}) aal data differ\"\n    mean_analytical, std_analytical = calculate_mean_stddev(\n        vec_analytical_aal[\"mean\"],\n        vec_analytical_aal[\"mean_squared\"],\n        no_of_periods,\n    )\n\n    mean_sample, std_sample = calculate_mean_stddev(\n        vecs_sample_aal[\"mean\"],\n        vecs_sample_aal[\"mean_squared\"],\n        no_of_periods * sample_size,\n    )\n\n    for summary_idx in range(len(vec_analytical_aal)):\n        if not vec_used_summary_id[summary_idx]:\n            continue\n        aal_data.append((summary_idx + 1, MEAN_TYPE_ANALYTICAL, mean_analytical[summary_idx], std_analytical[summary_idx]))\n    for summary_idx in range(len(vecs_sample_aal)):\n        if not vec_used_summary_id[summary_idx]:\n            continue\n        aal_data.append((summary_idx + 1, MEAN_TYPE_SAMPLE, mean_sample[summary_idx], std_sample[summary_idx]))\n\n    return aal_data\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate AAL csv data\nArgs:\n    vec_analytical_aal (ndarray[_AAL_REC_DTYPE]): Vector for Analytical AAL\n    vecs_sample_aal (ndarray[_AAL_REC_PERIODS_DTYPE]): Vector for Sample AAL\n    vec_used_summary_id (ndarray[bool]): vector to store if summary_id is used\n    sample_size (int): Sample Size\n    no_of_periods (int): Number of periods\nReturns:\n    aal_data (List[Tuple]): AAL csv data"
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::get_aal_data_meanonly@648",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "get_aal_data_meanonly",
      "lineno": 648,
      "end_lineno": 687,
      "business_stage": "other",
      "docstring": "Generate AAL csv data\nArgs:\n    vec_analytical_aal (ndarray[_AAL_REC_DTYPE]): Vector for Analytical AAL\n    vecs_sample_aal (ndarray[_AAL_REC_PERIODS_DTYPE]): Vector for Sample AAL\n    vec_used_summary_id (ndarray[bool]): vector to store if summary_id is used\n    sample_size (int): Sample Size\n    no_of_periods (int): Number of periods\nReturns:\n    aal_data (List[Tuple]): AAL csv data",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: get_aal_data_meanonly (lines 648-687)\n\ndef get_aal_data_meanonly(\n    vec_analytical_aal,\n    vecs_sample_aal,\n    vec_used_summary_id,\n    sample_size,\n    no_of_periods\n):\n    \"\"\"Generate AAL csv data\n    Args:\n        vec_analytical_aal (ndarray[_AAL_REC_DTYPE]): Vector for Analytical AAL\n        vecs_sample_aal (ndarray[_AAL_REC_PERIODS_DTYPE]): Vector for Sample AAL\n        vec_used_summary_id (ndarray[bool]): vector to store if summary_id is used\n        sample_size (int): Sample Size\n        no_of_periods (int): Number of periods\n    Returns:\n        aal_data (List[Tuple]): AAL csv data\n    \"\"\"\n    aal_data = []\n    assert len(vec_analytical_aal) == len(vecs_sample_aal), \\\n        f\"Lengths of analytical ({len(vec_analytical_aal)}) and sample ({len(vecs_sample_aal)}) aal data differ\"\n    mean_analytical, std_analytical = calculate_mean_stddev(\n        vec_analytical_aal[\"mean\"],\n        vec_analytical_aal[\"mean_squared\"],\n        no_of_periods,\n    )\n\n    mean_sample, std_sample = calculate_mean_stddev(\n        vecs_sample_aal[\"mean\"],\n        vecs_sample_aal[\"mean_squared\"],\n        no_of_periods * sample_size,\n    )\n\n    # aalmeanonlycalc orders output data differently, this if condition is here to match the output to the ktools output\n    for summary_idx in range(len(vec_analytical_aal)):\n        if not vec_used_summary_id[summary_idx]:\n            continue\n        aal_data.append((summary_idx + 1, MEAN_TYPE_ANALYTICAL, mean_analytical[summary_idx]))\n        aal_data.append((summary_idx + 1, MEAN_TYPE_SAMPLE, mean_sample[summary_idx]))\n\n    return aal_data\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate AAL csv data\nArgs:\n    vec_analytical_aal (ndarray[_AAL_REC_DTYPE]): Vector for Analytical AAL\n    vecs_sample_aal (ndarray[_AAL_REC_PERIODS_DTYPE]): Vector for Sample AAL\n    vec_used_summary_id (ndarray[bool]): vector to store if summary_id is used\n    sample_size (int): Sample Size\n    no_of_periods (int): Number of periods\nReturns:\n    aal_data (List[Tuple]): AAL csv data"
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::calculate_confidence_interval@691",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "calculate_confidence_interval",
      "lineno": 691,
      "end_lineno": 714,
      "business_stage": "other",
      "docstring": "Calculate the confidence interval based on standard error and confidence level.\nArgs:\n    std_err (float): The standard error.\n    confidence_level (float): The confidence level (e.g., 0.95 for 95%).\nReturns:\n    confidence interval (float): The confidence interval.",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: calculate_confidence_interval (lines 691-714)\n\ndef calculate_confidence_interval(std_err, confidence_level):\n    \"\"\"Calculate the confidence interval based on standard error and confidence level.\n    Args:\n        std_err (float): The standard error.\n        confidence_level (float): The confidence level (e.g., 0.95 for 95%).\n    Returns:\n        confidence interval (float): The confidence interval.\n    \"\"\"\n    # Compute p-value above 0.5\n    p_value = (1 + confidence_level) / 2\n    p_value = np.sqrt(-2 * np.log(1 - p_value))\n\n    # Approximation formula for z-value from Abramowitz & Stegun, Handbook\n    # of Mathematical Functions: with Formulas, Graphs, and Mathematical\n    # Tables, Dover Publications (1965), eq. 26.2.23\n    # Also see John D. Cook Consulting, https://www.johndcook.com/blog/cpp_phi_inverse/\n    c = np.array([2.515517, 0.802853, 0.010328])\n    d = np.array([1.432788, 0.189269, 0.001308])\n    z_value = p_value - (\n        ((c[2] * p_value + c[1]) * p_value + c[0]) /\n        (((d[2] * p_value + d[1]) * p_value + d[0]) * p_value + 1)\n    )\n    # Return the confidence interval\n    return std_err * z_value\n\n\"\"\"Docstring (excerpt)\"\"\"\nCalculate the confidence interval based on standard error and confidence level.\nArgs:\n    std_err (float): The standard error.\n    confidence_level (float): The confidence level (e.g., 0.95 for 95%).\nReturns:\n    confidence interval (float): The confidence interval."
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::get_alct_data@718",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "get_alct_data",
      "lineno": 718,
      "end_lineno": 785,
      "business_stage": "other",
      "docstring": "Generate ALCT csv data\nArgs:\n    vecs_sample_aal (ndarray[_AAL_REC_PERIODS_DTYPE]): Vector for Sample AAL\n    max_summary_id (int): Max summary_id\n    sample_size (int): Sample Size\n    no_of_periods (int): Number of periods\n    confidence (float): Confidence level between 0 and 1, default 0.95\nReturns:\n    alct_data (List[List]): ALCT csv data",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: get_alct_data (lines 718-785)\n\ndef get_alct_data(\n    vecs_sample_aal,\n    max_summary_id,\n    sample_size,\n    no_of_periods,\n    confidence,\n):\n    \"\"\"Generate ALCT csv data\n    Args:\n        vecs_sample_aal (ndarray[_AAL_REC_PERIODS_DTYPE]): Vector for Sample AAL\n        max_summary_id (int): Max summary_id\n        sample_size (int): Sample Size\n        no_of_periods (int): Number of periods\n        confidence (float): Confidence level between 0 and 1, default 0.95\n    Returns:\n        alct_data (List[List]): ALCT csv data\n    \"\"\"\n    alct_data = []\n\n    num_subsets = len(vecs_sample_aal) // max_summary_id\n    # Generate the subset sizes (last one is always sample_size)\n    subset_sizes = np.array([2 ** i for i in range(num_subsets)])\n    subset_sizes[-1] = sample_size\n\n    for summary_id in range(1, max_summary_id + 1):\n        # Get idxs for summary_id across all subset_sizes\n        idxs = np.array([i * max_summary_id + (summary_id - 1) for i in range(num_subsets)])\n        v_curr = vecs_sample_aal[idxs]\n\n        mean, std = calculate_mean_stddev(\n            v_curr[\"mean\"],\n            v_curr[\"mean_squared\"],\n            subset_sizes * no_of_periods,\n        )\n        mean_period = v_curr[\"mean_period\"] / (subset_sizes * subset_sizes)\n\n        var_vuln = (\n            (v_curr[\"mean_squared\"] - subset_sizes * mean_period)\n            / (subset_sizes * no_of_periods - subset_sizes)\n        ) / (subset_sizes * no_of_periods)\n        var_haz = (\n            subset_sizes * (mean_period - no_of_periods * mean * mean)\n            / (no_of_periods - 1)\n        ) / (subset_sizes * no_of_periods)\n\n        std_err = np.sqrt(var_vuln)\n        ci = calculate_confidence_interval(std_err, confidence)\n\n        std_err_haz = np.sqrt(var_haz)\n        std_err_vuln = np.sqrt(var_vuln)\n\n        lower_ci = np.where(ci > 0, mean - ci, 0)\n        upper_ci = np.where(ci > 0, mean + ci, 0)\n\n        curr_data = np.column_stack((\n            np.array([summary_id] * num_subsets),\n            mean,\n            std,\n            subset_sizes,\n            lower_ci,\n            upper_ci,\n            std_err, std_err / mean,\n            var_haz, std_err_haz, std_err_haz / mean,\n            var_vuln, std_err_vuln, std_err_vuln / mean,\n        ))\n        for row in curr_data:\n            alct_data.append(row)\n    return alct_data\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate ALCT csv data\nArgs:\n    vecs_sample_aal (ndarray[_AAL_REC_PERIODS_DTYPE]): Vector for Sample AAL\n    max_summary_id (int): Max summary_id\n    sample_size (int): Sample Size\n    no_of_periods (int): Number of periods\n    confidence (float): Confidence level between 0 and 1, default 0.95\nReturns:\n    alct_data (List[List]): ALCT csv data"
    },
    {
      "chunk_id": "oasislmf/pytools/aal/manager.py::run@788",
      "source_type": "code",
      "path": "oasislmf/pytools/aal/manager.py",
      "symbol_type": "function",
      "name": "run",
      "lineno": 788,
      "end_lineno": 938,
      "business_stage": "other",
      "docstring": "Runs AAL calculations\nArgs:\n    run_dir (str | os.PathLike): Path to directory containing required files structure\n    subfolder (str): Workspace subfolder inside <run_dir>/work/<subfolder>\n    aal_output_file (str, optional): Path to AAL output file. Defaults to None\n    alct_output_file (str, optional): Path to ALCT output file. Defaults to None\n    meanonly (bool): Boolean value to output AAL with mean only\n    noheader (bool): Boolean value to skip header in output file\n    confidence (float): Confidence level between 0 and 1, default 0.95\n    output_format (str): Output format extension. Defaults to \"csv\".",
      "content": "# File: oasislmf/pytools/aal/manager.py\n# function: run (lines 788-938)\n\ndef run(\n    run_dir,\n    subfolder,\n    aal_output_file=None,\n    alct_output_file=None,\n    meanonly=False,\n    noheader=False,\n    confidence=0.95,\n    output_format=\"csv\",\n):\n    \"\"\"Runs AAL calculations\n    Args:\n        run_dir (str | os.PathLike): Path to directory containing required files structure\n        subfolder (str): Workspace subfolder inside <run_dir>/work/<subfolder>\n        aal_output_file (str, optional): Path to AAL output file. Defaults to None\n        alct_output_file (str, optional): Path to ALCT output file. Defaults to None\n        meanonly (bool): Boolean value to output AAL with mean only\n        noheader (bool): Boolean value to skip header in output file\n        confidence (float): Confidence level between 0 and 1, default 0.95\n        output_format (str): Output format extension. Defaults to \"csv\".\n    \"\"\"\n    outmap = {\n        \"aal\": {\n            \"compute\": aal_output_file is not None,\n            \"file_path\": aal_output_file,\n            \"fmt\": AAL_fmt if not meanonly else AAL_meanonly_fmt,\n            \"headers\": AAL_headers if not meanonly else AAL_meanonly_headers,\n            \"file\": None,\n            \"dtype\": AAL_dtype if not meanonly else AAL_meanonly_dtype,\n        },\n        \"alct\": {\n            \"compute\": alct_output_file is not None,\n            \"file_path\": alct_output_file,\n            \"fmt\": ALCT_fmt,\n            \"headers\": ALCT_headers,\n            \"file\": None,\n            \"dtype\": ALCT_dtype,\n        },\n    }\n\n    output_format = \".\" + output_format\n    output_binary = output_format == \".bin\"\n    output_parquet = output_format == \".parquet\"\n    # Check for correct suffix\n    for path in [v[\"file_path\"] for v in outmap.values()]:\n        if path is None:\n            continue\n        if Path(path).suffix == \"\":  # Ignore suffix for pipes\n            continue\n        if (Path(path).suffix != output_format):\n            raise ValueError(f\"Invalid file extension for {output_format}, got {path},\")\n\n    if not all([v[\"compute\"] for v in outmap.values()]):\n        logger.warning(\"No output files specified\")\n\n    with ExitStack() as stack:\n        workspace_folder = Path(run_dir, \"work\", subfolder)\n        if not workspace_folder.is_dir():\n            raise RuntimeError(f\"Error: Unable to open directory {workspace_folder}\")\n\n        file_data = read_input_files(run_dir)\n        files_handles, sample_size, max_summary_id, memmaps = summary_index(\n            workspace_folder,\n            file_data[\"occ_map\"],\n            stack\n        )\n        # aal vec are Indexed on summary_id - 1\n        num_subsets = get_num_subsets(outmap[\"alct\"][\"compute\"], sample_size, max_summary_id)\n        vecs_sample_aal = np.zeros(num_subsets * max_summary_id, dtype=_AAL_REC_PERIOD_DTYPE)\n        vec_analytical_aal = np.zeros(max_summary_id, dtype=_AAL_REC_DTYPE)\n        vec_used_summary_id = np.zeros(max_summary_id, dtype=np.bool_)\n\n        # Run AAL calculations, populate above vecs\n        run_aal(\n            memmaps,\n            file_data[\"no_of_periods\"],\n            file_data[\"period_weights\"],\n            sample_size,\n            max_summary_id,\n            files_handles,\n\n\"\"\"Docstring (excerpt)\"\"\"\nRuns AAL calculations\nArgs:\n    run_dir (str | os.PathLike): Path to directory containing required files structure\n    subfolder (str): Workspace subfolder inside <run_dir>/work/<subfolder>\n    aal_output_file (str, optional): Path to AAL output file. Defaults to None\n    alct_output_file (str, optional): Path to ALCT output file. Defaults to None\n    meanonly (bool): Boolean value to output AAL with mean only\n    noheader (bool): Boolean value to skip header in output file\n    confidence (float): Confidence level between 0 and 1, default 0.95\n    output_format (str): Output format extension. Defaults to \"csv\"."
    },
    {
      "chunk_id": "oasislmf/pytools/common/data.py::generate_output_metadata@30",
      "source_type": "code",
      "path": "oasislmf/pytools/common/data.py",
      "symbol_type": "function",
      "name": "generate_output_metadata",
      "lineno": 30,
      "end_lineno": 42,
      "business_stage": "other",
      "docstring": "Generates *_header, *_dtype and *_fmt items given a list of tuples describing some output description\noutput description has type List(Tuple({name: str}, {type: Any}, {format: str}))\nArgs:\n    output_map (list(tuple(str, Any, str))): Dictionary mapping string name to  {output description}_output list\nReturns:\n    result (tuple(list[str], np.dtype, str)): Tuple containing the generated *_header list, *_dtype np.dtype, *_fmt csv format string",
      "content": "# File: oasislmf/pytools/common/data.py\n# function: generate_output_metadata (lines 30-42)\n\ndef generate_output_metadata(output):\n    \"\"\"Generates *_header, *_dtype and *_fmt items given a list of tuples describing some output description\n    output description has type List(Tuple({name: str}, {type: Any}, {format: str}))\n    Args:\n        output_map (list(tuple(str, Any, str))): Dictionary mapping string name to  {output description}_output list\n    Returns:\n        result (tuple(list[str], np.dtype, str)): Tuple containing the generated *_header list, *_dtype np.dtype, *_fmt csv format string\n    \"\"\"\n    headers = [c[0] for c in output]\n    dtype = np.dtype([(c[0], c[1]) for c in output])\n    fmt = ','.join([c[2] for c in output])\n    result = (headers, dtype, fmt)\n    return result\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerates *_header, *_dtype and *_fmt items given a list of tuples describing some output description\noutput description has type List(Tuple({name: str}, {type: Any}, {format: str}))\nArgs:\n    output_map (list(tuple(str, Any, str))): Dictionary mapping string name to  {output description}_output list\nReturns:\n    result (tuple(list[str], np.dtype, str)): Tuple containing the generated *_header list, *_dtype np.dtype, *_fmt csv format string"
    },
    {
      "chunk_id": "oasislmf/pytools/common/data.py::load_as_ndarray@274",
      "source_type": "code",
      "path": "oasislmf/pytools/common/data.py",
      "symbol_type": "function",
      "name": "load_as_ndarray",
      "lineno": 274,
      "end_lineno": 303,
      "business_stage": "other",
      "docstring": "load a file as a numpy ndarray\nuseful for multi-columns files\nArgs:\n    dir_path: path to the directory where the binary or csv file is stored\n    name: name of the file\n    _dtype: np.dtype\n    must_exist: raise FileNotFoundError if no file is present\n    col_map: name re-mapping to change name of csv columns\nReturns:\n    numpy ndarray",
      "content": "# File: oasislmf/pytools/common/data.py\n# function: load_as_ndarray (lines 274-303)\n\ndef load_as_ndarray(dir_path, name, _dtype, must_exist=True, col_map=None):\n    \"\"\"\n    load a file as a numpy ndarray\n    useful for multi-columns files\n    Args:\n        dir_path: path to the directory where the binary or csv file is stored\n        name: name of the file\n        _dtype: np.dtype\n        must_exist: raise FileNotFoundError if no file is present\n        col_map: name re-mapping to change name of csv columns\n    Returns:\n        numpy ndarray\n    \"\"\"\n\n    if os.path.isfile(os.path.join(dir_path, name + '.bin')):\n        return np.fromfile(os.path.join(dir_path, name + '.bin'), dtype=_dtype)\n    elif must_exist or os.path.isfile(os.path.join(dir_path, name + '.csv')):\n        # in csv column cam be out of order and have different name,\n        # we load with pandas and write each column to the ndarray\n        if col_map is None:\n            col_map = {}\n        with open(os.path.join(dir_path, name + '.csv')) as file_in:\n            cvs_dtype = {col_map.get(key, key): col_dtype for key, (col_dtype, _) in _dtype.fields.items()}\n            df = pd.read_csv(file_in, delimiter=',', dtype=cvs_dtype, usecols=list(cvs_dtype.keys()))\n            res = np.empty(df.shape[0], dtype=_dtype)\n            for name in _dtype.names:\n                res[name] = df[col_map.get(name, name)]\n            return res\n    else:\n        return np.empty(0, dtype=_dtype)\n\n\"\"\"Docstring (excerpt)\"\"\"\nload a file as a numpy ndarray\nuseful for multi-columns files\nArgs:\n    dir_path: path to the directory where the binary or csv file is stored\n    name: name of the file\n    _dtype: np.dtype\n    must_exist: raise FileNotFoundError if no file is present\n    col_map: name re-mapping to change name of csv columns\nReturns:\n    numpy ndarray"
    },
    {
      "chunk_id": "oasislmf/pytools/common/data.py::load_as_array@306",
      "source_type": "code",
      "path": "oasislmf/pytools/common/data.py",
      "symbol_type": "function",
      "name": "load_as_array",
      "lineno": 306,
      "end_lineno": 328,
      "business_stage": "other",
      "docstring": "load file as a single numpy array,\n useful for files with a binary version with only one type of value where their index correspond to an id.\n For example coverage.bin only contains tiv value for each coverage id\n coverage_id n correspond to index n-1\nArgs:\n    dir_path: path to the directory where the binary or csv file is stored\n    name: name of the file\n    _dtype: numpy dtype of the required array\n    must_exist: raise FileNotFoundError if no file is present\nReturns:\n    numpy array of dtype type",
      "content": "# File: oasislmf/pytools/common/data.py\n# function: load_as_array (lines 306-328)\n\ndef load_as_array(dir_path, name, _dtype, must_exist=True):\n    \"\"\"\n    load file as a single numpy array,\n     useful for files with a binary version with only one type of value where their index correspond to an id.\n     For example coverage.bin only contains tiv value for each coverage id\n     coverage_id n correspond to index n-1\n    Args:\n        dir_path: path to the directory where the binary or csv file is stored\n        name: name of the file\n        _dtype: numpy dtype of the required array\n        must_exist: raise FileNotFoundError if no file is present\n    Returns:\n        numpy array of dtype type\n    \"\"\"\n    fp = os.path.join(dir_path, name + '.bin')\n    if os.path.isfile(fp):\n        return np.fromfile(fp, dtype=_dtype)\n    elif must_exist or os.path.isfile(os.path.join(dir_path, name + '.csv')):\n        fp = os.path.join(dir_path, name + '.csv')\n        with open(fp) as file_in:\n            return np.loadtxt(file_in, dtype=_dtype, delimiter=',', skiprows=1, usecols=1)\n    else:\n        return np.empty(0, dtype=_dtype)\n\n\"\"\"Docstring (excerpt)\"\"\"\nload file as a single numpy array,\n useful for files with a binary version with only one type of value where their index correspond to an id.\n For example coverage.bin only contains tiv value for each coverage id\n coverage_id n correspond to index n-1\nArgs:\n    dir_path: path to the directory where the binary or csv file is stored\n    name: name of the file\n    _dtype: numpy dtype of the required array\n    must_exist: raise FileNotFoundError if no file is present\nReturns:\n    numpy array of dtype type"
    },
    {
      "chunk_id": "oasislmf/pytools/common/data.py::write_ndarray_to_fmt_csv@331",
      "source_type": "code",
      "path": "oasislmf/pytools/common/data.py",
      "symbol_type": "function",
      "name": "write_ndarray_to_fmt_csv",
      "lineno": 331,
      "end_lineno": 359,
      "business_stage": "other",
      "docstring": "Writes a custom dtype array with headers to csv with the provided row_fmt str\n\nThis function is a faster replacement for np.savetxt as it formats each row one at a time before writing to csv.\nWe create one large string, and formats all the data at once, and writes all the data at once.\n\nWARNING: untested with string types in custom data.\n\nArgs:\n    output_file (io.TextIOWrapper): CSV file\n    data (ndarray[<custom dtype>]): Custom dtype ndarray with column names\n    headers (list[str]): Column names for custom ndarray\n    row_fmt (str): Format for each row in csv",
      "content": "# File: oasislmf/pytools/common/data.py\n# function: write_ndarray_to_fmt_csv (lines 331-359)\n\ndef write_ndarray_to_fmt_csv(output_file, data, headers, row_fmt):\n    \"\"\"Writes a custom dtype array with headers to csv with the provided row_fmt str\n\n    This function is a faster replacement for np.savetxt as it formats each row one at a time before writing to csv.\n    We create one large string, and formats all the data at once, and writes all the data at once.\n\n    WARNING: untested with string types in custom data.\n\n    Args:\n        output_file (io.TextIOWrapper): CSV file\n        data (ndarray[<custom dtype>]): Custom dtype ndarray with column names\n        headers (list[str]): Column names for custom ndarray\n        row_fmt (str): Format for each row in csv\n    \"\"\"\n    if len(headers) != len(row_fmt.split(\",\")):\n        raise RuntimeError(f\"ERROR: write_ndarray_to_fmt_csv requires row_fmt ({row_fmt}) and headers ({headers}) to have the same length.\")\n\n    # Copy data as np.ravel does not work with custom dtype arrays\n    # Default type of np.empty is np.float64.\n    data_cpy = np.empty((data.shape[0], len(headers)))\n    for i in range(len(headers)):\n        data_cpy[:, i] = data[headers[i]]\n\n    # Create one large formatted string\n    final_fmt = \"\\n\".join([row_fmt] * data_cpy.shape[0])\n    str_data = final_fmt % tuple(np.ravel(data_cpy))\n\n    output_file.write(str_data)\n    output_file.write(\"\\n\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrites a custom dtype array with headers to csv with the provided row_fmt str\n\nThis function is a faster replacement for np.savetxt as it formats each row one at a time before writing to csv.\nWe create one large string, and formats all the data at once, and writes all the data at once.\n\nWARNING: untested with string types in custom data.\n\nArgs:\n    output_file (io.TextIOWrapper): CSV file\n    data (ndarray[<custom dtype>]): Custom dtype ndarray with column names\n    headers (list[str]): Column names for custom ndarray\n    row_fmt (str): Format for each row in csv"
    },
    {
      "chunk_id": "oasislmf/pytools/common/data.py::resolve_file@370",
      "source_type": "code",
      "path": "oasislmf/pytools/common/data.py",
      "symbol_type": "function",
      "name": "resolve_file",
      "lineno": 370,
      "end_lineno": 390,
      "business_stage": "other",
      "docstring": "Resolve file path to open file or use sys.stdin\n\nArgs:\n    path (str | os.PathLike): File path or \"-\" indicationg standard input/output.\n    mode (str): Mode to open file (\"r\", \"rb, \"w\", \"wb\").\n    stack (ExitStack): Context manager stack used to manage file lifecycle.\n\nReturns:\n    file (IO): A file-like object opened in the specified mode.",
      "content": "# File: oasislmf/pytools/common/data.py\n# function: resolve_file (lines 370-390)\n\ndef resolve_file(path, mode, stack):\n    \"\"\"Resolve file path to open file or use sys.stdin\n\n    Args:\n        path (str | os.PathLike): File path or \"-\" indicationg standard input/output.\n        mode (str): Mode to open file (\"r\", \"rb, \"w\", \"wb\").\n        stack (ExitStack): Context manager stack used to manage file lifecycle.\n\n    Returns:\n        file (IO): A file-like object opened in the specified mode.\n    \"\"\"\n    is_read = \"r\" in mode\n    is_binary = \"b\" in mode\n\n    if str(path) == \"-\":\n        if is_read:\n            return sys.stdin.buffer if is_binary else sys.stdin\n        else:\n            return sys.stdout.buffer if is_binary else sys.stdout\n    else:\n        return stack.enter_context(open(path, mode))\n\n\"\"\"Docstring (excerpt)\"\"\"\nResolve file path to open file or use sys.stdin\n\nArgs:\n    path (str | os.PathLike): File path or \"-\" indicationg standard input/output.\n    mode (str): Mode to open file (\"r\", \"rb, \"w\", \"wb\").\n    stack (ExitStack): Context manager stack used to manage file lifecycle.\n\nReturns:\n    file (IO): A file-like object opened in the specified mode."
    },
    {
      "chunk_id": "oasislmf/pytools/common/event_stream.py::stream_info_to_bytes@39",
      "source_type": "code",
      "path": "oasislmf/pytools/common/event_stream.py",
      "symbol_type": "function",
      "name": "stream_info_to_bytes",
      "lineno": 39,
      "end_lineno": 49,
      "business_stage": "hazard",
      "docstring": "From Stream source type and aggregation type produce the stream header\nArgs:\n    stream_source_type (np.int32):\n    stream_agg_type (np.int32):\n\nReturns:\n    return bytes",
      "content": "# File: oasislmf/pytools/common/event_stream.py\n# function: stream_info_to_bytes (lines 39-49)\n\ndef stream_info_to_bytes(stream_source_type, stream_agg_type):\n    \"\"\"\n    From Stream source type and aggregation type produce the stream header\n    Args:\n        stream_source_type (np.int32):\n        stream_agg_type (np.int32):\n\n    Returns:\n        return bytes\n    \"\"\"\n    return np.array([stream_agg_type], '<i4').tobytes()[:3] + np.int8(stream_source_type).tobytes()\n\n\"\"\"Docstring (excerpt)\"\"\"\nFrom Stream source type and aggregation type produce the stream header\nArgs:\n    stream_source_type (np.int32):\n    stream_agg_type (np.int32):\n\nReturns:\n    return bytes"
    },
    {
      "chunk_id": "oasislmf/pytools/common/event_stream.py::bytes_to_stream_types@52",
      "source_type": "code",
      "path": "oasislmf/pytools/common/event_stream.py",
      "symbol_type": "function",
      "name": "bytes_to_stream_types",
      "lineno": 52,
      "end_lineno": 61,
      "business_stage": "hazard",
      "docstring": "Read the stream header and return the information on stream type\nArgs:\n    stream_header: bytes\n\nReturns:\n    (stream source type (np.int32), stream aggregation type (np.int32))",
      "content": "# File: oasislmf/pytools/common/event_stream.py\n# function: bytes_to_stream_types (lines 52-61)\n\ndef bytes_to_stream_types(stream_header):\n    \"\"\"\n    Read the stream header and return the information on stream type\n    Args:\n        stream_header: bytes\n\n    Returns:\n        (stream source type (np.int32), stream aggregation type (np.int32))\n    \"\"\"\n    return np.frombuffer(stream_header[3:], 'i1')[0], np.frombuffer(stream_header[:3] + b'\\x00', '<i4')[0]\n\n\"\"\"Docstring (excerpt)\"\"\"\nRead the stream header and return the information on stream type\nArgs:\n    stream_header: bytes\n\nReturns:\n    (stream source type (np.int32), stream aggregation type (np.int32))"
    },
    {
      "chunk_id": "oasislmf/pytools/common/event_stream.py::read_stream_info@64",
      "source_type": "code",
      "path": "oasislmf/pytools/common/event_stream.py",
      "symbol_type": "function",
      "name": "read_stream_info",
      "lineno": 64,
      "end_lineno": 74,
      "business_stage": "hazard",
      "docstring": "from open stream object return the information that characterize the stream (stream_source_type, stream_agg_type, len_sample)\nArgs:\n    stream_obj: open stream\nReturns:\n    (stream_source_type, stream_agg_type, len_sample) as np.int32 triplet",
      "content": "# File: oasislmf/pytools/common/event_stream.py\n# function: read_stream_info (lines 64-74)\n\ndef read_stream_info(stream_obj):\n    \"\"\"\n    from open stream object return the information that characterize the stream (stream_source_type, stream_agg_type, len_sample)\n    Args:\n        stream_obj: open stream\n    Returns:\n        (stream_source_type, stream_agg_type, len_sample) as np.int32 triplet\n    \"\"\"\n    stream_source_type, stream_agg_type = bytes_to_stream_types(stream_obj.read(4))\n    len_sample = np.frombuffer(stream_obj.read(4), dtype=np.int32)[0]\n    return stream_source_type, stream_agg_type, len_sample\n\n\"\"\"Docstring (excerpt)\"\"\"\nfrom open stream object return the information that characterize the stream (stream_source_type, stream_agg_type, len_sample)\nArgs:\n    stream_obj: open stream\nReturns:\n    (stream_source_type, stream_agg_type, len_sample) as np.int32 triplet"
    },
    {
      "chunk_id": "oasislmf/pytools/common/event_stream.py::init_streams_in@94",
      "source_type": "code",
      "path": "oasislmf/pytools/common/event_stream.py",
      "symbol_type": "function",
      "name": "init_streams_in",
      "lineno": 94,
      "end_lineno": 106,
      "business_stage": "hazard",
      "docstring": "if files_in use stdin as stream in\notherwise open each path in files_in, read the header, check that they are the same, and return the streams and their info\nArgs:\n    files_in: none or a list of path\n    stack: contextlib stack to add the open stream to\n\nReturns:\n    list of open streams and their info",
      "content": "# File: oasislmf/pytools/common/event_stream.py\n# function: init_streams_in (lines 94-106)\n\ndef init_streams_in(files_in, stack):\n    \"\"\"\n    if files_in use stdin as stream in\n    otherwise open each path in files_in, read the header, check that they are the same, and return the streams and their info\n    Args:\n        files_in: none or a list of path\n        stack: contextlib stack to add the open stream to\n\n    Returns:\n        list of open streams and their info\n    \"\"\"\n    streams_in = get_streams_in(files_in, stack)\n    return streams_in, get_and_check_header_in(streams_in)\n\n\"\"\"Docstring (excerpt)\"\"\"\nif files_in use stdin as stream in\notherwise open each path in files_in, read the header, check that they are the same, and return the streams and their info\nArgs:\n    files_in: none or a list of path\n    stack: contextlib stack to add the open stream to\n\nReturns:\n    list of open streams and their info"
    },
    {
      "chunk_id": "oasislmf/pytools/common/event_stream.py::mv_read@110",
      "source_type": "code",
      "path": "oasislmf/pytools/common/event_stream.py",
      "symbol_type": "function",
      "name": "mv_read",
      "lineno": 110,
      "end_lineno": 122,
      "business_stage": "hazard",
      "docstring": "read a certain dtype from numpy byte view starting at cursor, return the value and the index of the end of the object\nArgs:\n    byte_mv: numpy byte view\n    cursor: index of where the object start\n    _dtype: data type of the object\n    itemsize: size of the data type\n\nReturns:\n    (object value, end of object index)",
      "content": "# File: oasislmf/pytools/common/event_stream.py\n# function: mv_read (lines 110-122)\n\ndef mv_read(byte_mv, cursor, _dtype, itemsize):\n    \"\"\"\n    read a certain dtype from numpy byte view starting at cursor, return the value and the index of the end of the object\n    Args:\n        byte_mv: numpy byte view\n        cursor: index of where the object start\n        _dtype: data type of the object\n        itemsize: size of the data type\n\n    Returns:\n        (object value, end of object index)\n    \"\"\"\n    return byte_mv[cursor:cursor + itemsize].view(_dtype)[0], cursor + itemsize\n\n\"\"\"Docstring (excerpt)\"\"\"\nread a certain dtype from numpy byte view starting at cursor, return the value and the index of the end of the object\nArgs:\n    byte_mv: numpy byte view\n    cursor: index of where the object start\n    _dtype: data type of the object\n    itemsize: size of the data type\n\nReturns:\n    (object value, end of object index)"
    },
    {
      "chunk_id": "oasislmf/pytools/common/event_stream.py::mv_write@126",
      "source_type": "code",
      "path": "oasislmf/pytools/common/event_stream.py",
      "symbol_type": "function",
      "name": "mv_write",
      "lineno": 126,
      "end_lineno": 140,
      "business_stage": "hazard",
      "docstring": "load an object into the numpy byte view at index cursor, return the index of the end of the object\nArgs:\n    byte_mv: numpy byte view\n    cursor: index of where the object start\n    _dtype: data type of the object\n    itemsize: size of the data type\n    value: value to write\n\nReturns:\n    end of object index",
      "content": "# File: oasislmf/pytools/common/event_stream.py\n# function: mv_write (lines 126-140)\n\ndef mv_write(byte_mv, cursor, _dtype, itemsize, value) -> int:\n    \"\"\"\n    load an object into the numpy byte view at index cursor, return the index of the end of the object\n    Args:\n        byte_mv: numpy byte view\n        cursor: index of where the object start\n        _dtype: data type of the object\n        itemsize: size of the data type\n        value: value to write\n\n    Returns:\n        end of object index\n    \"\"\"\n    byte_mv[cursor:cursor + itemsize].view(_dtype)[0] = value\n    return cursor + itemsize\n\n\"\"\"Docstring (excerpt)\"\"\"\nload an object into the numpy byte view at index cursor, return the index of the end of the object\nArgs:\n    byte_mv: numpy byte view\n    cursor: index of where the object start\n    _dtype: data type of the object\n    itemsize: size of the data type\n    value: value to write\n\nReturns:\n    end of object index"
    },
    {
      "chunk_id": "oasislmf/pytools/common/event_stream.py::mv_write_summary_header@144",
      "source_type": "code",
      "path": "oasislmf/pytools/common/event_stream.py",
      "symbol_type": "function",
      "name": "mv_write_summary_header",
      "lineno": 144,
      "end_lineno": 161,
      "business_stage": "hazard",
      "docstring": "write a summary header to the numpy byte view at index cursor, return the index of the end of the object\nArgs:\n    byte_mv: numpy byte view\n    cursor: index of where the object start\n    event_id: event id\n    summary_id: summary id\n    exposure_value: exposure value\n\nReturns:\n    end of object index",
      "content": "# File: oasislmf/pytools/common/event_stream.py\n# function: mv_write_summary_header (lines 144-161)\n\ndef mv_write_summary_header(byte_mv, cursor, event_id, summary_id, exposure_value) -> int:\n    \"\"\"\n    write a summary header to the numpy byte view at index cursor, return the index of the end of the object\n    Args:\n        byte_mv: numpy byte view\n        cursor: index of where the object start\n        event_id: event id\n        summary_id: summary id\n        exposure_value: exposure value\n\n    Returns:\n        end of object index\n    \"\"\"\n    # print(event_id, summary_id, exposure_value)\n    cursor = mv_write(byte_mv, cursor, oasis_int, oasis_int_size, event_id)\n    cursor = mv_write(byte_mv, cursor, oasis_int, oasis_int_size, summary_id)\n    cursor = mv_write(byte_mv, cursor, oasis_float, oasis_float_size, exposure_value)\n    return cursor\n\n\"\"\"Docstring (excerpt)\"\"\"\nwrite a summary header to the numpy byte view at index cursor, return the index of the end of the object\nArgs:\n    byte_mv: numpy byte view\n    cursor: index of where the object start\n    event_id: event id\n    summary_id: summary id\n    exposure_value: exposure value\n\nReturns:\n    end of object index"
    },
    {
      "chunk_id": "oasislmf/pytools/common/event_stream.py::mv_write_item_header@165",
      "source_type": "code",
      "path": "oasislmf/pytools/common/event_stream.py",
      "symbol_type": "function",
      "name": "mv_write_item_header",
      "lineno": 165,
      "end_lineno": 180,
      "business_stage": "hazard",
      "docstring": "write a item header to the numpy byte view at index cursor, return the index of the end of the object\nArgs:\n    byte_mv: numpy byte view\n    cursor: index of where the object start\n    event_id: event id\n    item_id: item id\n\nReturns:\n    end of object index",
      "content": "# File: oasislmf/pytools/common/event_stream.py\n# function: mv_write_item_header (lines 165-180)\n\ndef mv_write_item_header(byte_mv, cursor, event_id, item_id) -> int:\n    \"\"\"\n    write a item header to the numpy byte view at index cursor, return the index of the end of the object\n    Args:\n        byte_mv: numpy byte view\n        cursor: index of where the object start\n        event_id: event id\n        item_id: item id\n\n    Returns:\n        end of object index\n    \"\"\"\n    # print(event_id, item_id)\n    cursor = mv_write(byte_mv, cursor, oasis_int, oasis_int_size, event_id)\n    cursor = mv_write(byte_mv, cursor, oasis_int, oasis_int_size, item_id)\n    return cursor\n\n\"\"\"Docstring (excerpt)\"\"\"\nwrite a item header to the numpy byte view at index cursor, return the index of the end of the object\nArgs:\n    byte_mv: numpy byte view\n    cursor: index of where the object start\n    event_id: event id\n    item_id: item id\n\nReturns:\n    end of object index"
    },
    {
      "chunk_id": "oasislmf/pytools/common/event_stream.py::mv_write_sidx_loss@184",
      "source_type": "code",
      "path": "oasislmf/pytools/common/event_stream.py",
      "symbol_type": "function",
      "name": "mv_write_sidx_loss",
      "lineno": 184,
      "end_lineno": 199,
      "business_stage": "hazard",
      "docstring": "write sidx and loss to the numpy byte view at index cursor, return the index of the end of the object\nArgs:\n    byte_mv: numpy byte view\n    cursor: index of where the object start\n    sidx: sample id\n    loss: loss\n\nReturns:\n    end of object index",
      "content": "# File: oasislmf/pytools/common/event_stream.py\n# function: mv_write_sidx_loss (lines 184-199)\n\ndef mv_write_sidx_loss(byte_mv, cursor, sidx, loss) -> int:\n    \"\"\"\n    write sidx and loss to the numpy byte view at index cursor, return the index of the end of the object\n    Args:\n        byte_mv: numpy byte view\n        cursor: index of where the object start\n        sidx: sample id\n        loss: loss\n\n    Returns:\n        end of object index\n    \"\"\"\n    # print('    ', sidx, loss)\n    cursor = mv_write(byte_mv, cursor, oasis_int, oasis_int_size, sidx)\n    cursor = mv_write(byte_mv, cursor, oasis_float, oasis_float_size, loss)\n    return cursor\n\n\"\"\"Docstring (excerpt)\"\"\"\nwrite sidx and loss to the numpy byte view at index cursor, return the index of the end of the object\nArgs:\n    byte_mv: numpy byte view\n    cursor: index of where the object start\n    sidx: sample id\n    loss: loss\n\nReturns:\n    end of object index"
    },
    {
      "chunk_id": "oasislmf/pytools/common/event_stream.py::mv_write_delimiter@203",
      "source_type": "code",
      "path": "oasislmf/pytools/common/event_stream.py",
      "symbol_type": "function",
      "name": "mv_write_delimiter",
      "lineno": 203,
      "end_lineno": 216,
      "business_stage": "hazard",
      "docstring": "write the item delimiter (0,0) to the numpy byte view at index cursor, return the index of the end of the object\nArgs:\n    byte_mv: numpy byte view\n    cursor: index of where the object start\n\nReturns:\n    end of delimiter index",
      "content": "# File: oasislmf/pytools/common/event_stream.py\n# function: mv_write_delimiter (lines 203-216)\n\ndef mv_write_delimiter(byte_mv, cursor) -> int:\n    \"\"\"\n    write the item delimiter (0,0) to the numpy byte view at index cursor, return the index of the end of the object\n    Args:\n        byte_mv: numpy byte view\n        cursor: index of where the object start\n\n    Returns:\n        end of delimiter index\n    \"\"\"\n    cursor = mv_write(byte_mv, cursor, oasis_int, oasis_int_size, 0)\n    cursor = mv_write(byte_mv, cursor, oasis_float, oasis_float_size, 0)\n    # print('end', cursor)\n    return cursor\n\n\"\"\"Docstring (excerpt)\"\"\"\nwrite the item delimiter (0,0) to the numpy byte view at index cursor, return the index of the end of the object\nArgs:\n    byte_mv: numpy byte view\n    cursor: index of where the object start\n\nReturns:\n    end of delimiter index"
    },
    {
      "chunk_id": "oasislmf/pytools/common/event_stream.py::EventReader@219",
      "source_type": "code",
      "path": "oasislmf/pytools/common/event_stream.py",
      "symbol_type": "class",
      "name": "EventReader",
      "lineno": 219,
      "end_lineno": 380,
      "business_stage": "hazard",
      "docstring": "Abstract class to read event stream\n\nThis class provide a generic interface to read multiple event stream using:\n- selector : handle back pressure, the program is paused and don't use resource if nothing is in the stream buffer\n- memoryview : read a chuck (PIPE_CAPACITY) of data at a time then work on it using a numpy byte view of this buffer\n\nTo use those methods need to be implemented:\n- __init__(self, ...) the constructor with all data structure needed to read and store the event stream\n- read_buffer(self, byte_mv, cursor, valid_buff, event_id, item_id)\n    simply point to a local numba.jit function name read_buffer (a template is provided bellow)\n    this function should implement the specific logic of where and how to store the event information.\n\nThose to method may be overwritten\n- item_exit(self):\n    specific logic to do when an item is finished (only executed once the stream is finished but no 0,0 closure was present)\n\n- event_read_log(self):\n    what kpi to log when a full event is read\n\nusage snippet:\n    with ExitStack() as stack:\n        streams_in, (stream_type, stream_agg_type, len_sample) = init_streams_in(files_in, stack)\n        reader = CustomReader(<read relevant attributes>)\n        for event_id in reader.read_streams(streams_in):\n            <event logic>",
      "content": "# File: oasislmf/pytools/common/event_stream.py\n# class: EventReader (lines 219-380)\n\nclass EventReader:\n    \"\"\"\n    Abstract class to read event stream\n\n    This class provide a generic interface to read multiple event stream using:\n    - selector : handle back pressure, the program is paused and don't use resource if nothing is in the stream buffer\n    - memoryview : read a chuck (PIPE_CAPACITY) of data at a time then work on it using a numpy byte view of this buffer\n\n    To use those methods need to be implemented:\n    - __init__(self, ...) the constructor with all data structure needed to read and store the event stream\n    - read_buffer(self, byte_mv, cursor, valid_buff, event_id, item_id)\n        simply point to a local numba.jit function name read_buffer (a template is provided bellow)\n        this function should implement the specific logic of where and how to store the event information.\n\n    Those to method may be overwritten\n    - item_exit(self):\n        specific logic to do when an item is finished (only executed once the stream is finished but no 0,0 closure was present)\n\n    - event_read_log(self):\n        what kpi to log when a full event is read\n\n    usage snippet:\n        with ExitStack() as stack:\n            streams_in, (stream_type, stream_agg_type, len_sample) = init_streams_in(files_in, stack)\n            reader = CustomReader(<read relevant attributes>)\n            for event_id in reader.read_streams(streams_in):\n                <event logic>\n\n    \"\"\"\n\n    @staticmethod\n    def register_streams_in(selector_class, streams_in):\n        \"\"\"\n        Data from input process is generally sent by event block, meaning once a stream receive data, the complete event is\n        going to be sent in a short amount of time.\n        Therefore, we can focus on each stream one by one using their specific selector 'stream_selector'.\n\n        \"\"\"\n        main_selector = selector_class()\n        stream_data = []\n        for file_idx, stream_in in enumerate(streams_in):\n            mv = memoryview(bytearray(PIPE_CAPACITY))\n            byte_mv = np.frombuffer(buffer=mv, dtype='b')\n\n            stream_selector = selector_class()\n            stream_selector.register(stream_in, selectors.EVENT_READ)\n            data = {'mv': mv,\n                    'byte_mv': byte_mv,\n                    'cursor': 0,\n                    'valid_buff': 0,\n                    'stream_selector': stream_selector,\n                    'file_idx': file_idx,\n                    }\n            stream_data.append(data)\n            main_selector.register(stream_in, selectors.EVENT_READ, data)\n        return main_selector, stream_data\n\n    def read_streams(self, streams_in):\n        \"\"\"\n        read multiple stream input, yield each event id and load relevant value according to the specific read_buffer implemented in subclass\n        Args:\n            streams_in: streams to read\n\n        Returns:\n            event id generator\n        \"\"\"\n        try:\n            main_selector, stream_data = self.register_streams_in(selectors.DefaultSelector, streams_in)\n            self.logger.debug(\"Streams read with DefaultSelector\")\n        except PermissionError:  # Fall back option if stream_in contain regular files\n            main_selector, stream_data = self.register_streams_in(selectors.SelectSelector, streams_in)\n            self.logger.debug(\"Streams read with SelectSelector\")\n        try:\n            while main_selector.get_map():\n                for sKey, _ in main_selector.select():\n                    event = self.read_event(sKey.fileobj, main_selector, **sKey.data)\n\n                    if event:\n                        event_id, cursor, valid_buff = event\n                        sKey.data['cursor'] = cursor\n\n\"\"\"Docstring (excerpt)\"\"\"\nAbstract class to read event stream\n\nThis class provide a generic interface to read multiple event stream using:\n- selector : handle back pressure, the program is paused and don't use resource if nothing is in the stream buffer\n- memoryview : read a chuck (PIPE_CAPACITY) of data at a time then work on it using a numpy byte view of this buffer\n\nTo use those methods need to be implemented:\n- __init__(self, ...) the constructor with all data structure needed to read and store the event stream\n- read_buffer(self, byte_mv, cursor, valid_buff, event_id, item_id)\n    simply point to a local numba.jit function name read_buffer (a template is provided bellow)\n    this function should implement the specific logic of where and how to store the event information.\n\nThose to method may be overwritten\n- item_exit(self):\n    specific logic to do when an item is finished (only executed once the stream is finished but no 0,0 closure was present)\n\n- event_read_log(self):\n    what kpi to log when a full event is read\n\nusage snippet:\n    with ExitStack() as stack:\n        streams_in, (stream_type, stream_agg_type, len_sample) = init_streams_in(files_in, stack)\n        reader = CustomReader(<read relevant attributes>)\n        for event_id in reader.read_streams(streams_in):\n            <event logic>"
    },
    {
      "chunk_id": "oasislmf/pytools/common/event_stream.py::write_mv_to_stream@423",
      "source_type": "code",
      "path": "oasislmf/pytools/common/event_stream.py",
      "symbol_type": "function",
      "name": "write_mv_to_stream",
      "lineno": 423,
      "end_lineno": 439,
      "business_stage": "hazard",
      "docstring": "Write numpy byte array view to stream\n- use select to handle forward pressure\n- use a while loop in case the stream is non-blocking (meaning the ammount of byte written is not guarantied to be cursor len)\nArgs:\n    stream: stream to write to\n    byte_mv: numpy byte view of the buffer to write\n    cursor: ammount of byte to write",
      "content": "# File: oasislmf/pytools/common/event_stream.py\n# function: write_mv_to_stream (lines 423-439)\n\ndef write_mv_to_stream(stream, byte_mv, cursor):\n    \"\"\"\n    Write numpy byte array view to stream\n    - use select to handle forward pressure\n    - use a while loop in case the stream is non-blocking (meaning the ammount of byte written is not guarantied to be cursor len)\n    Args:\n        stream: stream to write to\n        byte_mv: numpy byte view of the buffer to write\n        cursor: ammount of byte to write\n\n    \"\"\"\n    written = 0\n    while written < cursor:\n        _, writable, exceptional = select([], [stream], [stream])\n        if exceptional:\n            raise IOError(f'error with input stream, {exceptional}')\n        written += stream.write(byte_mv[written:cursor].tobytes())\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrite numpy byte array view to stream\n- use select to handle forward pressure\n- use a while loop in case the stream is non-blocking (meaning the ammount of byte written is not guarantied to be cursor len)\nArgs:\n    stream: stream to write to\n    byte_mv: numpy byte view of the buffer to write\n    cursor: ammount of byte to write"
    },
    {
      "chunk_id": "oasislmf/pytools/common/event_stream.py::register_streams_in@250",
      "source_type": "code",
      "path": "oasislmf/pytools/common/event_stream.py",
      "symbol_type": "function",
      "name": "register_streams_in",
      "lineno": 250,
      "end_lineno": 274,
      "business_stage": "hazard",
      "docstring": "Data from input process is generally sent by event block, meaning once a stream receive data, the complete event is\ngoing to be sent in a short amount of time.\nTherefore, we can focus on each stream one by one using their specific selector 'stream_selector'.",
      "content": "# File: oasislmf/pytools/common/event_stream.py\n# function: register_streams_in (lines 250-274)\n\n    def register_streams_in(selector_class, streams_in):\n        \"\"\"\n        Data from input process is generally sent by event block, meaning once a stream receive data, the complete event is\n        going to be sent in a short amount of time.\n        Therefore, we can focus on each stream one by one using their specific selector 'stream_selector'.\n\n        \"\"\"\n        main_selector = selector_class()\n        stream_data = []\n        for file_idx, stream_in in enumerate(streams_in):\n            mv = memoryview(bytearray(PIPE_CAPACITY))\n            byte_mv = np.frombuffer(buffer=mv, dtype='b')\n\n            stream_selector = selector_class()\n            stream_selector.register(stream_in, selectors.EVENT_READ)\n            data = {'mv': mv,\n                    'byte_mv': byte_mv,\n                    'cursor': 0,\n                    'valid_buff': 0,\n                    'stream_selector': stream_selector,\n                    'file_idx': file_idx,\n                    }\n            stream_data.append(data)\n            main_selector.register(stream_in, selectors.EVENT_READ, data)\n        return main_selector, stream_data\n\n\"\"\"Docstring (excerpt)\"\"\"\nData from input process is generally sent by event block, meaning once a stream receive data, the complete event is\ngoing to be sent in a short amount of time.\nTherefore, we can focus on each stream one by one using their specific selector 'stream_selector'."
    },
    {
      "chunk_id": "oasislmf/pytools/common/event_stream.py::read_streams@276",
      "source_type": "code",
      "path": "oasislmf/pytools/common/event_stream.py",
      "symbol_type": "function",
      "name": "read_streams",
      "lineno": 276,
      "end_lineno": 320,
      "business_stage": "hazard",
      "docstring": "read multiple stream input, yield each event id and load relevant value according to the specific read_buffer implemented in subclass\nArgs:\n    streams_in: streams to read\n\nReturns:\n    event id generator",
      "content": "# File: oasislmf/pytools/common/event_stream.py\n# function: read_streams (lines 276-320)\n\n    def read_streams(self, streams_in):\n        \"\"\"\n        read multiple stream input, yield each event id and load relevant value according to the specific read_buffer implemented in subclass\n        Args:\n            streams_in: streams to read\n\n        Returns:\n            event id generator\n        \"\"\"\n        try:\n            main_selector, stream_data = self.register_streams_in(selectors.DefaultSelector, streams_in)\n            self.logger.debug(\"Streams read with DefaultSelector\")\n        except PermissionError:  # Fall back option if stream_in contain regular files\n            main_selector, stream_data = self.register_streams_in(selectors.SelectSelector, streams_in)\n            self.logger.debug(\"Streams read with SelectSelector\")\n        try:\n            while main_selector.get_map():\n                for sKey, _ in main_selector.select():\n                    event = self.read_event(sKey.fileobj, main_selector, **sKey.data)\n\n                    if event:\n                        event_id, cursor, valid_buff = event\n                        sKey.data['cursor'] = cursor\n                        sKey.data['valid_buff'] = valid_buff\n                        self.event_read_log(event_id)\n                        yield event_id\n\n            # Stream is read, we need to check if there is remaining event to be parsed\n            for data in stream_data:\n                if data['cursor'] < data['valid_buff']:\n                    byte_mv = data['byte_mv']\n                    cursor = data['cursor']\n                    valid_buff = data['valid_buff']\n                    file_idx = data['file_idx']\n                    yield_event = True\n                    while yield_event:\n                        cursor, event_id, item_id, yield_event = self.read_buffer(byte_mv, cursor, valid_buff, 0, 0, file_idx=file_idx)\n\n                        if event_id:\n                            self.item_exit()\n                            self.event_read_log(event_id)\n                            yield event_id\n\n        finally:\n            main_selector.close()\n\n\"\"\"Docstring (excerpt)\"\"\"\nread multiple stream input, yield each event id and load relevant value according to the specific read_buffer implemented in subclass\nArgs:\n    streams_in: streams to read\n\nReturns:\n    event id generator"
    },
    {
      "chunk_id": "oasislmf/pytools/common/event_stream.py::read_event@322",
      "source_type": "code",
      "path": "oasislmf/pytools/common/event_stream.py",
      "symbol_type": "function",
      "name": "read_event",
      "lineno": 322,
      "end_lineno": 371,
      "business_stage": "hazard",
      "docstring": "read one event from stream_in\nclose and remove the stream from main_selector when all is read\nArgs:\n    stream_in: stream to read\n    main_selector: selector that contain all the streams\n    stream_selector:  this stream selector\n    mv: buffer memoryview\n    byte_mv: numpy byte view of the buffer\n    cursor: current cursor of the memory view\n    valid_buff: valid data in memory view\n    file_idx: file index\n\nReturns:\n    event_id, cursor, valid_buff",
      "content": "# File: oasislmf/pytools/common/event_stream.py\n# function: read_event (lines 322-371)\n\n    def read_event(self, stream_in, main_selector, stream_selector, mv, byte_mv, cursor, valid_buff, file_idx):\n        \"\"\"\n        read one event from stream_in\n        close and remove the stream from main_selector when all is read\n        Args:\n            stream_in: stream to read\n            main_selector: selector that contain all the streams\n            stream_selector:  this stream selector\n            mv: buffer memoryview\n            byte_mv: numpy byte view of the buffer\n            cursor: current cursor of the memory view\n            valid_buff: valid data in memory view\n            file_idx: file index\n\n        Returns:\n            event_id, cursor, valid_buff\n        \"\"\"\n        event_id = 0\n        item_id = 0\n        try:\n            while True:\n                if valid_buff < PIPE_CAPACITY:\n                    stream_selector.select()\n                    len_read = stream_in.readinto1(mv[valid_buff:])\n                    valid_buff += len_read\n\n                    if len_read == 0:\n                        stream_selector.close()\n                        main_selector.unregister(stream_in)\n                        if event_id:\n                            self.item_exit()\n                            return event_id, cursor, valid_buff\n\n                        break\n                cursor, event_id, item_id, yield_event = self.read_buffer(byte_mv, cursor, valid_buff, event_id, item_id, file_idx=file_idx)\n\n                if yield_event:\n                    if 2 * cursor > valid_buff:\n                        mv[:valid_buff - cursor] = mv[cursor: valid_buff]\n                        valid_buff -= cursor\n                        cursor = 0\n                    return event_id, cursor, valid_buff\n                else:\n                    mv[:valid_buff - cursor] = mv[cursor: valid_buff]\n                    valid_buff -= cursor\n                    cursor = 0\n\n        except Exception as e:\n            self.logger.error(str(e))\n            raise OasisStreamException(\"Error reading stream\", e)\n\n\"\"\"Docstring (excerpt)\"\"\"\nread one event from stream_in\nclose and remove the stream from main_selector when all is read\nArgs:\n    stream_in: stream to read\n    main_selector: selector that contain all the streams\n    stream_selector:  this stream selector\n    mv: buffer memoryview\n    byte_mv: numpy byte view of the buffer\n    cursor: current cursor of the memory view\n    valid_buff: valid data in memory view\n    file_idx: file index\n\nReturns:\n    event_id, cursor, valid_buff"
    },
    {
      "chunk_id": "oasislmf/pytools/common/input_files.py::read_amplifications@37",
      "source_type": "code",
      "path": "oasislmf/pytools/common/input_files.py",
      "symbol_type": "function",
      "name": "read_amplifications",
      "lineno": 37,
      "end_lineno": 74,
      "business_stage": "other",
      "docstring": "Get array of amplification IDs from amplifications.bin, where index\ncorresponds to item ID.\n\namplifications.bin is binary file with layout:\n    reserved header (4-byte int),\n    item ID 1 (4-byte int), amplification ID a_1 (4-byte int),\n    ...\n    item ID n (4-byte int), amplification ID a_n (4-byte int)\n\nArgs:\n    run_dir (str): path to amplifications.bin file\n    filename (str | os.PathLike): amplifications file name\n    use_stdin (bool): Use standard input for file data, ignores run_dir/filename. Defaults to False.\nReturns:\n    items_amps (numpy.ndarray): array of amplification IDs, where index\n        corresponds to item ID",
      "content": "# File: oasislmf/pytools/common/input_files.py\n# function: read_amplifications (lines 37-74)\n\ndef read_amplifications(run_dir=\"\", filename=AMPLIFICATIONS_FILE, use_stdin=False):\n    \"\"\"\n    Get array of amplification IDs from amplifications.bin, where index\n    corresponds to item ID.\n\n    amplifications.bin is binary file with layout:\n        reserved header (4-byte int),\n        item ID 1 (4-byte int), amplification ID a_1 (4-byte int),\n        ...\n        item ID n (4-byte int), amplification ID a_n (4-byte int)\n\n    Args:\n        run_dir (str): path to amplifications.bin file\n        filename (str | os.PathLike): amplifications file name\n        use_stdin (bool): Use standard input for file data, ignores run_dir/filename. Defaults to False.\n    Returns:\n        items_amps (numpy.ndarray): array of amplification IDs, where index\n            corresponds to item ID\n    \"\"\"\n    header_size = 4\n    if use_stdin:\n        items_amps = np.frombuffer(sys.stdin.buffer.read(), dtype=np.int32, offset=header_size)\n    else:\n        amplification_file = Path(run_dir, filename)\n        if not amplification_file.exists():\n            raise FileNotFoundError('amplifications file not found.')\n        items_amps = np.fromfile(amplification_file, dtype=np.int32, offset=header_size)\n\n    # Check item IDs start from 1 and are contiguous\n    if items_amps[0] != 1:\n        raise ValueError(f'First item ID is {items_amps[0]}. Expected 1.')\n    items_amps = items_amps.reshape(len(items_amps) // 2, 2)\n    if not np.all(items_amps[1:, 0] - items_amps[:-1, 0] == 1):\n        raise ValueError('Item IDs in amplifications file are not contiguous')\n\n    items_amps = np.concatenate((np.array([0]), items_amps[:, 1]))\n\n    return items_amps\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet array of amplification IDs from amplifications.bin, where index\ncorresponds to item ID.\n\namplifications.bin is binary file with layout:\n    reserved header (4-byte int),\n    item ID 1 (4-byte int), amplification ID a_1 (4-byte int),\n    ...\n    item ID n (4-byte int), amplification ID a_n (4-byte int)\n\nArgs:\n    run_dir (str): path to amplifications.bin file\n    filename (str | os.PathLike): amplifications file name\n    use_stdin (bool): Use standard input for file data, ignores run_dir/filename. Defaults to False.\nReturns:\n    items_amps (numpy.ndarray): array of amplification IDs, where index\n        corresponds to item ID"
    },
    {
      "chunk_id": "oasislmf/pytools/common/input_files.py::read_correlations@77",
      "source_type": "code",
      "path": "oasislmf/pytools/common/input_files.py",
      "symbol_type": "function",
      "name": "read_correlations",
      "lineno": 77,
      "end_lineno": 119,
      "business_stage": "other",
      "docstring": "Load the correlations from the correlations file.\nArgs:\n    run_dir (str): path to correlations file\n    ignore_file_type (Set[str]): file extension to ignore when loading.\n    filename (str | os.PathLike): correlations file name\nReturns:\n    Tuple[Dict[int, int], List[int], Dict[int, int], List[Tuple[int, int]], List[int]]\n    vulnerability dictionary, vulnerability IDs, areaperil to vulnerability index dictionary,\n    areaperil ID to vulnerability index array, areaperil ID to vulnerability array",
      "content": "# File: oasislmf/pytools/common/input_files.py\n# function: read_correlations (lines 77-119)\n\ndef read_correlations(run_dir, ignore_file_type=set(), filename=CORRELATIONS_FILENAME):\n    \"\"\"Load the correlations from the correlations file.\n    Args:\n        run_dir (str): path to correlations file\n        ignore_file_type (Set[str]): file extension to ignore when loading.\n        filename (str | os.PathLike): correlations file name\n    Returns:\n        Tuple[Dict[int, int], List[int], Dict[int, int], List[Tuple[int, int]], List[int]]\n        vulnerability dictionary, vulnerability IDs, areaperil to vulnerability index dictionary,\n        areaperil ID to vulnerability index array, areaperil ID to vulnerability array\n    \"\"\"\n\n    for ext in [\"bin\", \"csv\"]:\n        if ext in ignore_file_type:\n            continue\n\n        correlations_file = Path(run_dir, filename).with_suffix(\".\" + ext)\n        if correlations_file.exists():\n            logger.debug(f\"loading {correlations_file}\")\n            if ext == \"bin\":\n                try:\n                    correlations = np.memmap(correlations_file, dtype=correlations_dtype, mode='r')\n                except ValueError:\n                    logger.debug(\"binary file is empty, numpy.memmap failed. trying to read correlations.csv.\")\n                    correlations = read_correlations(run_dir, ignore_file_type={'bin'}, filename=correlations_file.with_suffix(\".csv\").name)\n            elif ext == \"csv\":\n                # Check for header\n                with open(correlations_file, \"r\") as fin:\n                    first_line = fin.readline()\n                    first_line_elements = [header.strip() for header in first_line.strip().split(',')]\n                    has_header = first_line_elements == correlations_headers\n                correlations = np.loadtxt(\n                    correlations_file,\n                    dtype=correlations_dtype,\n                    delimiter=\",\",\n                    skiprows=1 if has_header else 0,\n                    ndmin=1\n                )\n            else:\n                raise RuntimeError(f\"Cannot read correlations file of type {ext}. Not Implemented.\")\n            return correlations\n\n    raise FileNotFoundError(f'correlations file not found at {run_dir}. Ignoring files with ext {ignore_file_type}.')\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoad the correlations from the correlations file.\nArgs:\n    run_dir (str): path to correlations file\n    ignore_file_type (Set[str]): file extension to ignore when loading.\n    filename (str | os.PathLike): correlations file name\nReturns:\n    Tuple[Dict[int, int], List[int], Dict[int, int], List[Tuple[int, int]], List[int]]\n    vulnerability dictionary, vulnerability IDs, areaperil to vulnerability index dictionary,\n    areaperil ID to vulnerability index array, areaperil ID to vulnerability array"
    },
    {
      "chunk_id": "oasislmf/pytools/common/input_files.py::read_coverages@122",
      "source_type": "code",
      "path": "oasislmf/pytools/common/input_files.py",
      "symbol_type": "function",
      "name": "read_coverages",
      "lineno": 122,
      "end_lineno": 181,
      "business_stage": "other",
      "docstring": "Load the coverages from the coverages file.\nArgs:\n    run_dir (str): path to coverages file\n    ignore_file_type (Set[str]): file extension to ignore when loading.\n    filename (str | os.PathLike): coverages file name\n    use_stdin (bool): Use standard input for file data, ignores run_dir/filename. Defaults to False.\nReturns:\n    numpy.array[oasis_float]: array with the coverage values for each coverage_id.",
      "content": "# File: oasislmf/pytools/common/input_files.py\n# function: read_coverages (lines 122-181)\n\ndef read_coverages(run_dir=\"\", ignore_file_type=set(), filename=COVERAGES_FILE, use_stdin=False):\n    \"\"\"Load the coverages from the coverages file.\n    Args:\n        run_dir (str): path to coverages file\n        ignore_file_type (Set[str]): file extension to ignore when loading.\n        filename (str | os.PathLike): coverages file name\n        use_stdin (bool): Use standard input for file data, ignores run_dir/filename. Defaults to False.\n    Returns:\n        numpy.array[oasis_float]: array with the coverage values for each coverage_id.\n    \"\"\"\n    supported_exts = [\"bin\", \"csv\"]\n\n    def read_csv_lines(lines):\n        # Check for header\n        first_line_elements = [header.strip() for header in lines[0].strip().split(',')]\n        has_header = first_line_elements == coverages_headers\n        data_lines = lines[1:] if has_header else lines\n        return np.loadtxt(\n            data_lines,\n            dtype=oasis_float,\n            delimiter=\",\",\n            ndmin=1\n        )[:, 1]\n\n    # STDIN\n    if use_stdin:\n        for ext in supported_exts:\n            if ext in ignore_file_type:\n                continue\n            if ext == \"bin\":\n                return np.frombuffer(sys.stdin.buffer.read(), dtype=oasis_float)\n            elif ext == \"csv\":\n                lines = sys.stdin.readlines()\n                return read_csv_lines(lines)\n            else:\n                raise RuntimeError(f\"Cannot read coverages file of type {ext}. Not Implemented.\")\n        raise RuntimeError(\n            f'coverages data not readable with use_stdin={use_stdin}, run_dir={run_dir}, filename={filename}. Ignoring files with ext {ignore_file_type}.')\n\n    # FILE\n    for ext in supported_exts:\n        if ext in ignore_file_type:\n            continue\n\n        coverages_file = Path(run_dir, filename).with_suffix(\".\" + ext)\n        if not coverages_file.exists():\n            continue\n\n        if ext == \"bin\":\n            return np.fromfile(coverages_file, dtype=oasis_float)\n        elif ext == \"csv\":\n            with ExitStack() as stack:\n                fin = stack.enter_context(open(coverages_file, \"r\"))\n                lines = fin.readlines()\n            return read_csv_lines(lines)\n        else:\n            raise RuntimeError(f\"Cannot read coverages file of type {ext}. Not Implemented.\")\n\n    raise RuntimeError(\n        f'coverages data not readable with use_stdin={use_stdin}, run_dir={run_dir}, filename={filename}. Ignoring files with ext {ignore_file_type}.')\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoad the coverages from the coverages file.\nArgs:\n    run_dir (str): path to coverages file\n    ignore_file_type (Set[str]): file extension to ignore when loading.\n    filename (str | os.PathLike): coverages file name\n    use_stdin (bool): Use standard input for file data, ignores run_dir/filename. Defaults to False.\nReturns:\n    numpy.array[oasis_float]: array with the coverage values for each coverage_id."
    },
    {
      "chunk_id": "oasislmf/pytools/common/input_files.py::read_event_rates@184",
      "source_type": "code",
      "path": "oasislmf/pytools/common/input_files.py",
      "symbol_type": "function",
      "name": "read_event_rates",
      "lineno": 184,
      "end_lineno": 211,
      "business_stage": "other",
      "docstring": "Reads event rates from a CSV file\nArgs:\n    run_dir (str | os.PathLike): Path to input files dir\n    filename (str | os.PathLike): event rates csv file name\nReturns:\n    unique_event_ids (ndarray[oasis_int]): unique event ids\n    event_rates (ndarray[oasis_float]): event rates",
      "content": "# File: oasislmf/pytools/common/input_files.py\n# function: read_event_rates (lines 184-211)\n\ndef read_event_rates(run_dir, filename=EVENTRATES_FILE):\n    \"\"\"Reads event rates from a CSV file\n    Args:\n        run_dir (str | os.PathLike): Path to input files dir\n        filename (str | os.PathLike): event rates csv file name\n    Returns:\n        unique_event_ids (ndarray[oasis_int]): unique event ids\n        event_rates (ndarray[oasis_float]): event rates\n    \"\"\"\n    event_rate_file = Path(run_dir, filename)\n    data = load_as_ndarray(\n        run_dir,\n        filename[:-4],\n        np.dtype([('event_id', oasis_int), ('rate', oasis_float)]),\n        must_exist=False,\n        col_map={\"event_id\": \"EventIds\", \"rate\": \"Event_rates\"}\n    )\n    if data is None or data.size == 0:\n        logger.info(f\"Event rate file {event_rate_file} is empty, proceeding without event rates.\")\n        return np.array([], dtype=oasis_int), np.array([], dtype=oasis_float)\n    unique_event_ids = data['event_id']\n    event_rates = data['rate']\n\n    # Make sure event_ids are sorted\n    sort_idx = np.argsort(unique_event_ids)\n    unique_event_ids = unique_event_ids[sort_idx]\n    event_rates = event_rates[sort_idx]\n    return unique_event_ids, event_rates\n\n\"\"\"Docstring (excerpt)\"\"\"\nReads event rates from a CSV file\nArgs:\n    run_dir (str | os.PathLike): Path to input files dir\n    filename (str | os.PathLike): event rates csv file name\nReturns:\n    unique_event_ids (ndarray[oasis_int]): unique event ids\n    event_rates (ndarray[oasis_float]): event rates"
    },
    {
      "chunk_id": "oasislmf/pytools/common/input_files.py::read_quantile@214",
      "source_type": "code",
      "path": "oasislmf/pytools/common/input_files.py",
      "symbol_type": "function",
      "name": "read_quantile",
      "lineno": 214,
      "end_lineno": 239,
      "business_stage": "other",
      "docstring": "Generate a quantile interval Dictionary based on sample size and quantile binary file\nArgs:\n    sample_size (int): Sample size\n    run_dir (str | os.PathLike): Path to input files dir\n    filename (str | os.PathLike): quantile binary file name\n    return_empty (bool): return an empty intervals array regardless of the existence of the quantile binary\nReturns:\n    intervals (quantile_interval_dtype): Numpy array emulating a dictionary for numba",
      "content": "# File: oasislmf/pytools/common/input_files.py\n# function: read_quantile (lines 214-239)\n\ndef read_quantile(sample_size, run_dir, filename=QUANTILE_FILE, return_empty=False):\n    \"\"\"Generate a quantile interval Dictionary based on sample size and quantile binary file\n    Args:\n        sample_size (int): Sample size\n        run_dir (str | os.PathLike): Path to input files dir\n        filename (str | os.PathLike): quantile binary file name\n        return_empty (bool): return an empty intervals array regardless of the existence of the quantile binary\n    Returns:\n        intervals (quantile_interval_dtype): Numpy array emulating a dictionary for numba\n    \"\"\"\n    intervals = []\n\n    if return_empty:\n        return np.array([], dtype=quantile_interval_dtype)\n    data = load_as_ndarray(run_dir, filename[:-4], quantile_dtype, must_exist=True)\n    for row in data:\n        q = row[\"quantile\"]\n        # Calculate interval index and fractional part\n        pos = (sample_size - 1) * q + 1\n        integer_part = int(pos)\n        fractional_part = pos - integer_part\n        intervals.append((q, integer_part, fractional_part))\n\n    # Convert to numpy array\n    intervals = np.array(intervals, dtype=quantile_interval_dtype)\n    return intervals\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate a quantile interval Dictionary based on sample size and quantile binary file\nArgs:\n    sample_size (int): Sample size\n    run_dir (str | os.PathLike): Path to input files dir\n    filename (str | os.PathLike): quantile binary file name\n    return_empty (bool): return an empty intervals array regardless of the existence of the quantile binary\nReturns:\n    intervals (quantile_interval_dtype): Numpy array emulating a dictionary for numba"
    },
    {
      "chunk_id": "oasislmf/pytools/common/input_files.py::read_occurrence_bin@242",
      "source_type": "code",
      "path": "oasislmf/pytools/common/input_files.py",
      "symbol_type": "function",
      "name": "read_occurrence_bin",
      "lineno": 242,
      "end_lineno": 294,
      "business_stage": "other",
      "docstring": "Read the occurrence binary file and returns an occurrence map\nArgs:\n    run_dir (str | os.PathLike): Path to input files dir\n    filename (str | os.PathLike): occurrence binary file name\n    use_stdin (bool): Use standard input for file data, ignores run_dir/filename. Defaults to False.\nReturns:\n    occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file",
      "content": "# File: oasislmf/pytools/common/input_files.py\n# function: read_occurrence_bin (lines 242-294)\n\ndef read_occurrence_bin(run_dir=\"\", filename=OCCURRENCE_FILE, use_stdin=False):\n    \"\"\"Read the occurrence binary file and returns an occurrence map\n    Args:\n        run_dir (str | os.PathLike): Path to input files dir\n        filename (str | os.PathLike): occurrence binary file name\n        use_stdin (bool): Use standard input for file data, ignores run_dir/filename. Defaults to False.\n    Returns:\n        occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file\n    \"\"\"\n    occurrence_fp = Path(run_dir, filename)\n    if use_stdin:\n        fin = np.frombuffer(sys.stdin.buffer.read(), dtype=\"u1\")\n    else:\n        fin = np.memmap(occurrence_fp, mode=\"r\", dtype=\"u1\")\n    cursor = 0\n    valid_buff = len(fin)\n\n    if valid_buff - cursor < np.dtype(np.int32).itemsize:\n        raise RuntimeError(\"Error: broken occurrence file, not enough data\")\n    date_opts, cursor = mv_read(fin, cursor, np.int32, np.dtype(np.int32).itemsize)\n\n    date_algorithm = date_opts & 1\n    granular_date = date_opts >> 1\n\n    # (event_id: int, period_no: int, occ_date_id: int)\n    record_size = np.dtype(np.int32).itemsize * 3\n    # (event_id: int, period_no: int, occ_date_id: long long)\n    if granular_date:\n        record_size = np.dtype(np.int32).itemsize * 2 + np.dtype(np.int64).itemsize\n    # Should not get here\n    if not date_algorithm and granular_date:\n        raise RuntimeError(\"FATAL: Unknown date algorithm\")\n\n    # Extract no_of_periods\n    if valid_buff - cursor < np.dtype(np.int32).itemsize:\n        raise RuntimeError(\"Error: broken occurrence file, not enough data\")\n    no_of_periods, cursor = mv_read(fin, cursor, np.int32, np.dtype(np.int32).itemsize)\n\n    num_records = (valid_buff - cursor) // record_size\n    if (valid_buff - cursor) % record_size != 0:\n        logger.warning(\n            f\"Occurrence File size (num_records: {num_records}) does not align with expected record size (record_size: {record_size})\"\n        )\n\n    occ_dtype = occurrence_dtype\n    if granular_date:\n        occ_dtype = occurrence_granular_dtype\n    occ_arr = np.zeros(0, dtype=occ_dtype)\n\n    if num_records > 0:\n        occ_arr = np.frombuffer(fin[cursor:cursor + num_records * record_size], dtype=occ_dtype)\n\n    return occ_arr, date_algorithm, granular_date, no_of_periods\n\n\"\"\"Docstring (excerpt)\"\"\"\nRead the occurrence binary file and returns an occurrence map\nArgs:\n    run_dir (str | os.PathLike): Path to input files dir\n    filename (str | os.PathLike): occurrence binary file name\n    use_stdin (bool): Use standard input for file data, ignores run_dir/filename. Defaults to False.\nReturns:\n    occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file"
    },
    {
      "chunk_id": "oasislmf/pytools/common/input_files.py::read_occurrence@297",
      "source_type": "code",
      "path": "oasislmf/pytools/common/input_files.py",
      "symbol_type": "function",
      "name": "read_occurrence",
      "lineno": 297,
      "end_lineno": 316,
      "business_stage": "other",
      "docstring": "Read the occurrence binary file and returns an occurrence map\nArgs:\n    run_dir (str | os.PathLike): Path to input files dir\n    filename (str | os.PathLike): occurrence binary file name\nReturns:\n    occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file",
      "content": "# File: oasislmf/pytools/common/input_files.py\n# function: read_occurrence (lines 297-316)\n\ndef read_occurrence(run_dir, filename=OCCURRENCE_FILE):\n    \"\"\"Read the occurrence binary file and returns an occurrence map\n    Args:\n        run_dir (str | os.PathLike): Path to input files dir\n        filename (str | os.PathLike): occurrence binary file name\n    Returns:\n        occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file\n    \"\"\"\n    occ_arr, date_algorithm, granular_date, no_of_periods = read_occurrence_bin(run_dir, filename=filename)\n\n    occ_dtype = occurrence_dtype\n    if granular_date:\n        occ_dtype = occurrence_granular_dtype\n\n    occ_map_valtype = occ_dtype[[\"period_no\", \"occ_date_id\"]]\n    NB_occ_map_valtype = nb.types.Array(nb.from_dtype(occ_map_valtype), 1, \"C\")\n\n    occ_map = _read_occ_arr(occ_arr, occ_map_valtype, NB_occ_map_valtype)\n\n    return occ_map, date_algorithm, granular_date, no_of_periods\n\n\"\"\"Docstring (excerpt)\"\"\"\nRead the occurrence binary file and returns an occurrence map\nArgs:\n    run_dir (str | os.PathLike): Path to input files dir\n    filename (str | os.PathLike): occurrence binary file name\nReturns:\n    occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file"
    },
    {
      "chunk_id": "oasislmf/pytools/common/input_files.py::occ_get_date@351",
      "source_type": "code",
      "path": "oasislmf/pytools/common/input_files.py",
      "symbol_type": "function",
      "name": "occ_get_date",
      "lineno": 351,
      "end_lineno": 378,
      "business_stage": "other",
      "docstring": "Returns date as year, month, day, hour, minute from occ_date_id\n\nArgs:\n    occ_date_id (np.int32 | np.int64): occurrence file date id (int64 for granular dates)\n    granular_date (bool): boolean for whether granular date should be extracted or not\n\nReturns:\n    (oasis_int, oasis_int, oasis_int, oasis_int, oasis_int): Returns year, month, date, hour, minute",
      "content": "# File: oasislmf/pytools/common/input_files.py\n# function: occ_get_date (lines 351-378)\n\ndef occ_get_date(occ_date_id, granular_date):\n    \"\"\"Returns date as year, month, day, hour, minute from occ_date_id\n\n    Args:\n        occ_date_id (np.int32 | np.int64): occurrence file date id (int64 for granular dates)\n        granular_date (bool): boolean for whether granular date should be extracted or not\n\n    Returns:\n        (oasis_int, oasis_int, oasis_int, oasis_int, oasis_int): Returns year, month, date, hour, minute\n    \"\"\"\n    days = occ_date_id / (1440 - 1439 * (not granular_date))\n\n    # Function void d(long long g, int& y, int& mm, int& dd) taken from pltcalc.cpp\n    y = (10000 * days + 14780) // 3652425\n    ddd = days - (365 * y + y // 4 - y // 100 + y // 400)\n    if ddd < 0:\n        y = y - 1\n        ddd = days - (365 * y + y // 4 - y // 100 + y // 400)\n    mi = (100 * ddd + 52) // 3060\n    mm = (mi + 2) % 12 + 1\n    y = y + (mi + 2) // 12\n    dd = ddd - (mi * 306 + 5) // 10 + 1\n\n    minutes = (occ_date_id % 1440) * granular_date\n    occ_hour = minutes // 60\n    occ_minutes = minutes % 60\n\n    return y, mm, dd, occ_hour, occ_minutes\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturns date as year, month, day, hour, minute from occ_date_id\n\nArgs:\n    occ_date_id (np.int32 | np.int64): occurrence file date id (int64 for granular dates)\n    granular_date (bool): boolean for whether granular date should be extracted or not\n\nReturns:\n    (oasis_int, oasis_int, oasis_int, oasis_int, oasis_int): Returns year, month, date, hour, minute"
    },
    {
      "chunk_id": "oasislmf/pytools/common/input_files.py::occ_get_date_id@382",
      "source_type": "code",
      "path": "oasislmf/pytools/common/input_files.py",
      "symbol_type": "function",
      "name": "occ_get_date_id",
      "lineno": 382,
      "end_lineno": 402,
      "business_stage": "other",
      "docstring": "Returns the occ_date_id from year, month, day, hour, minute and whether it is a granular date\nArgs:\n    granular_date (bool): boolean for whether granular date should be extracted or not\n    occ_year (int): Occurrence Year.\n    occ_month (int): Occurrence Month.\n    occ_day (int): Occurrence Day.\n    occ_hour (int): Occurrence Hour. Defaults to 0.\n    occ_minute (int): Occurrence Minute. Defaults to 0.\nReturns:\n    occ_date_id (np.int64): occurrence file date id (int64 for granular dates)",
      "content": "# File: oasislmf/pytools/common/input_files.py\n# function: occ_get_date_id (lines 382-402)\n\ndef occ_get_date_id(granular_date, occ_year, occ_month, occ_day, occ_hour=0, occ_minute=0):\n    \"\"\"Returns the occ_date_id from year, month, day, hour, minute and whether it is a granular date\n    Args:\n        granular_date (bool): boolean for whether granular date should be extracted or not\n        occ_year (int): Occurrence Year.\n        occ_month (int): Occurrence Month.\n        occ_day (int): Occurrence Day.\n        occ_hour (int): Occurrence Hour. Defaults to 0.\n        occ_minute (int): Occurrence Minute. Defaults to 0.\n    Returns:\n        occ_date_id (np.int64): occurrence file date id (int64 for granular dates)\n    \"\"\"\n    occ_month = (occ_month + 9) % 12\n    occ_year = occ_year - occ_month // 10\n    occ_date_id = np.int64(\n        365 * occ_year + occ_year // 4 - occ_year // 100 + occ_year // 400 + (occ_month * 306 + 5) // 10 + (occ_day - 1)\n    )\n\n    occ_date_id *= (1440 // (1440 - 1439 * granular_date))\n    occ_date_id += (60 * occ_hour + occ_minute)\n    return occ_date_id\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturns the occ_date_id from year, month, day, hour, minute and whether it is a granular date\nArgs:\n    granular_date (bool): boolean for whether granular date should be extracted or not\n    occ_year (int): Occurrence Year.\n    occ_month (int): Occurrence Month.\n    occ_day (int): Occurrence Day.\n    occ_hour (int): Occurrence Hour. Defaults to 0.\n    occ_minute (int): Occurrence Minute. Defaults to 0.\nReturns:\n    occ_date_id (np.int64): occurrence file date id (int64 for granular dates)"
    },
    {
      "chunk_id": "oasislmf/pytools/common/input_files.py::read_periods@405",
      "source_type": "code",
      "path": "oasislmf/pytools/common/input_files.py",
      "symbol_type": "function",
      "name": "read_periods",
      "lineno": 405,
      "end_lineno": 439,
      "business_stage": "other",
      "docstring": "Returns an array of period weights for each period between 1 and no_of_periods inclusive (with no gaps).\nArgs:\n    no_of_periods (int): Number of periods\n    run_dir (str | os.PathLike): Path to input files dir\n    filename (str | os.PathLike): periods binary file name\nReturns:\n    period_weights (ndarray[periods_dtype]): Period weights",
      "content": "# File: oasislmf/pytools/common/input_files.py\n# function: read_periods (lines 405-439)\n\ndef read_periods(no_of_periods, run_dir, filename=PERIODS_FILE):\n    \"\"\"Returns an array of period weights for each period between 1 and no_of_periods inclusive (with no gaps).\n    Args:\n        no_of_periods (int): Number of periods\n        run_dir (str | os.PathLike): Path to input files dir\n        filename (str | os.PathLike): periods binary file name\n    Returns:\n        period_weights (ndarray[periods_dtype]): Period weights\n    \"\"\"\n    periods_fp = Path(run_dir, filename)\n\n    if not periods_fp.exists():\n        # If no periods binary file found, the revert to using period weights reciprocal to no_of_periods\n        logger.warning(f\"Periods file not found at {periods_fp}, using reciprocal calculated period weights based on no_of_periods {no_of_periods}\")\n        period_weights = np.array(\n            [(i + 1, 1 / no_of_periods) for i in range(no_of_periods)],\n            dtype=periods_dtype\n        )\n        return period_weights\n\n    data = load_as_ndarray(run_dir, filename[:-4], periods_dtype, must_exist=True)\n    # Less data than no_of_periods\n    if len(data) != no_of_periods:\n        raise RuntimeError(f\"ERROR: no_of_periods does not match total period_no in period binary file {periods_fp}.\")\n\n    # Sort by period_no\n    period_weights = np.sort(data, order=\"period_no\")\n\n    # Identify any missing periods\n    expected_periods = np.arange(1, no_of_periods + 1)\n    actual_periods = period_weights['period_no']\n    missing_periods = np.setdiff1d(expected_periods, actual_periods)\n    if len(missing_periods) > 0:\n        raise RuntimeError(f\"ERROR: Missing period_no in period binary file {periods_fp}.\")\n    return period_weights\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturns an array of period weights for each period between 1 and no_of_periods inclusive (with no gaps).\nArgs:\n    no_of_periods (int): Number of periods\n    run_dir (str | os.PathLike): Path to input files dir\n    filename (str | os.PathLike): periods binary file name\nReturns:\n    period_weights (ndarray[periods_dtype]): Period weights"
    },
    {
      "chunk_id": "oasislmf/pytools/common/input_files.py::read_returnperiods@442",
      "source_type": "code",
      "path": "oasislmf/pytools/common/input_files.py",
      "symbol_type": "function",
      "name": "read_returnperiods",
      "lineno": 442,
      "end_lineno": 480,
      "business_stage": "other",
      "docstring": "Returns an array of return periods decreasing order with no duplicates.\nArgs:\n    use_return_period_file (bool): Bool to use Return Period File\n    run_dir (str | os.PathLike): Path to input files dir\n    filename (str | os.PathLike): return periods binary file name\nReturns:\n    return_periods (ndarray[np.int32]): Return Periods\n    use_return_period_file (bool): Bool to use Return Period File",
      "content": "# File: oasislmf/pytools/common/input_files.py\n# function: read_returnperiods (lines 442-480)\n\ndef read_returnperiods(use_return_period_file, run_dir, filename=RETURNPERIODS_FILE):\n    \"\"\"Returns an array of return periods decreasing order with no duplicates.\n    Args:\n        use_return_period_file (bool): Bool to use Return Period File\n        run_dir (str | os.PathLike): Path to input files dir\n        filename (str | os.PathLike): return periods binary file name\n    Returns:\n        return_periods (ndarray[np.int32]): Return Periods\n        use_return_period_file (bool): Bool to use Return Period File\n    \"\"\"\n    if not use_return_period_file:\n        return np.array([], dtype=returnperiods_dtype)[\"return_period\"], use_return_period_file\n    returnperiods_fp = Path(run_dir, filename)\n\n    if not returnperiods_fp.exists():\n        raise RuntimeError(f\"ERROR: Return Periods file not found at {returnperiods_fp}.\")\n\n    returnperiods = load_as_ndarray(\n        run_dir,\n        filename[:-4],\n        returnperiods_dtype,\n        must_exist=False\n    )\n\n    if len(returnperiods) == 0:\n        logger.warning(f\"WARNING: Empty return periods file at {returnperiods_fp}. Running without defined return periods option\")\n        return None, False\n\n    # Check return periods validity\n    # Return periods should be unique and decreasing in order\n    if len(returnperiods) != len(np.unique(returnperiods)):\n        raise RuntimeError(f\"ERROR: Invalid return periods file. Duplicate return periods found: {returnperiods}\")\n    lastrp = -1\n    for rp in returnperiods[\"return_period\"]:\n        if lastrp != -1 and lastrp <= rp:\n            raise RuntimeError(f\"ERROR: Invalid return periods file. Non-decreasing return periods found: {returnperiods}\")\n        lastrp = rp\n\n    return returnperiods[\"return_period\"], use_return_period_file\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturns an array of return periods decreasing order with no duplicates.\nArgs:\n    use_return_period_file (bool): Bool to use Return Period File\n    run_dir (str | os.PathLike): Path to input files dir\n    filename (str | os.PathLike): return periods binary file name\nReturns:\n    return_periods (ndarray[np.int32]): Return Periods\n    use_return_period_file (bool): Bool to use Return Period File"
    },
    {
      "chunk_id": "oasislmf/pytools/common/utils/nb_heapq.py::heap_push@79",
      "source_type": "code",
      "path": "oasislmf/pytools/common/utils/nb_heapq.py",
      "symbol_type": "function",
      "name": "heap_push",
      "lineno": 79,
      "end_lineno": 85,
      "business_stage": "other",
      "docstring": "Heapq heappush",
      "content": "# File: oasislmf/pytools/common/utils/nb_heapq.py\n# function: heap_push (lines 79-85)\n\ndef heap_push(heap, size, element):\n    \"\"\"Heapq heappush\"\"\"\n    if size >= len(heap):\n        heap = _resize_heap(heap, len(heap))\n    heap[size] = element\n    _sift_down(heap, 0, size)\n    return heap, size + 1\n\n\"\"\"Docstring (excerpt)\"\"\"\nHeapq heappush"
    },
    {
      "chunk_id": "oasislmf/pytools/common/utils/nb_heapq.py::heap_pop@89",
      "source_type": "code",
      "path": "oasislmf/pytools/common/utils/nb_heapq.py",
      "symbol_type": "function",
      "name": "heap_pop",
      "lineno": 89,
      "end_lineno": 99,
      "business_stage": "other",
      "docstring": "Heapq heappop",
      "content": "# File: oasislmf/pytools/common/utils/nb_heapq.py\n# function: heap_pop (lines 89-99)\n\ndef heap_pop(heap, size):\n    \"\"\"Heapq heappop\"\"\"\n    if size <= 0:\n        raise ValueError(\"Heap underflow: Cannot pop from an empty heap.\")\n    lastelt = heap[size - 1].copy()\n    if size - 1 > 0:\n        returnitem = heap[0].copy()\n        heap[0] = lastelt\n        _sift_up(heap, 0, size - 1)\n        return returnitem, heap, size - 1\n    return lastelt, heap, size - 1\n\n\"\"\"Docstring (excerpt)\"\"\"\nHeapq heappop"
    },
    {
      "chunk_id": "oasislmf/pytools/common/utils/nb_heapq.py::init_heap@103",
      "source_type": "code",
      "path": "oasislmf/pytools/common/utils/nb_heapq.py",
      "symbol_type": "function",
      "name": "init_heap",
      "lineno": 103,
      "end_lineno": 110,
      "business_stage": "other",
      "docstring": "Initialise heap with (num_compare + 2) * num_rows elements, where\nnum_compare is the number of elements to order by in lexicographical\norder, and the remaining two elements are the file and row idxs\n\nCurrently limited to dtype np.int32 as it is tricky to use custom dtypes",
      "content": "# File: oasislmf/pytools/common/utils/nb_heapq.py\n# function: init_heap (lines 103-110)\n\ndef init_heap(num_rows=4, num_compare=1):\n    \"\"\"Initialise heap with (num_compare + 2) * num_rows elements, where\n    num_compare is the number of elements to order by in lexicographical\n    order, and the remaining two elements are the file and row idxs\n\n    Currently limited to dtype np.int32 as it is tricky to use custom dtypes\n    \"\"\"\n    return np.zeros((num_rows, num_compare + 2), dtype=np.int32)\n\n\"\"\"Docstring (excerpt)\"\"\"\nInitialise heap with (num_compare + 2) * num_rows elements, where\nnum_compare is the number of elements to order by in lexicographical\norder, and the remaining two elements are the file and row idxs\n\nCurrently limited to dtype np.int32 as it is tricky to use custom dtypes"
    },
    {
      "chunk_id": "oasislmf/pytools/converters/data.py::get_tools_by_cli@8",
      "source_type": "code",
      "path": "oasislmf/pytools/converters/data.py",
      "symbol_type": "function",
      "name": "get_tools_by_cli",
      "lineno": 8,
      "end_lineno": 10,
      "business_stage": "other",
      "docstring": "Return a list of tool names supported by the given CLI command.",
      "content": "# File: oasislmf/pytools/converters/data.py\n# function: get_tools_by_cli (lines 8-10)\n\ndef get_tools_by_cli(tools_info: dict, cli_command: str) -> list[str]:\n    \"\"\"Return a list of tool names supported by the given CLI command.\"\"\"\n    return [tool for tool, info in tools_info.items() if cli_command in info.get(\"cli_support\", [])]\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturn a list of tool names supported by the given CLI command."
    },
    {
      "chunk_id": "oasislmf/pytools/converters/data.py::build_tool_info@13",
      "source_type": "code",
      "path": "oasislmf/pytools/converters/data.py",
      "symbol_type": "function",
      "name": "build_tool_info",
      "lineno": 13,
      "end_lineno": 32,
      "business_stage": "other",
      "docstring": "Generates the tools information dictionary for conversion tools from the files in TOOL_MODULE_PACKAGE\n\nReturns:\n    tool_indo (dict): TOOL_INFO dict",
      "content": "# File: oasislmf/pytools/converters/data.py\n# function: build_tool_info (lines 13-32)\n\ndef build_tool_info():\n    \"\"\" Generates the tools information dictionary for conversion tools from the files in TOOL_MODULE_PACKAGE\n\n    Returns:\n        tool_indo (dict): TOOL_INFO dict\n    \"\"\"\n    tool_info = {}\n\n    package = importlib.import_module(TOOL_MODULE_PACKAGE)\n    for _, module_name, _ in pkgutil.iter_modules(package.__path__):\n        mod = importlib.import_module(f\"{TOOL_MODULE_PACKAGE}.{module_name}\")\n\n        tool_info[module_name] = {\n            \"headers\": getattr(mod, \"headers\", None),\n            \"dtype\": getattr(mod, \"dtype\", None),\n            \"fmt\": getattr(mod, \"fmt\", None),\n            \"cli_support\": getattr(mod, \"cli_support\", []),\n        }\n\n    return tool_info\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerates the tools information dictionary for conversion tools from the files in TOOL_MODULE_PACKAGE\n\nReturns:\n    tool_indo (dict): TOOL_INFO dict"
    },
    {
      "chunk_id": "oasislmf/pytools/converters/bintocsv/manager.py::bintocsv@63",
      "source_type": "code",
      "path": "oasislmf/pytools/converters/bintocsv/manager.py",
      "symbol_type": "function",
      "name": "bintocsv",
      "lineno": 63,
      "end_lineno": 75,
      "business_stage": "other",
      "docstring": "Convert bin file to csv file based on file type\nArgs:\n    file_in (str | os.PathLike): Input file path\n    file_out (str | os.PathLike): Output file path\n    file_type (str): File type str from SUPPORTED_BINTOCSV\n    noheader (bool): Bool to not output header. Defaults to False.",
      "content": "# File: oasislmf/pytools/converters/bintocsv/manager.py\n# function: bintocsv (lines 63-75)\n\ndef bintocsv(file_in, file_out, file_type, noheader=False, **kwargs):\n    \"\"\"Convert bin file to csv file based on file type\n    Args:\n        file_in (str | os.PathLike): Input file path\n        file_out (str | os.PathLike): Output file path\n        file_type (str): File type str from SUPPORTED_BINTOCSV\n        noheader (bool): Bool to not output header. Defaults to False.\n    \"\"\"\n    with ExitStack() as stack:\n        file_out = resolve_file(file_out, \"w\", stack)\n\n        tocsv_func = TOCSV_FUNC_MAP.get(file_type, default_tocsv)\n        tocsv_func(stack, file_in, file_out, file_type, noheader, **kwargs)\n\n\"\"\"Docstring (excerpt)\"\"\"\nConvert bin file to csv file based on file type\nArgs:\n    file_in (str | os.PathLike): Input file path\n    file_out (str | os.PathLike): Output file path\n    file_type (str): File type str from SUPPORTED_BINTOCSV\n    noheader (bool): Bool to not output header. Defaults to False."
    },
    {
      "chunk_id": "oasislmf/pytools/converters/bintocsv/utils/cdf.py::get_cdf_data@16",
      "source_type": "code",
      "path": "oasislmf/pytools/converters/bintocsv/utils/cdf.py",
      "symbol_type": "function",
      "name": "get_cdf_data",
      "lineno": 16,
      "end_lineno": 44,
      "business_stage": "other",
      "docstring": "Get the cdf data produced by getmodel.\nNote that the input arrays are lists of cdf entries, namely\nthe shape on axis=0 is the number of entries.\nArgs:\n    event_id (int): event_id\n    damagecdfrecs (ndarray[damagecdfrec]): cdf record keys\n    recs (ndarray[ProbMean]): cdf record values\n    rec_idx_ptr (ndarray[int]): array with the indices of `rec` where each cdf record starts.\n    cdf_dtype (np.dtype): cdf numpy dtype.\nReturns:\n    data (ndarray[cdf_dtype]): cdf data extracted from recs/getmodel.",
      "content": "# File: oasislmf/pytools/converters/bintocsv/utils/cdf.py\n# function: get_cdf_data (lines 16-44)\n\ndef get_cdf_data(event_id, damagecdfrecs, recs, rec_idx_ptr, cdf_dtype):\n    \"\"\"Get the cdf data produced by getmodel.\n    Note that the input arrays are lists of cdf entries, namely\n    the shape on axis=0 is the number of entries.\n    Args:\n        event_id (int): event_id\n        damagecdfrecs (ndarray[damagecdfrec]): cdf record keys\n        recs (ndarray[ProbMean]): cdf record values\n        rec_idx_ptr (ndarray[int]): array with the indices of `rec` where each cdf record starts.\n        cdf_dtype (np.dtype): cdf numpy dtype.\n    Returns:\n        data (ndarray[cdf_dtype]): cdf data extracted from recs/getmodel.\n    \"\"\"\n    assert len(damagecdfrecs) == len(rec_idx_ptr) - 1, \"Number of cdfrecs groups does not match number of cdf keys found\"\n\n    data = np.zeros(len(recs), dtype=cdf_dtype)\n    Nbins = len(rec_idx_ptr) - 1\n    idx = 0\n    for group_idx in range(Nbins):\n        areaperil_id, vulnerability_id = damagecdfrecs[group_idx]\n        for bin_index, rec in enumerate(recs[rec_idx_ptr[group_idx]:rec_idx_ptr[group_idx + 1]]):\n            data[idx][\"event_id\"] = event_id\n            data[idx][\"areaperil_id\"] = areaperil_id\n            data[idx][\"vulnerability_id\"] = vulnerability_id\n            data[idx][\"bin_index\"] = bin_index + 1\n            data[idx][\"prob_to\"] = rec[\"prob_to\"]\n            data[idx][\"bin_mean\"] = rec[\"bin_mean\"]\n            idx += 1\n    return data\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet the cdf data produced by getmodel.\nNote that the input arrays are lists of cdf entries, namely\nthe shape on axis=0 is the number of entries.\nArgs:\n    event_id (int): event_id\n    damagecdfrecs (ndarray[damagecdfrec]): cdf record keys\n    recs (ndarray[ProbMean]): cdf record values\n    rec_idx_ptr (ndarray[int]): array with the indices of `rec` where each cdf record starts.\n    cdf_dtype (np.dtype): cdf numpy dtype.\nReturns:\n    data (ndarray[cdf_dtype]): cdf data extracted from recs/getmodel."
    },
    {
      "chunk_id": "oasislmf/pytools/converters/bintoparquet/manager.py::bintoparquet@32",
      "source_type": "code",
      "path": "oasislmf/pytools/converters/bintoparquet/manager.py",
      "symbol_type": "function",
      "name": "bintoparquet",
      "lineno": 32,
      "end_lineno": 41,
      "business_stage": "other",
      "docstring": "Convert bin file to parquet file based on file type\nArgs:\n    file_in (str | os.PathLike): Input file path\n    file_out (str | os.PathLike): Output file path\n    file_type (str): File type str from SUPPORTED_BINTOPARQUET",
      "content": "# File: oasislmf/pytools/converters/bintoparquet/manager.py\n# function: bintoparquet (lines 32-41)\n\ndef bintoparquet(file_in, file_out, file_type, **kwargs):\n    \"\"\"Convert bin file to parquet file based on file type\n    Args:\n        file_in (str | os.PathLike): Input file path\n        file_out (str | os.PathLike): Output file path\n        file_type (str): File type str from SUPPORTED_BINTOPARQUET\n    \"\"\"\n    with ExitStack() as stack:\n        file_out = resolve_file(file_out, \"wb\", stack)\n        default_toparquet(stack, file_in, file_out, file_type, **kwargs)\n\n\"\"\"Docstring (excerpt)\"\"\"\nConvert bin file to parquet file based on file type\nArgs:\n    file_in (str | os.PathLike): Input file path\n    file_out (str | os.PathLike): Output file path\n    file_type (str): File type str from SUPPORTED_BINTOPARQUET"
    },
    {
      "chunk_id": "oasislmf/pytools/converters/csvtobin/manager.py::csvtobin@50",
      "source_type": "code",
      "path": "oasislmf/pytools/converters/csvtobin/manager.py",
      "symbol_type": "function",
      "name": "csvtobin",
      "lineno": 50,
      "end_lineno": 61,
      "business_stage": "other",
      "docstring": "Convert csv file to bin file based on file type\nArgs:\n    file_in (str | os.PathLike): Input file path\n    file_out (str | os.PathLike): Output file path\n    file_type (str): File type str from SUPPORTED_CSVTOBIN",
      "content": "# File: oasislmf/pytools/converters/csvtobin/manager.py\n# function: csvtobin (lines 50-61)\n\ndef csvtobin(file_in, file_out, file_type, **kwargs):\n    \"\"\"Convert csv file to bin file based on file type\n    Args:\n        file_in (str | os.PathLike): Input file path\n        file_out (str | os.PathLike): Output file path\n        file_type (str): File type str from SUPPORTED_CSVTOBIN\n    \"\"\"\n    with ExitStack() as stack:\n        file_out = resolve_file(file_out, \"wb\", stack)\n\n        tobin_func = TOBIN_FUNC_MAP.get(file_type, default_tobin)\n        tobin_func(stack, file_in, file_out, file_type, **kwargs)\n\n\"\"\"Docstring (excerpt)\"\"\"\nConvert csv file to bin file based on file type\nArgs:\n    file_in (str | os.PathLike): Input file path\n    file_out (str | os.PathLike): Output file path\n    file_type (str): File type str from SUPPORTED_CSVTOBIN"
    },
    {
      "chunk_id": "oasislmf/pytools/converters/parquettobin/manager.py::parquettobin@23",
      "source_type": "code",
      "path": "oasislmf/pytools/converters/parquettobin/manager.py",
      "symbol_type": "function",
      "name": "parquettobin",
      "lineno": 23,
      "end_lineno": 32,
      "business_stage": "other",
      "docstring": "Convert parquet file to bin file based on file type\nArgs:\n    file_in (str | os.PathLike): Input file path\n    file_out (str | os.PathLike): Output file path\n    file_type (str): File type str from SUPPORTED_PARQUETTOBIN",
      "content": "# File: oasislmf/pytools/converters/parquettobin/manager.py\n# function: parquettobin (lines 23-32)\n\ndef parquettobin(file_in, file_out, file_type, **kwargs):\n    \"\"\"Convert parquet file to bin file based on file type\n    Args:\n        file_in (str | os.PathLike): Input file path\n        file_out (str | os.PathLike): Output file path\n        file_type (str): File type str from SUPPORTED_PARQUETTOBIN\n    \"\"\"\n    with ExitStack() as stack:\n        file_out = resolve_file(file_out, \"wb\", stack)\n        default_tobin(stack, file_in, file_out, file_type, **kwargs)\n\n\"\"\"Docstring (excerpt)\"\"\"\nConvert parquet file to bin file based on file type\nArgs:\n    file_in (str | os.PathLike): Input file path\n    file_out (str | os.PathLike): Output file path\n    file_type (str): File type str from SUPPORTED_PARQUETTOBIN"
    },
    {
      "chunk_id": "oasislmf/pytools/data_layer/footprint_layer.py::OperationEnum@34",
      "source_type": "code",
      "path": "oasislmf/pytools/data_layer/footprint_layer.py",
      "symbol_type": "class",
      "name": "OperationEnum",
      "lineno": 34,
      "end_lineno": 43,
      "business_stage": "other",
      "docstring": "Defines the different types of operations supported via bytes. To be passed through TCP port first to tell the\nserver what type of operation is required.",
      "content": "# File: oasislmf/pytools/data_layer/footprint_layer.py\n# class: OperationEnum (lines 34-43)\n\nclass OperationEnum(Enum):\n    \"\"\"\n    Defines the different types of operations supported via bytes. To be passed through TCP port first to tell the\n    server what type of operation is required.\n    \"\"\"\n    GET_DATA = (1).to_bytes(4, byteorder='big')\n    GET_NUM_INTENSITY_BINS = (2).to_bytes(4, byteorder='big')\n    REGISTER = (3).to_bytes(4, byteorder='big')\n    UNREGISTER = (4).to_bytes(4, byteorder='big')\n    POLL_DATA = (5).to_bytes(4, byteorder='big')\n\n\"\"\"Docstring (excerpt)\"\"\"\nDefines the different types of operations supported via bytes. To be passed through TCP port first to tell the\nserver what type of operation is required."
    },
    {
      "chunk_id": "oasislmf/pytools/data_layer/footprint_layer.py::FootprintLayer@46",
      "source_type": "code",
      "path": "oasislmf/pytools/data_layer/footprint_layer.py",
      "symbol_type": "class",
      "name": "FootprintLayer",
      "lineno": 46,
      "end_lineno": 202,
      "business_stage": "other",
      "docstring": "This class is responsible for accessing the footprint data via TCP ports.\n\nAttributes:\n    static_path (str): path to the static file to load the data\n    ignore_file_type (Set[str]): collection of file types to ignore when loading\n    file_data (Optional[Footprint]): footprint object to load\n    socket (Optional[socket.socket]): the TCP socket in which data is sent\n    count (int): the number of processes currently relying on the server\n    total_expected (int): the total number of reliant processes expected\n    total_served (int): the total number of processes that have ever registered through the server's lifetime",
      "content": "# File: oasislmf/pytools/data_layer/footprint_layer.py\n# class: FootprintLayer (lines 46-202)\n\nclass FootprintLayer:\n    \"\"\"\n    This class is responsible for accessing the footprint data via TCP ports.\n\n    Attributes:\n        static_path (str): path to the static file to load the data\n        ignore_file_type (Set[str]): collection of file types to ignore when loading\n        file_data (Optional[Footprint]): footprint object to load\n        socket (Optional[socket.socket]): the TCP socket in which data is sent\n        count (int): the number of processes currently relying on the server\n        total_expected (int): the total number of reliant processes expected\n        total_served (int): the total number of processes that have ever registered through the server's lifetime\n    \"\"\"\n\n    def __init__(\n        self,\n        storage: BaseStorage,\n        total_expected: int,\n        ignore_file_type: Set[str] = set(),\n        df_engine=\"oasis_data_manager.df_reader.reader.OasisPandasReader\",\n    ) -> None:\n        \"\"\"\n        The constructor for the FootprintLayer class.\n\n        Args:\n            ignore_file_type: (Set[str]) collection of file types to ignore when loading\n            total_expected: (int) the total number of reliant processes expected\n\n        \"\"\"\n        self.storage = storage\n        self.ignore_file_type: Set[str] = ignore_file_type\n        self.file_data: Optional[Footprint] = None\n        self.socket: Optional[socket.socket] = None\n        self.count: int = 0\n        self.total_expected: int = total_expected\n        self.total_served: int = 0\n        self.df_engine = df_engine\n        self._define_socket()\n\n    def _define_socket(self) -> None:\n        \"\"\"\n        Defines the self.socket attribute to the port and localhost.\n\n        Returns: None\n        \"\"\"\n        logging.info(f\"defining socket for TCP server: {datetime.datetime.now()}\")\n        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        server_address = (TCP_IP, TCP_PORT)\n        self.socket.bind(server_address)\n        self.socket.listen(PROCESSES_SUPPORTED)\n\n    def _establish_shutdown_procedure(self) -> None:\n        \"\"\"\n        Establishes the steps for shutdown, writing the pointer, and making sure that the pointer will be deleted\n        and the self.socket is shutdown once the process running the server is exited.\n\n        Returns: None\n        \"\"\"\n        logging.info(f\"establishing shutdown procedure: {datetime.datetime.now()}\")\n        # atexit.register(_shutdown_port, self.socket)\n        pass\n\n    @staticmethod\n    def _stream_footprint_data(event_data: np.array, connection: socket.socket, event_id: int) -> None:\n        \"\"\"\n        Serialises data then splits it into chunks of 500 in turn streaming through a connection.\n\n        Args:\n            event_data: (np.array) the data to be serialised and streamed through a connection\n            connection: (socket.socket) the connection that the data is going to be streamed through\n\n        Returns: None\n        \"\"\"\n        raw_data: bytes = pickle.dumps(event_data)\n\n        raw_data_buffer: List[bytes] = [raw_data[i:i + 60000] for i in range(0, len(raw_data), 60000)]\n\n        for chunk in raw_data_buffer:\n            connection.sendall(chunk)\n\n\"\"\"Docstring (excerpt)\"\"\"\nThis class is responsible for accessing the footprint data via TCP ports.\n\nAttributes:\n    static_path (str): path to the static file to load the data\n    ignore_file_type (Set[str]): collection of file types to ignore when loading\n    file_data (Optional[Footprint]): footprint object to load\n    socket (Optional[socket.socket]): the TCP socket in which data is sent\n    count (int): the number of processes currently relying on the server\n    total_expected (int): the total number of reliant processes expected\n    total_served (int): the total number of processes that have ever registered through the server's lifetime"
    },
    {
      "chunk_id": "oasislmf/pytools/data_layer/footprint_layer.py::FootprintLayerClient@205",
      "source_type": "code",
      "path": "oasislmf/pytools/data_layer/footprint_layer.py",
      "symbol_type": "class",
      "name": "FootprintLayerClient",
      "lineno": 205,
      "end_lineno": 313,
      "business_stage": "other",
      "docstring": "This class is responsible for connecting to the footprint server via TCP.",
      "content": "# File: oasislmf/pytools/data_layer/footprint_layer.py\n# class: FootprintLayerClient (lines 205-313)\n\nclass FootprintLayerClient:\n    \"\"\"\n    This class is responsible for connecting to the footprint server via TCP.\n    \"\"\"\n    @classmethod\n    def poll(cls) -> bool:\n        \"\"\"\n        Checks to see if data server is running.\n\n        Returns: (bool)\n        \"\"\"\n        try:\n            _ = cls._get_socket()\n            return True\n        except ConnectionRefusedError as e:\n            logging.error('Failed to find server: {}'.format(e))\n            return False\n\n    @classmethod\n    def _get_socket(cls) -> socket.socket:\n        \"\"\"\n        Gets the socket using the cls.TCP_IP and cls.TCP_PORT.\n\n        Returns: (socket.socket) a connected socket\n        \"\"\"\n        current_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        current_socket.connect((TCP_IP, TCP_PORT))\n        current_socket.settimeout(10)\n        return current_socket\n\n    @classmethod\n    def _define_shutdown_procedure(cls) -> None:\n        \"\"\"\n        Unregisters the client to the server on exit of the process.\n\n        Returns: None\n        \"\"\"\n        atexit.register(cls.unregister)\n\n    @classmethod\n    def register(cls) -> None:\n        \"\"\"\n        Registers the client with the server.\n\n        Returns: None\n        \"\"\"\n        connection_viable: bool = False\n        while connection_viable is False:\n            connection_viable = FootprintLayerClient.poll()\n\n        current_socket = cls._get_socket()\n        data: bytes = OperationEnum.REGISTER.value\n        current_socket.sendall(data)\n        current_socket.close()\n        # cls._define_shutdown_procedure()\n\n    @classmethod\n    def unregister(cls) -> None:\n        \"\"\"\n        Unregisters the client with the data server.\n\n        Returns: None\n        \"\"\"\n        current_socket = cls._get_socket()\n        data: bytes = OperationEnum.UNREGISTER.value\n        current_socket.sendall(data)\n        current_socket.close()\n        atexit.unregister(cls.unregister)\n\n    @classmethod\n    def get_number_of_intensity_bins(cls) -> int:\n        \"\"\"\n        Gets the number of intensity bins from the footprint data.\n\n        Returns: (int) the number of intensity bins\n        \"\"\"\n        current_socket = cls._get_socket()\n\n        data: bytes = OperationEnum.GET_NUM_INTENSITY_BINS.value\n        current_socket.sendall(data)\n\n\"\"\"Docstring (excerpt)\"\"\"\nThis class is responsible for connecting to the footprint server via TCP."
    },
    {
      "chunk_id": "oasislmf/pytools/data_layer/footprint_layer.py::listen@149",
      "source_type": "code",
      "path": "oasislmf/pytools/data_layer/footprint_layer.py",
      "symbol_type": "function",
      "name": "listen",
      "lineno": 149,
      "end_lineno": 202,
      "business_stage": "other",
      "docstring": "Fires off the process with an event loop serving footprint data.\n\nReturns: None",
      "content": "# File: oasislmf/pytools/data_layer/footprint_layer.py\n# function: listen (lines 149-202)\n\n    def listen(self) -> None:\n        \"\"\"\n        Fires off the process with an event loop serving footprint data.\n\n        Returns: None\n        \"\"\"\n        self._establish_shutdown_procedure()\n\n        with ExitStack() as stack:\n            footprint_obj = stack.enter_context(Footprint.load(self.storage,\n                                                               ignore_file_type=self.ignore_file_type,\n                                                               df_engine=self.df_engine))\n            self.file_data = footprint_obj\n            while True:\n                try:\n                    connection, client_address = self.socket.accept()\n                    data = connection.recv(16)\n\n                    if data:\n                        operation, event_id = self._extract_header(header_data=data)\n\n                        if operation == OperationEnum.GET_DATA:\n                            event_data = self.file_data.get_event(event_id=event_id)\n\n                            if event_id in self.file_data.footprint_index:\n                                logging.info(f'event_id \"{event_id}\" retrieved from footprint index')\n                                del self.file_data.footprint_index[event_id]\n                            else:\n                                logging.error(f'event_id \"{event_id}\" not in footprint_index')\n\n                            FootprintLayer._stream_footprint_data(event_data=event_data, connection=connection, event_id=event_id)\n\n                        elif operation == OperationEnum.GET_NUM_INTENSITY_BINS:\n\n                            number_of_intensity_bins = self.file_data.num_intensity_bins\n                            connection.sendall(number_of_intensity_bins.to_bytes(8, byteorder='big'))\n\n                        elif operation == OperationEnum.REGISTER:\n                            self.count += 1\n                            self.total_served += 1\n                            logging.info(f\"connection registered: {self.count} for {client_address} {datetime.datetime.now()}\")\n\n                        elif operation == OperationEnum.UNREGISTER:\n                            self.count -= 1\n                            logging.info(f\"connection unregistered: {self.count} for {client_address} {datetime.datetime.now()}\")\n                            if self.count <= 0 and self.total_expected == self.total_served:\n                                logging.info(f\"breaking event loop: {datetime.datetime.now()}\")\n                                self.socket.shutdown(socket.SHUT_RDWR)\n                                break\n                        connection.close()\n                # Catch all errors, send to logger and keep running\n                except Exception as e:\n                    logging.error(e)\n            connection.close()\n\n\"\"\"Docstring (excerpt)\"\"\"\nFires off the process with an event loop serving footprint data.\n\nReturns: None"
    },
    {
      "chunk_id": "oasislmf/pytools/data_layer/footprint_layer.py::poll@210",
      "source_type": "code",
      "path": "oasislmf/pytools/data_layer/footprint_layer.py",
      "symbol_type": "function",
      "name": "poll",
      "lineno": 210,
      "end_lineno": 221,
      "business_stage": "other",
      "docstring": "Checks to see if data server is running.\n\nReturns: (bool)",
      "content": "# File: oasislmf/pytools/data_layer/footprint_layer.py\n# function: poll (lines 210-221)\n\n    def poll(cls) -> bool:\n        \"\"\"\n        Checks to see if data server is running.\n\n        Returns: (bool)\n        \"\"\"\n        try:\n            _ = cls._get_socket()\n            return True\n        except ConnectionRefusedError as e:\n            logging.error('Failed to find server: {}'.format(e))\n            return False\n\n\"\"\"Docstring (excerpt)\"\"\"\nChecks to see if data server is running.\n\nReturns: (bool)"
    },
    {
      "chunk_id": "oasislmf/pytools/data_layer/footprint_layer.py::register@245",
      "source_type": "code",
      "path": "oasislmf/pytools/data_layer/footprint_layer.py",
      "symbol_type": "function",
      "name": "register",
      "lineno": 245,
      "end_lineno": 258,
      "business_stage": "other",
      "docstring": "Registers the client with the server.\n\nReturns: None",
      "content": "# File: oasislmf/pytools/data_layer/footprint_layer.py\n# function: register (lines 245-258)\n\n    def register(cls) -> None:\n        \"\"\"\n        Registers the client with the server.\n\n        Returns: None\n        \"\"\"\n        connection_viable: bool = False\n        while connection_viable is False:\n            connection_viable = FootprintLayerClient.poll()\n\n        current_socket = cls._get_socket()\n        data: bytes = OperationEnum.REGISTER.value\n        current_socket.sendall(data)\n        current_socket.close()\n\n\"\"\"Docstring (excerpt)\"\"\"\nRegisters the client with the server.\n\nReturns: None"
    },
    {
      "chunk_id": "oasislmf/pytools/data_layer/footprint_layer.py::unregister@262",
      "source_type": "code",
      "path": "oasislmf/pytools/data_layer/footprint_layer.py",
      "symbol_type": "function",
      "name": "unregister",
      "lineno": 262,
      "end_lineno": 272,
      "business_stage": "other",
      "docstring": "Unregisters the client with the data server.\n\nReturns: None",
      "content": "# File: oasislmf/pytools/data_layer/footprint_layer.py\n# function: unregister (lines 262-272)\n\n    def unregister(cls) -> None:\n        \"\"\"\n        Unregisters the client with the data server.\n\n        Returns: None\n        \"\"\"\n        current_socket = cls._get_socket()\n        data: bytes = OperationEnum.UNREGISTER.value\n        current_socket.sendall(data)\n        current_socket.close()\n        atexit.unregister(cls.unregister)\n\n\"\"\"Docstring (excerpt)\"\"\"\nUnregisters the client with the data server.\n\nReturns: None"
    },
    {
      "chunk_id": "oasislmf/pytools/data_layer/footprint_layer.py::get_number_of_intensity_bins@275",
      "source_type": "code",
      "path": "oasislmf/pytools/data_layer/footprint_layer.py",
      "symbol_type": "function",
      "name": "get_number_of_intensity_bins",
      "lineno": 275,
      "end_lineno": 287,
      "business_stage": "other",
      "docstring": "Gets the number of intensity bins from the footprint data.\n\nReturns: (int) the number of intensity bins",
      "content": "# File: oasislmf/pytools/data_layer/footprint_layer.py\n# function: get_number_of_intensity_bins (lines 275-287)\n\n    def get_number_of_intensity_bins(cls) -> int:\n        \"\"\"\n        Gets the number of intensity bins from the footprint data.\n\n        Returns: (int) the number of intensity bins\n        \"\"\"\n        current_socket = cls._get_socket()\n\n        data: bytes = OperationEnum.GET_NUM_INTENSITY_BINS.value\n        current_socket.sendall(data)\n        intensity_bins_data = current_socket.recv(8)\n        current_socket.close()\n        return int.from_bytes(intensity_bins_data, 'big')\n\n\"\"\"Docstring (excerpt)\"\"\"\nGets the number of intensity bins from the footprint data.\n\nReturns: (int) the number of intensity bins"
    },
    {
      "chunk_id": "oasislmf/pytools/data_layer/footprint_layer.py::get_event@290",
      "source_type": "code",
      "path": "oasislmf/pytools/data_layer/footprint_layer.py",
      "symbol_type": "function",
      "name": "get_event",
      "lineno": 290,
      "end_lineno": 313,
      "business_stage": "other",
      "docstring": "Gets the footprint data from the footprint based off the event_id.\n\nArgs:\n    event_id: (int) the event ID of the data required\n\nReturns: (np.array) footprint data based off the event_id",
      "content": "# File: oasislmf/pytools/data_layer/footprint_layer.py\n# function: get_event (lines 290-313)\n\n    def get_event(cls, event_id: int) -> np.array:\n        \"\"\"\n        Gets the footprint data from the footprint based off the event_id.\n\n        Args:\n            event_id: (int) the event ID of the data required\n\n        Returns: (np.array) footprint data based off the event_id\n        \"\"\"\n        current_socket = cls._get_socket()\n\n        data: bytes = OperationEnum.GET_DATA.value + int(event_id).to_bytes(8, byteorder='big')\n        current_socket.sendall(data)\n\n        raw_data_buffer: List[bytes] = []\n\n        while True:\n            raw_data = current_socket.recv(6000)\n            if not raw_data:\n                break\n            raw_data_buffer.append(raw_data)\n\n        if raw_data_buffer:\n            return pickle.loads(b\"\".join(raw_data_buffer))\n\n\"\"\"Docstring (excerpt)\"\"\"\nGets the footprint data from the footprint based off the event_id.\n\nArgs:\n    event_id: (int) the event ID of the data required\n\nReturns: (np.array) footprint data based off the event_id"
    },
    {
      "chunk_id": "oasislmf/pytools/data_layer/conversions/footprint.py::convert_bin_to_parquet_event@16",
      "source_type": "code",
      "path": "oasislmf/pytools/data_layer/conversions/footprint.py",
      "symbol_type": "function",
      "name": "convert_bin_to_parquet_event",
      "lineno": 16,
      "end_lineno": 55,
      "business_stage": "other",
      "docstring": "Converts the data from a binary file to a parquet file.\n\nArgs:\n    static_path: (str) the path to the static file\n\nReturns: None",
      "content": "# File: oasislmf/pytools/data_layer/conversions/footprint.py\n# function: convert_bin_to_parquet_event (lines 16-55)\n\ndef convert_bin_to_parquet_event(static_path: str, **kwargs) -> None:\n    \"\"\"\n    Converts the data from a binary file to a parquet file.\n\n    Args:\n        static_path: (str) the path to the static file\n\n    Returns: None\n    \"\"\"\n    root_path = os.path.join(static_path, 'footprint.parquet')\n    try:\n        shutil.rmtree(root_path)\n    except FileNotFoundError:\n        pass\n    with ExitStack() as stack:\n        storage = LocalStorage(\n            root_dir=static_path,\n            cache_dir=None,\n        )\n        footprint_obj = stack.enter_context(Footprint.load(storage,\n                                                           ignore_file_type={'csv', 'parquet'}))\n        index_data = footprint_obj.footprint_index\n\n        meta_data = {\n            \"num_intensity_bins\": footprint_obj.num_intensity_bins,\n            \"has_intensity_uncertainty\": True if footprint_obj.has_intensity_uncertainty == 1 else False\n        }\n\n        for event_id in tqdm(index_data.keys(), desc=\"processing events\"):\n            data_slice = footprint_obj.get_event(event_id)\n            df = pd.DataFrame(data_slice)\n            df[\"event_id\"] = event_id\n            pq.write_to_dataset(\n                pa.Table.from_pandas(df),\n                root_path=root_path,\n                partition_cols=['event_id'],\n                compression=\"BROTLI\"\n            )\n        with storage.open('footprint_parquet_meta.json', 'w') as outfile:\n            json.dump(meta_data, outfile)\n\n\"\"\"Docstring (excerpt)\"\"\"\nConverts the data from a binary file to a parquet file.\n\nArgs:\n    static_path: (str) the path to the static file\n\nReturns: None"
    },
    {
      "chunk_id": "oasislmf/pytools/data_layer/conversions/footprint.py::convert_bin_to_parquet_chunk@58",
      "source_type": "code",
      "path": "oasislmf/pytools/data_layer/conversions/footprint.py",
      "symbol_type": "function",
      "name": "convert_bin_to_parquet_chunk",
      "lineno": 58,
      "end_lineno": 162,
      "business_stage": "other",
      "docstring": "Converts the data from a binary file to a parquet file.\n\nArgs:\n    static_path: (str) the path to the static file\n    chunk_size: target raw size of the partition\n\nReturns: None",
      "content": "# File: oasislmf/pytools/data_layer/conversions/footprint.py\n# function: convert_bin_to_parquet_chunk (lines 58-162)\n\ndef convert_bin_to_parquet_chunk(static_path, chunk_size, **kwargs) -> None:\n    \"\"\"\n    Converts the data from a binary file to a parquet file.\n\n    Args:\n        static_path: (str) the path to the static file\n        chunk_size: target raw size of the partition\n\n    Returns: None\n    \"\"\"\n    root_path = os.path.join(static_path, 'footprint_chunk')\n    try:\n        shutil.rmtree(root_path)\n    except FileNotFoundError:\n        pass\n    os.mkdir(root_path)\n    density = (0.0, 0)\n    with ExitStack() as stack:\n        storage = LocalStorage(\n            root_dir=static_path,\n            cache_dir=None,\n        )\n        footprint_obj = stack.enter_context(\n            Footprint.load(storage, ignore_file_type={'csv', 'parquet'})\n        )\n        index_data = footprint_obj.footprint_index\n        meta_data = {\n            \"num_intensity_bins\": footprint_obj.num_intensity_bins,\n            \"has_intensity_uncertainty\": True\n            if footprint_obj.has_intensity_uncertainty == 1\n            else False\n        }\n\n        event_data = []\n        for event_id in tqdm(index_data.keys(), desc=\"parsing index file\"):\n            data_slice = footprint_obj.get_event(event_id)\n            df = pd.DataFrame(data_slice)\n            min_areaperil_id = min(df['areaperil_id'])\n            max_areaperil_id = max(df['areaperil_id'])\n            event_data.append((min_areaperil_id, max_areaperil_id, event_id))\n            if df.shape[0]:\n                if max_areaperil_id == min_areaperil_id:\n                    cur_density = 1\n                else:\n                    cur_density = df.shape[0] / (max_areaperil_id - min_areaperil_id)\n                density = ((density[0] * density[1] + cur_density) / (density[1] + 1),\n                           density[1] + 1)\n\n        event_data.sort(key=lambda x: x[0])\n\n        current_chunk = []\n        current_size = 0\n        count = 1\n        footprint_lookup = []\n\n        for min_apid, max_apid, event_id in tqdm(event_data, desc=\"processing chunks\"):\n            footprint_lookup.append({\n                'event_id': event_id,\n                'partition': count,\n                'min_areaperil_id': min_apid,\n                'max_areaperil_id': max_apid\n            })  # size?\n\n            data_slice = footprint_obj.get_event(event_id)\n            df = pd.DataFrame(data_slice)\n            df[\"event_id\"] = event_id\n\n            current_chunk.append(df)\n            current_size += df.memory_usage(deep=True).sum()\n\n            if (current_size < chunk_size):\n                continue\n\n            pq.write_table(\n                pa.Table.from_pandas(\n                    pd.concat(current_chunk, ignore_index=True)\n                ),\n                os.path.join(root_path, f'footprint_{count}.parquet'),\n                compression=\"BROTLI\"\n            )\n\n\"\"\"Docstring (excerpt)\"\"\"\nConverts the data from a binary file to a parquet file.\n\nArgs:\n    static_path: (str) the path to the static file\n    chunk_size: target raw size of the partition\n\nReturns: None"
    },
    {
      "chunk_id": "oasislmf/pytools/elt/manager.py::read_input_files@359",
      "source_type": "code",
      "path": "oasislmf/pytools/elt/manager.py",
      "symbol_type": "function",
      "name": "read_input_files",
      "lineno": 359,
      "end_lineno": 384,
      "business_stage": "other",
      "docstring": "Reads all input files and returns a dict of relevant data\nArgs:\n    run_dir (str | os.PathLike): Path to directory containing required files structure\n    compute_melt (bool): Compute MELT bool\n    compute_qelt (bool): Compute QELT bool\n    sample_size (int): Sample size\nReturns:\n    file_data (Dict[str, Any]): A dict of relevent data extracted from files",
      "content": "# File: oasislmf/pytools/elt/manager.py\n# function: read_input_files (lines 359-384)\n\ndef read_input_files(run_dir, compute_melt, compute_qelt, sample_size):\n    \"\"\"Reads all input files and returns a dict of relevant data\n    Args:\n        run_dir (str | os.PathLike): Path to directory containing required files structure\n        compute_melt (bool): Compute MELT bool\n        compute_qelt (bool): Compute QELT bool\n        sample_size (int): Sample size\n    Returns:\n        file_data (Dict[str, Any]): A dict of relevent data extracted from files\n    \"\"\"\n    unique_event_ids = np.array([], dtype=oasis_int)\n    event_rates = np.array([], dtype=oasis_float)\n    include_event_rate = False\n    if compute_melt:\n        unique_event_ids, event_rates = read_event_rates(Path(run_dir, \"input\"))\n        include_event_rate = unique_event_ids.size > 0\n\n    intervals = read_quantile(sample_size, Path(run_dir, \"input\"), return_empty=not compute_qelt)\n\n    file_data = {\n        \"unique_event_ids\": unique_event_ids,\n        \"event_rates\": event_rates,\n        \"include_event_rate\": include_event_rate,\n        \"intervals\": intervals,\n    }\n    return file_data\n\n\"\"\"Docstring (excerpt)\"\"\"\nReads all input files and returns a dict of relevant data\nArgs:\n    run_dir (str | os.PathLike): Path to directory containing required files structure\n    compute_melt (bool): Compute MELT bool\n    compute_qelt (bool): Compute QELT bool\n    sample_size (int): Sample size\nReturns:\n    file_data (Dict[str, Any]): A dict of relevent data extracted from files"
    },
    {
      "chunk_id": "oasislmf/pytools/elt/manager.py::run@387",
      "source_type": "code",
      "path": "oasislmf/pytools/elt/manager.py",
      "symbol_type": "function",
      "name": "run",
      "lineno": 387,
      "end_lineno": 520,
      "business_stage": "other",
      "docstring": "Runs ELT calculations\nArgs:\n    run_dir (str | os.PathLike): Path to directory containing required files structure\n    files_in (list[str]): Path to summary binary input file\n    selt_output_file (str, optional): Path to SELT output file. Defaults to None.\n    melt_output_file (str, optional): Path to MELT output file. Defaults to None.\n    qelt_output_file (str, optional): Path to QELT output file. Defaults to None.\n    noheader (bool): Boolean value to skip header in output file. Defaults to False.\n    output_format (str): Output format extension. Defaults to \"csv\".",
      "content": "# File: oasislmf/pytools/elt/manager.py\n# function: run (lines 387-520)\n\ndef run(\n    run_dir,\n    files_in,\n    selt_output_file=None,\n    melt_output_file=None,\n    qelt_output_file=None,\n    noheader=False,\n    output_format=\"csv\",\n):\n    \"\"\"Runs ELT calculations\n    Args:\n        run_dir (str | os.PathLike): Path to directory containing required files structure\n        files_in (list[str]): Path to summary binary input file\n        selt_output_file (str, optional): Path to SELT output file. Defaults to None.\n        melt_output_file (str, optional): Path to MELT output file. Defaults to None.\n        qelt_output_file (str, optional): Path to QELT output file. Defaults to None.\n        noheader (bool): Boolean value to skip header in output file. Defaults to False.\n        output_format (str): Output format extension. Defaults to \"csv\".\n    \"\"\"\n    outmap = {\n        \"selt\": {\n            \"compute\": selt_output_file is not None,\n            \"file_path\": selt_output_file,\n            \"fmt\": SELT_fmt,\n            \"headers\": SELT_headers,\n            \"file\": None,\n        },\n        \"melt\": {\n            \"compute\": melt_output_file is not None,\n            \"file_path\": melt_output_file,\n            \"fmt\": MELT_fmt,\n            \"headers\": MELT_headers,\n            \"file\": None,\n        },\n        \"qelt\": {\n            \"compute\": qelt_output_file is not None,\n            \"file_path\": qelt_output_file,\n            \"fmt\": QELT_fmt,\n            \"headers\": QELT_headers,\n            \"file\": None,\n        },\n    }\n\n    output_format = \".\" + output_format\n    output_binary = output_format == \".bin\"\n    output_parquet = output_format == \".parquet\"\n    # Check for correct suffix\n    for path in [v[\"file_path\"] for v in outmap.values()]:\n        if path is None:\n            continue\n        if Path(path).suffix == \"\":  # Ignore suffix for pipes\n            continue\n        if (Path(path).suffix != output_format):\n            raise ValueError(f\"Invalid file extension for {output_format}, got {path},\")\n\n    if run_dir is None:\n        run_dir = './work'\n\n    if not all([v[\"compute\"] for v in outmap.values()]):\n        logger.warning(\"No output files specified\")\n\n    with ExitStack() as stack:\n        if files_in == [\"-\"]:\n            files_in = None  # init_streams checks for None to read from sys.stdin.buffer\n\n        streams_in, (stream_source_type, stream_agg_type, len_sample) = init_streams_in(files_in, stack)\n        if stream_source_type != SUMMARY_STREAM_ID:\n            raise Exception(f\"unsupported stream type {stream_source_type}, {stream_agg_type}\")\n\n        file_data = read_input_files(\n            run_dir,\n            outmap[\"melt\"][\"compute\"],\n            outmap[\"qelt\"][\"compute\"],\n            len_sample\n        )\n        elt_reader = ELTReader(\n            len_sample,\n            outmap[\"selt\"][\"compute\"],\n            outmap[\"melt\"][\"compute\"],\n            outmap[\"qelt\"][\"compute\"],\n\n\"\"\"Docstring (excerpt)\"\"\"\nRuns ELT calculations\nArgs:\n    run_dir (str | os.PathLike): Path to directory containing required files structure\n    files_in (list[str]): Path to summary binary input file\n    selt_output_file (str, optional): Path to SELT output file. Defaults to None.\n    melt_output_file (str, optional): Path to MELT output file. Defaults to None.\n    qelt_output_file (str, optional): Path to QELT output file. Defaults to None.\n    noheader (bool): Boolean value to skip header in output file. Defaults to False.\n    output_format (str): Output format extension. Defaults to \"csv\"."
    },
    {
      "chunk_id": "oasislmf/pytools/eve/manager.py::read_events@17",
      "source_type": "code",
      "path": "oasislmf/pytools/eve/manager.py",
      "symbol_type": "function",
      "name": "read_events",
      "lineno": 17,
      "end_lineno": 23,
      "business_stage": "other",
      "docstring": "Read the event IDs from the binary events file.\n\nArgs:\n    input_file (str | os.PathLike): Path to binary events file.",
      "content": "# File: oasislmf/pytools/eve/manager.py\n# function: read_events (lines 17-23)\n\ndef read_events(input_file):\n    \"\"\"Read the event IDs from the binary events file.\n\n    Args:\n        input_file (str | os.PathLike): Path to binary events file.\n    \"\"\"\n    return np.fromfile(input_file, dtype=oasis_int)\n\n\"\"\"Docstring (excerpt)\"\"\"\nRead the event IDs from the binary events file.\n\nArgs:\n    input_file (str | os.PathLike): Path to binary events file."
    },
    {
      "chunk_id": "oasislmf/pytools/eve/manager.py::stream_events@26",
      "source_type": "code",
      "path": "oasislmf/pytools/eve/manager.py",
      "symbol_type": "function",
      "name": "stream_events",
      "lineno": 26,
      "end_lineno": 34,
      "business_stage": "other",
      "docstring": "Stream the output events.\n\nArgs:\n    events (Iterable): Iterable containing the events to stream.\n    stream_out (File object): File object with `write` method for handling output.",
      "content": "# File: oasislmf/pytools/eve/manager.py\n# function: stream_events (lines 26-34)\n\ndef stream_events(events, stream_out):\n    \"\"\"Stream the output events.\n\n    Args:\n        events (Iterable): Iterable containing the events to stream.\n        stream_out (File object): File object with `write` method for handling output.\n    \"\"\"\n    for e in events:\n        stream_out.write(np.int32(e).tobytes())\n\n\"\"\"Docstring (excerpt)\"\"\"\nStream the output events.\n\nArgs:\n    events (Iterable): Iterable containing the events to stream.\n    stream_out (File object): File object with `write` method for handling output."
    },
    {
      "chunk_id": "oasislmf/pytools/eve/manager.py::calculate_events_per_process@37",
      "source_type": "code",
      "path": "oasislmf/pytools/eve/manager.py",
      "symbol_type": "function",
      "name": "calculate_events_per_process",
      "lineno": 37,
      "end_lineno": 41,
      "business_stage": "other",
      "docstring": "Calculate number of events per process.",
      "content": "# File: oasislmf/pytools/eve/manager.py\n# function: calculate_events_per_process (lines 37-41)\n\ndef calculate_events_per_process(n_events, total_processes):\n    \"\"\"Calculate number of events per process.\n    \"\"\"\n    events_per_process, remainder = divmod(n_events, total_processes)\n    return events_per_process + bool(remainder)  # add 1 if remainder\n\n\"\"\"Docstring (excerpt)\"\"\"\nCalculate number of events per process."
    },
    {
      "chunk_id": "oasislmf/pytools/eve/manager.py::partition_events__no_shuffle@44",
      "source_type": "code",
      "path": "oasislmf/pytools/eve/manager.py",
      "symbol_type": "function",
      "name": "partition_events__no_shuffle",
      "lineno": 44,
      "end_lineno": 55,
      "business_stage": "other",
      "docstring": "Assign events in the order they are loaded to each process in turn. Only\noutput the event IDs allocated to the given `process_number`.\n\nArgs:\n    events (np.array): Array of ordered event IDs.\n    process_number (int): The process number to receive a partition of events.\n    total_processes (int): Total number of processes to distribute the events over.",
      "content": "# File: oasislmf/pytools/eve/manager.py\n# function: partition_events__no_shuffle (lines 44-55)\n\ndef partition_events__no_shuffle(events, process_number, total_processes):\n    \"\"\"Assign events in the order they are loaded to each process in turn. Only\n    output the event IDs allocated to the given `process_number`.\n\n    Args:\n        events (np.array): Array of ordered event IDs.\n        process_number (int): The process number to receive a partition of events.\n        total_processes (int): Total number of processes to distribute the events over.\n    \"\"\"\n    events_per_process = calculate_events_per_process(len(events), total_processes)\n    return events[(process_number - 1) * events_per_process:\n                  process_number * events_per_process]\n\n\"\"\"Docstring (excerpt)\"\"\"\nAssign events in the order they are loaded to each process in turn. Only\noutput the event IDs allocated to the given `process_number`.\n\nArgs:\n    events (np.array): Array of ordered event IDs.\n    process_number (int): The process number to receive a partition of events.\n    total_processes (int): Total number of processes to distribute the events over."
    },
    {
      "chunk_id": "oasislmf/pytools/eve/manager.py::partition_events__random_builtin@58",
      "source_type": "code",
      "path": "oasislmf/pytools/eve/manager.py",
      "symbol_type": "function",
      "name": "partition_events__random_builtin",
      "lineno": 58,
      "end_lineno": 72,
      "business_stage": "other",
      "docstring": "Shuffle the events randomly and allocate to each process using builtin\nshuffle. Only output the event IDs to the given `process_number`.\n\nNote that this can be memory intensive. For `len(events) > 10**5` recommend\nusing `partition_events__random`.\n\nArgs:\n    events (np.array): Array of ordered event IDs.\n    process_number (int): The process number to receive a partition of events.\n    total_processes (int): Total number of processes to distribute the events over.",
      "content": "# File: oasislmf/pytools/eve/manager.py\n# function: partition_events__random_builtin (lines 58-72)\n\ndef partition_events__random_builtin(events, process_number, total_processes):\n    \"\"\"Shuffle the events randomly and allocate to each process using builtin\n    shuffle. Only output the event IDs to the given `process_number`.\n\n    Note that this can be memory intensive. For `len(events) > 10**5` recommend\n    using `partition_events__random`.\n\n    Args:\n        events (np.array): Array of ordered event IDs.\n        process_number (int): The process number to receive a partition of events.\n        total_processes (int): Total number of processes to distribute the events over.\n    \"\"\"\n    rng = np.random.default_rng(NUMPY_RANDOM_SEED)\n    rng.shuffle(events)\n    return partition_events__no_shuffle(events, process_number, total_processes)\n\n\"\"\"Docstring (excerpt)\"\"\"\nShuffle the events randomly and allocate to each process using builtin\nshuffle. Only output the event IDs to the given `process_number`.\n\nNote that this can be memory intensive. For `len(events) > 10**5` recommend\nusing `partition_events__random`.\n\nArgs:\n    events (np.array): Array of ordered event IDs.\n    process_number (int): The process number to receive a partition of events.\n    total_processes (int): Total number of processes to distribute the events over."
    },
    {
      "chunk_id": "oasislmf/pytools/eve/manager.py::partition_events__random@75",
      "source_type": "code",
      "path": "oasislmf/pytools/eve/manager.py",
      "symbol_type": "function",
      "name": "partition_events__random",
      "lineno": 75,
      "end_lineno": 97,
      "business_stage": "other",
      "docstring": "Shuffle the events randomly and allocate to each process. Only output\nthe event IDs to the given `process_number`. Generates an iterator.\n\nRandomisation is implemented using the Fisher-Yates algorithm.\n\nArgs:\n    events (np.array): Array of ordered event IDs.\n    process_number (int): The process number to receive a partition of events.\n    total_processes (int): Total number of processes to distribute the events over.",
      "content": "# File: oasislmf/pytools/eve/manager.py\n# function: partition_events__random (lines 75-97)\n\ndef partition_events__random(events, process_number, total_processes):\n    \"\"\"Shuffle the events randomly and allocate to each process. Only output\n    the event IDs to the given `process_number`. Generates an iterator.\n\n    Randomisation is implemented using the Fisher-Yates algorithm.\n\n    Args:\n        events (np.array): Array of ordered event IDs.\n        process_number (int): The process number to receive a partition of events.\n        total_processes (int): Total number of processes to distribute the events over.\n    \"\"\"\n    rng = np.random.default_rng(NUMPY_RANDOM_SEED)\n\n    for i in range(len(events) - 1, 0, -1):\n        j = rng.integers(0, i + 1)\n\n        events[i], events[j] = events[j], events[i]\n\n        if (i - process_number) % total_processes == 0:\n            yield events[i]\n\n    if process_number % total_processes == 0:  # Event at 0 index\n        yield events[0]\n\n\"\"\"Docstring (excerpt)\"\"\"\nShuffle the events randomly and allocate to each process. Only output\nthe event IDs to the given `process_number`. Generates an iterator.\n\nRandomisation is implemented using the Fisher-Yates algorithm.\n\nArgs:\n    events (np.array): Array of ordered event IDs.\n    process_number (int): The process number to receive a partition of events.\n    total_processes (int): Total number of processes to distribute the events over."
    },
    {
      "chunk_id": "oasislmf/pytools/eve/manager.py::partition_events__round_robin@100",
      "source_type": "code",
      "path": "oasislmf/pytools/eve/manager.py",
      "symbol_type": "function",
      "name": "partition_events__round_robin",
      "lineno": 100,
      "end_lineno": 109,
      "business_stage": "other",
      "docstring": "Partition the events sequentially in a round robin style per process.\nOnly output the events allocated to the given `process_number`.\n\nArgs:\n    events (np.array): Array of ordered event IDs.\n    process_number (int): The process number to receive a partition of events.\n    total_processes (int): Total number of processes to distribute the events over.",
      "content": "# File: oasislmf/pytools/eve/manager.py\n# function: partition_events__round_robin (lines 100-109)\n\ndef partition_events__round_robin(events, process_number, total_processes):\n    \"\"\"Partition the events sequentially in a round robin style per process.\n    Only output the events allocated to the given `process_number`.\n\n    Args:\n        events (np.array): Array of ordered event IDs.\n        process_number (int): The process number to receive a partition of events.\n        total_processes (int): Total number of processes to distribute the events over.\n    \"\"\"\n    return events[np.arange(process_number - 1, len(events), total_processes)]\n\n\"\"\"Docstring (excerpt)\"\"\"\nPartition the events sequentially in a round robin style per process.\nOnly output the events allocated to the given `process_number`.\n\nArgs:\n    events (np.array): Array of ordered event IDs.\n    process_number (int): The process number to receive a partition of events.\n    total_processes (int): Total number of processes to distribute the events over."
    },
    {
      "chunk_id": "oasislmf/pytools/eve/manager.py::run@112",
      "source_type": "code",
      "path": "oasislmf/pytools/eve/manager.py",
      "symbol_type": "function",
      "name": "run",
      "lineno": 112,
      "end_lineno": 177,
      "business_stage": "other",
      "docstring": "Generate event ID partitions as a binary data stream with shuffling. By\ndefault the events are shuffled by assiging to processes one by one\ncyclically.\n\nArgs:\n    input_file (str | os.PathLike): Path to binary events file. If None\n    then defaults to DEFAULT_EVENTS_FILE.\n    process_number (int): The process number to receive a partition of events.\n    total_processes (int): Total number of processes to distribute the events over.\n    no_shuffle (bool, optional): Disable shuffling events. Events are split\n        and distributed into blocks in the order they are input. Takes\n        priority over `randomise(_builtin)`.\n    randomise (bool, optional): Shuffle events randomly in the blocks and\n        stream events on the fly. If `no_shuffle` is `True` then it takes\n        priority.\n    randomise_builtin (bool, optional): Shuffle events randomly in the blocks using\n        builtin shuffle. If `no_shuffle` or `randomise` is `True` then it\n        they take priority.\n    output_file (str | os.PathLike): Path to output file. If '-' then outputs to stdout.",
      "content": "# File: oasislmf/pytools/eve/manager.py\n# function: run (lines 112-177)\n\ndef run(input_file, process_number, total_processes, no_shuffle=False,\n        randomise=False, randomise_builtin=False, output_file='-',\n        ):\n    \"\"\"Generate event ID partitions as a binary data stream with shuffling. By\n    default the events are shuffled by assiging to processes one by one\n    cyclically.\n\n    Args:\n        input_file (str | os.PathLike): Path to binary events file. If None\n        then defaults to DEFAULT_EVENTS_FILE.\n        process_number (int): The process number to receive a partition of events.\n        total_processes (int): Total number of processes to distribute the events over.\n        no_shuffle (bool, optional): Disable shuffling events. Events are split\n            and distributed into blocks in the order they are input. Takes\n            priority over `randomise(_builtin)`.\n        randomise (bool, optional): Shuffle events randomly in the blocks and\n            stream events on the fly. If `no_shuffle` is `True` then it takes\n            priority.\n        randomise_builtin (bool, optional): Shuffle events randomly in the blocks using\n            builtin shuffle. If `no_shuffle` or `randomise` is `True` then it\n            they take priority.\n        output_file (str | os.PathLike): Path to output file. If '-' then outputs to stdout.\n    \"\"\"\n    if input_file is None:\n        input_file = DEFAULT_EVENTS_FILE\n\n    # Check input file is valid\n    input_file = Path(input_file)\n    if not input_file.exists():\n        raise FileNotFoundError(f\"ERROR: File \\'{input_file}\\' does not exist.\")\n    if not input_file.is_file():\n        raise ValueError(f\"ERROR: \\'{input_file}\\' is not a file.\")\n\n    # Check shuffle and randomise settings\n    if no_shuffle and (randomise or randomise_builtin):\n        logger.warning(\"Warning: `no_shuffle` and `randomise(_builtin)` options are incompatible. Ignoring `randomise(_builtin)`.\")\n        randomise = False\n        randomise_builtin = False\n\n    if randomise and randomise_builtin:\n        logger.warning(\"Warning: `randomise` and `randomise_builtin` options are incompatible. Ignoring `randomise_builtin`.\")\n        randomise_builtin = False\n\n    events = read_events(input_file)\n\n    if no_shuffle:\n        event_partitions = partition_events__no_shuffle(events,\n                                                        process_number,\n                                                        total_processes)\n    elif randomise:\n        event_partitions = partition_events__random(events,\n                                                    process_number,\n                                                    total_processes)\n    elif randomise_builtin:\n        event_partitions = partition_events__random_builtin(events,\n                                                            process_number,\n                                                            total_processes)\n    else:\n        event_partitions = partition_events__round_robin(events,\n                                                         process_number,\n                                                         total_processes)\n\n    with ExitStack() as stack:\n        stream_out = resolve_file(output_file, 'wb', stack)\n\n        stream_events(event_partitions, stream_out)\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate event ID partitions as a binary data stream with shuffling. By\ndefault the events are shuffled by assiging to processes one by one\ncyclically.\n\nArgs:\n    input_file (str | os.PathLike): Path to binary events file. If None\n    then defaults to DEFAULT_EVENTS_FILE.\n    process_number (int): The process number to receive a partition of events.\n    total_processes (int): Total number of processes to distribute the events over.\n    no_shuffle (bool, optional): Disable shuffling events. Events are split\n        and distributed into blocks in the order they are input. Takes\n        priority over `randomise(_builtin)`.\n    randomise (bool, optional): Shuffle events randomly in the blocks and\n        stream events on the fly. If `no_shuffle` is `True` then it takes\n        priority.\n    randomise_builtin (bool, optional): Shuffle events randomly in the blocks using\n        builtin shuffle. If `no_shuffle` or `randomise` is `True` then it\n        they take priority.\n    output_file (str | os.PathLike): Path to output file. If '-' then outputs to stdout."
    },
    {
      "chunk_id": "oasislmf/pytools/fm/back_allocation.py::back_alloc_extra_a2@6",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/back_allocation.py",
      "symbol_type": "function",
      "name": "back_alloc_extra_a2",
      "lineno": 6,
      "end_lineno": 132,
      "business_stage": "exposure",
      "docstring": "back allocation of loss and extra to the base children,\nThe function modifies in-place array loss_in and items loss and extra in loss_val and extra val\nArgs:\n    base_children_len: number of base children\n    temp_children_queue: array of base children\n    nodes_array: array of all node info\n    p: profile index\n    node_val_len: number of actual values in sidx\n    node_sidx: sidx for this node\n    sidx_indptr: index to sidx pointer\n    sidx_indexes: index of sidx for nodes\n    sidx_val: sidx values\n    loss_in: loss before applying profile\n    loss_out: loss after applying profile\n    temp_node_loss: array to store loss factor\n    loss_indptr: index to loss pointer\n    loss_val: loss values\n    extra: extra after applying profile\n    temp_node_extras: extra after applying profile(dense)\n    extras_indptr: index to extra pointer\n    extras_val: extra values",
      "content": "# File: oasislmf/pytools/fm/back_allocation.py\n# function: back_alloc_extra_a2 (lines 6-132)\n\ndef back_alloc_extra_a2(base_children_len, temp_children_queue, nodes_array, p,\n                        node_val_len, node_sidx, sidx_indptr, sidx_indexes, sidx_val,\n                        loss_in, loss_out, temp_node_loss, loss_indptr, loss_val,\n                        extra, temp_node_extras, extras_indptr, extras_val):\n    \"\"\"\n    back allocation of loss and extra to the base children,\n    The function modifies in-place array loss_in and items loss and extra in loss_val and extra val\n    Args:\n        base_children_len: number of base children\n        temp_children_queue: array of base children\n        nodes_array: array of all node info\n        p: profile index\n        node_val_len: number of actual values in sidx\n        node_sidx: sidx for this node\n        sidx_indptr: index to sidx pointer\n        sidx_indexes: index of sidx for nodes\n        sidx_val: sidx values\n        loss_in: loss before applying profile\n        loss_out: loss after applying profile\n        temp_node_loss: array to store loss factor\n        loss_indptr: index to loss pointer\n        loss_val: loss values\n        extra: extra after applying profile\n        temp_node_extras: extra after applying profile(dense)\n        extras_indptr: index to extra pointer\n        extras_val: extra values\n    \"\"\"\n    if base_children_len == 1:  # this is a base children, we only need to assign loss_in\n        loss_in[:] = loss_out\n    else:\n        # back allocation rules:\n        # if deductible grows, deductible and loss are allocated based on loss\n        # else it means it is reallocated to loss,\n        #   if underlimit is still >0 then extra loss and deductible are allocated based on underlimit\n        #   else                           extra loss and deductible are allocated based on deductible\n        # if overlimit grows, more loss is over limit so it is reallocated based on loss\n        # else it is reallocated based on overlimit\n        # if underlimit grows, more loss has been deducted so we reallocate based on loss\n        # else it is reallocated based on underlimit\n\n        for i in range(node_val_len):\n            diff = extra[i, DEDUCTIBLE] - temp_node_extras[p, node_sidx[i], DEDUCTIBLE]\n            if diff >= 0:\n                realloc = 0\n                if loss_in[i] > 0:\n                    temp_node_extras[p, node_sidx[i], DEDUCTIBLE] = diff / loss_in[i]\n                    temp_node_loss[p, node_sidx[i]] = loss_out[i] / loss_in[i]\n                else:\n                    temp_node_extras[p, node_sidx[i], DEDUCTIBLE] = 0\n                    temp_node_loss[p, node_sidx[i]] = 0\n            else:\n                realloc = diff  # to loss or to over\n\n                if extra[i, UNDERLIMIT] > 0:\n                    temp_node_extras[p, node_sidx[i], DEDUCTIBLE] = diff / temp_node_extras[p, node_sidx[i], UNDERLIMIT]\n                else:\n                    temp_node_extras[p, node_sidx[i], DEDUCTIBLE] = diff / temp_node_extras[p, node_sidx[i], DEDUCTIBLE]\n                temp_node_loss[p, node_sidx[i]] = loss_out[i] / (loss_in[i] - diff)\n\n            diff = extra[i, OVERLIMIT] - temp_node_extras[p, node_sidx[i], OVERLIMIT]\n            if diff > 0:\n                temp_node_extras[p, node_sidx[i], OVERLIMIT] = diff / (loss_in[i] - realloc)\n            elif diff == 0:\n                temp_node_extras[p, node_sidx[i], OVERLIMIT] = 0\n            else:  # we set it to <0 to be able to check it later\n                temp_node_extras[p, node_sidx[i], OVERLIMIT] = - extra[i, OVERLIMIT] / temp_node_extras[\n                    p, node_sidx[i], OVERLIMIT]\n\n            diff = extra[i, UNDERLIMIT] - temp_node_extras[p, node_sidx[i], UNDERLIMIT]\n            if diff > 0:\n                temp_node_extras[p, node_sidx[i], UNDERLIMIT] = diff / loss_in[i]\n            elif diff == 0:\n                temp_node_extras[p, node_sidx[i], UNDERLIMIT] = 0\n            else:  # we set it to <0 to be able to check it later\n                temp_node_extras[p, node_sidx[i], UNDERLIMIT] = - extra[i, UNDERLIMIT] / temp_node_extras[\n                    p, node_sidx[i], UNDERLIMIT]\n\n            loss_in[i] = loss_out[i]\n\n        for base_child_i in range(base_children_len):\n\n\"\"\"Docstring (excerpt)\"\"\"\nback allocation of loss and extra to the base children,\nThe function modifies in-place array loss_in and items loss and extra in loss_val and extra val\nArgs:\n    base_children_len: number of base children\n    temp_children_queue: array of base children\n    nodes_array: array of all node info\n    p: profile index\n    node_val_len: number of actual values in sidx\n    node_sidx: sidx for this node\n    sidx_indptr: index to sidx pointer\n    sidx_indexes: index of sidx for nodes\n    sidx_val: sidx values\n    loss_in: loss before applying profile\n    loss_out: loss after applying profile\n    temp_node_loss: array to store loss factor\n    loss_indptr: index to loss pointer\n    loss_val: loss values\n    extra: extra after applying profile\n    temp_node_extras: extra after applying profile(dense)\n    extras_indptr: index to extra pointer\n    extras_val: extra values"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/back_allocation.py::back_alloc_a2@138",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/back_allocation.py",
      "symbol_type": "function",
      "name": "back_alloc_a2",
      "lineno": 138,
      "end_lineno": 181,
      "business_stage": "exposure",
      "docstring": "back allocation of loss to the base children. The function modifies in-place array loss_in and items loss in loss_val.\nArgs:\n    base_children_len: number of base children\n    temp_children_queue: array of base children\n    nodes_array: array of all node info\n    p: profile index\n    node_val_len: number of actual values in sidx\n    node_sidx: sidx for this node\n    sidx_indptr: index to sidx pointer\n    sidx_indexes: index of sidx for nodes\n    sidx_val: sidx values\n    loss_in: loss before applying profile\n    loss_out: loss after applying profile\n    temp_node_loss: array to store loss factor\n    loss_indptr: index to loss pointer\n    loss_val: loss values",
      "content": "# File: oasislmf/pytools/fm/back_allocation.py\n# function: back_alloc_a2 (lines 138-181)\n\ndef back_alloc_a2(base_children_len, temp_children_queue, nodes_array, p,\n                  node_val_len, node_sidx, sidx_indptr, sidx_indexes, sidx_val,\n                  loss_in, loss_out, temp_node_loss, loss_indptr, loss_val):\n    \"\"\"\n    back allocation of loss to the base children. The function modifies in-place array loss_in and items loss in loss_val.\n    Args:\n        base_children_len: number of base children\n        temp_children_queue: array of base children\n        nodes_array: array of all node info\n        p: profile index\n        node_val_len: number of actual values in sidx\n        node_sidx: sidx for this node\n        sidx_indptr: index to sidx pointer\n        sidx_indexes: index of sidx for nodes\n        sidx_val: sidx values\n        loss_in: loss before applying profile\n        loss_out: loss after applying profile\n        temp_node_loss: array to store loss factor\n        loss_indptr: index to loss pointer\n        loss_val: loss values\n\n    \"\"\"\n    if base_children_len == 1:\n        loss_in[:] = loss_out\n    else:\n        for i in range(node_val_len):\n            if loss_out[i]:\n                temp_node_loss[p, node_sidx[i]] = loss_out[i] / loss_in[i]\n            else:\n                temp_node_loss[p, node_sidx[i]] = 0\n            loss_in[i] = loss_out[i]\n\n        for base_child_i in range(base_children_len):\n            child = nodes_array[temp_children_queue[base_child_i]]\n\n            child_sidx_start = sidx_indptr[sidx_indexes[child['node_id']]]\n            child_sidx_end = sidx_indptr[sidx_indexes[child['node_id']] + 1]\n            child_val_len = child_sidx_end - child_sidx_start\n\n            child_sidx = sidx_val[child_sidx_start: child_sidx_end]\n            child_loss = loss_val[loss_indptr[child['loss'] + p]: loss_indptr[child['loss'] + p] + child_val_len]\n\n            for i in range(child_val_len):\n                child_loss[i] = child_loss[i] * temp_node_loss[p, child_sidx[i]]\n\n\"\"\"Docstring (excerpt)\"\"\"\nback allocation of loss to the base children. The function modifies in-place array loss_in and items loss in loss_val.\nArgs:\n    base_children_len: number of base children\n    temp_children_queue: array of base children\n    nodes_array: array of all node info\n    p: profile index\n    node_val_len: number of actual values in sidx\n    node_sidx: sidx for this node\n    sidx_indptr: index to sidx pointer\n    sidx_indexes: index of sidx for nodes\n    sidx_val: sidx values\n    loss_in: loss before applying profile\n    loss_out: loss after applying profile\n    temp_node_loss: array to store loss factor\n    loss_indptr: index to loss pointer\n    loss_val: loss values"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/back_allocation.py::back_alloc_layer@186",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/back_allocation.py",
      "symbol_type": "function",
      "name": "back_alloc_layer",
      "lineno": 186,
      "end_lineno": 208,
      "business_stage": "exposure",
      "docstring": "Args:\n    layer_len: number of layers in the node\n    node_val_len: number of actual values in sidx\n    node_loss_indptr: index of the loss pointer of the node\n    loss_in: loss before applying profile\n    loss_out: loss after applying profile\n    loss_indptr: index to loss pointer\n    loss_val: loss values\n    temp_node_loss_layer_ba: sparse array to loss after layer back alloc",
      "content": "# File: oasislmf/pytools/fm/back_allocation.py\n# function: back_alloc_layer (lines 186-208)\n\ndef back_alloc_layer(layer_len, node_val_len, node_loss_indptr,\n                     loss_in, loss_out, loss_indptr, loss_val,\n                     temp_node_loss_layer_ba):\n    \"\"\"\n    Args:\n        layer_len: number of layers in the node\n        node_val_len: number of actual values in sidx\n        node_loss_indptr: index of the loss pointer of the node\n        loss_in: loss before applying profile\n        loss_out: loss after applying profile\n        loss_indptr: index to loss pointer\n        loss_val: loss values\n        temp_node_loss_layer_ba: sparse array to loss after layer back alloc\n    \"\"\"\n    for i in range(node_val_len):\n        if loss_out[i]:\n            loss_factor = loss_out[i] / loss_in[i]\n        else:\n            loss_factor = 0\n\n        for l in range(layer_len):\n            layer_loss_indptr = loss_indptr[node_loss_indptr + l]\n            temp_node_loss_layer_ba[l, i] = loss_val[layer_loss_indptr + i] * loss_factor\n\n\"\"\"Docstring (excerpt)\"\"\"\nArgs:\n    layer_len: number of layers in the node\n    node_val_len: number of actual values in sidx\n    node_loss_indptr: index of the loss pointer of the node\n    loss_in: loss before applying profile\n    loss_out: loss after applying profile\n    loss_indptr: index to loss pointer\n    loss_val: loss values\n    temp_node_loss_layer_ba: sparse array to loss after layer back alloc"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/back_allocation.py::back_alloc_layer_extra@212",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/back_allocation.py",
      "symbol_type": "function",
      "name": "back_alloc_layer_extra",
      "lineno": 212,
      "end_lineno": 300,
      "business_stage": "exposure",
      "docstring": "Args:\n    layer_len: number of layers in the node\n    node_val_len: number of actual values in sidx\n    node_loss_indptr: index of the loss pointer of the node\n    node_extra_indptr: index of the extra pointer of the node\n    loss_in: loss before applying profile\n    loss_out: loss after applying profile\n    loss_indptr: index to loss pointer\n    loss_val: loss values\n    temp_node_loss_layer_ba: sparse array to loss after layer back alloc\n    extras_indptr: index to extra pointer\n    extras_val: extra values\n    temp_node_extras_layer_merge: node extra after profile\n    temp_node_extras_layer_merge_save: node extra before profile",
      "content": "# File: oasislmf/pytools/fm/back_allocation.py\n# function: back_alloc_layer_extra (lines 212-300)\n\ndef back_alloc_layer_extra(layer_len, node_val_len, node_loss_indptr, node_extra_indptr,\n                           loss_in, loss_out, loss_indptr, loss_val,\n                           temp_node_loss_layer_ba,\n                           extras_indptr, extras_val,\n                           temp_node_extras_layer_merge, temp_node_extras_layer_merge_save\n                           ):\n    \"\"\"\n\n    Args:\n        layer_len: number of layers in the node\n        node_val_len: number of actual values in sidx\n        node_loss_indptr: index of the loss pointer of the node\n        node_extra_indptr: index of the extra pointer of the node\n        loss_in: loss before applying profile\n        loss_out: loss after applying profile\n        loss_indptr: index to loss pointer\n        loss_val: loss values\n        temp_node_loss_layer_ba: sparse array to loss after layer back alloc\n        extras_indptr: index to extra pointer\n        extras_val: extra values\n        temp_node_extras_layer_merge: node extra after profile\n        temp_node_extras_layer_merge_save: node extra before profile\n\n    \"\"\"\n    for i in range(node_val_len):\n        deductible_delta = temp_node_extras_layer_merge[i, DEDUCTIBLE] - temp_node_extras_layer_merge_save[i, DEDUCTIBLE]\n        if deductible_delta >= 0:  # deductible increase no loss reallocation, deductible and loss are allocated based on loss\n            if loss_in[i] > 0:\n                ded_factor = deductible_delta / loss_in[i]\n                loss_factor = loss_out[i] / loss_in[i]\n            else:\n                ded_factor = 0\n                loss_factor = 0\n            realloc = 0\n        else:\n            realloc = deductible_delta\n            realloc_numerator = UNDERLIMIT if temp_node_extras_layer_merge[i, UNDERLIMIT] > 0 else DEDUCTIBLE\n            ded_factor = realloc / temp_node_extras_layer_merge_save[i, realloc_numerator]\n            loss_factor = loss_out[i] / (loss_in[i] - realloc)\n\n        overlimit_delta = (temp_node_extras_layer_merge[i, OVERLIMIT]\n                           - temp_node_extras_layer_merge_save[i, OVERLIMIT])\n        if overlimit_delta > 0:\n            overlimit_factor = overlimit_delta / (loss_in[i] - realloc)\n        elif overlimit_delta == 0:\n            overlimit_factor = 0\n        else:\n            overlimit_factor = - temp_node_extras_layer_merge[i, OVERLIMIT] / temp_node_extras_layer_merge_save[i, OVERLIMIT]\n\n        underlimit_delta = (temp_node_extras_layer_merge[i, UNDERLIMIT]\n                            - temp_node_extras_layer_merge_save[i, UNDERLIMIT])\n\n        if underlimit_delta > 0:\n            underlimit_factor = underlimit_delta / loss_in[i]\n        elif underlimit_delta == 0:\n            underlimit_factor = 0\n        else:\n            underlimit_factor = - temp_node_extras_layer_merge[i, UNDERLIMIT] / temp_node_extras_layer_merge_save[i, UNDERLIMIT]\n\n        for l in range(layer_len):\n            layer_loss_indptr = loss_indptr[node_loss_indptr + l]\n            layer_extra_indptr = extras_indptr[node_extra_indptr + l]\n            if ded_factor < 0:\n                if underlimit_factor == 0:\n                    l_realloc = ded_factor * extras_val[layer_extra_indptr + i, DEDUCTIBLE]\n                else:\n                    l_realloc = ded_factor * extras_val[layer_extra_indptr + i, UNDERLIMIT]\n\n                if overlimit_factor >= 0:\n                    extras_val[layer_extra_indptr + i, OVERLIMIT] += (overlimit_factor *\n                                                                      (loss_val[layer_loss_indptr + i] - l_realloc))\n                else:\n                    extras_val[layer_extra_indptr + i, OVERLIMIT] *= - overlimit_factor\n                temp_node_loss_layer_ba[l, i] = (loss_val[layer_loss_indptr + i] - l_realloc) * loss_factor\n                extras_val[layer_extra_indptr + i, DEDUCTIBLE] += l_realloc\n                extras_val[layer_extra_indptr + i, UNDERLIMIT] *= -underlimit_factor\n            else:\n                if overlimit_factor >= 0:\n                    extras_val[layer_extra_indptr + i, OVERLIMIT] += overlimit_factor * loss_val[layer_loss_indptr + i]\n                else:\n\n\"\"\"Docstring (excerpt)\"\"\"\nArgs:\n    layer_len: number of layers in the node\n    node_val_len: number of actual values in sidx\n    node_loss_indptr: index of the loss pointer of the node\n    node_extra_indptr: index of the extra pointer of the node\n    loss_in: loss before applying profile\n    loss_out: loss after applying profile\n    loss_indptr: index to loss pointer\n    loss_val: loss values\n    temp_node_loss_layer_ba: sparse array to loss after layer back alloc\n    extras_indptr: index to extra pointer\n    extras_val: extra values\n    temp_node_extras_layer_merge: node extra after profile\n    temp_node_extras_layer_merge_save: node extra before profile"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/compute_sparse.py::get_base_children@16",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/compute_sparse.py",
      "symbol_type": "function",
      "name": "get_base_children",
      "lineno": 16,
      "end_lineno": 49,
      "business_stage": "fm",
      "docstring": "fill up temp_children_queue with all the base children of node from index 0 and return the number of base children\nArgs:\n    node: top node\n    children: array of all the children with loss value for each node\n    nodes_array: array of information on all nodes\n    temp_children_queue: empty array where we write base children.\n\nReturns:\n    number of base children",
      "content": "# File: oasislmf/pytools/fm/compute_sparse.py\n# function: get_base_children (lines 16-49)\n\ndef get_base_children(node, children, nodes_array, temp_children_queue):\n    \"\"\"\n    fill up temp_children_queue with all the base children of node from index 0 and return the number of base children\n    Args:\n        node: top node\n        children: array of all the children with loss value for each node\n        nodes_array: array of information on all nodes\n        temp_children_queue: empty array where we write base children.\n\n    Returns:\n        number of base children\n    \"\"\"\n    len_children = children[node['children']]\n    if len_children:\n        temp_children_queue[:len_children] = children[node['children'] + 1: node['children'] + len_children + 1]\n        temp_children_queue[len_children] = null_index\n        queue_end = len_children\n        i = 0\n        child_i = 0\n        while temp_children_queue[i] != null_index:\n            parent = nodes_array[temp_children_queue[i]]\n            len_children = children[parent['children']]\n            if len_children:\n                temp_children_queue[queue_end: queue_end + len_children] = children[parent['children'] + 1: parent['children'] + len_children + 1]\n                queue_end += len_children\n                temp_children_queue[queue_end] = null_index\n            else:\n                temp_children_queue[child_i] = temp_children_queue[i]\n                child_i += 1\n            i += 1\n    else:\n        temp_children_queue[0] = node['node_id']\n        child_i = 1\n    return child_i\n\n\"\"\"Docstring (excerpt)\"\"\"\nfill up temp_children_queue with all the base children of node from index 0 and return the number of base children\nArgs:\n    node: top node\n    children: array of all the children with loss value for each node\n    nodes_array: array of information on all nodes\n    temp_children_queue: empty array where we write base children.\n\nReturns:\n    number of base children"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/compute_sparse.py::first_time_layer@53",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/compute_sparse.py",
      "symbol_type": "function",
      "name": "first_time_layer",
      "lineno": 53,
      "end_lineno": 68,
      "business_stage": "fm",
      "docstring": "first time there is a back allocation with multiple layer, we duplicate loss and extra from layer 1 to the other layers",
      "content": "# File: oasislmf/pytools/fm/compute_sparse.py\n# function: first_time_layer (lines 53-68)\n\ndef first_time_layer(profile_len, base_children_len, temp_children_queue, compute_idx, nodes_array,\n                     sidx_indptr, sidx_indexes,\n                     loss_indptr, loss_val\n                     ):\n    \"\"\"\n    first time there is a back allocation with multiple layer, we duplicate loss and extra from layer 1 to the other layers\n    \"\"\"\n    for base_child_i in range(base_children_len):\n        child = nodes_array[temp_children_queue[base_child_i]]\n        child_val_len = sidx_indptr[sidx_indexes[child['node_id']] + 1] - sidx_indptr[sidx_indexes[child['node_id']]]\n        child_loss_val_layer_0 = loss_val[loss_indptr[child['loss']]:\n                                          loss_indptr[child['loss']] + child_val_len]\n        for p in range(1, profile_len):\n            loss_indptr[child['loss'] + p] = compute_idx['loss_ptr_i']\n            loss_val[compute_idx['loss_ptr_i']: compute_idx['loss_ptr_i'] + child_val_len] = child_loss_val_layer_0\n            compute_idx['loss_ptr_i'] += child_val_len\n\n\"\"\"Docstring (excerpt)\"\"\"\nfirst time there is a back allocation with multiple layer, we duplicate loss and extra from layer 1 to the other layers"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/compute_sparse.py::first_time_layer_extra@72",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/compute_sparse.py",
      "symbol_type": "function",
      "name": "first_time_layer_extra",
      "lineno": 72,
      "end_lineno": 99,
      "business_stage": "fm",
      "docstring": "first time there is a back allocation with multiple layer, we duplicate loss and extra from layer 1 to the other layers",
      "content": "# File: oasislmf/pytools/fm/compute_sparse.py\n# function: first_time_layer_extra (lines 72-99)\n\ndef first_time_layer_extra(profile_len, base_children_len, temp_children_queue, compute_idx, nodes_array,\n                           sidx_indptr, sidx_indexes,\n                           loss_indptr, loss_val,\n                           extras_indptr, extras_val,\n                           ):\n    \"\"\"\n    first time there is a back allocation with multiple layer, we duplicate loss and extra from layer 1 to the other layers\n    \"\"\"\n    for base_child_i in range(base_children_len):\n        child = nodes_array[temp_children_queue[base_child_i]]\n        child_val_len = sidx_indptr[sidx_indexes[child['node_id']] + 1] - sidx_indptr[sidx_indexes[child['node_id']]]\n        child_loss_val_layer_0 = loss_val[loss_indptr[child['loss']]:\n                                          loss_indptr[child['loss']] + child_val_len]\n        if base_children_len == 1:  # aggregation case\n            child_extra_val_layer_0 = extras_val[extras_indptr[child['extra']]:\n                                                 extras_indptr[child['extra']] + child_val_len]\n        else:  # back allocation case\n            child_extra_val_layer_0 = np.zeros_like(extras_val[extras_indptr[child['extra']]:\n                                                               extras_indptr[child['extra']] + child_val_len])\n\n        for p in range(1, profile_len):\n            loss_indptr[child['loss'] + p] = compute_idx['loss_ptr_i']\n            loss_val[compute_idx['loss_ptr_i']: compute_idx['loss_ptr_i'] + child_val_len] = child_loss_val_layer_0\n            compute_idx['loss_ptr_i'] += child_val_len\n\n            extras_indptr[child['extra'] + p] = compute_idx['extras_ptr_i']\n            extras_val[compute_idx['extras_ptr_i']: compute_idx['extras_ptr_i'] + child_val_len] = child_extra_val_layer_0\n            compute_idx['extras_ptr_i'] += child_val_len\n\n\"\"\"Docstring (excerpt)\"\"\"\nfirst time there is a back allocation with multiple layer, we duplicate loss and extra from layer 1 to the other layers"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/compute_sparse.py::aggregate_children_extras@103",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/compute_sparse.py",
      "symbol_type": "function",
      "name": "aggregate_children_extras",
      "lineno": 103,
      "end_lineno": 196,
      "business_stage": "fm",
      "docstring": "aggregate the loss and extra of the children of the node that is currently computed\nArgs:\n    node: node that we compute\n    len_children: number of children of the node\n    nodes_array: array of information on all nodes\n    children: array of all the children with loss value for each node\n    temp_children_queue: array storing all the base children of the node\n    compute_idx: single element named array containing all the pointer needed to tract the computation (compute_idx_dtype)\n    temp_node_sidx: dense array to store if sample id has value for this node\n    sidx_indexes: index of sidx for nodes\n    sidx_indptr: : index to sidx pointer\n    sidx_val: sidx values\n    all_sidx: list of all sidx in this computation\n    temp_node_loss: dense array storing the sum of children loss\n    loss_indptr: index to the loss pointer\n    loss_val: loss values\n    temp_node_extras: dense array storing the sum of children extra\n    extras_indptr: index to the extra pointer\n    extras_val: extra values",
      "content": "# File: oasislmf/pytools/fm/compute_sparse.py\n# function: aggregate_children_extras (lines 103-196)\n\ndef aggregate_children_extras(node, len_children, nodes_array, children, temp_children_queue, compute_idx,\n                              temp_node_sidx, sidx_indexes, sidx_indptr, sidx_val, all_sidx,\n                              temp_node_loss, loss_indptr, loss_val,\n                              temp_node_extras, extras_indptr, extras_val):\n    \"\"\"\n    aggregate the loss and extra of the children of the node that is currently computed\n    Args:\n        node: node that we compute\n        len_children: number of children of the node\n        nodes_array: array of information on all nodes\n        children: array of all the children with loss value for each node\n        temp_children_queue: array storing all the base children of the node\n        compute_idx: single element named array containing all the pointer needed to tract the computation (compute_idx_dtype)\n        temp_node_sidx: dense array to store if sample id has value for this node\n        sidx_indexes: index of sidx for nodes\n        sidx_indptr: : index to sidx pointer\n        sidx_val: sidx values\n        all_sidx: list of all sidx in this computation\n        temp_node_loss: dense array storing the sum of children loss\n        loss_indptr: index to the loss pointer\n        loss_val: loss values\n        temp_node_extras: dense array storing the sum of children extra\n        extras_indptr: index to the extra pointer\n        extras_val: extra values\n    \"\"\"\n    sidx_created = False\n    node_sidx_start = compute_idx['sidx_ptr_i']\n    node_sidx_end = 0\n    sidx_indexes[node['node_id']] = compute_idx['sidx_i']\n    compute_idx['sidx_i'] += 1\n\n    for p in range(node['profile_len']):\n        p_temp_node_loss = temp_node_loss[p]\n        p_temp_node_extras = temp_node_extras[p]\n\n        for c in range(node['children'] + 1, node['children'] + len_children + 1):\n            child = nodes_array[children[c]]\n            child_sidx_val = sidx_val[sidx_indptr[sidx_indexes[child['node_id']]]:\n                                      sidx_indptr[sidx_indexes[child['node_id']] + 1]]\n\n            if p == 1 and loss_indptr[child['loss'] + p] == loss_indptr[child['loss']]:\n                # this is the first time child branch has multiple layer we create views for root children\n                base_children_len = get_base_children(child, children, nodes_array, temp_children_queue)\n                # print('new layers', child['level_id'], child['agg_id'], node['profile_len'])\n                first_time_layer_extra(\n                    node['profile_len'], base_children_len, temp_children_queue, compute_idx, nodes_array,\n                    sidx_indptr, sidx_indexes,\n                    loss_indptr, loss_val,\n                    extras_indptr, extras_val,\n                )\n\n            child_loss = loss_val[loss_indptr[child['loss'] + p]:\n                                  loss_indptr[child['loss'] + p] + child_sidx_val.shape[0]]\n            child_extra = extras_val[extras_indptr[child['extra'] + p]:\n                                     extras_indptr[child['extra'] + p] + child_sidx_val.shape[0]]\n            # print('child', child['level_id'], child['agg_id'], p, loss_indptr[child['loss'] + p], child_loss[0], child['extra'], extras_indptr[child['extra'] + p])\n            for indptr in range(child_sidx_val.shape[0]):\n                temp_node_sidx[child_sidx_val[indptr]] = True\n                p_temp_node_loss[child_sidx_val[indptr]] += child_loss[indptr]\n                p_temp_node_extras[child_sidx_val[indptr]] += child_extra[indptr]\n        # print('res', p, p_temp_node_loss[-3], p_temp_node_extras[-3])\n\n        loss_indptr[node['loss'] + p] = compute_idx['loss_ptr_i']\n        extras_indptr[node['extra'] + p] = compute_idx['extras_ptr_i']\n        if sidx_created:\n            for node_sidx_cur in range(node_sidx_start, node_sidx_end):\n                loss_val[compute_idx['loss_ptr_i']] = p_temp_node_loss[sidx_val[node_sidx_cur]]\n                compute_idx['loss_ptr_i'] += 1\n                extras_val[compute_idx['extras_ptr_i']] = p_temp_node_extras[sidx_val[node_sidx_cur]]\n                compute_idx['extras_ptr_i'] += 1\n\n        else:\n            for sidx in all_sidx:\n                if temp_node_sidx[sidx]:\n                    sidx_val[compute_idx['sidx_ptr_i']] = sidx\n                    compute_idx['sidx_ptr_i'] += 1\n\n                    loss_val[compute_idx['loss_ptr_i']] = p_temp_node_loss[sidx]\n                    compute_idx['loss_ptr_i'] += 1\n\n\"\"\"Docstring (excerpt)\"\"\"\naggregate the loss and extra of the children of the node that is currently computed\nArgs:\n    node: node that we compute\n    len_children: number of children of the node\n    nodes_array: array of information on all nodes\n    children: array of all the children with loss value for each node\n    temp_children_queue: array storing all the base children of the node\n    compute_idx: single element named array containing all the pointer needed to tract the computation (compute_idx_dtype)\n    temp_node_sidx: dense array to store if sample id has value for this node\n    sidx_indexes: index of sidx for nodes\n    sidx_indptr: : index to sidx pointer\n    sidx_val: sidx values\n    all_sidx: list of all sidx in this computation\n    temp_node_loss: dense array storing the sum of children loss\n    loss_indptr: index to the loss pointer\n    loss_val: loss values\n    temp_node_extras: dense array storing the sum of children extra\n    extras_indptr: index to the extra pointer\n    extras_val: extra values"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/compute_sparse.py::aggregate_children@200",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/compute_sparse.py",
      "symbol_type": "function",
      "name": "aggregate_children",
      "lineno": 200,
      "end_lineno": 272,
      "business_stage": "fm",
      "docstring": "aggregate the loss of the children of the node that is currently computed\nArgs:\n    node: node that we compute\n    len_children: number of children of the node\n    nodes_array: array of information on all nodes\n    children: array of all the children with loss value for each node\n    temp_children_queue: array storing all the base children of the node\n    compute_idx: single element named array containing all the pointer needed to tract the computation (compute_idx_dtype)\n    temp_node_sidx: dense array to store if sample id has value for this node\n    sidx_indexes: index of sidx for nodes\n    sidx_indptr: : index to sidx pointer\n    sidx_val: sidx values\n    all_sidx: list of all sidx in this computation\n    temp_node_loss: dense array storing the sum of children loss\n    loss_indptr: index to the loss pointer\n    loss_val: loss values",
      "content": "# File: oasislmf/pytools/fm/compute_sparse.py\n# function: aggregate_children (lines 200-272)\n\ndef aggregate_children(node, len_children, nodes_array, children, temp_children_queue, compute_idx,\n                       temp_node_sidx, sidx_indexes, sidx_indptr, sidx_val, all_sidx,\n                       temp_node_loss, loss_indptr, loss_val):\n    \"\"\"\n    aggregate the loss of the children of the node that is currently computed\n    Args:\n        node: node that we compute\n        len_children: number of children of the node\n        nodes_array: array of information on all nodes\n        children: array of all the children with loss value for each node\n        temp_children_queue: array storing all the base children of the node\n        compute_idx: single element named array containing all the pointer needed to tract the computation (compute_idx_dtype)\n        temp_node_sidx: dense array to store if sample id has value for this node\n        sidx_indexes: index of sidx for nodes\n        sidx_indptr: : index to sidx pointer\n        sidx_val: sidx values\n        all_sidx: list of all sidx in this computation\n        temp_node_loss: dense array storing the sum of children loss\n        loss_indptr: index to the loss pointer\n        loss_val: loss values\n    \"\"\"\n    sidx_created = False\n    node_sidx_start = compute_idx['sidx_ptr_i']\n    node_sidx_end = 0\n    sidx_indexes[node['node_id']] = compute_idx['sidx_i']\n    compute_idx['sidx_i'] += 1\n    for p in range(node['profile_len']):\n        p_temp_node_loss = temp_node_loss[p]\n        for c in range(node['children'] + 1, node['children'] + len_children + 1):\n            child = nodes_array[children[c]]\n            child_sidx_val = sidx_val[sidx_indptr[sidx_indexes[child['node_id']]]:\n                                      sidx_indptr[sidx_indexes[child['node_id']] + 1]]\n            if p == 1 and loss_indptr[child['loss'] + p] == loss_indptr[child['loss']]:\n                # this is the first time child branch has multiple layer we create views for root children\n                base_children_len = get_base_children(child, children, nodes_array, temp_children_queue)\n                first_time_layer(\n                    node['profile_len'], base_children_len, temp_children_queue, compute_idx, nodes_array,\n                    sidx_indptr, sidx_indexes,\n                    loss_indptr, loss_val\n                )\n            child_loss = loss_val[loss_indptr[child['loss'] + p]:\n                                  loss_indptr[child['loss'] + p] + child_sidx_val.shape[0]]\n\n            for indptr in range(child_sidx_val.shape[0]):\n                temp_node_sidx[child_sidx_val[indptr]] = True\n                p_temp_node_loss[child_sidx_val[indptr]] += child_loss[indptr]\n\n        loss_indptr[node['loss'] + p] = compute_idx['loss_ptr_i']\n        if sidx_created:\n            for node_sidx_cur in range(node_sidx_start, node_sidx_end):\n                loss_val[compute_idx['loss_ptr_i']] = p_temp_node_loss[sidx_val[node_sidx_cur]]\n                compute_idx['loss_ptr_i'] += 1\n\n        else:\n            for sidx in all_sidx:\n                if temp_node_sidx[sidx]:\n                    sidx_val[compute_idx['sidx_ptr_i']] = sidx\n                    compute_idx['sidx_ptr_i'] += 1\n                    temp_node_sidx[sidx] = False\n\n                    loss_val[compute_idx['loss_ptr_i']] = p_temp_node_loss[sidx]\n                    compute_idx['loss_ptr_i'] += 1\n\n            node_sidx_end = compute_idx['sidx_ptr_i']\n            node_val_len = node_sidx_end - node_sidx_start\n            sidx_indptr[compute_idx['sidx_i']] = compute_idx['sidx_ptr_i']\n            sidx_created = True\n\n    # fill up all layer if necessary\n    for layer in range(node['profile_len'], node['layer_len']):\n        loss_indptr[node['loss'] + layer] = loss_indptr[node['loss']]\n\n    return node_val_len\n\n\"\"\"Docstring (excerpt)\"\"\"\naggregate the loss of the children of the node that is currently computed\nArgs:\n    node: node that we compute\n    len_children: number of children of the node\n    nodes_array: array of information on all nodes\n    children: array of all the children with loss value for each node\n    temp_children_queue: array storing all the base children of the node\n    compute_idx: single element named array containing all the pointer needed to tract the computation (compute_idx_dtype)\n    temp_node_sidx: dense array to store if sample id has value for this node\n    sidx_indexes: index of sidx for nodes\n    sidx_indptr: : index to sidx pointer\n    sidx_val: sidx values\n    all_sidx: list of all sidx in this computation\n    temp_node_loss: dense array storing the sum of children loss\n    loss_indptr: index to the loss pointer\n    loss_val: loss values"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/compute_sparse.py::set_parent_next_compute@276",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/compute_sparse.py",
      "symbol_type": "function",
      "name": "set_parent_next_compute",
      "lineno": 276,
      "end_lineno": 293,
      "business_stage": "fm",
      "docstring": "Set the parent node that needs to be computed at the next level\nArgs:\n    parent_id: id of the parent\n    child_id: id of the child\n    nodes_array: array of information on all nodes\n    children: array of all the children with loss value for each node\n    computes: array of node that need to be computed.\n    compute_idx: single element named array containing all the pointer needed to tract the computation (compute_idx_dtype)",
      "content": "# File: oasislmf/pytools/fm/compute_sparse.py\n# function: set_parent_next_compute (lines 276-293)\n\ndef set_parent_next_compute(parent_id, child_id, nodes_array, children, computes, compute_idx):\n    \"\"\"\n    Set the parent node that needs to be computed at the next level\n    Args:\n        parent_id: id of the parent\n        child_id: id of the child\n        nodes_array: array of information on all nodes\n        children: array of all the children with loss value for each node\n        computes: array of node that need to be computed.\n        compute_idx: single element named array containing all the pointer needed to tract the computation (compute_idx_dtype)\n    \"\"\"\n    parent = nodes_array[parent_id]\n    parent_children_len = children[parent['children']] + 1\n    children[parent['children']] = parent_children_len\n    children[parent['children'] + parent_children_len] = child_id\n    if parent_children_len == 1:  # first time parent is seen\n        computes[compute_idx['next_compute_i']] = parent_id\n        compute_idx['next_compute_i'] += 1\n\n\"\"\"Docstring (excerpt)\"\"\"\nSet the parent node that needs to be computed at the next level\nArgs:\n    parent_id: id of the parent\n    child_id: id of the child\n    nodes_array: array of information on all nodes\n    children: array of all the children with loss value for each node\n    computes: array of node that need to be computed.\n    compute_idx: single element named array containing all the pointer needed to tract the computation (compute_idx_dtype)"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/compute_sparse.py::compute_event@317",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/compute_sparse.py",
      "symbol_type": "function",
      "name": "compute_event",
      "lineno": 317,
      "end_lineno": 727,
      "business_stage": "fm",
      "docstring": "compute an entire event, result losses are stored inplace in loss_val\nArgs:\n    compute_info: general information on the computation (financial_structure.compute_info_dtype)\n    keep_input_loss: if true compute the net loss instead of the insured loss\n    nodes_array: array of information on all nodes\n    node_parents_array: array of node to parent node\n    node_profiles_array: array of profile for each node\n    len_array: length of array needed to store loss and extra as dence array\n    max_sidx_val: maximum sidx value\n    sidx_indptr: index to sidx pointer\n    sidx_indexes: index of sidx for nodes\n    sidx_val: sidx values\n    loss_indptr: index of loss for nodes\n    loss_val: loss values\n    extras_indptr: index of extra for nodes\n    extras_val: extra values\n    children: array of all the children with loss value for each node\n    computes: array of node that need to be computed. it is filled up as the computation carries on\n    compute_idx: single element named array containing all the pointer needed to tract the computation (compute_idx_dtype)\n    item_parent_i: each node may have multiple parent that are selected one after another. This array keeps track of which parent index to select\n    fm_profile: array of all the profiles\n    stepped: (True or None) determine if this computation contain stepped profile (using None instead of False to allow jit compilation)",
      "content": "# File: oasislmf/pytools/fm/compute_sparse.py\n# function: compute_event (lines 317-727)\n\ndef compute_event(compute_info,\n                  keep_input_loss,\n                  nodes_array,\n                  node_parents_array,\n                  node_profiles_array,\n                  len_array, max_sidx_val, sidx_indexes, sidx_indptr, sidx_val, loss_indptr, loss_val, extras_indptr, extras_val,\n                  children,\n                  computes,\n                  compute_idx,\n                  item_parent_i,\n                  fm_profile,\n                  stepped):\n    \"\"\"\n    compute an entire event, result losses are stored inplace in loss_val\n    Args:\n        compute_info: general information on the computation (financial_structure.compute_info_dtype)\n        keep_input_loss: if true compute the net loss instead of the insured loss\n        nodes_array: array of information on all nodes\n        node_parents_array: array of node to parent node\n        node_profiles_array: array of profile for each node\n        len_array: length of array needed to store loss and extra as dence array\n        max_sidx_val: maximum sidx value\n        sidx_indptr: index to sidx pointer\n        sidx_indexes: index of sidx for nodes\n        sidx_val: sidx values\n        loss_indptr: index of loss for nodes\n        loss_val: loss values\n        extras_indptr: index of extra for nodes\n        extras_val: extra values\n        children: array of all the children with loss value for each node\n        computes: array of node that need to be computed. it is filled up as the computation carries on\n        compute_idx: single element named array containing all the pointer needed to tract the computation (compute_idx_dtype)\n        item_parent_i: each node may have multiple parent that are selected one after another. This array keeps track of which parent index to select\n        fm_profile: array of all the profiles\n        stepped: (True or None) determine if this computation contain stepped profile (using None instead of False to allow jit compilation)\n\n    \"\"\"\n    compute_idx['sidx_i'] = compute_idx['next_compute_i']\n    compute_idx['sidx_ptr_i'] = compute_idx['loss_ptr_i'] = sidx_indptr[compute_idx['next_compute_i']]\n    compute_idx['extras_ptr_i'] = 0\n    compute_idx['compute_i'] = 0\n\n    # dense array to store if sample id has value for this node\n    temp_node_sidx = np.zeros(len_array, dtype=oasis_int)\n\n    # sparse array to store the loss after profile is applied\n    temp_node_loss_sparse = np.zeros(len_array, dtype=oasis_float)\n\n    # sparse array to store the merged loss of all layer\n    temp_node_loss_layer_merge = np.zeros(len_array, dtype=oasis_float)\n\n    # sparse array to loss after layer back alloc\n    temp_node_loss_layer_ba = np.zeros((compute_info['max_layer'], len_array), dtype=oasis_float)\n\n    # temp_node_loss: dense array storing the sum of children loss, then the loss factor in children back alloc\n    temp_node_loss = np.zeros((compute_info['max_layer'], len_array), dtype=np.float64)\n\n    # temp_node_extras: dense array storing the sum of children extra, then the loss factor in children back alloc\n    temp_node_extras = np.zeros((compute_info['max_layer'], len_array, 3), dtype=oasis_float)\n\n    # temp_node_extras_layer_merge: sparse array to store the merged extra of all layer\n    temp_node_extras_layer_merge = np.zeros((len_array, 3), dtype=oasis_float)\n\n    # temp_node_extras_layer_merge_save: sparse array to keep the value of extra when profile is apply on merged layer\n    temp_node_extras_layer_merge_save = np.zeros((len_array, 3), dtype=oasis_float)\n\n    # temp_children_queue: array storing all the base children of the node\n    temp_children_queue = np.empty(nodes_array.shape[0], dtype=oasis_int)\n\n    # create all sidx array\n    all_sidx = np.empty(max_sidx_val + EXTRA_VALUES, dtype=oasis_int)\n    all_sidx[0] = MAX_LOSS_IDX\n    all_sidx[1] = TIV_IDX\n    all_sidx[2] = MEAN_IDX\n    all_sidx[3:] = np.arange(1, max_sidx_val + 1)\n\n    is_allocation_rule_a0 = compute_info['allocation_rule'] == 0\n    is_allocation_rule_a1 = compute_info['allocation_rule'] == 1\n    is_allocation_rule_a2 = compute_info['allocation_rule'] == 2\n\n\"\"\"Docstring (excerpt)\"\"\"\ncompute an entire event, result losses are stored inplace in loss_val\nArgs:\n    compute_info: general information on the computation (financial_structure.compute_info_dtype)\n    keep_input_loss: if true compute the net loss instead of the insured loss\n    nodes_array: array of information on all nodes\n    node_parents_array: array of node to parent node\n    node_profiles_array: array of profile for each node\n    len_array: length of array needed to store loss and extra as dence array\n    max_sidx_val: maximum sidx value\n    sidx_indptr: index to sidx pointer\n    sidx_indexes: index of sidx for nodes\n    sidx_val: sidx values\n    loss_indptr: index of loss for nodes\n    loss_val: loss values\n    extras_indptr: index of extra for nodes\n    extras_val: extra values\n    children: array of all the children with loss value for each node\n    computes: array of node that need to be computed. it is filled up as the computation carries on\n    compute_idx: single element named array containing all the pointer needed to tract the computation (compute_idx_dtype)\n    item_parent_i: each node may have multiple parent that are selected one after another. This array keeps track of which parent index to select\n    fm_profile: array of all the profiles\n    stepped: (True or None) determine if this computation contain stepped profile (using None instead of False to allow jit compilation)"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/compute_sparse.py::init_variable@730",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/compute_sparse.py",
      "symbol_type": "function",
      "name": "init_variable",
      "lineno": 730,
      "end_lineno": 765,
      "business_stage": "fm",
      "docstring": "extras, loss contains the same index as sidx\ntherefore we can use only sidx_indexes to tract the length of each node values",
      "content": "# File: oasislmf/pytools/fm/compute_sparse.py\n# function: init_variable (lines 730-765)\n\ndef init_variable(compute_info, max_sidx_val, temp_dir, low_memory):\n    \"\"\"\n    extras, loss contains the same index as sidx\n    therefore we can use only sidx_indexes to tract the length of each node values\n    \"\"\"\n    max_sidx_count = max_sidx_val + EXTRA_VALUES\n    len_array = max_sidx_val + 6\n\n    if low_memory:\n        sidx_val = np.memmap(os.path.join(temp_dir, \"sidx_val.bin\"), mode='w+',\n                             shape=(compute_info['node_len'] * max_sidx_count), dtype=oasis_int)\n        loss_val = np.memmap(os.path.join(temp_dir, \"loss_val.bin\"), mode='w+',\n                             shape=(compute_info['loss_len'] * max_sidx_count), dtype=oasis_float)\n        extras_val = np.memmap(os.path.join(temp_dir, \"extras_val.bin\"), mode='w+',\n                               shape=(compute_info['extra_len'] * max_sidx_count, 3), dtype=oasis_float)\n    else:\n        sidx_val = np.zeros((compute_info['node_len'] * max_sidx_count), dtype=oasis_int)\n        loss_val = np.zeros((compute_info['loss_len'] * max_sidx_count), dtype=oasis_float)\n        extras_val = np.zeros((compute_info['extra_len'] * max_sidx_count, 3), dtype=oasis_float)\n\n    sidx_indptr = np.zeros(compute_info['node_len'] + 1, dtype=np.int64)\n    loss_indptr = np.zeros(compute_info['loss_len'] + 1, dtype=np.int64)\n    extras_indptr = np.zeros(compute_info['extra_len'] + 1, dtype=np.int64)\n\n    sidx_indexes = np.empty(compute_info['node_len'], dtype=oasis_int)\n    children = np.zeros(compute_info['children_len'], dtype=np.uint32)\n    computes = np.zeros(compute_info['compute_len'], dtype=np.uint32)\n\n    pass_through = np.zeros(compute_info['items_len'] + 1, dtype=oasis_float)\n    item_parent_i = np.ones(compute_info['items_len'] + 1, dtype=np.uint32)\n\n    compute_idx = np.empty(1, dtype=compute_idx_dtype)[0]\n    compute_idx['next_compute_i'] = 0\n\n    return (max_sidx_val, max_sidx_count, len_array, sidx_indexes, sidx_indptr, sidx_val, loss_indptr, loss_val,\n            pass_through, extras_indptr, extras_val, children, computes, item_parent_i, compute_idx)\n\n\"\"\"Docstring (excerpt)\"\"\"\nextras, loss contains the same index as sidx\ntherefore we can use only sidx_indexes to tract the length of each node values"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/compute_sparse.py::reset_variable@769",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/compute_sparse.py",
      "symbol_type": "function",
      "name": "reset_variable",
      "lineno": 769,
      "end_lineno": 779,
      "business_stage": "fm",
      "docstring": "reset the per event array\nArgs:\n    children: array of all the children with loss value for each node\n    compute_idx: single element named array containing all the pointer needed to tract the computation (compute_idx_dtype)\n    computes: array of node to compute",
      "content": "# File: oasislmf/pytools/fm/compute_sparse.py\n# function: reset_variable (lines 769-779)\n\ndef reset_variable(children, compute_idx, computes):\n    \"\"\"\n    reset the per event array\n    Args:\n        children: array of all the children with loss value for each node\n        compute_idx: single element named array containing all the pointer needed to tract the computation (compute_idx_dtype)\n        computes: array of node to compute\n    \"\"\"\n    computes[:compute_idx['next_compute_i']].fill(0)\n    children.fill(0)\n    compute_idx['next_compute_i'] = 0\n\n\"\"\"Docstring (excerpt)\"\"\"\nreset the per event array\nArgs:\n    children: array of all the children with loss value for each node\n    compute_idx: single element named array containing all the pointer needed to tract the computation (compute_idx_dtype)\n    computes: array of node to compute"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/financial_structure.py::load_static@70",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/financial_structure.py",
      "symbol_type": "function",
      "name": "load_static",
      "lineno": 70,
      "end_lineno": 105,
      "business_stage": "fm",
      "docstring": "Load the raw financial data from static_path as numpy ndarray\nfirst check if .bin file is present then try .cvs\ntry loading profile_step before falling back to normal profile,\n\n:param static_path: str\n        static_path\n:return:\n    programme : link between nodes\n    policytc : info on layer\n    profile : policy profile can be profile_step or profile\n    xref : node to output_id\n    items : items (item_id and coverage_id mapping)\n    coverages : Tiv value for each coverage id\n:raise:\n    FileNotFoundError if one of the static is missing",
      "content": "# File: oasislmf/pytools/fm/financial_structure.py\n# function: load_static (lines 70-105)\n\ndef load_static(static_path):\n    \"\"\"\n    Load the raw financial data from static_path as numpy ndarray\n    first check if .bin file is present then try .cvs\n    try loading profile_step before falling back to normal profile,\n\n    :param static_path: str\n            static_path\n    :return:\n        programme : link between nodes\n        policytc : info on layer\n        profile : policy profile can be profile_step or profile\n        xref : node to output_id\n        items : items (item_id and coverage_id mapping)\n        coverages : Tiv value for each coverage id\n    :raise:\n        FileNotFoundError if one of the static is missing\n    \"\"\"\n    programme = load_as_ndarray(static_path, 'fm_programme', fm_programme_dtype)\n    policytc = load_as_ndarray(static_path, 'fm_policytc', fm_policytc_dtype)\n    profile = load_as_ndarray(static_path, 'fm_profile_step', fm_profile_step_dtype, False)\n    if len(profile) == 0:\n        profile = load_as_ndarray(static_path, 'fm_profile', fm_profile_dtype)\n        stepped = None\n    else:\n        stepped = True\n    xref = load_as_ndarray(static_path, 'fm_xref', fm_xref_dtype)\n\n    items = load_as_ndarray(static_path, 'items', items_dtype, must_exist=False)[['item_id', 'coverage_id']]\n    coverages = load_as_array(static_path, 'coverages', oasis_float, must_exist=False)\n    if np.unique(items['coverage_id']).shape[0] != coverages.shape[0]:\n        # one of the file is missing we default to empty array\n        items = np.empty(0, dtype=items_dtype)\n        coverages = np.empty(0, dtype=oasis_float)\n\n    return programme, policytc, profile, stepped, xref, items, coverages\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoad the raw financial data from static_path as numpy ndarray\nfirst check if .bin file is present then try .cvs\ntry loading profile_step before falling back to normal profile,\n\n:param static_path: str\n        static_path\n:return:\n    programme : link between nodes\n    policytc : info on layer\n    profile : policy profile can be profile_step or profile\n    xref : node to output_id\n    items : items (item_id and coverage_id mapping)\n    coverages : Tiv value for each coverage id\n:raise:\n    FileNotFoundError if one of the static is missing"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/financial_structure.py::does_nothing@109",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/financial_structure.py",
      "symbol_type": "function",
      "name": "does_nothing",
      "lineno": 109,
      "end_lineno": 125,
      "business_stage": "fm",
      "docstring": "evaluate if the profile is just doing nothing to the loss.\nthis allows to save some memory and compulation time and memory during the calculation\n:param profile: np.array of fm_profile_dtype or fm_profile_step_dtype\n        profile\n:return:\n    boolean : True is profile is actually doing nothing",
      "content": "# File: oasislmf/pytools/fm/financial_structure.py\n# function: does_nothing (lines 109-125)\n\ndef does_nothing(profile):\n    \"\"\"\n    evaluate if the profile is just doing nothing to the loss.\n    this allows to save some memory and compulation time and memory during the calculation\n    :param profile: np.array of fm_profile_dtype or fm_profile_step_dtype\n            profile\n    :return:\n        boolean : True is profile is actually doing nothing\n    \"\"\"\n    return ((profile['calcrule_id'] == 100) or\n            (profile['calcrule_id'] == 12 and almost_equal(profile['deductible1'], 0)) or\n            (profile['calcrule_id'] == 15 and almost_equal(profile['limit1'], 1)) or\n            (profile['calcrule_id'] == 16 and almost_equal(profile['deductible1'], 0)) or\n            (profile['calcrule_id'] == 34 and almost_equal(profile['deductible1'], 0)\n                and almost_equal(profile['attachment1'], 0)\n                and almost_equal(profile['share1'], 1))\n            )\n\n\"\"\"Docstring (excerpt)\"\"\"\nevaluate if the profile is just doing nothing to the loss.\nthis allows to save some memory and compulation time and memory during the calculation\n:param profile: np.array of fm_profile_dtype or fm_profile_step_dtype\n        profile\n:return:\n    boolean : True is profile is actually doing nothing"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/financial_structure.py::extract_financial_structure@294",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/financial_structure.py",
      "symbol_type": "function",
      "name": "extract_financial_structure",
      "lineno": 294,
      "end_lineno": 614,
      "business_stage": "fm",
      "docstring": ":param allocation_rule:\n    option to indicate out the loss are allocated to the output\n:param fm_programme:\n    structure of the levels\n:param fm_policytc:\n    structure of the layers and policy_id to apply\n:param fm_profile:\n    definition of the policy_id\n:param fm_xref:\n    mapping between the output of the allocation and output item_id\n:return:\n    compute_infos:\n    nodes_array:\n    node_parents_array:\n    node_profiles_array:\n    output_array:",
      "content": "# File: oasislmf/pytools/fm/financial_structure.py\n# function: extract_financial_structure (lines 294-614)\n\ndef extract_financial_structure(allocation_rule, fm_programme, fm_policytc, fm_profile, stepped, fm_xref, items, coverages):\n    \"\"\"\n    :param allocation_rule:\n        option to indicate out the loss are allocated to the output\n    :param fm_programme:\n        structure of the levels\n    :param fm_policytc:\n        structure of the layers and policy_id to apply\n    :param fm_profile:\n        definition of the policy_id\n    :param fm_xref:\n        mapping between the output of the allocation and output item_id\n    :return:\n        compute_infos:\n        nodes_array:\n        node_parents_array:\n        node_profiles_array:\n        output_array:\n    \"\"\"\n    ##### profile_id_to_profile_index ####\n    # policies may have multiple step, crate a mapping between profile_id and the start and end index in fm_profile file\n    max_profile_id = np.max(fm_profile['profile_id'])\n    profile_id_to_profile_index = np.empty(max_profile_id + 1, dtype=profile_index_dtype)\n    has_tiv_policy = Dict.empty(nb_oasis_int, nb_oasis_int)\n    last_profile_id = 0  # real profile_id start at 1\n    for i in range(fm_profile.shape[0]):\n        if fm_profile[i]['calcrule_id'] in need_tiv_policy:\n            has_tiv_policy[fm_profile[i]['profile_id']] = nb_oasis_int(0)\n        profile_id_to_profile_index[fm_profile[i]['profile_id']]['i_end'] = i + 1\n        if last_profile_id != fm_profile[i]['profile_id']:\n            profile_id_to_profile_index[fm_profile[i]['profile_id']]['i_start'] = i\n            last_profile_id = fm_profile[i]['profile_id']\n\n    # in fm_programme check if multi-peril and get size of each levels\n    max_level = np.max(fm_programme['level_id'])\n    level_node_len = np.zeros(max_level + 1, dtype=oasis_int)\n    multi_peril = False\n    for i in range(fm_programme.shape[0]):\n        programme = fm_programme[i]\n        if programme['level_id'] == 1 and programme['from_agg_id'] != programme['to_agg_id']:\n            multi_peril = True\n        if level_node_len[programme['level_id'] - 1] < programme['from_agg_id']:\n            level_node_len[programme['level_id'] - 1] = programme['from_agg_id']\n\n        if level_node_len[programme['level_id']] < programme['to_agg_id']:\n            level_node_len[programme['level_id']] = programme['to_agg_id']\n\n    ##### fm_policytc (level_id agg_id layer_id => profile_id) #####\n    # programme_node_to_profiles dict of (level_id, agg_id) => list of (layer_id, policy_index_start, policy_index_end)\n    # for each policy needing tiv, we duplicate the policy for each node to then later on calculate the % tiv parameters\n    programme_node_to_profiles = Dict.empty(node_type, List.empty_list(layer_type))\n    programme_node_to_layers = Dict.empty(node_type, List.empty_list(layer_type))\n    i_new_fm_profile = fm_profile.shape[0]\n    new_fm_profile_list = List.empty_list(np.int64)\n    for i in range(fm_policytc.shape[0]):\n        policytc = fm_policytc[i]\n        programme_node = (nb_oasis_int(policytc['level_id']), nb_oasis_int(policytc['agg_id']))\n        i_start = profile_id_to_profile_index[nb_oasis_int(policytc['profile_id'])]['i_start']\n        i_end = profile_id_to_profile_index[nb_oasis_int(policytc['profile_id'])]['i_end']\n\n        if policytc['profile_id'] in has_tiv_policy:\n            if has_tiv_policy[policytc['profile_id']]:\n                for j in range(i_start, i_end):\n                    new_fm_profile_list.append(j)\n                i_start, i_end = i_new_fm_profile, i_new_fm_profile + i_end - i_start\n                i_new_fm_profile = i_end\n            else:\n                has_tiv_policy[policytc['profile_id']] = nb_oasis_int(1)\n\n        layer = (nb_oasis_int(policytc['layer_id']), nb_oasis_int(i_start), nb_oasis_int(i_end))\n\n        if programme_node not in programme_node_to_profiles:\n            _list = List.empty_list(layer_type)\n            _list.append(layer)\n            programme_node_to_profiles[programme_node] = _list\n        else:\n            programme_node_to_profiles[programme_node].append(layer)\n\n    # create a new fm_profile with all the needed duplicated %tiv profiles\n    if i_new_fm_profile - fm_profile.shape[0]:\n\n\"\"\"Docstring (excerpt)\"\"\"\n:param allocation_rule:\n    option to indicate out the loss are allocated to the output\n:param fm_programme:\n    structure of the levels\n:param fm_policytc:\n    structure of the layers and policy_id to apply\n:param fm_profile:\n    definition of the policy_id\n:param fm_xref:\n    mapping between the output of the allocation and output item_id\n:return:\n    compute_infos:\n    nodes_array:\n    node_parents_array:\n    node_profiles_array:\n    output_array:"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/financial_structure.py::create_financial_structure@617",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/financial_structure.py",
      "symbol_type": "function",
      "name": "create_financial_structure",
      "lineno": 617,
      "end_lineno": 648,
      "business_stage": "fm",
      "docstring": ":param allocation_rule: int\n        back-allocation rule\n:param static_path: string\n        path to the static files\n:return:\n    compute_queue : the step of the computation to perform on each event\n    node_indexes : map node to index of item in result array\n    index_dependencies : map node to its dependent indexes\n    node_profile : map node to profile\n    output_item_index : list of item_id, index to put in the output",
      "content": "# File: oasislmf/pytools/fm/financial_structure.py\n# function: create_financial_structure (lines 617-648)\n\ndef create_financial_structure(allocation_rule, static_path):\n    \"\"\"\n    :param allocation_rule: int\n            back-allocation rule\n    :param static_path: string\n            path to the static files\n    :return:\n        compute_queue : the step of the computation to perform on each event\n        node_indexes : map node to index of item in result array\n        index_dependencies : map node to its dependent indexes\n        node_profile : map node to profile\n        output_item_index : list of item_id, index to put in the output\n    \"\"\"\n\n    if allocation_rule not in allowed_allocation_rule:\n        raise ValueError(f\"allocation_rule must be in {allowed_allocation_rule}, found {allocation_rule}\")\n    if allocation_rule == 3:\n        allocation_rule = 2\n\n    fm_programme, fm_policytc, fm_profile, stepped, fm_xref, items, coverages = load_static(static_path)\n    financial_structure = extract_financial_structure(allocation_rule, fm_programme, fm_policytc, fm_profile,\n                                                      stepped, fm_xref, items, coverages)\n    compute_info, nodes_array, node_parents_array, node_profiles_array, output_array, fm_profile = financial_structure\n    logger.info(f'nodes_array has {len(nodes_array)} elements')\n    logger.info(f'compute_info : {dict(zip(compute_info.dtype.names, compute_info[0]))}')\n\n    np.save(os.path.join(static_path, f'compute_info_{allocation_rule}'), compute_info)\n    np.save(os.path.join(static_path, f'nodes_array_{allocation_rule}'), nodes_array)\n    np.save(os.path.join(static_path, f'node_parents_array_{allocation_rule}'), node_parents_array)\n    np.save(os.path.join(static_path, f'node_profiles_array_{allocation_rule}'), node_profiles_array)\n    np.save(os.path.join(static_path, f'output_array_{allocation_rule}'), output_array)\n    np.save(os.path.join(static_path, 'fm_profile'), fm_profile)\n\n\"\"\"Docstring (excerpt)\"\"\"\n:param allocation_rule: int\n        back-allocation rule\n:param static_path: string\n        path to the static files\n:return:\n    compute_queue : the step of the computation to perform on each event\n    node_indexes : map node to index of item in result array\n    index_dependencies : map node to its dependent indexes\n    node_profile : map node to profile\n    output_item_index : list of item_id, index to put in the output"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_1@19",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_1",
      "lineno": 19,
      "end_lineno": 30,
      "business_stage": "fm",
      "docstring": "Deductible and limit",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_1 (lines 19-30)\n\ndef calcrule_1(policy, loss_out, loss_in):\n    \"\"\"\n    Deductible and limit\n    \"\"\"\n    lim = policy['limit1'] + policy['deductible1']\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= policy['deductible1']:\n            loss_out[i] = 0\n        elif loss_in[i] <= lim:\n            loss_out[i] = loss_in[i] - policy['deductible1']\n        else:\n            loss_out[i] = policy['limit1']\n\n\"\"\"Docstring (excerpt)\"\"\"\nDeductible and limit"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_2@34",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_2",
      "lineno": 34,
      "end_lineno": 48,
      "business_stage": "fm",
      "docstring": "Deductible, attachment, limit and share",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_2 (lines 34-48)\n\ndef calcrule_2(policy, loss_out, loss_in):\n    \"\"\"\n    Deductible, attachment, limit and share\n\n    \"\"\"\n    ded_att = policy['deductible1'] + policy['attachment1']\n    lim = policy['limit1'] + ded_att\n    maxi = policy['limit1'] * policy['share1']\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= ded_att:\n            loss_out[i] = 0\n        elif loss_in[i] <= lim:\n            loss_out[i] = (loss_in[i] - ded_att) * policy['share1']\n        else:\n            loss_out[i] = maxi\n\n\"\"\"Docstring (excerpt)\"\"\"\nDeductible, attachment, limit and share"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_3@52",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_3",
      "lineno": 52,
      "end_lineno": 62,
      "business_stage": "fm",
      "docstring": "Franchise deductible and limit",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_3 (lines 52-62)\n\ndef calcrule_3(policy, loss_out, loss_in):\n    \"\"\"\n    Franchise deductible and limit\n    \"\"\"\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= policy['deductible1']:\n            loss_out[i] = 0\n        elif loss_in[i] <= policy['limit1']:\n            loss_out[i] = loss_in[i]\n        else:\n            loss_out[i] = policy['limit1']\n\n\"\"\"Docstring (excerpt)\"\"\"\nFranchise deductible and limit"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_5@66",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_5",
      "lineno": 66,
      "end_lineno": 77,
      "business_stage": "fm",
      "docstring": "Deductible and limit as a proportion of loss",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_5 (lines 66-77)\n\ndef calcrule_5(policy, loss_out, loss_in):\n    \"\"\"\n    Deductible and limit as a proportion of loss\n    \"\"\"\n    effective_deductible = loss_in * policy['deductible1']\n    effective_limit = loss_in * policy['limit1']\n    if policy['deductible1'] + policy['limit1'] >= 1:  # always under limit\n        for i in range(loss_in.shape[0]):\n            loss_out[i] = loss_in[i] - effective_deductible[i]\n\n    else:  # always over limit\n        loss_out[:] = effective_limit\n\n\"\"\"Docstring (excerpt)\"\"\"\nDeductible and limit as a proportion of loss"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_12@81",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_12",
      "lineno": 81,
      "end_lineno": 89,
      "business_stage": "fm",
      "docstring": "Deductible only",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_12 (lines 81-89)\n\ndef calcrule_12(policy, loss_out, loss_in):\n    \"\"\"\n    Deductible only\n    \"\"\"\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= policy['deductible1']:\n            loss_out[i] = 0\n        else:\n            loss_out[i] = loss_in[i] - policy['deductible1']\n\n\"\"\"Docstring (excerpt)\"\"\"\nDeductible only"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_14@93",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_14",
      "lineno": 93,
      "end_lineno": 101,
      "business_stage": "fm",
      "docstring": "Limit only",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_14 (lines 93-101)\n\ndef calcrule_14(policy, loss_out, loss_in):\n    \"\"\"\n    Limit only\n    \"\"\"\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= policy['limit1']:\n            loss_out[i] = loss_in[i]\n        else:\n            loss_out[i] = policy['limit1']\n\n\"\"\"Docstring (excerpt)\"\"\"\nLimit only"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_15@105",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_15",
      "lineno": 105,
      "end_lineno": 116,
      "business_stage": "fm",
      "docstring": "deductible and limit % loss",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_15 (lines 105-116)\n\ndef calcrule_15(policy, loss_out, loss_in):\n    \"\"\"\n    deductible and limit % loss\n    \"\"\"\n    effective_limit = policy['deductible1'] / (1 - policy['limit1'])\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= policy['deductible1']:\n            loss_out[i] = 0\n        elif loss_in[i] <= effective_limit:\n            loss_out[i] = loss_in[i] - policy['deductible1']\n        else:\n            loss_out[i] = loss_in[i] * policy['limit1']\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible and limit % loss"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_16@120",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_16",
      "lineno": 120,
      "end_lineno": 124,
      "business_stage": "fm",
      "docstring": "deductible % loss",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_16 (lines 120-124)\n\ndef calcrule_16(policy, loss_out, loss_in):\n    \"\"\"\n    deductible % loss\n    \"\"\"\n    loss_out[:] = loss_in * (1 - policy['deductible1'])\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible % loss"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_17@128",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_17",
      "lineno": 128,
      "end_lineno": 144,
      "business_stage": "fm",
      "docstring": "deductible % loss with attachment, limit and share",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_17 (lines 128-144)\n\ndef calcrule_17(policy, loss_out, loss_in):\n    \"\"\"\n    deductible % loss with attachment, limit and share\n    \"\"\"\n    if policy['deductible1'] >= 1:\n        loss_out.fill(0)\n    else:\n        post_ded_attachment = policy['attachment1'] / (1 - policy['deductible1'])\n        post_ded_attachment_limit = (policy['attachment1'] + policy['limit1']) / (1 - policy['deductible1'])\n        maxi = policy['limit1'] * policy['share1']\n        for i in range(loss_in.shape[0]):\n            if loss_in[i] <= post_ded_attachment:\n                loss_out[i] = 0\n            elif loss_in[i] <= post_ded_attachment_limit:\n                loss_out[i] = (loss_in[i] * (1 - policy['deductible1']) - policy['attachment1']) * policy['share1']\n            else:\n                loss_out[i] = maxi\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible % loss with attachment, limit and share"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_20@148",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_20",
      "lineno": 148,
      "end_lineno": 156,
      "business_stage": "fm",
      "docstring": "reverse franchise deductible",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_20 (lines 148-156)\n\ndef calcrule_20(policy, loss_out, loss_in):\n    \"\"\"\n    reverse franchise deductible\n    \"\"\"\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] > policy['deductible1']:\n            loss_out[i] = 0\n        else:\n            loss_out[i] = loss_in[i]\n\n\"\"\"Docstring (excerpt)\"\"\"\nreverse franchise deductible"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_22@160",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_22",
      "lineno": 160,
      "end_lineno": 174,
      "business_stage": "fm",
      "docstring": "reinsurance % ceded, limit and % placed",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_22 (lines 160-174)\n\ndef calcrule_22(policy, loss_out, loss_in):\n    \"\"\"\n    reinsurance % ceded, limit and % placed\n    \"\"\"\n    if policy['share1'] == 0:\n        loss_out.fill(0)\n    else:\n        pre_share_limit = policy['limit1'] / policy['share1']\n        all_share = policy['share1'] * policy['share2'] * policy['share3']\n        maxi = policy['limit1'] * policy['share2'] * policy['share3']\n        for i in range(loss_in.shape[0]):\n            if loss_in[i] <= pre_share_limit:\n                loss_out[i] = loss_in[i] * all_share\n            else:\n                loss_out[i] = maxi\n\n\"\"\"Docstring (excerpt)\"\"\"\nreinsurance % ceded, limit and % placed"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_23@178",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_23",
      "lineno": 178,
      "end_lineno": 188,
      "business_stage": "fm",
      "docstring": "reinsurance limit and % placed",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_23 (lines 178-188)\n\ndef calcrule_23(policy, loss_out, loss_in):\n    \"\"\"\n    reinsurance limit and % placed\n    \"\"\"\n    all_share = policy['share2'] * policy['share3']\n    maxi = policy['limit1'] * all_share\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= policy['limit1']:\n            loss_out[i] = loss_in[i] * all_share\n        else:\n            loss_out[i] = maxi\n\n\"\"\"Docstring (excerpt)\"\"\"\nreinsurance limit and % placed"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_24@192",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_24",
      "lineno": 192,
      "end_lineno": 210,
      "business_stage": "fm",
      "docstring": "reinsurance excess terms",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_24 (lines 192-210)\n\ndef calcrule_24(policy, loss_out, loss_in):\n    \"\"\"\n    reinsurance excess terms\n    \"\"\"\n    if policy['share1'] == 0:\n        loss_out.fill(0)\n    else:\n        pre_share_attachment = policy['attachment1'] / policy['share1']\n        pre_share_attachment_limit = (policy['limit1'] + policy['attachment1']) / policy['share1']\n        attachment_share = policy['attachment1'] * policy['share2'] * policy['share3']\n        all_share = policy['share1'] * policy['share2'] * policy['share3']\n        maxi = policy['limit1'] * policy['share2'] * policy['share3']\n        for i in range(loss_in.shape[0]):\n            if loss_in[i] <= pre_share_attachment:\n                loss_out[i] = 0\n            elif loss_in[i] <= pre_share_attachment_limit:\n                loss_out[i] = loss_in[i] * all_share - attachment_share\n            else:\n                loss_out[i] = maxi\n\n\"\"\"Docstring (excerpt)\"\"\"\nreinsurance excess terms"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_25@214",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_25",
      "lineno": 214,
      "end_lineno": 218,
      "business_stage": "fm",
      "docstring": "reinsurance proportional terms",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_25 (lines 214-218)\n\ndef calcrule_25(policy, loss_out, loss_in):\n    \"\"\"\n    reinsurance proportional terms\n    \"\"\"\n    loss_out[:] = loss_in * (policy['share1'] * policy['share2'] * policy['share3'])\n\n\"\"\"Docstring (excerpt)\"\"\"\nreinsurance proportional terms"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_28@222",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_28",
      "lineno": 222,
      "end_lineno": 232,
      "business_stage": "fm",
      "docstring": "% loss step payout\nnote that 1 was added to scale1 in the precompute step",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_28 (lines 222-232)\n\ndef calcrule_28(policy, loss_out, loss_in):\n    \"\"\"\n    % loss step payout\n    note that 1 was added to scale1 in the precompute step\n    \"\"\"\n    if policy['step_id'] == 1:\n        loss_out.fill(0)\n    for i in range(loss_in.shape[0]):\n        if policy['trigger_start'] <= loss_in[i] < policy['trigger_end']:\n            loss = max(policy['payout_start'] * loss_in[i] - policy['deductible1'], 0)\n            loss_out[i] = (loss + min(loss * policy['scale2'], policy['limit2'])) * policy['scale1']\n\n\"\"\"Docstring (excerpt)\"\"\"\n% loss step payout\nnote that 1 was added to scale1 in the precompute step"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_281@236",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_281",
      "lineno": 236,
      "end_lineno": 244,
      "business_stage": "fm",
      "docstring": "conditional coverage",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_281 (lines 236-244)\n\ndef calcrule_281(policy, loss_out, loss_in):\n    \"\"\"\n    conditional coverage\n    \"\"\"\n    if policy['step_id'] == 1:\n        loss_out.fill(0)\n    for i in range(loss_in.shape[0]):\n        if policy['trigger_start'] <= loss_in[i] < policy['trigger_end']:\n            loss_out[i] += min(loss_out[i] * policy['scale2'], policy['limit2']) * policy['scale1']\n\n\"\"\"Docstring (excerpt)\"\"\"\nconditional coverage"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_32@248",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_32",
      "lineno": 248,
      "end_lineno": 257,
      "business_stage": "fm",
      "docstring": "monetary amount trigger and % loss step payout with limit",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_32 (lines 248-257)\n\ndef calcrule_32(policy, loss_out, loss_in):\n    \"\"\"\n    monetary amount trigger and % loss step payout with limit\n    \"\"\"\n    if policy['step_id'] == 1:\n        loss_out.fill(0)\n    for i in range(loss_in.shape[0]):\n        if policy['trigger_start'] <= loss_in[i]:\n            loss = min(policy['payout_start'] * loss_in[i], policy['limit1'])\n            loss_out[i] += (loss + min(loss * policy['scale2'], policy['limit2'])) * policy['scale1']\n\n\"\"\"Docstring (excerpt)\"\"\"\nmonetary amount trigger and % loss step payout with limit"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_33@261",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_33",
      "lineno": 261,
      "end_lineno": 274,
      "business_stage": "fm",
      "docstring": "deductible % loss with limit",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_33 (lines 261-274)\n\ndef calcrule_33(policy, loss_out, loss_in):\n    \"\"\"\n    deductible % loss with limit\n\n    \"\"\"\n    if policy['deductible1'] >= 1:\n        loss_out.fill(0)\n    else:\n        post_ded_limit = policy['limit1'] / (1 - policy['deductible1'])\n        for i in range(loss_in.shape[0]):\n            if loss_in[i] <= post_ded_limit:\n                loss_out[i] = loss_in[i] * (1 - policy['deductible1'])\n            else:\n                loss_out[i] = policy['limit1']\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible % loss with limit"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_34@278",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_34",
      "lineno": 278,
      "end_lineno": 289,
      "business_stage": "fm",
      "docstring": "deductible with attachment and share\n\nTODO: compare to the cpp, as there is shares, deductible won't be use later on so no need to compute it",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_34 (lines 278-289)\n\ndef calcrule_34(policy, loss_out, loss_in):\n    \"\"\"\n    deductible with attachment and share\n\n    TODO: compare to the cpp, as there is shares, deductible won't be use later on so no need to compute it\n    \"\"\"\n    ded_att = policy['deductible1'] + policy['attachment1']\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= ded_att:\n            loss_out[i] = 0\n        else:\n            loss_out[i] = (loss_in[i] - ded_att) * policy['share1']\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible with attachment and share\n\nTODO: compare to the cpp, as there is shares, deductible won't be use later on so no need to compute it"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_37@293",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_37",
      "lineno": 293,
      "end_lineno": 302,
      "business_stage": "fm",
      "docstring": "% loss step payout",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_37 (lines 293-302)\n\ndef calcrule_37(policy, loss_out, loss_in):\n    \"\"\"\n    % loss step payout\n    \"\"\"\n    if policy['step_id'] == 1:\n        loss_out.fill(0)\n    for i in range(loss_in.shape[0]):\n        if policy['trigger_start'] <= loss_in[i] < policy['trigger_end']:\n            loss = min(max(policy['payout_start'] * loss_in[i] - policy['deductible1'], 0), policy['limit1'])\n            loss_out[i] = (loss + min(loss * policy['scale2'], policy['limit2'])) * policy['scale1']\n\n\"\"\"Docstring (excerpt)\"\"\"\n% loss step payout"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_38@306",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_38",
      "lineno": 306,
      "end_lineno": 314,
      "business_stage": "fm",
      "docstring": "conditional coverage",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_38 (lines 306-314)\n\ndef calcrule_38(policy, loss_out, loss_in):\n    \"\"\"\n    conditional coverage\n    \"\"\"\n    if policy['step_id'] == 1:\n        loss_out.fill(0)\n    for i in range(loss_in.shape[0]):\n        if policy['trigger_start'] <= loss_in[i] < policy['trigger_end']:\n            loss_out[i] = (loss_out[i] + min(loss_out[i] * policy['scale2'], policy['limit2'])) * (policy['scale1'])\n\n\"\"\"Docstring (excerpt)\"\"\"\nconditional coverage"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_39@318",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_39",
      "lineno": 318,
      "end_lineno": 326,
      "business_stage": "fm",
      "docstring": "Franchise deductible",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_39 (lines 318-326)\n\ndef calcrule_39(policy, loss_out, loss_in):\n    \"\"\"\n    Franchise deductible\n    \"\"\"\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= policy['deductible1']:\n            loss_out[i] = 0\n        else:\n            loss_out[i] = loss_in[i]\n\n\"\"\"Docstring (excerpt)\"\"\"\nFranchise deductible"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_40@330",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_40",
      "lineno": 330,
      "end_lineno": 341,
      "business_stage": "fm",
      "docstring": "BI deductible (waiting period) and limit (period of interest)",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_40 (lines 330-341)\n\ndef calcrule_40(policy, loss_out, loss_in):\n    \"\"\"\n    BI deductible (waiting period) and limit (period of interest)\n    \"\"\"\n    lim = policy['limit1'] + policy['deductible1']\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= policy['deductible1']:\n            loss_out[i] = 0\n        elif loss_in[i] <= lim:\n            loss_out[i] = loss_in[i] - policy['deductible1']\n        else:\n            loss_out[i] = policy['limit1']\n\n\"\"\"Docstring (excerpt)\"\"\"\nBI deductible (waiting period) and limit (period of interest)"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy.py::calcrule_41@345",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy.py",
      "symbol_type": "function",
      "name": "calcrule_41",
      "lineno": 345,
      "end_lineno": 353,
      "business_stage": "fm",
      "docstring": "No BI deductible (waiting period) and limit only (period of interest)",
      "content": "# File: oasislmf/pytools/fm/policy.py\n# function: calcrule_41 (lines 345-353)\n\ndef calcrule_41(policy, loss_out, loss_in):\n    \"\"\"\n    No BI deductible (waiting period) and limit only (period of interest)\n    \"\"\"\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= policy['limit1']:\n            loss_out[i] = loss_in[i]\n        else:\n            loss_out[i] = policy['limit1']\n\n\"\"\"Docstring (excerpt)\"\"\"\nNo BI deductible (waiting period) and limit only (period of interest)"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::deductible_over_max@22",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "deductible_over_max",
      "lineno": 22,
      "end_lineno": 46,
      "business_stage": "fm",
      "docstring": "deductible is over maximum deductible, we reduce the loss, therefore increase the loss up to under_limit\n\nunder limit is always the minimum between the limit - loss and  the sub_node under_limit + the applied deductible\nso we are sure that if deductible[i] > max_ded_left, we are sure that under_limit is the good cap\nwe are sure that is there is no sub node with limit loss_delta < under_limit\n\nloss delta can be negative so in this case we have to be careful it is not bigger than loss_in",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: deductible_over_max (lines 22-46)\n\ndef deductible_over_max(i, loss_out, loss_in, deductible, over_limit, under_limit, max_deductible):\n    \"\"\"\n    deductible is over maximum deductible, we reduce the loss, therefore increase the loss up to under_limit\n\n    under limit is always the minimum between the limit - loss and  the sub_node under_limit + the applied deductible\n    so we are sure that if deductible[i] > max_ded_left, we are sure that under_limit is the good cap\n    we are sure that is there is no sub node with limit loss_delta < under_limit\n\n    loss delta can be negative so in this case we have to be careful it is not bigger than loss_in\n\n    \"\"\"\n    loss_delta = deductible[i] - max_deductible\n    if loss_delta > under_limit[i]:\n        loss_out[i] = loss_in[i] + under_limit[i]\n        over_limit[i] += loss_delta - under_limit[i]\n        deductible[i] = max_deductible\n        under_limit[i] = 0\n    elif loss_in[i] >= -loss_delta:\n        loss_out[i] = loss_in[i] + loss_delta\n        under_limit[i] -= loss_delta\n        deductible[i] = max_deductible\n    else:\n        loss_out[i] = 0\n        deductible[i] += loss_in[i]\n        under_limit[i] += loss_in[i]\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible is over maximum deductible, we reduce the loss, therefore increase the loss up to under_limit\n\nunder limit is always the minimum between the limit - loss and  the sub_node under_limit + the applied deductible\nso we are sure that if deductible[i] > max_ded_left, we are sure that under_limit is the good cap\nwe are sure that is there is no sub node with limit loss_delta < under_limit\n\nloss delta can be negative so in this case we have to be careful it is not bigger than loss_in"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::deductible_under_min@50",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "deductible_under_min",
      "lineno": 50,
      "end_lineno": 83,
      "business_stage": "fm",
      "docstring": "Deductible is under the minimum, we raise the deductible from over_limit first then loss if over_limit is not enough",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: deductible_under_min (lines 50-83)\n\ndef deductible_under_min(i, loss_out, loss_in, effective_deductible, over_limit, under_limit, min_deductible, deductible):\n    \"\"\"\n    Deductible is under the minimum, we raise the deductible from over_limit first then loss if over_limit is not enough\n\n    \"\"\"\n    loss_delta = min_deductible - deductible - effective_deductible[i]\n    if loss_delta <= over_limit[i]:  # we have enough over_limit to cover loss_delta\n        if loss_in[i] > deductible:  # we have enough loss to cover deductible\n            loss_out[i] = loss_in[i]\n            over_limit[i] -= loss_delta\n            effective_deductible[i] = min_deductible\n        elif (over_limit[i] - loss_delta) + loss_in[i] > deductible:  # not enough loss, we also reduce the over_limit\n            loss_out[i] = 0\n            over_limit[i] -= loss_delta + deductible\n            effective_deductible[i] = min_deductible\n            under_limit[i] += loss_in[i]\n        else:\n            effective_deductible[i] += loss_in[i] + over_limit[i]\n            loss_out[i] = 0\n            over_limit[i] = 0\n            under_limit[i] += loss_in[i]\n\n    else:\n        loss_not_over_limit = loss_delta - over_limit[i]\n        if loss_in[i] > loss_not_over_limit + deductible:  # we have enough loss after we use over_limit pool\n            loss_out[i] = loss_in[i] - loss_not_over_limit - deductible\n            over_limit[i] = 0\n            effective_deductible[i] = min_deductible\n            under_limit[i] += loss_not_over_limit\n        else:\n            loss_out[i] = 0\n            effective_deductible[i] += loss_in[i] + over_limit[i]\n            under_limit[i] += loss_in[i]\n            over_limit[i] = 0\n\n\"\"\"Docstring (excerpt)\"\"\"\nDeductible is under the minimum, we raise the deductible from over_limit first then loss if over_limit is not enough"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_1@87",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_1",
      "lineno": 87,
      "end_lineno": 105,
      "business_stage": "fm",
      "docstring": "Deductible and limit",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_1 (lines 87-105)\n\ndef calcrule_1(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    Deductible and limit\n    \"\"\"\n    lim = policy['limit1'] + policy['deductible1']\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= policy['deductible1']:\n            under_limit[i] = min2(under_limit[i] + loss_in[i], policy['limit1'])\n            deductible[i] += loss_in[i]\n            loss_out[i] = 0\n        elif loss_in[i] <= lim:\n            under_limit[i] = min2(under_limit[i] + policy['deductible1'], lim - loss_in[i])\n            deductible[i] += policy['deductible1']\n            loss_out[i] = loss_in[i] - policy['deductible1']\n        else:\n            over_limit[i] += loss_in[i] - lim\n            under_limit[i] = 0\n            deductible[i] += policy['deductible1']\n            loss_out[i] = policy['limit1']\n\n\"\"\"Docstring (excerpt)\"\"\"\nDeductible and limit"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_2@109",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_2",
      "lineno": 109,
      "end_lineno": 123,
      "business_stage": "fm",
      "docstring": "Deductible, attachment, limit and share",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_2 (lines 109-123)\n\ndef calcrule_2(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    Deductible, attachment, limit and share\n\n    \"\"\"\n    ded_att = policy['deductible1'] + policy['attachment1']\n    lim = policy['limit1'] + ded_att\n    maxi = policy['limit1'] * policy['share1']\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= ded_att:\n            loss_out[i] = 0\n        elif loss_in[i] <= lim:\n            loss_out[i] = (loss_in[i] - ded_att) * policy['share1']\n        else:\n            loss_out[i] = maxi\n\n\"\"\"Docstring (excerpt)\"\"\"\nDeductible, attachment, limit and share"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_3@127",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_3",
      "lineno": 127,
      "end_lineno": 142,
      "business_stage": "fm",
      "docstring": "Franchise deductible and limit",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_3 (lines 127-142)\n\ndef calcrule_3(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    Franchise deductible and limit\n    \"\"\"\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= policy['deductible1']:\n            under_limit[i] = min2(under_limit[i] + loss_in[i], policy['limit1'])\n            deductible[i] += loss_in[i]\n            loss_out[i] = 0\n        elif loss_in[i] <= policy['limit1']:\n            under_limit[i] = min2(under_limit[i], policy['limit1'] - loss_in[i])\n            loss_out[i] = loss_in[i]\n        else:\n            under_limit[i] = 0\n            over_limit[i] += loss_in[i] - policy['limit1']\n            loss_out[i] = policy['limit1']\n\n\"\"\"Docstring (excerpt)\"\"\"\nFranchise deductible and limit"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_5@146",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_5",
      "lineno": 146,
      "end_lineno": 161,
      "business_stage": "fm",
      "docstring": "Deductible and limit as a proportion of loss",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_5 (lines 146-161)\n\ndef calcrule_5(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    Deductible and limit as a proportion of loss\n    \"\"\"\n    effective_deductible = loss_in * policy['deductible1']\n    effective_limit = loss_in * policy['limit1']\n    deductible += effective_deductible\n    if policy['deductible1'] + policy['limit1'] >= 1:  # always under limit\n        for i in range(loss_in.shape[0]):\n            loss_out[i] = loss_in[i] - effective_deductible[i]\n            under_limit[i] = min2(effective_limit[i] - loss_out[i], under_limit[i] + effective_deductible[i])\n\n    else:  # always over limit\n        loss_out[:] = effective_limit\n        over_limit += loss_in - effective_deductible - effective_limit\n        under_limit[:] = 0\n\n\"\"\"Docstring (excerpt)\"\"\"\nDeductible and limit as a proportion of loss"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_7@165",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_7",
      "lineno": 165,
      "end_lineno": 193,
      "business_stage": "fm",
      "docstring": "deductible, minimum and maximum deductible, with limit",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_7 (lines 165-193)\n\ndef calcrule_7(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    deductible, minimum and maximum deductible, with limit\n    \"\"\"\n\n    max_ded_left = policy['deductible3'] - policy['deductible1']\n    min_ded_left = policy['deductible2'] - policy['deductible1']\n\n    for i in range(loss_in.shape[0]):\n        if deductible[i] > max_ded_left:\n            deductible_over_max(i, loss_out, loss_in, deductible, over_limit, under_limit, policy['deductible3'])\n        elif deductible[i] < min_ded_left:\n            deductible_under_min(i, loss_out, loss_in, deductible, over_limit, under_limit, policy['deductible2'], policy['deductible1'])\n        else:\n            if loss_in[i] > policy['deductible1']:\n                loss_out[i] = loss_in[i] - policy['deductible1']\n                deductible[i] += policy['deductible1']\n                under_limit[i] += policy['deductible1']\n            else:\n                loss_out[i] = 0\n                deductible[i] += loss_in[i]\n                under_limit[i] += loss_in[i]\n\n        if loss_out[i] > policy['limit1']:\n            over_limit[i] += loss_out[i] - policy['limit1']\n            under_limit[i] = 0\n            loss_out[i] = policy['limit1']\n        else:\n            under_limit[i] = min2(policy['limit1'] - loss_out[i], under_limit[i])\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible, minimum and maximum deductible, with limit"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_8@197",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_8",
      "lineno": 197,
      "end_lineno": 220,
      "business_stage": "fm",
      "docstring": "deductible and minimum deductible, with limit",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_8 (lines 197-220)\n\ndef calcrule_8(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    deductible and minimum deductible, with limit\n    \"\"\"\n    min_ded_left = policy['deductible2'] - policy['deductible1']\n    for i in range(loss_in.shape[0]):\n        if deductible[i] < min_ded_left:\n            deductible_under_min(i, loss_out, loss_in, deductible, over_limit, under_limit, policy['deductible2'], policy['deductible1'])\n        else:\n            if loss_in[i] > policy['deductible1']:\n                loss_out[i] = loss_in[i] - policy['deductible1']\n                deductible[i] += policy['deductible1']\n                under_limit[i] += policy['deductible1']\n            else:\n                loss_out[i] = 0\n                deductible[i] += loss_in[i]\n                under_limit[i] += loss_in[i]\n\n        if loss_out[i] > policy['limit1']:\n            over_limit[i] += loss_out[i] - policy['limit1']\n            under_limit[i] = 0\n            loss_out[i] = policy['limit1']\n        else:\n            under_limit[i] = min2(policy['limit1'] - loss_out[i], under_limit[i])\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible and minimum deductible, with limit"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_10@224",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_10",
      "lineno": 224,
      "end_lineno": 241,
      "business_stage": "fm",
      "docstring": "deductible and maximum deductible",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_10 (lines 224-241)\n\ndef calcrule_10(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    deductible and maximum deductible\n    \"\"\"\n    max_ded_left = policy['deductible3'] - policy['deductible1']\n\n    for i in range(loss_in.shape[0]):\n        if deductible[i] > max_ded_left:\n            deductible_over_max(i, loss_out, loss_in, deductible, over_limit, under_limit, policy['deductible3'])\n        else:\n            if loss_in[i] > policy['deductible1']:\n                loss_out[i] = loss_in[i] - policy['deductible1']\n                deductible[i] += policy['deductible1']\n                under_limit[i] += policy['deductible1']\n            else:\n                loss_out[i] = 0\n                deductible[i] += loss_in[i]\n                under_limit[i] += loss_in[i]\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible and maximum deductible"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_11@245",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_11",
      "lineno": 245,
      "end_lineno": 262,
      "business_stage": "fm",
      "docstring": "deductible and minimum deductible",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_11 (lines 245-262)\n\ndef calcrule_11(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    deductible and minimum deductible\n    \"\"\"\n    min_ded_left = policy['deductible2'] - policy['deductible1']\n\n    for i in range(loss_in.shape[0]):\n        if deductible[i] < min_ded_left:\n            deductible_under_min(i, loss_out, loss_in, deductible, over_limit, under_limit, policy['deductible2'], policy['deductible1'])\n        else:\n            if loss_in[i] > policy['deductible1']:\n                loss_out[i] = loss_in[i] - policy['deductible1']\n                deductible[i] += policy['deductible1']\n                under_limit[i] += policy['deductible1']\n            else:\n                loss_out[i] = 0\n                deductible[i] += loss_in[i]\n                under_limit[i] += loss_in[i]\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible and minimum deductible"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_12@266",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_12",
      "lineno": 266,
      "end_lineno": 278,
      "business_stage": "fm",
      "docstring": "Deductible only",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_12 (lines 266-278)\n\ndef calcrule_12(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    Deductible only\n    \"\"\"\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= policy['deductible1']:\n            under_limit[i] += loss_in[i]\n            deductible[i] += loss_in[i]\n            loss_out[i] = 0\n        else:\n            under_limit[i] += policy['deductible1']\n            deductible[i] += policy['deductible1']\n            loss_out[i] = loss_in[i] - policy['deductible1']\n\n\"\"\"Docstring (excerpt)\"\"\"\nDeductible only"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_13@282",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_13",
      "lineno": 282,
      "end_lineno": 303,
      "business_stage": "fm",
      "docstring": "deductible, minimum and maximum deductible",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_13 (lines 282-303)\n\ndef calcrule_13(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    deductible, minimum and maximum deductible\n    \"\"\"\n\n    max_ded_left = policy['deductible3'] - policy['deductible1']\n    min_ded_left = policy['deductible2'] - policy['deductible1']\n\n    for i in range(loss_in.shape[0]):\n        if deductible[i] > max_ded_left:\n            deductible_over_max(i, loss_out, loss_in, deductible, over_limit, under_limit, policy['deductible3'])\n        elif deductible[i] < min_ded_left:\n            deductible_under_min(i, loss_out, loss_in, deductible, over_limit, under_limit, policy['deductible2'], policy['deductible1'])\n        else:\n            if loss_in[i] > policy['deductible1']:\n                loss_out[i] = loss_in[i] - policy['deductible1']\n                deductible[i] += policy['deductible1']\n                under_limit[i] += policy['deductible1']\n            else:\n                loss_out[i] = 0\n                deductible[i] += loss_in[i]\n                under_limit[i] += loss_in[i]\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible, minimum and maximum deductible"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_14@307",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_14",
      "lineno": 307,
      "end_lineno": 318,
      "business_stage": "fm",
      "docstring": "Limit only",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_14 (lines 307-318)\n\ndef calcrule_14(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    Limit only\n    \"\"\"\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= policy['limit1']:\n            under_limit[i] = min2(policy['limit1'] - loss_in[i], under_limit[i])\n            loss_out[i] = loss_in[i]\n        else:\n            over_limit[i] += loss_in[i] - policy['limit1']\n            under_limit[i] = 0\n            loss_out[i] = policy['limit1']\n\n\"\"\"Docstring (excerpt)\"\"\"\nLimit only"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_15@322",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_15",
      "lineno": 322,
      "end_lineno": 340,
      "business_stage": "fm",
      "docstring": "deductible and limit % loss",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_15 (lines 322-340)\n\ndef calcrule_15(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    deductible and limit % loss\n    \"\"\"\n    effective_limit = policy['deductible1'] / (1 - policy['limit1'])\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= policy['deductible1']:\n            under_limit[i] = min2(effective_limit, under_limit[i] + loss_in[i])\n            loss_out[i] = 0\n            deductible[i] += loss_in[i]\n        elif loss_in[i] <= effective_limit:\n            under_limit[i] = min2(effective_limit - loss_in[i], under_limit[i] + policy['deductible1'])\n            loss_out[i] = loss_in[i] - policy['deductible1']\n            deductible[i] += policy['deductible1']\n        else:\n            loss_out[i] = loss_in[i] * policy['limit1']\n            deductible[i] += policy['deductible1']\n            over_limit[i] += loss_in[i] - loss_out[i] - policy['deductible1']\n            under_limit[i] = 0\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible and limit % loss"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_16@344",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_16",
      "lineno": 344,
      "end_lineno": 351,
      "business_stage": "fm",
      "docstring": "deductible % loss",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_16 (lines 344-351)\n\ndef calcrule_16(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    deductible % loss\n    \"\"\"\n    effective_deductible = loss_in * policy['deductible1']\n    deductible += effective_deductible\n    under_limit += effective_deductible\n    loss_out[:] = loss_in - effective_deductible\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible % loss"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_17@355",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_17",
      "lineno": 355,
      "end_lineno": 372,
      "business_stage": "fm",
      "docstring": "deductible % loss with attachment, limit and share",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_17 (lines 355-372)\n\ndef calcrule_17(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    deductible % loss with attachment, limit and share\n    \"\"\"\n    if policy['deductible1'] >= 1:\n        loss_out.fill(0)\n    else:\n        post_ded_attachment = policy['attachment1'] / (1 - policy['deductible1'])\n        post_ded_attachment_limit = (policy['attachment1'] + policy['limit1']) / (1 - policy['deductible1'])\n        maxi = policy['limit1'] * policy['share1']\n        for i in range(loss_in.shape[0]):\n            effective_deductible = loss_in[i] * policy['deductible1']\n            if loss_in[i] <= post_ded_attachment:\n                loss_out[i] = 0\n            elif loss_in[i] <= post_ded_attachment_limit:\n                loss_out[i] = (loss_in[i] - effective_deductible - policy['attachment1']) * policy['share1']\n            else:\n                loss_out[i] = maxi\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible % loss with attachment, limit and share"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_19@376",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_19",
      "lineno": 376,
      "end_lineno": 397,
      "business_stage": "fm",
      "docstring": "deductible % loss with min and/or max deductible\n\nTODO: check if we can assume 0 <= policy['deductible1'] <= 1",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_19 (lines 376-397)\n\ndef calcrule_19(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    deductible % loss with min and/or max deductible\n\n    TODO: check if we can assume 0 <= policy['deductible1'] <= 1\n    \"\"\"\n\n    for i in range(loss_in.shape[0]):\n        effective_deductible = loss_in[i] * policy['deductible1']\n        if effective_deductible + deductible[i] > policy['deductible3'] > 0:\n            deductible_over_max(i, loss_out, loss_in, deductible, over_limit, under_limit, policy['deductible3'])\n        elif effective_deductible + deductible[i] < policy['deductible2']:\n            deductible_under_min(i, loss_out, loss_in, deductible, over_limit, under_limit, policy['deductible2'], effective_deductible)\n        else:\n            if loss_in[i] > effective_deductible:\n                loss_out[i] = loss_in[i] - effective_deductible\n                deductible[i] += effective_deductible\n                under_limit[i] += effective_deductible\n            else:\n                loss_out[i] = 0\n                deductible[i] += loss_in[i]\n                under_limit[i] += loss_in[i]\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible % loss with min and/or max deductible\n\nTODO: check if we can assume 0 <= policy['deductible1'] <= 1"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_20@401",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_20",
      "lineno": 401,
      "end_lineno": 409,
      "business_stage": "fm",
      "docstring": "reverse franchise deductible",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_20 (lines 401-409)\n\ndef calcrule_20(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    reverse franchise deductible\n    \"\"\"\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] > policy['deductible1']:\n            loss_out[i] = 0\n        else:\n            loss_out[i] = loss_in[i]\n\n\"\"\"Docstring (excerpt)\"\"\"\nreverse franchise deductible"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_22@413",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_22",
      "lineno": 413,
      "end_lineno": 427,
      "business_stage": "fm",
      "docstring": "reinsurance % ceded, limit and % placed",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_22 (lines 413-427)\n\ndef calcrule_22(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    reinsurance % ceded, limit and % placed\n    \"\"\"\n    if policy['share1'] == 0:\n        loss_out.fill(0)\n    else:\n        pre_share_limit = policy['limit1'] / policy['share1']\n        all_share = policy['share1'] * policy['share2'] * policy['share3']\n        maxi = policy['limit1'] * policy['share2'] * policy['share3']\n        for i in range(loss_in.shape[0]):\n            if loss_in[i] <= pre_share_limit:\n                loss_out[i] = loss_in[i] * all_share\n            else:\n                loss_out[i] = maxi\n\n\"\"\"Docstring (excerpt)\"\"\"\nreinsurance % ceded, limit and % placed"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_23@431",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_23",
      "lineno": 431,
      "end_lineno": 441,
      "business_stage": "fm",
      "docstring": "reinsurance limit and % placed",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_23 (lines 431-441)\n\ndef calcrule_23(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    reinsurance limit and % placed\n    \"\"\"\n    all_share = policy['share2'] * policy['share3']\n    maxi = policy['limit1'] * all_share\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= policy['limit1']:\n            loss_out[i] = loss_in[i] * all_share\n        else:\n            loss_out[i] = maxi\n\n\"\"\"Docstring (excerpt)\"\"\"\nreinsurance limit and % placed"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_24@445",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_24",
      "lineno": 445,
      "end_lineno": 463,
      "business_stage": "fm",
      "docstring": "reinsurance excess terms",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_24 (lines 445-463)\n\ndef calcrule_24(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    reinsurance excess terms\n    \"\"\"\n    if policy['share1'] == 0:\n        loss_out.fill(0)\n    else:\n        pre_share_attachment = policy['attachment1'] / policy['share1']\n        pre_share_attachment_limit = (policy['limit1'] + policy['attachment1']) / policy['share1']\n        attachment_share = policy['attachment1'] * policy['share2'] * policy['share3']\n        all_share = policy['share1'] * policy['share2'] * policy['share3']\n        maxi = policy['limit1'] * policy['share2'] * policy['share3']\n        for i in range(loss_in.shape[0]):\n            if loss_in[i] <= pre_share_attachment:\n                loss_out[i] = 0\n            elif loss_in[i] <= pre_share_attachment_limit:\n                loss_out[i] = loss_in[i] * all_share - attachment_share\n            else:\n                loss_out[i] = maxi\n\n\"\"\"Docstring (excerpt)\"\"\"\nreinsurance excess terms"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_25@467",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_25",
      "lineno": 467,
      "end_lineno": 471,
      "business_stage": "fm",
      "docstring": "reinsurance proportional terms",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_25 (lines 467-471)\n\ndef calcrule_25(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    reinsurance proportional terms\n    \"\"\"\n    loss_out[:] = loss_in * (policy['share1'] * policy['share2'] * policy['share3'])\n\n\"\"\"Docstring (excerpt)\"\"\"\nreinsurance proportional terms"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_26@475",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_26",
      "lineno": 475,
      "end_lineno": 503,
      "business_stage": "fm",
      "docstring": "deductible % loss with min and/or max deductible and limit\n\nTODO: check if we can assume 0 <= policy['deductible1'] <= 1",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_26 (lines 475-503)\n\ndef calcrule_26(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    deductible % loss with min and/or max deductible and limit\n\n    TODO: check if we can assume 0 <= policy['deductible1'] <= 1\n    \"\"\"\n\n    for i in range(loss_in.shape[0]):\n        effective_deductible = loss_in[i] * policy['deductible1']\n        if effective_deductible + deductible[i] > policy['deductible3'] > 0:\n            deductible_over_max(i, loss_out, loss_in, deductible, over_limit, under_limit, policy['deductible3'])\n        elif effective_deductible + deductible[i] < policy['deductible2']:\n            deductible_under_min(i, loss_out, loss_in, deductible, over_limit, under_limit, policy['deductible2'], effective_deductible)\n        else:\n            if loss_in[i] > effective_deductible:\n                loss_out[i] = loss_in[i] - effective_deductible\n                deductible[i] += effective_deductible\n                under_limit[i] += effective_deductible\n            else:\n                loss_out[i] = 0\n                deductible[i] += loss_in[i]\n                under_limit[i] += loss_in[i]\n\n        if loss_out[i] > policy['limit1']:\n            over_limit[i] += loss_out[i] - policy['limit1']\n            under_limit[i] = 0\n            loss_out[i] = policy['limit1']\n        else:\n            under_limit[i] = min2(policy['limit1'] - loss_out[i], under_limit[i])\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible % loss with min and/or max deductible and limit\n\nTODO: check if we can assume 0 <= policy['deductible1'] <= 1"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_27@507",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_27",
      "lineno": 507,
      "end_lineno": 515,
      "business_stage": "fm",
      "docstring": "step payout with limit",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_27 (lines 507-515)\n\ndef calcrule_27(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    step payout with limit\n    \"\"\"\n    if policy['step_id'] == 1:\n        loss_out.fill(0)\n    for i in range(loss_in.shape[0]):\n        if (0 < loss_in[i] or 0 < deductible[i]) and policy['trigger_start'] <= loss_in[i] < policy['trigger_end']:\n            loss_out[i] += policy['payout_start']\n\n\"\"\"Docstring (excerpt)\"\"\"\nstep payout with limit"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_28@519",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_28",
      "lineno": 519,
      "end_lineno": 523,
      "business_stage": "fm",
      "docstring": "% loss step payout",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_28 (lines 519-523)\n\ndef calcrule_28(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    % loss step payout\n    \"\"\"\n    _calcrule_28(policy, loss_out, loss_in)\n\n\"\"\"Docstring (excerpt)\"\"\"\n% loss step payout"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_32@527",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_32",
      "lineno": 527,
      "end_lineno": 531,
      "business_stage": "fm",
      "docstring": "monetary amount trigger and % loss step payout with limit",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_32 (lines 527-531)\n\ndef calcrule_32(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    monetary amount trigger and % loss step payout with limit\n    \"\"\"\n    _calcrule_32(policy, loss_out, loss_in)\n\n\"\"\"Docstring (excerpt)\"\"\"\nmonetary amount trigger and % loss step payout with limit"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_33@535",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_33",
      "lineno": 535,
      "end_lineno": 554,
      "business_stage": "fm",
      "docstring": "deductible % loss with limit",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_33 (lines 535-554)\n\ndef calcrule_33(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    deductible % loss with limit\n\n    \"\"\"\n    if policy['deductible1'] >= 1:\n        loss_out.fill(0)\n        deductible += loss_in\n    else:\n        post_ded_limit = policy['limit1'] / (1 - policy['deductible1'])\n        for i in range(loss_in.shape[0]):\n            effective_deductible = loss_in[i] * policy['deductible1']\n            deductible[i] += effective_deductible\n            if loss_in[i] <= post_ded_limit:\n                loss_out[i] = loss_in[i] - effective_deductible\n                under_limit[i] = min2(under_limit[i] + effective_deductible, policy['limit1'] - loss_out[i])\n            else:\n                over_limit[i] += loss_in[i] - effective_deductible - policy['limit1']\n                under_limit[i] = 0\n                loss_out[i] = policy['limit1']\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible % loss with limit"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_34@558",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_34",
      "lineno": 558,
      "end_lineno": 564,
      "business_stage": "fm",
      "docstring": "deductible with attachment and share\n\nTODO: compare to the cpp, as there is shares, deductible won't be use later on so no need to compute it",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_34 (lines 558-564)\n\ndef calcrule_34(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    deductible with attachment and share\n\n    TODO: compare to the cpp, as there is shares, deductible won't be use later on so no need to compute it\n    \"\"\"\n    _calcrule_34(policy, loss_out, loss_in)\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible with attachment and share\n\nTODO: compare to the cpp, as there is shares, deductible won't be use later on so no need to compute it"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_35@568",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_35",
      "lineno": 568,
      "end_lineno": 596,
      "business_stage": "fm",
      "docstring": "deductible % loss with min and/or max deductible and limit % loss\n\nTODO: check if we can assume 0 <= policy['deductible1'] <= 1",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_35 (lines 568-596)\n\ndef calcrule_35(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    deductible % loss with min and/or max deductible and limit % loss\n\n    TODO: check if we can assume 0 <= policy['deductible1'] <= 1\n    \"\"\"\n\n    for i in range(loss_in.shape[0]):\n        effective_deductible = loss_in[i] * policy['deductible1']\n        if effective_deductible + deductible[i] > policy['deductible3'] > 0:\n            deductible_over_max(i, loss_out, loss_in, deductible, over_limit, under_limit, policy['deductible3'])\n        elif effective_deductible + deductible[i] < policy['deductible2']:\n            deductible_under_min(i, loss_out, loss_in, deductible, over_limit, under_limit, policy['deductible2'], effective_deductible)\n        else:\n            if loss_in[i] > effective_deductible:\n                loss_out[i] = loss_in[i] - effective_deductible\n                deductible[i] += effective_deductible\n                under_limit[i] += effective_deductible\n            else:\n                loss_out[i] = 0\n                deductible[i] += loss_in[i]\n                under_limit[i] += loss_in[i]\n        limit = loss_in[i] * policy['limit1']\n        if loss_out[i] > limit:\n            over_limit[i] += loss_out[i] - limit\n            under_limit[i] = 0\n            loss_out[i] = limit\n        else:\n            under_limit[i] = min2(limit - loss_out[i], under_limit[i])\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible % loss with min and/or max deductible and limit % loss\n\nTODO: check if we can assume 0 <= policy['deductible1'] <= 1"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_36@600",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_36",
      "lineno": 600,
      "end_lineno": 629,
      "business_stage": "fm",
      "docstring": "deductible with min and/or max deductible and limit % loss",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_36 (lines 600-629)\n\ndef calcrule_36(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    deductible with min and/or max deductible and limit % loss\n    \"\"\"\n\n    max_ded_left = policy['deductible3'] - policy['deductible1']\n    min_ded_left = policy['deductible2'] - policy['deductible1']\n\n    for i in range(loss_in.shape[0]):\n        if deductible[i] > max_ded_left > 0:\n            deductible_over_max(i, loss_out, loss_in, deductible, over_limit, under_limit, policy['deductible3'])\n        elif deductible[i] < min_ded_left:\n            deductible_under_min(i, loss_out, loss_in, deductible, over_limit, under_limit, policy['deductible2'], policy['deductible1'])\n        else:\n            if loss_in[i] > policy['deductible1']:\n                loss_out[i] = loss_in[i] - policy['deductible1']\n                deductible[i] += policy['deductible1']\n                under_limit[i] += policy['deductible1']\n            else:\n                loss_out[i] = 0\n                deductible[i] += loss_in[i]\n                under_limit[i] += loss_in[i]\n\n        limit = loss_in[i] * policy['limit1']\n        if loss_out[i] > limit:\n            over_limit[i] += loss_out[i] - limit\n            under_limit[i] = 0\n            loss_out[i] = limit\n        else:\n            under_limit[i] = min2(limit - loss_out[i], under_limit[i])\n\n\"\"\"Docstring (excerpt)\"\"\"\ndeductible with min and/or max deductible and limit % loss"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_37@633",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_37",
      "lineno": 633,
      "end_lineno": 643,
      "business_stage": "fm",
      "docstring": "% loss step payout",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_37 (lines 633-643)\n\ndef calcrule_37(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    % loss step payout\n    \"\"\"\n    _calcrule_37(policy, loss_out, loss_in)\n    if policy['step_id'] == 1:\n        loss_out.fill(0)\n    for i in range(loss_in.shape[0]):\n        if policy['trigger_start'] <= loss_in[i] < policy['trigger_end']:\n            loss = min(max(policy['payout_start'] * loss_in[i] - policy['deductible1'], 0), policy['limit1'])\n            loss_out[i] = (loss + min(loss * policy['scale2'], policy['limit2'])) * policy['scale1']\n\n\"\"\"Docstring (excerpt)\"\"\"\n% loss step payout"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_38@647",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_38",
      "lineno": 647,
      "end_lineno": 651,
      "business_stage": "fm",
      "docstring": "conditional coverage",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_38 (lines 647-651)\n\ndef calcrule_38(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    conditional coverage\n    \"\"\"\n    _calcrule_38(policy, loss_out, loss_in)\n\n\"\"\"Docstring (excerpt)\"\"\"\nconditional coverage"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_39@655",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_39",
      "lineno": 655,
      "end_lineno": 665,
      "business_stage": "fm",
      "docstring": "Franchise deductible",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_39 (lines 655-665)\n\ndef calcrule_39(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    Franchise deductible\n    \"\"\"\n    for i in range(loss_in.shape[0]):\n        if loss_in[i] <= policy['deductible1']:\n            under_limit[i] += loss_in[i]\n            deductible[i] += loss_in[i]\n            loss_out[i] = 0\n        else:\n            loss_out[i] = loss_in[i]\n\n\"\"\"Docstring (excerpt)\"\"\"\nFranchise deductible"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_40@669",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_40",
      "lineno": 669,
      "end_lineno": 673,
      "business_stage": "fm",
      "docstring": "BI deductible (waiting period) and limit (period of interest)",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_40 (lines 669-673)\n\ndef calcrule_40(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    BI deductible (waiting period) and limit (period of interest)\n    \"\"\"\n    _calcrule_40(policy, loss_out, loss_in)\n\n\"\"\"Docstring (excerpt)\"\"\"\nBI deductible (waiting period) and limit (period of interest)"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/policy_extras.py::calcrule_41@677",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/policy_extras.py",
      "symbol_type": "function",
      "name": "calcrule_41",
      "lineno": 677,
      "end_lineno": 681,
      "business_stage": "fm",
      "docstring": "No BI deductible (waiting period) and limit only (period of interest)",
      "content": "# File: oasislmf/pytools/fm/policy_extras.py\n# function: calcrule_41 (lines 677-681)\n\ndef calcrule_41(policy, loss_out, loss_in, deductible, over_limit, under_limit):\n    \"\"\"\n    No BI deductible (waiting period) and limit only (period of interest)\n    \"\"\"\n    _calcrule_41(policy, loss_out, loss_in)\n\n\"\"\"Docstring (excerpt)\"\"\"\nNo BI deductible (waiting period) and limit only (period of interest)"
    },
    {
      "chunk_id": "oasislmf/pytools/fm/stream_sparse.py::FMReader@117",
      "source_type": "code",
      "path": "oasislmf/pytools/fm/stream_sparse.py",
      "symbol_type": "class",
      "name": "FMReader",
      "lineno": 117,
      "end_lineno": 155,
      "business_stage": "fm",
      "docstring": "when reading the stream we store relenvant value into a slithly modified version of the CSR sparse matrix where\nthe column indices for row i are stored in indices[indptr[i]:indptr[i+1]]\nand their corresponding values are stored in data[indptr[i]:indptr[i+1]].\n\nnodes_array: array containing all the static information on the nodes\nloss_indptr: array containing the indexes of the beginning and end of samples of an item\nloss_sidx: array containing the sidx of the samples\nloss_val: array containing the loss of the samples",
      "content": "# File: oasislmf/pytools/fm/stream_sparse.py\n# class: FMReader (lines 117-155)\n\nclass FMReader(EventReader):\n    \"\"\"\n    when reading the stream we store relenvant value into a slithly modified version of the CSR sparse matrix where\n    the column indices for row i are stored in indices[indptr[i]:indptr[i+1]]\n    and their corresponding values are stored in data[indptr[i]:indptr[i+1]].\n\n    nodes_array: array containing all the static information on the nodes\n    loss_indptr: array containing the indexes of the beginning and end of samples of an item\n    loss_sidx: array containing the sidx of the samples\n    loss_val: array containing the loss of the samples\n    \"\"\"\n\n    def __init__(self, nodes_array, sidx_indexes, sidx_indptr, sidx_val, loss_indptr, loss_val, pass_through,\n                 len_array, computes, compute_idx):\n        self.nodes_array = nodes_array\n        self.sidx_indexes = sidx_indexes\n        self.sidx_indptr = sidx_indptr\n        self.sidx_val = sidx_val\n        self.loss_indptr = loss_indptr\n        self.loss_val = loss_val\n        self.pass_through = pass_through\n        self.len_array = len_array\n        self.computes = computes\n        self.compute_idx = compute_idx\n        self.logger = logger\n\n    def read_buffer(self, byte_mv, cursor, valid_buff, event_id, item_id, **kwargs):\n        return read_buffer(\n            byte_mv, cursor, valid_buff, event_id, item_id,\n            self.nodes_array, self.sidx_indexes, self.sidx_indptr,\n            self.sidx_val, self.loss_indptr, self.loss_val, self.pass_through,\n            self.computes, self.compute_idx\n        )\n\n    def item_exit(self):\n        reset_empty_items(self.compute_idx, self.sidx_indptr, self.sidx_val, self.loss_val, self.computes)\n\n    def event_read_log(self, event_id):\n        logger.debug(event_log_msg(event_id, self.sidx_indptr, self.len_array, self.compute_idx['next_compute_i']))\n\n\"\"\"Docstring (excerpt)\"\"\"\nwhen reading the stream we store relenvant value into a slithly modified version of the CSR sparse matrix where\nthe column indices for row i are stored in indices[indptr[i]:indptr[i+1]]\nand their corresponding values are stored in data[indptr[i]:indptr[i+1]].\n\nnodes_array: array containing all the static information on the nodes\nloss_indptr: array containing the indexes of the beginning and end of samples of an item\nloss_sidx: array containing the sidx of the samples\nloss_val: array containing the loss of the samples"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/footprint.py::df_to_numpy@49",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/footprint.py",
      "symbol_type": "function",
      "name": "df_to_numpy",
      "lineno": 49,
      "end_lineno": 68,
      "business_stage": "other",
      "docstring": "Args:\n    dataframe: DataFrame to convert to numpy\n    dtype: numpy dtype of the output ndarray\n    columns: optional dict-like object (with get method) mapping np_column => dataframe_column if they are different\nReturns:\n    numpy nd array\n\n>>> dataframe = pd.DataFrame({'a':[1,2], 'b':[0.0, 1.0]})\n>>> dtype = np.dtype([('a', np.int64), ('c', np.float32),])\n>>> columns = {'c': 'b'}\n>>> df_to_numpy(dataframe, dtype, columns)\narray([(1, 0.), (2, 1.)], dtype=[('a', '<i8'), ('c', '<f4')])",
      "content": "# File: oasislmf/pytools/getmodel/footprint.py\n# function: df_to_numpy (lines 49-68)\n\ndef df_to_numpy(dataframe, dtype, columns={}) -> np.array:\n    \"\"\"\n\n    Args:\n        dataframe: DataFrame to convert to numpy\n        dtype: numpy dtype of the output ndarray\n        columns: optional dict-like object (with get method) mapping np_column => dataframe_column if they are different\n    Returns:\n        numpy nd array\n\n    >>> dataframe = pd.DataFrame({'a':[1,2], 'b':[0.0, 1.0]})\n    >>> dtype = np.dtype([('a', np.int64), ('c', np.float32),])\n    >>> columns = {'c': 'b'}\n    >>> df_to_numpy(dataframe, dtype, columns)\n    array([(1, 0.), (2, 1.)], dtype=[('a', '<i8'), ('c', '<f4')])\n    \"\"\"\n    numpy_data = np.empty(len(dataframe), dtype=dtype)\n    for np_column in dtype.fields.keys():\n        numpy_data[:][np_column] = dataframe[columns.get(np_column, np_column)].to_numpy()\n    return numpy_data\n\n\"\"\"Docstring (excerpt)\"\"\"\nArgs:\n    dataframe: DataFrame to convert to numpy\n    dtype: numpy dtype of the output ndarray\n    columns: optional dict-like object (with get method) mapping np_column => dataframe_column if they are different\nReturns:\n    numpy nd array\n\n>>> dataframe = pd.DataFrame({'a':[1,2], 'b':[0.0, 1.0]})\n>>> dtype = np.dtype([('a', np.int64), ('c', np.float32),])\n>>> columns = {'c': 'b'}\n>>> df_to_numpy(dataframe, dtype, columns)\narray([(1, 0.), (2, 1.)], dtype=[('a', '<i8'), ('c', '<f4')])"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/footprint.py::OasisFootPrintError@85",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/footprint.py",
      "symbol_type": "class",
      "name": "OasisFootPrintError",
      "lineno": 85,
      "end_lineno": 97,
      "business_stage": "other",
      "docstring": "Raises exceptions when loading footprints.",
      "content": "# File: oasislmf/pytools/getmodel/footprint.py\n# class: OasisFootPrintError (lines 85-97)\n\nclass OasisFootPrintError(Exception):\n    \"\"\"\n    Raises exceptions when loading footprints.\n    \"\"\"\n\n    def __init__(self, message: str) -> None:\n        \"\"\"\n        The constructor of the OasisFootPrintError class.\n\n        Args:\n            message: (str) the message to be raised\n        \"\"\"\n        super().__init__(message)\n\n\"\"\"Docstring (excerpt)\"\"\"\nRaises exceptions when loading footprints."
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/footprint.py::Footprint@100",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/footprint.py",
      "symbol_type": "class",
      "name": "Footprint",
      "lineno": 100,
      "end_lineno": 230,
      "business_stage": "other",
      "docstring": "This class is the base class for the footprint loaders.\n\nAttributes:\n    storage (BaseStorage): the storage object used to lookup files\n    stack (ExitStack): the context manager that combines other context managers and cleanup functions",
      "content": "# File: oasislmf/pytools/getmodel/footprint.py\n# class: Footprint (lines 100-230)\n\nclass Footprint:\n    \"\"\"\n    This class is the base class for the footprint loaders.\n\n    Attributes:\n        storage (BaseStorage): the storage object used to lookup files\n        stack (ExitStack): the context manager that combines other context managers and cleanup functions\n    \"\"\"\n\n    def __init__(\n            self, storage: BaseStorage,\n            df_engine=\"oasis_data_manager.df_reader.reader.OasisPandasReader\",\n            areaperil_ids=None\n    ) -> None:\n        \"\"\"\n        The constructor for the Footprint class.\n\n        Args:\n            storage (BaseStorage): the storage object used to lookup files\n            areaperil_ids (list): areaperil_ids that will be useful\n        \"\"\"\n        self.storage = storage\n        self.stack = ExitStack()\n        self.df_engine = df_engine\n        if areaperil_ids is not None:\n            self.areaperil_ids = areaperil_ids\n        else:\n            self.areaperil_ids = None\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        self.stack.__exit__(exc_type, exc_value, exc_traceback)\n\n    @staticmethod\n    def get_footprint_fmt_priorities():\n        \"\"\"\n        Get list of footprint file format classes in order of priority.\n\n        Returns: (list) footprint file format classes\n        \"\"\"\n        format_to_class = {\n            'parquet_chunk': FootprintParquetChunk,\n            'parquet': FootprintParquet, 'csv': FootprintCsv,\n            'binZ': FootprintBinZ, 'bin': FootprintBin,\n            'parquet_dynamic': FootprintParquetDynamic,\n        }\n        priorities = [format_to_class[fmt] for fmt in fp_format_priorities if fmt in format_to_class]\n\n        return priorities\n\n    @classmethod\n    def load(\n        cls,\n        storage: BaseStorage,\n        ignore_file_type=set(),\n        df_engine=\"oasis_data_manager.df_reader.reader.OasisPandasReader\",\n        areaperil_ids=None,\n        **kwargs\n    ):\n        \"\"\"\n        Loads the loading classes defined in this file checking to see if the files are in the static path\n        whilst doing so. The loading goes through the hierarchy with the following order:\n\n        -> parquet\n        -> compressed binary file\n        -> binary file\n        -> CSV file\n        -> parquet (with dynamic generation)\n\n        If the compressed binary file is present, this will be loaded. If it\n        is not, then the binary file will be loaded\n        and so on.\n\n        Args:\n            storage (BaseStorage): the storage object used to lookup files\n            ignore_file_type (Set[str]): type of file to be skipped in the hierarchy. This can be a choice of:\n\n            parquet\n            json\n            z\n            bin\n\n\"\"\"Docstring (excerpt)\"\"\"\nThis class is the base class for the footprint loaders.\n\nAttributes:\n    storage (BaseStorage): the storage object used to lookup files\n    stack (ExitStack): the context manager that combines other context managers and cleanup functions"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/footprint.py::FootprintCsv@233",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/footprint.py",
      "symbol_type": "class",
      "name": "FootprintCsv",
      "lineno": 233,
      "end_lineno": 280,
      "business_stage": "other",
      "docstring": "This class is responsible for loading footprint data from CSV.\n\nAttributes (when in context):\n    footprint (np.array[footprint_event_dtype]): event data loaded from the CSV file\n    num_intensity_bins (int): number of intensity bins in the data\n    has_intensity_uncertainty (bool): if the data has uncertainty\n    footprint_index (dict): map of footprint IDs with the index in the data",
      "content": "# File: oasislmf/pytools/getmodel/footprint.py\n# class: FootprintCsv (lines 233-280)\n\nclass FootprintCsv(Footprint):\n    \"\"\"\n    This class is responsible for loading footprint data from CSV.\n\n    Attributes (when in context):\n        footprint (np.array[footprint_event_dtype]): event data loaded from the CSV file\n        num_intensity_bins (int): number of intensity bins in the data\n        has_intensity_uncertainty (bool): if the data has uncertainty\n        footprint_index (dict): map of footprint IDs with the index in the data\n    \"\"\"\n    footprint_filenames = [csvfootprint_filename]\n\n    def __enter__(self):\n        self.reader = pd.read_csv()\n        self.reader = self.get_df_reader(\"footprint.csv\", dtype=footprint_event_dtype)\n\n        self.num_intensity_bins = self.reader.query(lambda df: df['intensity_bin_id'].max())\n\n        self.has_intensity_uncertainty = self.reader.query(\n            lambda df: df.groupby(\n                ['event_id', 'areaperil_id']\n            ).size().max() > 1\n        )\n\n        def _fn(df):\n            footprint_index_df = df.groupby('event_id', as_index=False).size()\n            footprint_index_df['offset'] = (footprint_index_df['size'].cumsum() - footprint_index_df['size'])\n\n            footprint_index_df.set_index('event_id', inplace=True)\n            return footprint_index_df\n        self.footprint_index = self.reader.query(_fn).to_dict('index')\n\n        return self\n\n    def get_event(self, event_id):\n        \"\"\"\n        Gets the event from self.footprint based off the event ID passed in.\n\n        Args:\n            event_id: (int) the ID belonging to the Event being extracted\n\n        Returns: (np.array[footprint_event_dtype]) the event that was extracted\n        \"\"\"\n        event_info = self.footprint_index.get(event_id)\n        if event_info is None:\n            return\n        else:\n            return self.prepare_df_data(self.reader.filter(lambda df: df[df[\"event_id\"] == event_id]).as_pandas())\n\n\"\"\"Docstring (excerpt)\"\"\"\nThis class is responsible for loading footprint data from CSV.\n\nAttributes (when in context):\n    footprint (np.array[footprint_event_dtype]): event data loaded from the CSV file\n    num_intensity_bins (int): number of intensity bins in the data\n    has_intensity_uncertainty (bool): if the data has uncertainty\n    footprint_index (dict): map of footprint IDs with the index in the data"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/footprint.py::FootprintBin@283",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/footprint.py",
      "symbol_type": "class",
      "name": "FootprintBin",
      "lineno": 283,
      "end_lineno": 341,
      "business_stage": "other",
      "docstring": "This class is responsible loading the event data from the footprint binary files.\n\nAttributes (when in context):\n    footprint (mmap.mmap): loaded data from the binary file which has header and then Event data\n    num_intensity_bins (int): number of intensity bins in the data\n    has_intensity_uncertainty (bool): if the data has uncertainty\n    footprint_index (dict): map of footprint IDs with the index in the data",
      "content": "# File: oasislmf/pytools/getmodel/footprint.py\n# class: FootprintBin (lines 283-341)\n\nclass FootprintBin(Footprint):\n    \"\"\"\n    This class is responsible loading the event data from the footprint binary files.\n\n    Attributes (when in context):\n        footprint (mmap.mmap): loaded data from the binary file which has header and then Event data\n        num_intensity_bins (int): number of intensity bins in the data\n        has_intensity_uncertainty (bool): if the data has uncertainty\n        footprint_index (dict): map of footprint IDs with the index in the data\n    \"\"\"\n    footprint_filenames = [footprint_filename, footprint_index_filename]\n\n    def __enter__(self):\n        footprint_file = self.stack.enter_context(self.storage.with_fileno(footprint_filename))\n\n        self.footprint = mmap.mmap(footprint_file.fileno(), length=0, access=mmap.ACCESS_READ)\n\n        footprint_header = np.frombuffer(bytearray(self.footprint[:FootprintHeader.size]), dtype=FootprintHeader)\n\n        self.num_intensity_bins = int(footprint_header['num_intensity_bins'])\n        self.has_intensity_uncertainty = int(footprint_header['has_intensity_uncertainty'] & intensityMask)\n\n        f = self.stack.enter_context(self.storage.with_fileno(footprint_index_filename))\n        footprint_mmap = np.memmap(f, dtype=EventIndexBin, mode='r')\n\n        self.footprint_index = pd.DataFrame(footprint_mmap, columns=footprint_mmap.dtype.names).set_index('event_id').to_dict('index')\n        try:\n            lookup_file = self.storage.with_fileno(footprint_bin_lookup)\n            with lookup_file as f:\n                lookup = mmap.mmap(f.fileno(), length=0, access=mmap.ACCESS_READ)\n            df = pickle.loads(lookup)\n\n            self.events_dict = {\n                row.event_id: (row.min_areaperil_id, row.max_areaperil_id)\n                for row in df.itertuples(index=False)\n            }\n        except FileNotFoundError:\n            self.events_dict = None\n\n        return self\n\n    def get_event(self, event_id):\n        \"\"\"\n        Gets the event from self.footprint based off the event ID passed in.\n\n        Args:\n            event_id: (int) the ID belonging to the Event being extracted\n\n        Returns: (np.array(Event)) the event that was extracted\n        \"\"\"\n        if self.events_dict:\n            if not self.areaperil_in_range(event_id, self.events_dict):\n                return None\n\n        event_info = self.footprint_index.get(event_id)\n        if event_info is None:\n            return\n        else:\n            return np.frombuffer(self.footprint[event_info['offset']: event_info['offset'] + event_info['size']], Event)\n\n\"\"\"Docstring (excerpt)\"\"\"\nThis class is responsible loading the event data from the footprint binary files.\n\nAttributes (when in context):\n    footprint (mmap.mmap): loaded data from the binary file which has header and then Event data\n    num_intensity_bins (int): number of intensity bins in the data\n    has_intensity_uncertainty (bool): if the data has uncertainty\n    footprint_index (dict): map of footprint IDs with the index in the data"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/footprint.py::FootprintBinZ@344",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/footprint.py",
      "symbol_type": "class",
      "name": "FootprintBinZ",
      "lineno": 344,
      "end_lineno": 412,
      "business_stage": "other",
      "docstring": "This class is responsible for loading event data from compressed event data.\n\nAttributes (when in context):\n    zfootprint (mmap.mmap): loaded data from the compressed binary file which has header and then Event data\n    num_intensity_bins (int): number of intensity bins in the data\n    has_intensity_uncertainty (bool): if the data has uncertainty\n    uncompressed_size (int): the size in which the data is when it is decompressed\n    index_dtype (Union[EventIndexBinZ, EventIndexBin]) the data type\n    footprint_index (dict): map of footprint IDs with the index in the data",
      "content": "# File: oasislmf/pytools/getmodel/footprint.py\n# class: FootprintBinZ (lines 344-412)\n\nclass FootprintBinZ(Footprint):\n    \"\"\"\n    This class is responsible for loading event data from compressed event data.\n\n    Attributes (when in context):\n        zfootprint (mmap.mmap): loaded data from the compressed binary file which has header and then Event data\n        num_intensity_bins (int): number of intensity bins in the data\n        has_intensity_uncertainty (bool): if the data has uncertainty\n        uncompressed_size (int): the size in which the data is when it is decompressed\n        index_dtype (Union[EventIndexBinZ, EventIndexBin]) the data type\n        footprint_index (dict): map of footprint IDs with the index in the data\n    \"\"\"\n    footprint_filenames = [zfootprint_filename, zfootprint_index_filename]\n\n    def __enter__(self):\n        zfootprint_file = self.stack.enter_context(self.storage.with_fileno(zfootprint_filename))\n        self.zfootprint = mmap.mmap(zfootprint_file.fileno(), length=0, access=mmap.ACCESS_READ)\n\n        footprint_header = np.frombuffer(bytearray(self.zfootprint[:FootprintHeader.size]), dtype=FootprintHeader)\n\n        self.num_intensity_bins = int(footprint_header['num_intensity_bins'])\n        self.has_intensity_uncertainty = int(footprint_header['has_intensity_uncertainty'] & intensityMask)\n        self.uncompressed_size = int((footprint_header['has_intensity_uncertainty'] & uncompressedMask) >> 1)\n\n        if self.uncompressed_size:\n            self.index_dtype = EventIndexBinZ\n        else:\n            self.index_dtype = EventIndexBin\n\n        f = self.stack.enter_context(self.storage.with_fileno(zfootprint_index_filename))\n\n        zfootprint_mmap = np.memmap(f, dtype=self.index_dtype, mode='r')\n        self.footprint_index = pd.DataFrame(zfootprint_mmap, columns=zfootprint_mmap.dtype.names).set_index('event_id').to_dict('index')\n\n        try:\n            lookup_file = self.storage.with_fileno(footprint_bin_lookup)\n            with lookup_file as f:\n                lookup = mmap.mmap(f.fileno(), length=0, access=mmap.ACCESS_READ)\n            df = pickle.loads(lookup)\n\n            self.events_dict = {\n                row.event_id: (row.min_areaperil_id, row.max_areaperil_id)\n                for row in df.itertuples(index=False)\n            }\n        except FileNotFoundError:\n            self.events_dict = None\n\n        return self\n\n    def get_event(self, event_id):\n        \"\"\"\n        Gets the event from self.zfootprint based off the event ID passed in.\n\n        Args:\n            event_id: (int) the ID belonging to the Event being extracted\n\n        Returns: (np.array[Event]) the event that was extracted\n        \"\"\"\n\n        event_info = self.footprint_index.get(event_id)\n        if event_info is None:\n            return\n        elif self.events_dict:\n            if not self.areaperil_in_range(event_id, self.events_dict):\n                return None\n        else:\n            zdata = self.zfootprint[event_info['offset']: event_info['offset'] + event_info['size']]\n            data = decompress(zdata)\n            return np.frombuffer(data, Event)\n\n\"\"\"Docstring (excerpt)\"\"\"\nThis class is responsible for loading event data from compressed event data.\n\nAttributes (when in context):\n    zfootprint (mmap.mmap): loaded data from the compressed binary file which has header and then Event data\n    num_intensity_bins (int): number of intensity bins in the data\n    has_intensity_uncertainty (bool): if the data has uncertainty\n    uncompressed_size (int): the size in which the data is when it is decompressed\n    index_dtype (Union[EventIndexBinZ, EventIndexBin]) the data type\n    footprint_index (dict): map of footprint IDs with the index in the data"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/footprint.py::FootprintParquet@415",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/footprint.py",
      "symbol_type": "class",
      "name": "FootprintParquet",
      "lineno": 415,
      "end_lineno": 455,
      "business_stage": "other",
      "docstring": "This class is responsible for loading event data from parquet event data.\n\nAttributes (when in context):\n    num_intensity_bins (int): number of intensity bins in the data\n    has_intensity_uncertainty (bool): if the data has uncertainty\n    footprint_index (dict): map of footprint IDs with the index in the data",
      "content": "# File: oasislmf/pytools/getmodel/footprint.py\n# class: FootprintParquet (lines 415-455)\n\nclass FootprintParquet(Footprint):\n    \"\"\"\n    This class is responsible for loading event data from parquet event data.\n\n    Attributes (when in context):\n        num_intensity_bins (int): number of intensity bins in the data\n        has_intensity_uncertainty (bool): if the data has uncertainty\n        footprint_index (dict): map of footprint IDs with the index in the data\n    \"\"\"\n    footprint_filenames: List[str] = [parquetfootprint_filename, parquetfootprint_meta_filename]\n\n    def __enter__(self):\n        with self.storage.open(parquetfootprint_meta_filename, 'r') as outfile:\n            meta_data: Dict[str, Union[int, bool]] = json.load(outfile)\n\n        self.num_intensity_bins = int(meta_data['num_intensity_bins'])\n        self.has_intensity_uncertainty = int(meta_data['has_intensity_uncertainty'] & intensityMask)\n\n        if self.areaperil_ids is not None:\n            self.areaperil_ids_filter = [(\"areaperil_id\", \"in\", self.areaperil_ids)]\n        else:\n            self.areaperil_ids_filter = None\n\n        return self\n\n    def get_event(self, event_id: int):\n        \"\"\"\n        Gets the event data from the partitioned parquet data file.\n\n        Args:\n            event_id: (int) the ID belonging to the Event being extracted\n\n        Returns: (np.array[Event]) the event that was extracted\n        \"\"\"\n        dir_path = f\"footprint.parquet/event_id={event_id}/\"\n        if self.storage.exists(dir_path):\n            reader = self.get_df_reader(dir_path, filters=self.areaperil_ids_filter)\n            numpy_data = self.prepare_df_data(data_frame=reader.as_pandas())\n            return numpy_data\n        else:\n            return np.empty(0, dtype=Event)\n\n\"\"\"Docstring (excerpt)\"\"\"\nThis class is responsible for loading event data from parquet event data.\n\nAttributes (when in context):\n    num_intensity_bins (int): number of intensity bins in the data\n    has_intensity_uncertainty (bool): if the data has uncertainty\n    footprint_index (dict): map of footprint IDs with the index in the data"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/footprint.py::FootprintParquetDynamic@517",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/footprint.py",
      "symbol_type": "class",
      "name": "FootprintParquetDynamic",
      "lineno": 517,
      "end_lineno": 599,
      "business_stage": "other",
      "docstring": "This class is responsible for loading event data from parquet dynamic event sets and maps\nIt will build the footprint from the underlying event defintion and hazard case files\n\nAttributes (when in context):\n    num_intensity_bins (int): number of intensity bins in the data\n    has_intensity_uncertainty (bool): if the data has uncertainty. Only \"no\" is supported\n    return periods (list): the list of return periods in the model. Not currently used",
      "content": "# File: oasislmf/pytools/getmodel/footprint.py\n# class: FootprintParquetDynamic (lines 517-599)\n\nclass FootprintParquetDynamic(Footprint):\n    \"\"\"\n    This class is responsible for loading event data from parquet dynamic event sets and maps\n    It will build the footprint from the underlying event defintion and hazard case files\n\n    Attributes (when in context):\n        num_intensity_bins (int): number of intensity bins in the data\n        has_intensity_uncertainty (bool): if the data has uncertainty. Only \"no\" is supported\n        return periods (list): the list of return periods in the model. Not currently used\n    \"\"\"\n    footprint_filenames: List[str] = [event_defintion_filename, hazard_case_filename, parquetfootprint_meta_filename]\n\n    def __enter__(self):\n        with self.storage.open(parquetfootprint_meta_filename, 'r') as outfile:\n            meta_data: Dict[str, Union[int, bool]] = json.load(outfile)\n\n        self.num_intensity_bins = int(meta_data['num_intensity_bins'])\n        self.has_intensity_uncertainty = int(meta_data['has_intensity_uncertainty'] & intensityMask)\n\n        self.df_location_sections = pd.read_csv('input/sections.csv')\n        self.location_sections = set(list(self.df_location_sections['section_id']))\n        if self.areaperil_ids is None:\n            self.areaperil_ids = pd.read_csv('input/keys.csv', usecols=['AreaPerilID']).AreaPerilID.unique()\n\n        return self\n\n    def get_event(self, event_id: int):\n        \"\"\"\n        Gets the event data from the partitioned parquet data file.\n\n        Args:\n            event_id: (int) the ID belonging to the Event being extracted\n\n        Returns: (np.array[Event]) the event that was extracted\n        \"\"\"\n        event_defintion_reader = self.get_df_reader(event_defintion_filename, filters=[(\"event_id\", \"==\", event_id)])\n        df_event_defintion = event_defintion_reader.as_pandas()\n        event_sections = list(df_event_defintion['section_id'])\n        sections = list(set(event_sections) & self.location_sections)\n\n        if len(sections) > 0:\n            df_hazard_case = {}\n            for section in sections:\n                hazard_case_reader = self.get_df_reader(\n                    f'{hazard_case_filename}/section_id={int(section)}',\n                    filters=[(\"areaperil_id\", \"in\", self.areaperil_ids)]\n                )\n                df_hazard_case[section] = hazard_case_reader.as_pandas()\n                df_hazard_case[section]['section_id'] = section\n            df_hazard_case = pd.concat(df_hazard_case, ignore_index=True)\n\n            from_cols = ['section_id', 'areaperil_id', 'intensity']\n            to_cols = from_cols + ['interpolation', 'return_period']\n\n            df_hazard_case_from = df_hazard_case.merge(\n                df_event_defintion, left_on=['section_id', 'return_period'], right_on=['section_id', 'rp_from'])[from_cols].rename(\n                    columns={'intensity': 'from_intensity'})\n\n            df_hazard_case_to = df_hazard_case.merge(\n                df_event_defintion, left_on=['section_id', 'return_period'], right_on=['section_id', 'rp_to'])[to_cols].rename(\n                    columns={'intensity': 'to_intensity'})\n\n            df_footprint = df_hazard_case_from.merge(df_hazard_case_to, on=['section_id', 'areaperil_id'], how='outer')\n            df_footprint['from_intensity'] = df_footprint['from_intensity'].fillna(0)\n\n            if len(df_footprint.index) > 0:\n                df_footprint['intensity'] = np.floor(df_footprint.from_intensity + (\n                    (df_footprint.to_intensity - df_footprint.from_intensity) * df_footprint.interpolation))\n                df_footprint['intensity'] = df_footprint['intensity'].astype('int')\n                df_footprint = df_footprint.sort_values('intensity', ascending=False)\n                df_footprint = df_footprint.drop_duplicates(subset=['areaperil_id'], keep='first')\n                df_footprint['intensity_bin_id'] = 0  # Placeholder for intensity bin ID\n                df_footprint['probability'] = 1\n            else:\n                df_footprint.loc[:, 'intensity'] = []\n                df_footprint.loc[:, 'intensity_bin_id'] = []\n                df_footprint.loc[:, 'probability'] = []\n\n            numpy_data = np.empty(len(df_footprint), dtype=EventDynamic)\n            for column in ['areaperil_id', 'intensity_bin_id', 'intensity', 'probability', 'return_period']:\n\n\"\"\"Docstring (excerpt)\"\"\"\nThis class is responsible for loading event data from parquet dynamic event sets and maps\nIt will build the footprint from the underlying event defintion and hazard case files\n\nAttributes (when in context):\n    num_intensity_bins (int): number of intensity bins in the data\n    has_intensity_uncertainty (bool): if the data has uncertainty. Only \"no\" is supported\n    return periods (list): the list of return periods in the model. Not currently used"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/footprint.py::get_footprint_fmt_priorities@133",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/footprint.py",
      "symbol_type": "function",
      "name": "get_footprint_fmt_priorities",
      "lineno": 133,
      "end_lineno": 147,
      "business_stage": "other",
      "docstring": "Get list of footprint file format classes in order of priority.\n\nReturns: (list) footprint file format classes",
      "content": "# File: oasislmf/pytools/getmodel/footprint.py\n# function: get_footprint_fmt_priorities (lines 133-147)\n\n    def get_footprint_fmt_priorities():\n        \"\"\"\n        Get list of footprint file format classes in order of priority.\n\n        Returns: (list) footprint file format classes\n        \"\"\"\n        format_to_class = {\n            'parquet_chunk': FootprintParquetChunk,\n            'parquet': FootprintParquet, 'csv': FootprintCsv,\n            'binZ': FootprintBinZ, 'bin': FootprintBin,\n            'parquet_dynamic': FootprintParquetDynamic,\n        }\n        priorities = [format_to_class[fmt] for fmt in fp_format_priorities if fmt in format_to_class]\n\n        return priorities\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet list of footprint file format classes in order of priority.\n\nReturns: (list) footprint file format classes"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/footprint.py::load@150",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/footprint.py",
      "symbol_type": "function",
      "name": "load",
      "lineno": 150,
      "end_lineno": 201,
      "business_stage": "other",
      "docstring": "Loads the loading classes defined in this file checking to see if the files are in the static path\nwhilst doing so. The loading goes through the hierarchy with the following order:\n\n-> parquet\n-> compressed binary file\n-> binary file\n-> CSV file\n-> parquet (with dynamic generation)\n\nIf the compressed binary file is present, this will be loaded. If it\nis not, then the binary file will be loaded\nand so on.\n\nArgs:\n    storage (BaseStorage): the storage object used to lookup files\n    ignore_file_type (Set[str]): type of file to be skipped in the hierarchy. This can be a choice of:\n\n    parquet\n    json\n    z\n    bin\n    idx\n\nReturns: (Union[FootprintBinZ, FootprintBin, FootprintCsv]) the loaded class",
      "content": "# File: oasislmf/pytools/getmodel/footprint.py\n# function: load (lines 150-201)\n\n    def load(\n        cls,\n        storage: BaseStorage,\n        ignore_file_type=set(),\n        df_engine=\"oasis_data_manager.df_reader.reader.OasisPandasReader\",\n        areaperil_ids=None,\n        **kwargs\n    ):\n        \"\"\"\n        Loads the loading classes defined in this file checking to see if the files are in the static path\n        whilst doing so. The loading goes through the hierarchy with the following order:\n\n        -> parquet\n        -> compressed binary file\n        -> binary file\n        -> CSV file\n        -> parquet (with dynamic generation)\n\n        If the compressed binary file is present, this will be loaded. If it\n        is not, then the binary file will be loaded\n        and so on.\n\n        Args:\n            storage (BaseStorage): the storage object used to lookup files\n            ignore_file_type (Set[str]): type of file to be skipped in the hierarchy. This can be a choice of:\n\n            parquet\n            json\n            z\n            bin\n            idx\n\n        Returns: (Union[FootprintBinZ, FootprintBin, FootprintCsv]) the loaded class\n        \"\"\"\n        for footprint_class in cls.get_footprint_fmt_priorities():\n            for filename in footprint_class.footprint_filenames:\n                if (not storage.exists(filename) or filename.rsplit('.', 1)[-1] in ignore_file_type):\n                    valid = False\n                    break\n            else:\n                valid = True\n            if valid:\n                for filename in footprint_class.footprint_filenames:\n                    logger.debug(f\"loading {filename}\")\n                return footprint_class(storage, df_engine=df_engine, areaperil_ids=areaperil_ids)\n        else:\n            if storage.isfile(\"footprint.parquet\"):\n                raise OasisFootPrintError(\n                    message=\"footprint.parquet needs to be partitioned in order to work, please see: \"\n                    \"oasislmf.pytools.data_layer.conversions.footprint => convert_bin_to_parquet\"\n                )\n            raise OasisFootPrintError(message=\"no valid footprint found\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoads the loading classes defined in this file checking to see if the files are in the static path\nwhilst doing so. The loading goes through the hierarchy with the following order:\n\n-> parquet\n-> compressed binary file\n-> binary file\n-> CSV file\n-> parquet (with dynamic generation)\n\nIf the compressed binary file is present, this will be loaded. If it\nis not, then the binary file will be loaded\nand so on.\n\nArgs:\n    storage (BaseStorage): the storage object used to lookup files\n    ignore_file_type (Set[str]): type of file to be skipped in the hierarchy. This can be a choice of:\n\n    parquet\n    json\n    z\n    bin\n    idx\n\nReturns: (Union[FootprintBinZ, FootprintBin, FootprintCsv]) the loaded class"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/footprint.py::prepare_df_data@214",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/footprint.py",
      "symbol_type": "function",
      "name": "prepare_df_data",
      "lineno": 214,
      "end_lineno": 220,
      "business_stage": "other",
      "docstring": "Reads footprint data from a parquet file.\n\nReturns: (np.array) footprint data loaded from the parquet file",
      "content": "# File: oasislmf/pytools/getmodel/footprint.py\n# function: prepare_df_data (lines 214-220)\n\n    def prepare_df_data(data_frame: pd.DataFrame) -> np.array:\n        \"\"\"\n        Reads footprint data from a parquet file.\n\n        Returns: (np.array) footprint data loaded from the parquet file\n        \"\"\"\n        return df_to_numpy(data_frame, Event)\n\n\"\"\"Docstring (excerpt)\"\"\"\nReads footprint data from a parquet file.\n\nReturns: (np.array) footprint data loaded from the parquet file"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/footprint.py::get_event@267",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/footprint.py",
      "symbol_type": "function",
      "name": "get_event",
      "lineno": 267,
      "end_lineno": 280,
      "business_stage": "other",
      "docstring": "Gets the event from self.footprint based off the event ID passed in.\n\nArgs:\n    event_id: (int) the ID belonging to the Event being extracted\n\nReturns: (np.array[footprint_event_dtype]) the event that was extracted",
      "content": "# File: oasislmf/pytools/getmodel/footprint.py\n# function: get_event (lines 267-280)\n\n    def get_event(self, event_id):\n        \"\"\"\n        Gets the event from self.footprint based off the event ID passed in.\n\n        Args:\n            event_id: (int) the ID belonging to the Event being extracted\n\n        Returns: (np.array[footprint_event_dtype]) the event that was extracted\n        \"\"\"\n        event_info = self.footprint_index.get(event_id)\n        if event_info is None:\n            return\n        else:\n            return self.prepare_df_data(self.reader.filter(lambda df: df[df[\"event_id\"] == event_id]).as_pandas())\n\n\"\"\"Docstring (excerpt)\"\"\"\nGets the event from self.footprint based off the event ID passed in.\n\nArgs:\n    event_id: (int) the ID belonging to the Event being extracted\n\nReturns: (np.array[footprint_event_dtype]) the event that was extracted"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/footprint.py::get_event@324",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/footprint.py",
      "symbol_type": "function",
      "name": "get_event",
      "lineno": 324,
      "end_lineno": 341,
      "business_stage": "other",
      "docstring": "Gets the event from self.footprint based off the event ID passed in.\n\nArgs:\n    event_id: (int) the ID belonging to the Event being extracted\n\nReturns: (np.array(Event)) the event that was extracted",
      "content": "# File: oasislmf/pytools/getmodel/footprint.py\n# function: get_event (lines 324-341)\n\n    def get_event(self, event_id):\n        \"\"\"\n        Gets the event from self.footprint based off the event ID passed in.\n\n        Args:\n            event_id: (int) the ID belonging to the Event being extracted\n\n        Returns: (np.array(Event)) the event that was extracted\n        \"\"\"\n        if self.events_dict:\n            if not self.areaperil_in_range(event_id, self.events_dict):\n                return None\n\n        event_info = self.footprint_index.get(event_id)\n        if event_info is None:\n            return\n        else:\n            return np.frombuffer(self.footprint[event_info['offset']: event_info['offset'] + event_info['size']], Event)\n\n\"\"\"Docstring (excerpt)\"\"\"\nGets the event from self.footprint based off the event ID passed in.\n\nArgs:\n    event_id: (int) the ID belonging to the Event being extracted\n\nReturns: (np.array(Event)) the event that was extracted"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/footprint.py::get_event@393",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/footprint.py",
      "symbol_type": "function",
      "name": "get_event",
      "lineno": 393,
      "end_lineno": 412,
      "business_stage": "other",
      "docstring": "Gets the event from self.zfootprint based off the event ID passed in.\n\nArgs:\n    event_id: (int) the ID belonging to the Event being extracted\n\nReturns: (np.array[Event]) the event that was extracted",
      "content": "# File: oasislmf/pytools/getmodel/footprint.py\n# function: get_event (lines 393-412)\n\n    def get_event(self, event_id):\n        \"\"\"\n        Gets the event from self.zfootprint based off the event ID passed in.\n\n        Args:\n            event_id: (int) the ID belonging to the Event being extracted\n\n        Returns: (np.array[Event]) the event that was extracted\n        \"\"\"\n\n        event_info = self.footprint_index.get(event_id)\n        if event_info is None:\n            return\n        elif self.events_dict:\n            if not self.areaperil_in_range(event_id, self.events_dict):\n                return None\n        else:\n            zdata = self.zfootprint[event_info['offset']: event_info['offset'] + event_info['size']]\n            data = decompress(zdata)\n            return np.frombuffer(data, Event)\n\n\"\"\"Docstring (excerpt)\"\"\"\nGets the event from self.zfootprint based off the event ID passed in.\n\nArgs:\n    event_id: (int) the ID belonging to the Event being extracted\n\nReturns: (np.array[Event]) the event that was extracted"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/footprint.py::get_event@440",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/footprint.py",
      "symbol_type": "function",
      "name": "get_event",
      "lineno": 440,
      "end_lineno": 455,
      "business_stage": "other",
      "docstring": "Gets the event data from the partitioned parquet data file.\n\nArgs:\n    event_id: (int) the ID belonging to the Event being extracted\n\nReturns: (np.array[Event]) the event that was extracted",
      "content": "# File: oasislmf/pytools/getmodel/footprint.py\n# function: get_event (lines 440-455)\n\n    def get_event(self, event_id: int):\n        \"\"\"\n        Gets the event data from the partitioned parquet data file.\n\n        Args:\n            event_id: (int) the ID belonging to the Event being extracted\n\n        Returns: (np.array[Event]) the event that was extracted\n        \"\"\"\n        dir_path = f\"footprint.parquet/event_id={event_id}/\"\n        if self.storage.exists(dir_path):\n            reader = self.get_df_reader(dir_path, filters=self.areaperil_ids_filter)\n            numpy_data = self.prepare_df_data(data_frame=reader.as_pandas())\n            return numpy_data\n        else:\n            return np.empty(0, dtype=Event)\n\n\"\"\"Docstring (excerpt)\"\"\"\nGets the event data from the partitioned parquet data file.\n\nArgs:\n    event_id: (int) the ID belonging to the Event being extracted\n\nReturns: (np.array[Event]) the event that was extracted"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/footprint.py::get_event@479",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/footprint.py",
      "symbol_type": "function",
      "name": "get_event",
      "lineno": 479,
      "end_lineno": 507,
      "business_stage": "other",
      "docstring": "Gets the event data from the partitioned\nparquetfootprint_chunked_filename data file.\n\nArgs:\n    event_id: (int) the ID belonging to the Event being extracted\n\nReturns: (np.array[Event]) the event that was extracted",
      "content": "# File: oasislmf/pytools/getmodel/footprint.py\n# function: get_event (lines 479-507)\n\n    def get_event(self, event_id: int):\n        \"\"\"\n        Gets the event data from the partitioned\n        parquetfootprint_chunked_filename data file.\n\n        Args:\n            event_id: (int) the ID belonging to the Event being extracted\n\n        Returns: (np.array[Event]) the event that was extracted\n        \"\"\"\n        event_info = self.footprint_lookup_map.get(event_id)\n        if event_info is None:\n            return None\n\n        partition = event_info[\"partition\"]\n        if partition != self.current_partition:\n            self.current_partition = partition\n            self.current_df = self.get_df_reader(os.path.join(parquetfootprint_chunked_dir, f\"footprint_{partition}.parquet\"),\n                                                 filters=self.areaperil_ids_filter).as_pandas()\n            if len(self.current_df):\n                self.event_map = get_event_map(self.current_df[\"event_id\"].to_numpy())\n            else:\n                self.event_map = {}\n\n        if event_id in self.event_map:\n            start_idx, end_idx = self.event_map[event_id]\n            return self.prepare_df_data(data_frame=self.current_df[start_idx: end_idx])\n        else:\n            return None\n\n\"\"\"Docstring (excerpt)\"\"\"\nGets the event data from the partitioned\nparquetfootprint_chunked_filename data file.\n\nArgs:\n    event_id: (int) the ID belonging to the Event being extracted\n\nReturns: (np.array[Event]) the event that was extracted"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/footprint.py::get_event@543",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/footprint.py",
      "symbol_type": "function",
      "name": "get_event",
      "lineno": 543,
      "end_lineno": 599,
      "business_stage": "other",
      "docstring": "Gets the event data from the partitioned parquet data file.\n\nArgs:\n    event_id: (int) the ID belonging to the Event being extracted\n\nReturns: (np.array[Event]) the event that was extracted",
      "content": "# File: oasislmf/pytools/getmodel/footprint.py\n# function: get_event (lines 543-599)\n\n    def get_event(self, event_id: int):\n        \"\"\"\n        Gets the event data from the partitioned parquet data file.\n\n        Args:\n            event_id: (int) the ID belonging to the Event being extracted\n\n        Returns: (np.array[Event]) the event that was extracted\n        \"\"\"\n        event_defintion_reader = self.get_df_reader(event_defintion_filename, filters=[(\"event_id\", \"==\", event_id)])\n        df_event_defintion = event_defintion_reader.as_pandas()\n        event_sections = list(df_event_defintion['section_id'])\n        sections = list(set(event_sections) & self.location_sections)\n\n        if len(sections) > 0:\n            df_hazard_case = {}\n            for section in sections:\n                hazard_case_reader = self.get_df_reader(\n                    f'{hazard_case_filename}/section_id={int(section)}',\n                    filters=[(\"areaperil_id\", \"in\", self.areaperil_ids)]\n                )\n                df_hazard_case[section] = hazard_case_reader.as_pandas()\n                df_hazard_case[section]['section_id'] = section\n            df_hazard_case = pd.concat(df_hazard_case, ignore_index=True)\n\n            from_cols = ['section_id', 'areaperil_id', 'intensity']\n            to_cols = from_cols + ['interpolation', 'return_period']\n\n            df_hazard_case_from = df_hazard_case.merge(\n                df_event_defintion, left_on=['section_id', 'return_period'], right_on=['section_id', 'rp_from'])[from_cols].rename(\n                    columns={'intensity': 'from_intensity'})\n\n            df_hazard_case_to = df_hazard_case.merge(\n                df_event_defintion, left_on=['section_id', 'return_period'], right_on=['section_id', 'rp_to'])[to_cols].rename(\n                    columns={'intensity': 'to_intensity'})\n\n            df_footprint = df_hazard_case_from.merge(df_hazard_case_to, on=['section_id', 'areaperil_id'], how='outer')\n            df_footprint['from_intensity'] = df_footprint['from_intensity'].fillna(0)\n\n            if len(df_footprint.index) > 0:\n                df_footprint['intensity'] = np.floor(df_footprint.from_intensity + (\n                    (df_footprint.to_intensity - df_footprint.from_intensity) * df_footprint.interpolation))\n                df_footprint['intensity'] = df_footprint['intensity'].astype('int')\n                df_footprint = df_footprint.sort_values('intensity', ascending=False)\n                df_footprint = df_footprint.drop_duplicates(subset=['areaperil_id'], keep='first')\n                df_footprint['intensity_bin_id'] = 0  # Placeholder for intensity bin ID\n                df_footprint['probability'] = 1\n            else:\n                df_footprint.loc[:, 'intensity'] = []\n                df_footprint.loc[:, 'intensity_bin_id'] = []\n                df_footprint.loc[:, 'probability'] = []\n\n            numpy_data = np.empty(len(df_footprint), dtype=EventDynamic)\n            for column in ['areaperil_id', 'intensity_bin_id', 'intensity', 'probability', 'return_period']:\n                numpy_data[:][column] = df_footprint[column].to_numpy()\n\n            return numpy_data\n\n\"\"\"Docstring (excerpt)\"\"\"\nGets the event data from the partitioned parquet data file.\n\nArgs:\n    event_id: (int) the ID belonging to the Event being extracted\n\nReturns: (np.array[Event]) the event that was extracted"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::load_items@92",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "load_items",
      "lineno": 92,
      "end_lineno": 148,
      "business_stage": "other",
      "docstring": "Processes the Items loaded from the file extracting meta data around the vulnerability data.\n\nArgs:\n    items: (List[item_dtype]) Data loaded from the vulnerability file\n    valid_area_peril_id: array of area_peril_id to be included (if none, all are included)\n\nReturns: (Tuple[Dict[int, int], List[int], Dict[int, int], List[Tuple[int, int]], List[int]])\n         vulnerability dictionary, vulnerability IDs, areaperil to vulnerability index dictionary,\n         areaperil ID to vulnerability index array, areaperil ID to vulnerability array",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: load_items (lines 92-148)\n\ndef load_items(items, valid_area_peril_id):\n    \"\"\"\n    Processes the Items loaded from the file extracting meta data around the vulnerability data.\n\n    Args:\n        items: (List[item_dtype]) Data loaded from the vulnerability file\n        valid_area_peril_id: array of area_peril_id to be included (if none, all are included)\n\n    Returns: (Tuple[Dict[int, int], List[int], Dict[int, int], List[Tuple[int, int]], List[int]])\n             vulnerability dictionary, vulnerability IDs, areaperil to vulnerability index dictionary,\n             areaperil ID to vulnerability index array, areaperil ID to vulnerability array\n    \"\"\"\n    areaperil_to_vulns_size = 0\n    areaperil_dict = Dict()\n    vuln_dict = Dict()\n    vuln_idx = 0\n    for i in range(items.shape[0]):\n        item = items[i]\n\n        # filter areaperil_id\n        if valid_area_peril_id is not None and item['areaperil_id'] not in valid_area_peril_id:\n            continue\n\n        # insert the vulnerability index if not in there\n        if item['vulnerability_id'] not in vuln_dict:\n            vuln_dict[item['vulnerability_id']] = np.int32(vuln_idx)\n            vuln_idx += 1\n\n        # insert an area dictionary into areaperil_dict under the key of areaperil ID\n        if item['areaperil_id'] not in areaperil_dict:\n            area_vuln = Dict()\n            area_vuln[item['vulnerability_id']] = 0\n            areaperil_dict[item['areaperil_id']] = area_vuln\n            areaperil_to_vulns_size += 1\n        else:\n            if item['vulnerability_id'] not in areaperil_dict[item['areaperil_id']]:\n                areaperil_to_vulns_size += 1\n                areaperil_dict[item['areaperil_id']][item['vulnerability_id']] = 0\n\n    areaperil_to_vulns_idx_dict = Dict()\n    areaperil_to_vulns_idx_array = np.empty(len(areaperil_dict), dtype=Index_type)\n    areaperil_to_vulns = np.empty(areaperil_to_vulns_size, dtype=np.int32)\n\n    areaperil_i = 0\n    vulnerability_i = 0\n\n    for areaperil_id, vulns in areaperil_dict.items():\n        areaperil_to_vulns_idx_dict[areaperil_id] = areaperil_i\n        areaperil_to_vulns_idx_array[areaperil_i]['start'] = vulnerability_i\n\n        for vuln_id in sorted(vulns):  # sorted is not necessary but doesn't impede the perf and align with cpp getmodel\n            areaperil_to_vulns[vulnerability_i] = vuln_id\n            vulnerability_i += 1\n        areaperil_to_vulns_idx_array[areaperil_i]['end'] = vulnerability_i\n        areaperil_i += 1\n\n    return vuln_dict, areaperil_to_vulns_idx_dict, areaperil_to_vulns_idx_array, areaperil_to_vulns\n\n\"\"\"Docstring (excerpt)\"\"\"\nProcesses the Items loaded from the file extracting meta data around the vulnerability data.\n\nArgs:\n    items: (List[item_dtype]) Data loaded from the vulnerability file\n    valid_area_peril_id: array of area_peril_id to be included (if none, all are included)\n\nReturns: (Tuple[Dict[int, int], List[int], Dict[int, int], List[Tuple[int, int]], List[int]])\n         vulnerability dictionary, vulnerability IDs, areaperil to vulnerability index dictionary,\n         areaperil ID to vulnerability index array, areaperil ID to vulnerability array"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::get_items@151",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "get_items",
      "lineno": 151,
      "end_lineno": 173,
      "business_stage": "other",
      "docstring": "Loads the items from the items file.\n\nArgs:\n    input_path: (str) the path pointing to the file\n    ignore_file_type: set(str) file extension to ignore when loading\n\nReturns: (Tuple[Dict[int, int], List[int], Dict[int, int], List[Tuple[int, int]], List[int]])\n         vulnerability dictionary, vulnerability IDs, areaperil to vulnerability index dictionary,\n         areaperil ID to vulnerability index array, areaperil ID to vulnerability array",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: get_items (lines 151-173)\n\ndef get_items(input_path, ignore_file_type=set(), valid_area_peril_id=None):\n    \"\"\"\n    Loads the items from the items file.\n\n    Args:\n        input_path: (str) the path pointing to the file\n        ignore_file_type: set(str) file extension to ignore when loading\n\n    Returns: (Tuple[Dict[int, int], List[int], Dict[int, int], List[Tuple[int, int]], List[int]])\n             vulnerability dictionary, vulnerability IDs, areaperil to vulnerability index dictionary,\n             areaperil ID to vulnerability index array, areaperil ID to vulnerability array\n    \"\"\"\n    input_files = set(os.listdir(input_path))\n    if \"items.bin\" in input_files and \"bin\" not in ignore_file_type:\n        logger.debug(f\"loading {os.path.join(input_path, 'items.csv')}\")\n        items = np.memmap(os.path.join(input_path, \"items.bin\"), dtype=items_dtype, mode='r')\n    elif \"items.csv\" in input_files and \"csv\" not in ignore_file_type:\n        logger.debug(f\"loading {os.path.join(input_path, 'items.csv')}\")\n        items = np.loadtxt(os.path.join(input_path, \"items.csv\"), dtype=items_dtype, delimiter=\",\", skiprows=1, ndmin=1)\n    else:\n        raise FileNotFoundError(f'items file not found at {input_path}')\n\n    return load_items(items, valid_area_peril_id)\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoads the items from the items file.\n\nArgs:\n    input_path: (str) the path pointing to the file\n    ignore_file_type: set(str) file extension to ignore when loading\n\nReturns: (Tuple[Dict[int, int], List[int], Dict[int, int], List[Tuple[int, int]], List[int]])\n         vulnerability dictionary, vulnerability IDs, areaperil to vulnerability index dictionary,\n         areaperil ID to vulnerability index array, areaperil ID to vulnerability array"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::get_intensity_bin_dict@176",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "get_intensity_bin_dict",
      "lineno": 176,
      "end_lineno": 202,
      "business_stage": "other",
      "docstring": "Loads the intensity bin dictionary file and creates a dictionary to map intensities to bins\nUsed in the dynamic footprint generation as intensitys can be adjusted for defences at runtime\n\nArgs:\n    input_path: (str) the path pointing to the file\n\nReturns: (Dict[(int, int), int])\n         intensity bin dict,\n         with index of peril_id (encoded) and intensity value, and value of bin index",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: get_intensity_bin_dict (lines 176-202)\n\ndef get_intensity_bin_dict(input_path):\n    \"\"\"\n    Loads the intensity bin dictionary file and creates a dictionary to map intensities to bins\n    Used in the dynamic footprint generation as intensitys can be adjusted for defences at runtime\n\n    Args:\n        input_path: (str) the path pointing to the file\n\n    Returns: (Dict[(int, int), int])\n             intensity bin dict,\n             with index of peril_id (encoded) and intensity value, and value of bin index\n    \"\"\"\n    input_files = set(os.listdir(input_path))\n    intensity_bin_dict = Dict.empty(nb_Tuple((nb_int32, nb_int32)), nb_int32)\n    if \"intensity_bin_dict.csv\" in input_files:\n        logger.debug(f\"loading {os.path.join(input_path, 'intensity_bin_dict.csv')}\")\n        data = pd.read_csv(os.path.join(input_path, \"intensity_bin_dict.csv\"))\n        data = data[['peril_id', 'intensity', 'intensity_bin']]\n        data['peril_id'] = data['peril_id'].apply(encode_peril_id)\n        data = data.to_records(index=False).tolist()\n        data = np.array(data, dtype=np.int32)\n        for d in data:\n            intensity_bin_dict[(d[0], d[1])] = d[2]\n    else:\n        raise FileNotFoundError(f'intensity_bin_dict file not found at {input_path}')\n\n    return intensity_bin_dict\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoads the intensity bin dictionary file and creates a dictionary to map intensities to bins\nUsed in the dynamic footprint generation as intensitys can be adjusted for defences at runtime\n\nArgs:\n    input_path: (str) the path pointing to the file\n\nReturns: (Dict[(int, int), int])\n         intensity bin dict,\n         with index of peril_id (encoded) and intensity value, and value of bin index"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::encode_peril_id@205",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "encode_peril_id",
      "lineno": 205,
      "end_lineno": 215,
      "business_stage": "other",
      "docstring": "Encode a string to an integer.\n\nArgs:\n    peril_id (str): 3-digit Oasis peril code (also works with numeric codes).\n\nReturns:\n    int: The encoded peril_id.",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: encode_peril_id (lines 205-215)\n\ndef encode_peril_id(peril_id):\n    \"\"\"Encode a string to an integer.\n\n    Args:\n        peril_id (str): 3-digit Oasis peril code (also works with numeric codes).\n\n    Returns:\n        int: The encoded peril_id.\n    \"\"\"\n\n    return sum(ord(c) << (8 * i) for i, c in enumerate(str(peril_id).upper()))\n\n\"\"\"Docstring (excerpt)\"\"\"\nEncode a string to an integer.\n\nArgs:\n    peril_id (str): 3-digit Oasis peril code (also works with numeric codes).\n\nReturns:\n    int: The encoded peril_id."
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::load_vulns_bin_idx@231",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "load_vulns_bin_idx",
      "lineno": 231,
      "end_lineno": 258,
      "business_stage": "other",
      "docstring": "Loads the vulnerability binary index file.\n\nArgs:\n    vulns_bin: (List[VulnerabilityRow]) vulnerability data from the vulnerability file\n    vulns_idx_bin: (List[VulnerabilityIndex]) vulnerability index data from the vulnerability idx file\n    vuln_dict: (Dict[int, int]) maps the vulnerability ID with the index in the vulnerability array\n    num_damage_bins: (int) number of damage bins in the data\n    num_intensity_bins: (int) the number of intensity bins\n\nReturns: (List[List[List[floats]]]) vulnerability data grouped by intensity bin and damage bin",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: load_vulns_bin_idx (lines 231-258)\n\ndef load_vulns_bin_idx(vulns_bin, vulns_idx_bin, vuln_dict,\n                       num_damage_bins, num_intensity_bins, rowsize):\n    \"\"\"\n    Loads the vulnerability binary index file.\n\n    Args:\n        vulns_bin: (List[VulnerabilityRow]) vulnerability data from the vulnerability file\n        vulns_idx_bin: (List[VulnerabilityIndex]) vulnerability index data from the vulnerability idx file\n        vuln_dict: (Dict[int, int]) maps the vulnerability ID with the index in the vulnerability array\n        num_damage_bins: (int) number of damage bins in the data\n        num_intensity_bins: (int) the number of intensity bins\n\n    Returns: (List[List[List[floats]]]) vulnerability data grouped by intensity bin and damage bin\n    \"\"\"\n    vuln_array = np.zeros((len(vuln_dict), num_damage_bins, num_intensity_bins), dtype=oasis_float)\n    vuln_ids = np.full(len(vuln_dict), null_index)\n    for idx_i in range(vulns_idx_bin.shape[0]):\n        vuln_idx = vulns_idx_bin[idx_i]\n        if vuln_idx['vulnerability_id'] in vuln_dict:\n            vuln_ids[vuln_dict[vuln_idx['vulnerability_id']]] = vuln_idx['vulnerability_id']\n            cur_vuln_array = vuln_array[vuln_dict[vuln_idx['vulnerability_id']]]\n            start = (vuln_idx['offset'] - vuln_offset) // rowsize\n            end = start + vuln_idx['size'] // rowsize\n            for vuln_i in range(start, end):\n                vuln = vulns_bin[vuln_i]\n                load_vuln_probability(cur_vuln_array, vuln, vuln_idx['vulnerability_id'])\n\n    return vuln_array, vuln_ids\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoads the vulnerability binary index file.\n\nArgs:\n    vulns_bin: (List[VulnerabilityRow]) vulnerability data from the vulnerability file\n    vulns_idx_bin: (List[VulnerabilityIndex]) vulnerability index data from the vulnerability idx file\n    vuln_dict: (Dict[int, int]) maps the vulnerability ID with the index in the vulnerability array\n    num_damage_bins: (int) number of damage bins in the data\n    num_intensity_bins: (int) the number of intensity bins\n\nReturns: (List[List[List[floats]]]) vulnerability data grouped by intensity bin and damage bin"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::load_vulns_bin_idx_adjusted@262",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "load_vulns_bin_idx_adjusted",
      "lineno": 262,
      "end_lineno": 312,
      "business_stage": "other",
      "docstring": "Loads the vulnerability binary index file, prioritizing the data in the adjustments file over the data in the\nvulnerability file.\n\nArgs:\n    vulns_bin: (List[VulnerabilityRow]) vulnerability data from the vulnerability file\n    vulns_idx_bin: (List[VulnerabilityIndex]) vulnerability index data from the vulnerability idx file\n    vuln_dict: (Dict[int, int]) maps the vulnerability ID with the index in the vulnerability array\n    num_damage_bins: (int) number of damage bins in the data\n    num_intensity_bins: (int) the number of intensity bins\n    adj_vuln_data: (List[vulnerability_dtype]) vulnerability adjustment data, sorted by vuln_id\n\nReturns: (List[List[List[floats]]]) vulnerability data grouped by intensity bin and damage bin",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: load_vulns_bin_idx_adjusted (lines 262-312)\n\ndef load_vulns_bin_idx_adjusted(vulns_bin, vulns_idx_bin, vuln_dict,\n                                num_damage_bins, num_intensity_bins, rowsize, adj_vuln_data=None):\n    \"\"\"\n    Loads the vulnerability binary index file, prioritizing the data in the adjustments file over the data in the\n    vulnerability file.\n\n    Args:\n        vulns_bin: (List[VulnerabilityRow]) vulnerability data from the vulnerability file\n        vulns_idx_bin: (List[VulnerabilityIndex]) vulnerability index data from the vulnerability idx file\n        vuln_dict: (Dict[int, int]) maps the vulnerability ID with the index in the vulnerability array\n        num_damage_bins: (int) number of damage bins in the data\n        num_intensity_bins: (int) the number of intensity bins\n        adj_vuln_data: (List[vulnerability_dtype]) vulnerability adjustment data, sorted by vuln_id\n\n    Returns: (List[List[List[floats]]]) vulnerability data grouped by intensity bin and damage bin\n    \"\"\"\n    vuln_array = np.zeros((len(vuln_dict), num_damage_bins, num_intensity_bins), dtype=oasis_float)\n    vuln_ids = np.full(len(vuln_dict), null_index)\n    adj_vuln_index = 0\n\n    for idx_i in range(vulns_idx_bin.shape[0]):\n        vuln_idx = vulns_idx_bin[idx_i]\n        vuln_id = vuln_idx['vulnerability_id']\n\n        # Check if current vulnerability id is in the adjustment data\n        while adj_vuln_data is not None and adj_vuln_index < len(adj_vuln_data) and adj_vuln_data[adj_vuln_index]['vulnerability_id'] < vuln_id:\n            adj_vuln_index += 1\n\n        if vuln_id in vuln_dict:\n            vuln_ids[vuln_dict[vuln_id]] = vuln_id\n            cur_vuln_array = vuln_array[vuln_dict[vuln_id]]\n            start = (vuln_idx['offset'] - vuln_offset) // rowsize\n            end = start + vuln_idx['size'] // rowsize\n\n            # Apply data from vulns_bin or adj_vuln_data\n            for vuln_i in range(start, end):\n                if (adj_vuln_data is not None and adj_vuln_index < len(adj_vuln_data) and adj_vuln_data[adj_vuln_index]['vulnerability_id'] == vuln_id):\n                    load_vuln_probability(cur_vuln_array, adj_vuln_data[adj_vuln_index], vuln_id)\n                    adj_vuln_index += 1\n                else:\n                    load_vuln_probability(cur_vuln_array, vulns_bin[vuln_i], vuln_id)\n\n    # Add remaining adj_vuln_data\n    while adj_vuln_data is not None and adj_vuln_index < len(adj_vuln_data):\n        adj_vuln = adj_vuln_data[adj_vuln_index]\n        vuln_id = adj_vuln['vulnerability_id']\n        if vuln_id in vuln_dict:\n            load_vuln_probability(vuln_array[vuln_dict[vuln_id]], adj_vuln, vuln_id)\n        adj_vuln_index += 1\n\n    return vuln_array, vuln_ids\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoads the vulnerability binary index file, prioritizing the data in the adjustments file over the data in the\nvulnerability file.\n\nArgs:\n    vulns_bin: (List[VulnerabilityRow]) vulnerability data from the vulnerability file\n    vulns_idx_bin: (List[VulnerabilityIndex]) vulnerability index data from the vulnerability idx file\n    vuln_dict: (Dict[int, int]) maps the vulnerability ID with the index in the vulnerability array\n    num_damage_bins: (int) number of damage bins in the data\n    num_intensity_bins: (int) the number of intensity bins\n    adj_vuln_data: (List[vulnerability_dtype]) vulnerability adjustment data, sorted by vuln_id\n\nReturns: (List[List[List[floats]]]) vulnerability data grouped by intensity bin and damage bin"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::load_vulns_bin@316",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "load_vulns_bin",
      "lineno": 316,
      "end_lineno": 344,
      "business_stage": "other",
      "docstring": "Loads the vulnerability data grouped by the intensity and damage bins.\n\nArgs:\n    vuln_bin: (List[Vulnerability]) vulnerability data from the vulnerability file\n    vuln_dict: (Dict[int, int]) maps the vulnerability ID with the index in the vulnerability array\n    num_damage_bins: (int) number of damage bins in the data\n    num_intensity_bins: (int) the number of intensity bins\n\nReturns: (List[List[List[floats]]]) vulnerability data grouped by intensity bin and damage bin",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: load_vulns_bin (lines 316-344)\n\ndef load_vulns_bin(vulns_bin, vuln_dict, num_damage_bins, num_intensity_bins):\n    \"\"\"\n    Loads the vulnerability data grouped by the intensity and damage bins.\n\n    Args:\n        vuln_bin: (List[Vulnerability]) vulnerability data from the vulnerability file\n        vuln_dict: (Dict[int, int]) maps the vulnerability ID with the index in the vulnerability array\n        num_damage_bins: (int) number of damage bins in the data\n        num_intensity_bins: (int) the number of intensity bins\n\n    Returns: (List[List[List[floats]]]) vulnerability data grouped by intensity bin and damage bin\n    \"\"\"\n    vuln_array = np.zeros((len(vuln_dict), num_damage_bins, num_intensity_bins), dtype=oasis_float)\n    vuln_ids = np.full(len(vuln_dict), null_index)\n    cur_vulnerability_id = -1\n\n    for vuln_i in range(vulns_bin.shape[0]):\n        vuln = vulns_bin[vuln_i]\n        if vuln['vulnerability_id'] != cur_vulnerability_id:\n            if vuln['vulnerability_id'] in vuln_dict:\n                cur_vulnerability_id = vuln['vulnerability_id']\n                vuln_ids[vuln_dict[cur_vulnerability_id]] = cur_vulnerability_id\n                cur_vuln_array = vuln_array[vuln_dict[cur_vulnerability_id]]\n            else:\n                cur_vulnerability_id = -1\n        if cur_vulnerability_id != -1:\n            load_vuln_probability(cur_vuln_array, vuln, cur_vulnerability_id)\n\n    return vuln_array, vuln_ids\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoads the vulnerability data grouped by the intensity and damage bins.\n\nArgs:\n    vuln_bin: (List[Vulnerability]) vulnerability data from the vulnerability file\n    vuln_dict: (Dict[int, int]) maps the vulnerability ID with the index in the vulnerability array\n    num_damage_bins: (int) number of damage bins in the data\n    num_intensity_bins: (int) the number of intensity bins\n\nReturns: (List[List[List[floats]]]) vulnerability data grouped by intensity bin and damage bin"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::load_vulns_bin_adjusted@348",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "load_vulns_bin_adjusted",
      "lineno": 348,
      "end_lineno": 395,
      "business_stage": "other",
      "docstring": "Loads the vulnerability data grouped by the intensity and damage bins, prioritizing the data\nin the adjustments file over the data in the vulnerability file.\n\nArgs:\n    vuln_bin: (List[Vulnerability]) vulnerability data from the vulnerability file\n    vuln_dict: (Dict[int, int]) maps the vulnerability ID with the index in the vulnerability array\n    num_damage_bins: (int) number of damage bins in the data\n    num_intensity_bins: (int) the number of intensity bins\n    adj_vuln_data: (List[vulnerability_dtype]) vulnerability adjustment data, sorted by vuln_id\n\nReturns: (List[List[List[floats]]]) vulnerability data grouped by intensity bin and damage bin",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: load_vulns_bin_adjusted (lines 348-395)\n\ndef load_vulns_bin_adjusted(vulns_bin, vuln_dict, num_damage_bins, num_intensity_bins, adj_vuln_data=None):\n    \"\"\"\n    Loads the vulnerability data grouped by the intensity and damage bins, prioritizing the data\n    in the adjustments file over the data in the vulnerability file.\n\n    Args:\n        vuln_bin: (List[Vulnerability]) vulnerability data from the vulnerability file\n        vuln_dict: (Dict[int, int]) maps the vulnerability ID with the index in the vulnerability array\n        num_damage_bins: (int) number of damage bins in the data\n        num_intensity_bins: (int) the number of intensity bins\n        adj_vuln_data: (List[vulnerability_dtype]) vulnerability adjustment data, sorted by vuln_id\n\n    Returns: (List[List[List[floats]]]) vulnerability data grouped by intensity bin and damage bin\n    \"\"\"\n    vuln_array = np.zeros((len(vuln_dict), num_damage_bins, num_intensity_bins), dtype=oasis_float)\n    vuln_ids = np.full(len(vuln_dict), null_index)\n    ids_to_replace = set()\n    adj_vuln_index = 0\n\n    # Create list of ids to replace if adj_vuln_data is provided\n    if adj_vuln_data is not None:\n        for adj_vuln in adj_vuln_data:\n            ids_to_replace.add(adj_vuln['vulnerability_id'])\n\n    vuln_i = 0\n\n    while vuln_i < len(vulns_bin):\n        vuln = vulns_bin[vuln_i]\n        vuln_id = vuln['vulnerability_id']\n        if vuln_id in vuln_dict:\n            vuln_ids[vuln_dict[vuln_id]] = vuln_id\n            if vuln_id in ids_to_replace:\n                # Advance to current vuln_id\n                while adj_vuln_index < len(adj_vuln_data) and adj_vuln_data[adj_vuln_index]['vulnerability_id'] < vuln_id:\n                    adj_vuln_index += 1\n                # process current vuln_id\n                while adj_vuln_index < len(adj_vuln_data) and adj_vuln_data[adj_vuln_index]['vulnerability_id'] == vuln_id:\n                    load_vuln_probability(vuln_array[vuln_dict[vuln_id]], adj_vuln_data[adj_vuln_index], vuln_id)\n                    adj_vuln_index += 1\n                # Skip remaining vulns_bin entries with the same vulnerability_id\n                while vuln_i < len(vulns_bin) and vulns_bin[vuln_i]['vulnerability_id'] == vuln_id:\n                    vuln_i += 1\n                continue\n            else:\n                # Use data from vulns_bin\n                load_vuln_probability(vuln_array[vuln_dict[vuln_id]], vuln, vuln_id)\n        vuln_i += 1\n    return vuln_array, vuln_ids\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoads the vulnerability data grouped by the intensity and damage bins, prioritizing the data\nin the adjustments file over the data in the vulnerability file.\n\nArgs:\n    vuln_bin: (List[Vulnerability]) vulnerability data from the vulnerability file\n    vuln_dict: (Dict[int, int]) maps the vulnerability ID with the index in the vulnerability array\n    num_damage_bins: (int) number of damage bins in the data\n    num_intensity_bins: (int) the number of intensity bins\n    adj_vuln_data: (List[vulnerability_dtype]) vulnerability adjustment data, sorted by vuln_id\n\nReturns: (List[List[List[floats]]]) vulnerability data grouped by intensity bin and damage bin"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::update_vulns_dictionary@399",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "update_vulns_dictionary",
      "lineno": 399,
      "end_lineno": 409,
      "business_stage": "other",
      "docstring": "Updates the indexes of the vulnerability IDs (usually used in loading vulnerability data from parquet file).\n\nArgs:\n    vuln_dict: (Dict[int, int]) vulnerability dict that maps the vulnerability IDs (key) with the index (value)\n    vulns_id_array: (List[int]) list of vulnerability IDs loaded from the parquet file",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: update_vulns_dictionary (lines 399-409)\n\ndef update_vulns_dictionary(vuln_dict, vulns_id_array):\n    \"\"\"\n    Updates the indexes of the vulnerability IDs (usually used in loading vulnerability data from parquet file).\n\n    Args:\n        vuln_dict: (Dict[int, int]) vulnerability dict that maps the vulnerability IDs (key) with the index (value)\n        vulns_id_array: (List[int]) list of vulnerability IDs loaded from the parquet file\n\n    \"\"\"\n    for i in range(vulns_id_array.shape[0]):\n        vuln_dict[vulns_id_array[i]] = np.int32(i)\n\n\"\"\"Docstring (excerpt)\"\"\"\nUpdates the indexes of the vulnerability IDs (usually used in loading vulnerability data from parquet file).\n\nArgs:\n    vuln_dict: (Dict[int, int]) vulnerability dict that maps the vulnerability IDs (key) with the index (value)\n    vulns_id_array: (List[int]) list of vulnerability IDs loaded from the parquet file"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::update_vuln_array_with_adj_data@413",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "update_vuln_array_with_adj_data",
      "lineno": 413,
      "end_lineno": 428,
      "business_stage": "other",
      "docstring": "Update the vulnerability array with adjustment data (used for parquet loading).\n\nArgs:\n    vuln_array: (3D array) The vulnerability data array.\n    vuln_dict: (Dict[int, int]) Maps vulnerability IDs to indices in vuln_array.\n    adj_vuln_data: (List[vulnerability_dtype]) The vulnerability adjustment data.\n\nReturns: (3D array) The updated vulnerability data array.",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: update_vuln_array_with_adj_data (lines 413-428)\n\ndef update_vuln_array_with_adj_data(vuln_array, vuln_dict, adj_vuln_data):\n    \"\"\"\n    Update the vulnerability array with adjustment data (used for parquet loading).\n\n    Args:\n        vuln_array: (3D array) The vulnerability data array.\n        vuln_dict: (Dict[int, int]) Maps vulnerability IDs to indices in vuln_array.\n        adj_vuln_data: (List[vulnerability_dtype]) The vulnerability adjustment data.\n\n    Returns: (3D array) The updated vulnerability data array.\n    \"\"\"\n    for adj_vuln in adj_vuln_data:\n        vuln_id = adj_vuln['vulnerability_id']\n        if vuln_id in vuln_dict:\n            load_vuln_probability(vuln_array[vuln_dict[vuln_id]], adj_vuln, vuln_id)\n    return vuln_array\n\n\"\"\"Docstring (excerpt)\"\"\"\nUpdate the vulnerability array with adjustment data (used for parquet loading).\n\nArgs:\n    vuln_array: (3D array) The vulnerability data array.\n    vuln_dict: (Dict[int, int]) Maps vulnerability IDs to indices in vuln_array.\n    adj_vuln_data: (List[vulnerability_dtype]) The vulnerability adjustment data.\n\nReturns: (3D array) The updated vulnerability data array."
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::create_vulns_id@432",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "create_vulns_id",
      "lineno": 432,
      "end_lineno": 446,
      "business_stage": "other",
      "docstring": "Creates a vulnerability array where the index of the array correlates with the index of the vulnerability.\n\nArgs:\n    vuln_dict: (Dict) maps the vulnerability of the id (key) with the vulnerability ID (value)\n\nReturns: (List[int]) list of vulnerability IDs",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: create_vulns_id (lines 432-446)\n\ndef create_vulns_id(vuln_dict):\n    \"\"\"\n    Creates a vulnerability array where the index of the array correlates with the index of the vulnerability.\n\n    Args:\n        vuln_dict: (Dict) maps the vulnerability of the id (key) with the vulnerability ID (value)\n\n    Returns: (List[int]) list of vulnerability IDs\n    \"\"\"\n    vulns_id = np.empty(len(vuln_dict), dtype=np.int32)\n\n    for vuln_id, vuln_idx in vuln_dict.items():\n        vulns_id[vuln_idx] = vuln_id\n\n    return vulns_id\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreates a vulnerability array where the index of the array correlates with the index of the vulnerability.\n\nArgs:\n    vuln_dict: (Dict) maps the vulnerability of the id (key) with the vulnerability ID (value)\n\nReturns: (List[int]) list of vulnerability IDs"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::get_vulns@449",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "get_vulns",
      "lineno": 449,
      "end_lineno": 538,
      "business_stage": "other",
      "docstring": "Loads the vulnerabilities from the file.\n\nArgs:\n    storage: (str) the storage manager for fetching model data\n    run_dir: (str) the path to the run folder (used to load the analysis settings)\n    vuln_dict: (Dict[int, int]) maps the vulnerability ID with the index in the vulnerability array\n    num_intensity_bins: (int) the number of intensity bins\n    ignore_file_type: set(str) file extension to ignore when loading\n\nReturns: (Tuple[List[List[float]], int, np.array[int]) vulnerability data, vulnerabilities id, number of damage bins",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: get_vulns (lines 449-538)\n\ndef get_vulns(\n        storage: BaseStorage, run_dir, vuln_dict, num_intensity_bins, ignore_file_type=set(), df_engine=\"oasis_data_manager.df_reader.reader.OasisPandasReader\"):\n    \"\"\"\n    Loads the vulnerabilities from the file.\n\n    Args:\n        storage: (str) the storage manager for fetching model data\n        run_dir: (str) the path to the run folder (used to load the analysis settings)\n        vuln_dict: (Dict[int, int]) maps the vulnerability ID with the index in the vulnerability array\n        num_intensity_bins: (int) the number of intensity bins\n        ignore_file_type: set(str) file extension to ignore when loading\n\n    Returns: (Tuple[List[List[float]], int, np.array[int]) vulnerability data, vulnerabilities id, number of damage bins\n    \"\"\"\n    input_files = set(storage.listdir())\n    vuln_adj = get_vulnerability_replacements(run_dir, vuln_dict)\n\n    if vulnerability_dataset in input_files and \"parquet\" not in ignore_file_type:\n        source_url = storage.get_storage_url(vulnerability_dataset, encode_params=False)[1]\n        with storage.open(parquetvulnerability_meta_filename, 'r') as outfile:\n            meta_data = json.load(outfile)\n        logger.debug(f\"loading {source_url}\")\n\n        df_reader_config = clean_config(InputReaderConfig(filepath=vulnerability_dataset, engine=df_engine))\n        df_reader_config[\"engine\"][\"options\"][\"storage\"] = storage\n        reader = get_df_reader(df_reader_config, filters=[[('vulnerability_id', '==', vuln_id)] for vuln_id in vuln_dict.keys()])\n        df = reader.as_pandas()\n        num_damage_bins = meta_data['num_damage_bins']\n        vuln_array = np.vstack(df['vuln_array'].to_numpy()).reshape(len(df['vuln_array']),\n                                                                    num_damage_bins,\n                                                                    num_intensity_bins)\n        vuln_ids = df['vulnerability_id'].to_numpy()\n        missing_vuln_ids = set(vuln_dict).difference(vuln_ids)\n        if missing_vuln_ids:\n            raise Exception(f\"Vulnerability_ids {missing_vuln_ids} are missing\"\n                            f\" from {source_url}\")\n        update_vulns_dictionary(vuln_dict, vuln_ids)\n        # update vulnerability array with adjustment data if present\n        if vuln_adj is not None and len(vuln_adj) > 0:\n            vuln_array = update_vuln_array_with_adj_data(vuln_array, vuln_dict, vuln_adj)\n\n    else:\n        if \"vulnerability.bin\" in input_files and 'bin' not in ignore_file_type:\n            source_url = storage.get_storage_url('vulnerability.bin', encode_params=False)[1]\n            logger.debug(f\"loading {source_url}\")\n            with storage.open(\"vulnerability.bin\", 'rb') as f:\n                header = np.frombuffer(f.read(8), 'i4')\n                num_damage_bins = header[0]\n\n            if \"vulnerability.idx\" in input_files and 'idx' not in ignore_file_type:\n                logger.debug(f\"loading {storage.get_storage_url('vulnerability.idx', encode_params=False)[1]}\")\n                with storage.open(\"vulnerability.bin\") as f:\n                    vulns_bin = np.memmap(f, dtype=VulnerabilityRow, offset=4, mode='r')\n\n                with storage.open(\"vulnerability.idx\") as f:\n                    vulns_idx_bin = np.memmap(f, dtype=VulnerabilityIndex, mode='r')\n\n                if vuln_adj is not None and len(vuln_adj) > 0:\n                    vuln_array, valid_vuln_ids = load_vulns_bin_idx_adjusted(vulns_bin, vulns_idx_bin, vuln_dict,\n                                                                             num_damage_bins, num_intensity_bins, VulnerabilityRow.dtype.itemsize, vuln_adj)\n                else:\n                    vuln_array, valid_vuln_ids = load_vulns_bin_idx(vulns_bin, vulns_idx_bin, vuln_dict,\n                                                                    num_damage_bins, num_intensity_bins, VulnerabilityRow.dtype.itemsize)\n            else:\n                with storage.with_fileno(\"vulnerability.bin\") as f:\n                    vulns_bin = np.memmap(f, dtype=vulnerability_dtype, offset=4, mode='r')\n                if vuln_adj is not None and len(vuln_adj) > 0:\n                    vuln_array, valid_vuln_ids = load_vulns_bin_adjusted(vulns_bin, vuln_dict, num_damage_bins, num_intensity_bins, vuln_adj)\n                else:\n                    vuln_array, valid_vuln_ids = load_vulns_bin(vulns_bin, vuln_dict, num_damage_bins, num_intensity_bins)\n\n        elif \"vulnerability.csv\" in input_files and \"csv\" not in ignore_file_type:\n            source_url = storage.get_storage_url('vulnerability.csv', encode_params=False)[1]\n            logger.debug(f\"loading {source_url}\")\n            with storage.open(\"vulnerability.csv\") as f:\n                vuln_csv = np.loadtxt(f, dtype=vulnerability_dtype, delimiter=\",\", skiprows=1, ndmin=1)\n            num_damage_bins = max(vuln_csv['damage_bin_id'])\n            if vuln_adj is not None and len(vuln_adj) > 0:\n                vuln_array, valid_vuln_ids = load_vulns_bin_adjusted(vuln_csv, vuln_dict, num_damage_bins, num_intensity_bins, vuln_adj)\n            else:\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoads the vulnerabilities from the file.\n\nArgs:\n    storage: (str) the storage manager for fetching model data\n    run_dir: (str) the path to the run folder (used to load the analysis settings)\n    vuln_dict: (Dict[int, int]) maps the vulnerability ID with the index in the vulnerability array\n    num_intensity_bins: (int) the number of intensity bins\n    ignore_file_type: set(str) file extension to ignore when loading\n\nReturns: (Tuple[List[List[float]], int, np.array[int]) vulnerability data, vulnerabilities id, number of damage bins"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::get_vulnerability_replacements@541",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "get_vulnerability_replacements",
      "lineno": 541,
      "end_lineno": 583,
      "business_stage": "other",
      "docstring": "Loads the vulnerability adjustment file.\n\nArgs:\n    path: (str) the path pointing to the run directory\n    vuln_dict: (Dict[int, int]) list of vulnerability IDs\n\nReturns: (List[vulnerability_dtype]) vulnerability replacement data",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: get_vulnerability_replacements (lines 541-583)\n\ndef get_vulnerability_replacements(run_dir, vuln_dict):\n    \"\"\"\n    Loads the vulnerability adjustment file.\n\n    Args:\n        path: (str) the path pointing to the run directory\n        vuln_dict: (Dict[int, int]) list of vulnerability IDs\n\n    Returns: (List[vulnerability_dtype]) vulnerability replacement data\n    \"\"\"\n    settings_path = os.path.join(run_dir, \"analysis_settings.json\")\n\n    if not os.path.exists(settings_path):\n        logger.warning(f\"analysis_settings.json not found in {run_dir}.\")\n        return None\n\n    if not validate_vulnerability_replacements(settings_path):\n        return None\n\n    vulnerability_replacements_key = analysis_settings_loader(settings_path).get('vulnerability_adjustments')\n\n    # Inputting the data directly into the analysis_settings.json file takes precedence over the file path\n    vulnerability_replacements_field = vulnerability_replacements_key.get('replace_data', None)\n    if vulnerability_replacements_field is None:\n        vulnerability_replacements_field = vulnerability_replacements_key.get('replace_file', None)\n\n    if isinstance(vulnerability_replacements_field, dict):\n        # Convert dict to flat array equivalent to csv format\n        flat_data = []\n        for v_id, adjustments in vulnerability_replacements_field.items():\n            for adj in adjustments:\n                flat_data.append((v_id, *adj))\n        vuln_adj = np.array(flat_data, dtype=vulnerability_dtype)\n    elif isinstance(vulnerability_replacements_field, str):\n        # Load csv file\n        absolute_path = os.path.abspath(vulnerability_replacements_field)\n        logger.debug(f\"loading {absolute_path}\")\n        vuln_adj = np.loadtxt(absolute_path, dtype=vulnerability_dtype, delimiter=\",\", skiprows=1, ndmin=1)\n    vuln_adj = np.array([adj_vuln for adj_vuln in vuln_adj if adj_vuln['vulnerability_id'] in vuln_dict],\n                        dtype=vuln_adj.dtype)\n    vuln_adj.sort(order='vulnerability_id')\n    logger.info(\"Vulnerability adjustments found in analysis settings.\")\n    return vuln_adj\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoads the vulnerability adjustment file.\n\nArgs:\n    path: (str) the path pointing to the run directory\n    vuln_dict: (Dict[int, int]) list of vulnerability IDs\n\nReturns: (List[vulnerability_dtype]) vulnerability replacement data"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::get_mean_damage_bins@586",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "get_mean_damage_bins",
      "lineno": 586,
      "end_lineno": 596,
      "business_stage": "other",
      "docstring": "Loads the mean damage bins from the damage_bin_dict file, namely, the `interpolation` value for each bin.\n\nArgs:\n    storage: (BaseStorage) the storage connector for fetching the model data\n    ignore_file_type: set(str) file extension to ignore when loading\n\nReturns: (List[Union[damagebindictionary]]) loaded data from the damage_bin_dict file",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: get_mean_damage_bins (lines 586-596)\n\ndef get_mean_damage_bins(storage: BaseStorage, ignore_file_type=set()):\n    \"\"\"\n    Loads the mean damage bins from the damage_bin_dict file, namely, the `interpolation` value for each bin.\n\n    Args:\n        storage: (BaseStorage) the storage connector for fetching the model data\n        ignore_file_type: set(str) file extension to ignore when loading\n\n    Returns: (List[Union[damagebindictionary]]) loaded data from the damage_bin_dict file\n    \"\"\"\n    return get_damage_bins(storage, ignore_file_type)['interpolation']\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoads the mean damage bins from the damage_bin_dict file, namely, the `interpolation` value for each bin.\n\nArgs:\n    storage: (BaseStorage) the storage connector for fetching the model data\n    ignore_file_type: set(str) file extension to ignore when loading\n\nReturns: (List[Union[damagebindictionary]]) loaded data from the damage_bin_dict file"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::get_damage_bins@599",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "get_damage_bins",
      "lineno": 599,
      "end_lineno": 620,
      "business_stage": "other",
      "docstring": "Loads the damage bins from the damage_bin_dict file.\n\nArgs:\n    storage: (BaseStorage) the storage connector for fetching the model data\n    ignore_file_type: set(str) file extension to ignore when loading\n\nReturns: (List[Union[damagebindictionary]]) loaded data from the damage_bin_dict file",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: get_damage_bins (lines 599-620)\n\ndef get_damage_bins(storage: BaseStorage, ignore_file_type=set()):\n    \"\"\"\n    Loads the damage bins from the damage_bin_dict file.\n\n    Args:\n        storage: (BaseStorage) the storage connector for fetching the model data\n        ignore_file_type: set(str) file extension to ignore when loading\n\n    Returns: (List[Union[damagebindictionary]]) loaded data from the damage_bin_dict file\n    \"\"\"\n    input_files = set(storage.listdir())\n\n    if \"damage_bin_dict.bin\" in input_files and 'bin' not in ignore_file_type:\n        logger.debug(f\"loading {storage.get_storage_url('damage_bin_dict.bin', encode_params=False)[1]}\")\n        with storage.with_fileno(\"damage_bin_dict.bin\") as f:\n            return np.fromfile(f, dtype=damagebin_dtype)\n    elif \"damage_bin_dict.csv\" in input_files and 'csv' not in ignore_file_type:\n        logger.debug(f\"loading {storage.get_storage_url('damage_bin_dict.csv', encode_params=False)[1]}\")\n        with storage.open(\"damage_bin_dict.csv\") as f:\n            return np.loadtxt(f, dtype=damagebin_dtype, skiprows=1, delimiter=',', ndmin=1)\n    else:\n        raise FileNotFoundError(f\"damage_bin_dict file not found at {storage.get_storage_url('', encode_params=False)[1]}\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoads the damage bins from the damage_bin_dict file.\n\nArgs:\n    storage: (BaseStorage) the storage connector for fetching the model data\n    ignore_file_type: set(str) file extension to ignore when loading\n\nReturns: (List[Union[damagebindictionary]]) loaded data from the damage_bin_dict file"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::damage_bin_prob@624",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "damage_bin_prob",
      "lineno": 624,
      "end_lineno": 643,
      "business_stage": "other",
      "docstring": "Calculate the probability of an event happening and then causing damage.\nNote: vulns is a 1-d array containing 1 damage bin of the damage probability distribution as a\nfunction of hazard intensity.\n\nArgs:\n    p: (float) the probability to be updated\n    intensities_min: (int) minimum intensity bin id\n    intensities_max: (int) maximum intensity bin id\n    vulns: (List[float]) slice of damage probability distribution given hazard intensity\n    intensities: (List[float]) intensity probability distribution\n\nReturns: (float) the updated probability",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: damage_bin_prob (lines 624-643)\n\ndef damage_bin_prob(p, intensities_min, intensities_max, vulns, intensities):\n    \"\"\"\n    Calculate the probability of an event happening and then causing damage.\n    Note: vulns is a 1-d array containing 1 damage bin of the damage probability distribution as a\n    function of hazard intensity.\n\n    Args:\n        p: (float) the probability to be updated\n        intensities_min: (int) minimum intensity bin id\n        intensities_max: (int) maximum intensity bin id\n        vulns: (List[float]) slice of damage probability distribution given hazard intensity\n        intensities: (List[float]) intensity probability distribution\n\n    Returns: (float) the updated probability\n    \"\"\"\n    i = intensities_min\n    while i < intensities_max:\n        p += vulns[i] * intensities[i]\n        i += 1\n    return p\n\n\"\"\"Docstring (excerpt)\"\"\"\nCalculate the probability of an event happening and then causing damage.\nNote: vulns is a 1-d array containing 1 damage bin of the damage probability distribution as a\nfunction of hazard intensity.\n\nArgs:\n    p: (float) the probability to be updated\n    intensities_min: (int) minimum intensity bin id\n    intensities_max: (int) maximum intensity bin id\n    vulns: (List[float]) slice of damage probability distribution given hazard intensity\n    intensities: (List[float]) intensity probability distribution\n\nReturns: (float) the updated probability"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::do_result@647",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "do_result",
      "lineno": 647,
      "end_lineno": 692,
      "business_stage": "other",
      "docstring": "Calculate the result concerning an event ID.\n\nArgs:\n    vulns_id: (List[int]) list of vulnerability IDs\n    vuln_array: (List[List[list]]) list of vulnerabilities and their data\n    mean_damage_bins: (List[float]) the mean of each damage bin (len(mean_damage_bins) == num_damage_bins)\n    int32_mv: (List[int]) FILL IN LATER\n    num_damage_bins: (int) number of damage bins in the data\n    intensities_min: (int) minimum intensity bin id\n    intensities_max: (int) maximum intensity bin id\n    intensities: (List[float]) intensity probability distribution\n    event_id: (int) the event ID that concerns the result being calculated\n    areaperil_id: (List[int]) the areaperil ID that concerns the result being calculated\n    vuln_i: (int) the index concerning the vulnerability inside the vuln_array\n    cursor: (int) PLEASE FILL IN\n\nReturns: (int) PLEASE FILL IN",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: do_result (lines 647-692)\n\ndef do_result(vulns_id, vuln_array, mean_damage_bins,\n              int32_mv, num_damage_bins,\n              intensities_min, intensities_max, intensities,\n              event_id, areaperil_id, vuln_i, cursor):\n    \"\"\"\n    Calculate the result concerning an event ID.\n\n    Args:\n        vulns_id: (List[int]) list of vulnerability IDs\n        vuln_array: (List[List[list]]) list of vulnerabilities and their data\n        mean_damage_bins: (List[float]) the mean of each damage bin (len(mean_damage_bins) == num_damage_bins)\n        int32_mv: (List[int]) FILL IN LATER\n        num_damage_bins: (int) number of damage bins in the data\n        intensities_min: (int) minimum intensity bin id\n        intensities_max: (int) maximum intensity bin id\n        intensities: (List[float]) intensity probability distribution\n        event_id: (int) the event ID that concerns the result being calculated\n        areaperil_id: (List[int]) the areaperil ID that concerns the result being calculated\n        vuln_i: (int) the index concerning the vulnerability inside the vuln_array\n        cursor: (int) PLEASE FILL IN\n\n    Returns: (int) PLEASE FILL IN\n    \"\"\"\n    int32_mv[cursor], cursor = event_id, cursor + 1\n    int32_mv[cursor:cursor + areaperil_int_relative_size] = areaperil_id.view(oasis_int)\n    cursor += areaperil_int_relative_size\n    int32_mv[cursor], cursor = vulns_id[vuln_i], cursor + 1\n\n    cur_vuln_mat = vuln_array[vuln_i]\n    p = 0\n    cursor_start = cursor\n    cursor += 1\n    oasis_float_mv = int32_mv[cursor: cursor + num_damage_bins * results_relative_size].view(oasis_float)\n    result_cursor = 0\n    damage_bin_i = 0\n\n    while damage_bin_i < num_damage_bins:\n        p = damage_bin_prob(p, intensities_min, intensities_max, cur_vuln_mat[damage_bin_i], intensities)\n        oasis_float_mv[result_cursor], result_cursor = p, result_cursor + 1\n        oasis_float_mv[result_cursor], result_cursor = mean_damage_bins[damage_bin_i], result_cursor + 1\n        damage_bin_i += 1\n        if p >= 0.999999940:\n            break\n\n    int32_mv[cursor_start] = damage_bin_i\n    return cursor + (result_cursor * oasis_float_relative_size)\n\n\"\"\"Docstring (excerpt)\"\"\"\nCalculate the result concerning an event ID.\n\nArgs:\n    vulns_id: (List[int]) list of vulnerability IDs\n    vuln_array: (List[List[list]]) list of vulnerabilities and their data\n    mean_damage_bins: (List[float]) the mean of each damage bin (len(mean_damage_bins) == num_damage_bins)\n    int32_mv: (List[int]) FILL IN LATER\n    num_damage_bins: (int) number of damage bins in the data\n    intensities_min: (int) minimum intensity bin id\n    intensities_max: (int) maximum intensity bin id\n    intensities: (List[float]) intensity probability distribution\n    event_id: (int) the event ID that concerns the result being calculated\n    areaperil_id: (List[int]) the areaperil ID that concerns the result being calculated\n    vuln_i: (int) the index concerning the vulnerability inside the vuln_array\n    cursor: (int) PLEASE FILL IN\n\nReturns: (int) PLEASE FILL IN"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::doCdf@696",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "doCdf",
      "lineno": 696,
      "end_lineno": 779,
      "business_stage": "other",
      "docstring": "Calculates the cumulative distribution function (cdf) for an event ID.\n\nArgs:\n    event_id: (int) the event ID the the CDF is being calculated to.\n    num_intensity_bins: (int) the number of intensity bins for the CDF\n    footprint: (List[Tuple[int, int, float]]) information about the footprint with event_id, areaperil_id,\n                                              probability\n    areaperil_to_vulns_idx_dict: (Dict[int, int]) maps the areaperil ID with the ENTER_HERE\n    areaperil_to_vulns_idx_array: (List[Tuple[int, int]]) the index where the areaperil ID starts and finishes\n    areaperil_to_vulns: (List[int]) maps the areaperil ID to the vulnerability ID\n    vuln_array: (List[list]) FILL IN LATER\n    vulns_id: (List[int]) list of vulnerability IDs\n    num_damage_bins: (int) number of damage bins in the data\n    mean_damage_bins: (List[float]) the mean of each damage bin (len(mean_damage_bins) == num_damage_bins)\n    int32_mv: (List[int]) FILL IN LATER\n    max_result_relative_size: (int) the maximum result size\n\nReturns: (int)",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: doCdf (lines 696-779)\n\ndef doCdf(event_id,\n          num_intensity_bins, footprint,\n          areaperil_to_vulns_idx_dict, areaperil_to_vulns_idx_array, areaperil_to_vulns,\n          vuln_array, vulns_id, num_damage_bins, mean_damage_bins,\n          int32_mv, max_result_relative_size):\n    \"\"\"\n    Calculates the cumulative distribution function (cdf) for an event ID.\n\n    Args:\n        event_id: (int) the event ID the the CDF is being calculated to.\n        num_intensity_bins: (int) the number of intensity bins for the CDF\n        footprint: (List[Tuple[int, int, float]]) information about the footprint with event_id, areaperil_id,\n                                                  probability\n        areaperil_to_vulns_idx_dict: (Dict[int, int]) maps the areaperil ID with the ENTER_HERE\n        areaperil_to_vulns_idx_array: (List[Tuple[int, int]]) the index where the areaperil ID starts and finishes\n        areaperil_to_vulns: (List[int]) maps the areaperil ID to the vulnerability ID\n        vuln_array: (List[list]) FILL IN LATER\n        vulns_id: (List[int]) list of vulnerability IDs\n        num_damage_bins: (int) number of damage bins in the data\n        mean_damage_bins: (List[float]) the mean of each damage bin (len(mean_damage_bins) == num_damage_bins)\n        int32_mv: (List[int]) FILL IN LATER\n        max_result_relative_size: (int) the maximum result size\n\n    Returns: (int)\n    \"\"\"\n    if not footprint.shape[0]:\n        return 0\n\n    intensities_min = num_intensity_bins\n    intensities_max = 0\n    intensities = np.zeros(num_intensity_bins, dtype=oasis_float)\n\n    areaperil_id = np.zeros(1, dtype=areaperil_int)\n    has_vuln = False\n    cursor = 0\n\n    for footprint_i in range(footprint.shape[0]):\n        event_row = footprint[footprint_i]\n        if areaperil_id[0] != event_row['areaperil_id']:\n            if has_vuln and intensities_min <= intensities_max:\n                areaperil_to_vulns_idx = areaperil_to_vulns_idx_array[areaperil_to_vulns_idx_dict[areaperil_id[0]]]\n                intensities_max += 1\n                for vuln_idx in range(areaperil_to_vulns_idx['start'], areaperil_to_vulns_idx['end']):\n                    vuln_i = areaperil_to_vulns[vuln_idx]\n                    if cursor + max_result_relative_size > buff_int_size:\n                        yield cursor * oasis_int_size\n                        cursor = 0\n\n                    cursor = do_result(vulns_id, vuln_array, mean_damage_bins,\n                                       int32_mv, num_damage_bins,\n                                       intensities_min, intensities_max, intensities,\n                                       event_id, areaperil_id, vuln_i, cursor)\n\n            areaperil_id[0] = event_row['areaperil_id']\n            has_vuln = areaperil_id[0] in areaperil_to_vulns_idx_dict\n\n            if has_vuln:\n                intensities[intensities_min: intensities_max] = 0\n                intensities_min = num_intensity_bins\n                intensities_max = 0\n        if has_vuln:\n            if event_row['probability'] > 0:\n                intensity_bin_i = event_row['intensity_bin_id'] - 1\n                intensities[intensity_bin_i] = event_row['probability']\n                if intensity_bin_i > intensities_max:\n                    intensities_max = intensity_bin_i\n                if intensity_bin_i < intensities_min:\n                    intensities_min = intensity_bin_i\n\n    if has_vuln and intensities_min <= intensities_max:\n        areaperil_to_vulns_idx = areaperil_to_vulns_idx_array[areaperil_to_vulns_idx_dict[areaperil_id[0]]]\n        intensities_max += 1\n        for vuln_idx in range(areaperil_to_vulns_idx['start'], areaperil_to_vulns_idx['end']):\n            vuln_i = areaperil_to_vulns[vuln_idx]\n            if cursor + max_result_relative_size > buff_int_size:\n                yield cursor * oasis_int_size\n                cursor = 0\n\n            cursor = do_result(vulns_id, vuln_array, mean_damage_bins,\n                               int32_mv, num_damage_bins,\n\n\"\"\"Docstring (excerpt)\"\"\"\nCalculates the cumulative distribution function (cdf) for an event ID.\n\nArgs:\n    event_id: (int) the event ID the the CDF is being calculated to.\n    num_intensity_bins: (int) the number of intensity bins for the CDF\n    footprint: (List[Tuple[int, int, float]]) information about the footprint with event_id, areaperil_id,\n                                              probability\n    areaperil_to_vulns_idx_dict: (Dict[int, int]) maps the areaperil ID with the ENTER_HERE\n    areaperil_to_vulns_idx_array: (List[Tuple[int, int]]) the index where the areaperil ID starts and finishes\n    areaperil_to_vulns: (List[int]) maps the areaperil ID to the vulnerability ID\n    vuln_array: (List[list]) FILL IN LATER\n    vulns_id: (List[int]) list of vulnerability IDs\n    num_damage_bins: (int) number of damage bins in the data\n    mean_damage_bins: (List[float]) the mean of each damage bin (len(mean_damage_bins) == num_damage_bins)\n    int32_mv: (List[int]) FILL IN LATER\n    max_result_relative_size: (int) the maximum result size\n\nReturns: (int)"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/manager.py::run@789",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/manager.py",
      "symbol_type": "function",
      "name": "run",
      "lineno": 789,
      "end_lineno": 916,
      "business_stage": "other",
      "docstring": "Runs the main process of the getmodel process.\n\nArgs:\n    run_dir: (str) the directory of where the process is running\n    file_in: (Optional[str]) the path to the input directory\n    file_out: (Optional[str]) the path to the output directory\n    ignore_file_type: set(str) file extension to ignore when loading\n    data_server: (bool) if set to True runs the data server\n    peril_filter (list[int]): list of perils to include in the computation (if None, all perils will be included).\n    df_engine: (str) The engine to use when loading dataframes\n\nReturns: None",
      "content": "# File: oasislmf/pytools/getmodel/manager.py\n# function: run (lines 789-916)\n\ndef run(\n    run_dir,\n    file_in,\n    file_out,\n    ignore_file_type,\n    data_server,\n    peril_filter,\n    df_engine=\"oasis_data_manager.df_reader.reader.OasisPandasReader\",\n    analysis_pk=None\n):\n    \"\"\"\n    Runs the main process of the getmodel process.\n\n    Args:\n        run_dir: (str) the directory of where the process is running\n        file_in: (Optional[str]) the path to the input directory\n        file_out: (Optional[str]) the path to the output directory\n        ignore_file_type: set(str) file extension to ignore when loading\n        data_server: (bool) if set to True runs the data server\n        peril_filter (list[int]): list of perils to include in the computation (if None, all perils will be included).\n        df_engine: (str) The engine to use when loading dataframes\n\n    Returns: None\n    \"\"\"\n    model_storage = get_storage_from_config_path(\n        os.path.join(run_dir, 'model_storage.json'),\n        os.path.join(run_dir, 'static'),\n    )\n    input_path = os.path.join(run_dir, 'input')\n    ignore_file_type = set(ignore_file_type)\n\n    if data_server:\n        logger.debug(\"data server active\")\n        FootprintLayerClient.register()\n        logger.debug(\"registered with data server\")\n        atexit.register(FootprintLayerClient.unregister)\n    else:\n        logger.debug(\"data server not active\")\n\n    with ExitStack() as stack:\n        if file_in is None:\n            streams_in = sys.stdin.buffer\n        else:\n            streams_in = stack.enter_context(open(file_in, 'rb'))\n\n        if file_out is None:\n            stream_out = sys.stdout.buffer\n        else:\n            stream_out = stack.enter_context(open(file_out, 'wb'))\n\n        event_id_mv = memoryview(bytearray(4))\n        event_ids = np.ndarray(1, buffer=event_id_mv, dtype='i4')\n\n        # load keys.csv to determine included AreaPerilID from peril_filter\n        if os.path.exists(os.path.join(input_path, 'keys.csv')):\n            keys_df = pd.read_csv(os.path.join(input_path, 'keys.csv'), dtype=Keys)\n            if peril_filter:\n                valid_area_peril_id = np.unique(keys_df.loc[keys_df['PerilID'].isin(peril_filter), 'AreaPerilID'])\n                logger.debug(\n                    f'Peril specific run: ({peril_filter}), {len(valid_area_peril_id)} AreaPerilID included out of {len(keys_df)}')\n            else:\n                valid_area_peril_id = keys_df['AreaPerilID']\n        else:\n            valid_area_peril_id = None\n\n        logger.debug('init items')\n        vuln_dict, areaperil_to_vulns_idx_dict, areaperil_to_vulns_idx_array, areaperil_to_vulns = get_items(\n            input_path, ignore_file_type, valid_area_peril_id if peril_filter else None)\n\n        logger.debug('init footprint')\n        footprint_obj = stack.enter_context(\n            Footprint.load(model_storage, ignore_file_type, df_engine=df_engine,\n                           areaperil_ids=list(areaperil_to_vulns_idx_dict.keys())))\n\n        if data_server:\n            num_intensity_bins: int = FootprintLayerClient.get_number_of_intensity_bins()\n            logger.info(f\"got {num_intensity_bins} intensity bins from server\")\n        else:\n            num_intensity_bins: int = footprint_obj.num_intensity_bins\n\n\"\"\"Docstring (excerpt)\"\"\"\nRuns the main process of the getmodel process.\n\nArgs:\n    run_dir: (str) the directory of where the process is running\n    file_in: (Optional[str]) the path to the input directory\n    file_out: (Optional[str]) the path to the output directory\n    ignore_file_type: set(str) file extension to ignore when loading\n    data_server: (bool) if set to True runs the data server\n    peril_filter (list[int]): list of perils to include in the computation (if None, all perils will be included).\n    df_engine: (str) The engine to use when loading dataframes\n\nReturns: None"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/vulnerability.py::get_vuln_info@32",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/vulnerability.py",
      "symbol_type": "function",
      "name": "get_vuln_info",
      "lineno": 32,
      "end_lineno": 54,
      "business_stage": "other",
      "docstring": "Extracts meta data from the vulnerability data.\n\nArgs:\n    vulns_bin: (List[vulnerability_dtype]) vulnerability data from the file\n\nReturns: (Tuple[int, int, int]) number of vulnerability IDs, number of intensity bins, number of damage bins",
      "content": "# File: oasislmf/pytools/getmodel/vulnerability.py\n# function: get_vuln_info (lines 32-54)\n\ndef get_vuln_info(vulns_bin) -> Tuple[int, int, int]:\n    \"\"\"\n    Extracts meta data from the vulnerability data.\n\n    Args:\n        vulns_bin: (List[vulnerability_dtype]) vulnerability data from the file\n\n    Returns: (Tuple[int, int, int]) number of vulnerability IDs, number of intensity bins, number of damage bins\n    \"\"\"\n    vulnerability_ids_set = set()\n    num_intensity_bins = 0\n    num_damage_bins = 0\n\n    for vuln_i in range(vulns_bin.shape[0]):\n        vuln = vulns_bin[vuln_i]\n        vulnerability_ids_set.add(vuln['vulnerability_id'])\n        if num_intensity_bins < vuln['intensity_bin_id']:\n            num_intensity_bins = vuln['intensity_bin_id']\n\n        if num_damage_bins < vuln['damage_bin_id']:\n            num_damage_bins = vuln['damage_bin_id']\n\n    return len(vulnerability_ids_set), num_intensity_bins, num_damage_bins\n\n\"\"\"Docstring (excerpt)\"\"\"\nExtracts meta data from the vulnerability data.\n\nArgs:\n    vulns_bin: (List[vulnerability_dtype]) vulnerability data from the file\n\nReturns: (Tuple[int, int, int]) number of vulnerability IDs, number of intensity bins, number of damage bins"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/vulnerability.py::get_array@58",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/vulnerability.py",
      "symbol_type": "function",
      "name": "get_array",
      "lineno": 58,
      "end_lineno": 96,
      "business_stage": "other",
      "docstring": "Flattens the vulnerability data into a one-dimensional array for each vulnerability ID.\n\nNOTE => Numba: cannot cache generator for the moment to work properlly, data on the same vulnerability_id must\n        all be in one block\n\nArgs:\n    vulns_bin: (List[vulnerability_dtype]) vulnerability data from file\n    num_intensity_bins: (int) the number of intensity bins in the data\n    num_damage_bins: (int) the number of damage bins in the data\n    max_vulnerability_id_size: (int) the size of the vulnerability_ids array inside the function, this will\n                                     be the size of the vulnerability IDs array that is returned\n\nReturns: (Tuple[List[int], List[List[List[float]]]]) array of vulnerability IDs, vulnerability data",
      "content": "# File: oasislmf/pytools/getmodel/vulnerability.py\n# function: get_array (lines 58-96)\n\ndef get_array(vulns_bin, num_intensity_bins, num_damage_bins, max_vulnerability_id_size) -> Tuple[List[int], List[List[List[float]]]]:\n    \"\"\"\n    Flattens the vulnerability data into a one-dimensional array for each vulnerability ID.\n\n    NOTE => Numba: cannot cache generator for the moment to work properlly, data on the same vulnerability_id must\n            all be in one block\n\n    Args:\n        vulns_bin: (List[vulnerability_dtype]) vulnerability data from file\n        num_intensity_bins: (int) the number of intensity bins in the data\n        num_damage_bins: (int) the number of damage bins in the data\n        max_vulnerability_id_size: (int) the size of the vulnerability_ids array inside the function, this will\n                                         be the size of the vulnerability IDs array that is returned\n\n    Returns: (Tuple[List[int], List[List[List[float]]]]) array of vulnerability IDs, vulnerability data\n    \"\"\"\n    vulnerability_ids = np.empty(max_vulnerability_id_size, dtype=np.int32)\n    vuln_array = np.zeros((vulnerability_ids.shape[0], num_damage_bins, num_intensity_bins), dtype=oasis_float)\n\n    cursor = 0\n    vulnerability_id_index = 0\n    vulnerability_id = -1\n\n    while cursor < vulns_bin.shape[0]:\n        vuln: vulnerability_dtype = vulns_bin[cursor]\n        if vuln['vulnerability_id'] != vulnerability_id:\n            if vulnerability_id_index == max_vulnerability_id_size:\n                yield vulnerability_ids, vuln_array\n                vuln_array.fill(0)\n                vulnerability_id_index = 0\n            vulnerability_id = vulns_bin[cursor]['vulnerability_id']\n            vulnerability_ids[vulnerability_id_index] = vulnerability_id\n            cur_vuln_array = vuln_array[vulnerability_id_index]\n            vulnerability_id_index += 1\n\n        cur_vuln_array[vuln['damage_bin_id'] - 1, vuln['intensity_bin_id'] - 1] = vuln['probability']\n        cursor += 1\n    if vulnerability_id_index:\n        yield vulnerability_ids[:vulnerability_id_index], vuln_array[:vulnerability_id_index]\n\n\"\"\"Docstring (excerpt)\"\"\"\nFlattens the vulnerability data into a one-dimensional array for each vulnerability ID.\n\nNOTE => Numba: cannot cache generator for the moment to work properlly, data on the same vulnerability_id must\n        all be in one block\n\nArgs:\n    vulns_bin: (List[vulnerability_dtype]) vulnerability data from file\n    num_intensity_bins: (int) the number of intensity bins in the data\n    num_damage_bins: (int) the number of damage bins in the data\n    max_vulnerability_id_size: (int) the size of the vulnerability_ids array inside the function, this will\n                                     be the size of the vulnerability IDs array that is returned\n\nReturns: (Tuple[List[int], List[List[List[float]]]]) array of vulnerability IDs, vulnerability data"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/vulnerability.py::iter_table@99",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/vulnerability.py",
      "symbol_type": "function",
      "name": "iter_table",
      "lineno": 99,
      "end_lineno": 121,
      "business_stage": "other",
      "docstring": "Loops through the vulnerability data, converting it into one-dimensional arrays, in-turn converting this into\nPyArrow arrays and then converting into Tables.\n\nArgs:\n    vulns_bin: (List[vulnerability_dtype]) vulnerability data from file\n    num_intensity_bins: (int) the number of intensity bins in the data\n    num_damage_bins: (int) the number of damage bins in the data\n    info: (dict) meta data around the vulnerability data. It has to store:\n                                                            num_vulnerability_id => number of vulnerability IDs\n                                                            num_intensity_bins => number of intensity bins\n                                                            num_damage_bins => number of damage bins\n    max_vulnerability_id_size: (int) the size of the vulnerability_ids array\n\nReturns:",
      "content": "# File: oasislmf/pytools/getmodel/vulnerability.py\n# function: iter_table (lines 99-121)\n\ndef iter_table(vulns_bin, num_intensity_bins, num_damage_bins, info, max_vulnerability_id_size):\n    \"\"\"\n    Loops through the vulnerability data, converting it into one-dimensional arrays, in-turn converting this into\n    PyArrow arrays and then converting into Tables.\n\n    Args:\n        vulns_bin: (List[vulnerability_dtype]) vulnerability data from file\n        num_intensity_bins: (int) the number of intensity bins in the data\n        num_damage_bins: (int) the number of damage bins in the data\n        info: (dict) meta data around the vulnerability data. It has to store:\n                                                                num_vulnerability_id => number of vulnerability IDs\n                                                                num_intensity_bins => number of intensity bins\n                                                                num_damage_bins => number of damage bins\n        max_vulnerability_id_size: (int) the size of the vulnerability_ids array\n\n    Returns:\n    \"\"\"\n    for vulnerability_ids, vuln_array in get_array(vulns_bin, num_intensity_bins, num_damage_bins, max_vulnerability_id_size):\n        arr_vulnerability_ids = pa.array(vulnerability_ids)\n        arr_vulnerability = pa.FixedSizeListArray.from_arrays(vuln_array.ravel(), num_intensity_bins * num_damage_bins)\n        vulnerability_table = pa.Table.from_arrays([arr_vulnerability_ids, arr_vulnerability], names=[\n                                                   'vulnerability_id', 'vuln_array'], metadata=info)\n        yield vulnerability_table\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoops through the vulnerability data, converting it into one-dimensional arrays, in-turn converting this into\nPyArrow arrays and then converting into Tables.\n\nArgs:\n    vulns_bin: (List[vulnerability_dtype]) vulnerability data from file\n    num_intensity_bins: (int) the number of intensity bins in the data\n    num_damage_bins: (int) the number of damage bins in the data\n    info: (dict) meta data around the vulnerability data. It has to store:\n                                                            num_vulnerability_id => number of vulnerability IDs\n                                                            num_intensity_bins => number of intensity bins\n                                                            num_damage_bins => number of damage bins\n    max_vulnerability_id_size: (int) the size of the vulnerability_ids array\n\nReturns:"
    },
    {
      "chunk_id": "oasislmf/pytools/getmodel/vulnerability.py::vulnerability_to_parquet@124",
      "source_type": "code",
      "path": "oasislmf/pytools/getmodel/vulnerability.py",
      "symbol_type": "function",
      "name": "vulnerability_to_parquet",
      "lineno": 124,
      "end_lineno": 155,
      "business_stage": "other",
      "docstring": "Converts the vulnerability data to parquet file and saves it.\n\nArgs:\n    run_dir: (str) the directory of the data where this process is to take place\n\nReturns: None",
      "content": "# File: oasislmf/pytools/getmodel/vulnerability.py\n# function: vulnerability_to_parquet (lines 124-155)\n\ndef vulnerability_to_parquet(run_dir) -> None:\n    \"\"\"\n    Converts the vulnerability data to parquet file and saves it.\n\n    Args:\n        run_dir: (str) the directory of the data where this process is to take place\n\n    Returns: None\n    \"\"\"\n    logger.debug(f'retrieving vulnerability info from {os.path.join(run_dir, vulnerability_filename)}')\n    vulns_bin = np.memmap(os.path.join(run_dir, vulnerability_filename), dtype=vulnerability_dtype, offset=4, mode='r')\n    num_vulnerability_id, num_intensity_bins, num_damage_bins = get_vuln_info(vulns_bin)\n\n    info = {\"num_vulnerability_id\": str(num_vulnerability_id),\n            \"num_intensity_bins\": str(num_intensity_bins),\n            \"num_damage_bins\": str(num_damage_bins),\n            }\n\n    logger.debug(f'{info}')\n    with open(pathlib.Path(os.path.join(run_dir, parquetvulnerability_meta_filename)), 'w') as f:\n        json.dump({key: int(val) for key, val in info.items()}, f)\n\n    dataset_path = pathlib.Path(os.path.join(run_dir, vulnerability_dataset))\n    dataset_path.mkdir(exist_ok=True)\n    for filepath in dataset_path.glob(vulnerability_parquet_filename.format('*')):\n        os.remove(filepath)\n\n    max_vulnerability_id_size = vulnerability_bloc_size // (num_intensity_bins * num_damage_bins * oasis_float.itemsize)\n    num_step = ceil(num_vulnerability_id / max_vulnerability_id_size)\n    for i, vuln_table in enumerate(iter_table(vulns_bin, num_intensity_bins, num_damage_bins, info, max_vulnerability_id_size)):\n        logger.debug(f\"step {i + 1}/{num_step}\")\n        pq.write_table(vuln_table, os.path.join(dataset_path, vulnerability_parquet_filename.format(i)))\n\n\"\"\"Docstring (excerpt)\"\"\"\nConverts the vulnerability data to parquet file and saves it.\n\nArgs:\n    run_dir: (str) the directory of the data where this process is to take place\n\nReturns: None"
    },
    {
      "chunk_id": "oasislmf/pytools/gul/core.py::get_gul@14",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/core.py",
      "symbol_type": "function",
      "name": "get_gul",
      "lineno": 14,
      "end_lineno": 55,
      "business_stage": "gul",
      "docstring": "Compute the ground-up loss using linear or quadratic interpolaiton if necessary.\n\nArgs:\n    bin_from (oasis_float): bin minimum damage.\n    bin_to (oasis_float): bin maximum damage.\n    bin_mean (oasis_float): bin mean damage (`interpolation` column in damagebins file).\n    prob_from (oasis_float): bin minimum probability\n    prob_to (oasis_float): bin maximum probability\n    rval (float64): the random cdf value.\n    bin_scaling (oasis_float): scaling on the bins.\n\nReturns:\n    float64: the computed ground-up loss",
      "content": "# File: oasislmf/pytools/gul/core.py\n# function: get_gul (lines 14-55)\n\ndef get_gul(bin_from, bin_to, bin_mean, prob_from, prob_to, rval, bin_scaling):\n    \"\"\"Compute the ground-up loss using linear or quadratic interpolaiton if necessary.\n\n    Args:\n        bin_from (oasis_float): bin minimum damage.\n        bin_to (oasis_float): bin maximum damage.\n        bin_mean (oasis_float): bin mean damage (`interpolation` column in damagebins file).\n        prob_from (oasis_float): bin minimum probability\n        prob_to (oasis_float): bin maximum probability\n        rval (float64): the random cdf value.\n        bin_scaling (oasis_float): scaling on the bins.\n\n    Returns:\n        float64: the computed ground-up loss\n    \"\"\"\n    bin_width = bin_to - bin_from\n\n    # point-like bin\n    if bin_width == 0.:\n        gul = bin_scaling * bin_to\n\n        return gul\n\n    bin_height = prob_to - prob_from\n    rval_bin_offset = rval - prob_from\n\n    # linear interpolation\n    x = np.float64((bin_mean - bin_from) / bin_width)\n    if np.abs(x - 0.5) <= 5e-6:\n        # this condition requires 1 less operation\n        gul = bin_scaling * (bin_from + rval_bin_offset * bin_width / bin_height)\n\n        return gul\n\n    # quadratic interpolation\n    aa = 3. * bin_height / bin_width**2 * (2. * x - 1.)\n    bb = 2. * bin_height / bin_width * (2. - 3. * x)\n    cc = - rval_bin_offset\n\n    gul = bin_scaling * (bin_from + (sqrt(bb**2. - 4. * aa * cc) - bb) / (2. * aa))\n\n    return gul\n\n\"\"\"Docstring (excerpt)\"\"\"\nCompute the ground-up loss using linear or quadratic interpolaiton if necessary.\n\nArgs:\n    bin_from (oasis_float): bin minimum damage.\n    bin_to (oasis_float): bin maximum damage.\n    bin_mean (oasis_float): bin mean damage (`interpolation` column in damagebins file).\n    prob_from (oasis_float): bin minimum probability\n    prob_to (oasis_float): bin maximum probability\n    rval (float64): the random cdf value.\n    bin_scaling (oasis_float): scaling on the bins.\n\nReturns:\n    float64: the computed ground-up loss"
    },
    {
      "chunk_id": "oasislmf/pytools/gul/core.py::setmaxloss@81",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/core.py",
      "symbol_type": "function",
      "name": "setmaxloss",
      "lineno": 81,
      "end_lineno": 100,
      "business_stage": "gul",
      "docstring": "Set maximum losses.\nFor each sample idx, find the maximum loss across all items and set to zero\nall the losses smaller than the maximum loss. If the maximum loss occurs in `N` items,\nthen set the loss in all these items as the maximum loss divided by `N`.\n\nArgs:\n    losses (numpy.array[oasis_float]): losses for all item_ids and sample idx.\n\nReturns:\n    numpy.array[oasis_float]: losses for all item_ids and sample idx.",
      "content": "# File: oasislmf/pytools/gul/core.py\n# function: setmaxloss (lines 81-100)\n\ndef setmaxloss(losses):\n    \"\"\"Set maximum losses.\n    For each sample idx, find the maximum loss across all items and set to zero\n    all the losses smaller than the maximum loss. If the maximum loss occurs in `N` items,\n    then set the loss in all these items as the maximum loss divided by `N`.\n\n    Args:\n        losses (numpy.array[oasis_float]): losses for all item_ids and sample idx.\n\n    Returns:\n        numpy.array[oasis_float]: losses for all item_ids and sample idx.\n    \"\"\"\n    # losses array layout is [NA, normal sidx (1 to n), special sidx (NUM_IDX)]\n    setmaxloss_i(losses, TIV_IDX)\n    setmaxloss_i(losses, MAX_LOSS_IDX)\n    setmaxloss_i(losses, MEAN_IDX)\n    for sidx in range(1, losses.shape[0] - NUM_IDX):\n        setmaxloss_i(losses, sidx)\n\n    return losses\n\n\"\"\"Docstring (excerpt)\"\"\"\nSet maximum losses.\nFor each sample idx, find the maximum loss across all items and set to zero\nall the losses smaller than the maximum loss. If the maximum loss occurs in `N` items,\nthen set the loss in all these items as the maximum loss divided by `N`.\n\nArgs:\n    losses (numpy.array[oasis_float]): losses for all item_ids and sample idx.\n\nReturns:\n    numpy.array[oasis_float]: losses for all item_ids and sample idx."
    },
    {
      "chunk_id": "oasislmf/pytools/gul/core.py::split_tiv_classic@104",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/core.py",
      "symbol_type": "function",
      "name": "split_tiv_classic",
      "lineno": 104,
      "end_lineno": 120,
      "business_stage": "gul",
      "docstring": "Split the total insured value (TIV). If the total loss of all the items\nin `gulitems` exceeds the total insured value, re-scale the losses in the\nsame proportion to the losses.\n\nArgs:\n    gulitems (numpy.array[oasis_float]): array containing losses of all items.\n    tiv (oasis_float): total insured value.",
      "content": "# File: oasislmf/pytools/gul/core.py\n# function: split_tiv_classic (lines 104-120)\n\ndef split_tiv_classic(gulitems, tiv):\n    \"\"\"Split the total insured value (TIV). If the total loss of all the items\n    in `gulitems` exceeds the total insured value, re-scale the losses in the\n    same proportion to the losses.\n\n    Args:\n        gulitems (numpy.array[oasis_float]): array containing losses of all items.\n        tiv (oasis_float): total insured value.\n    \"\"\"\n    total_loss = np.sum(gulitems)\n\n    if total_loss > tiv:\n        f = tiv / total_loss\n\n        for j in range(gulitems.shape[0]):\n            # editing in-place the np array\n            gulitems[j] *= f\n\n\"\"\"Docstring (excerpt)\"\"\"\nSplit the total insured value (TIV). If the total loss of all the items\nin `gulitems` exceeds the total insured value, re-scale the losses in the\nsame proportion to the losses.\n\nArgs:\n    gulitems (numpy.array[oasis_float]): array containing losses of all items.\n    tiv (oasis_float): total insured value."
    },
    {
      "chunk_id": "oasislmf/pytools/gul/core.py::split_tiv_multiplicative@124",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/core.py",
      "symbol_type": "function",
      "name": "split_tiv_multiplicative",
      "lineno": 124,
      "end_lineno": 149,
      "business_stage": "gul",
      "docstring": "Split the total insured value (TIV) using a multiplicative formula for the\ntotal loss as tiv * (1 - (1-A)*(1-B)*(1-C)...), where A, B, C are damage ratios\ncomputed as the ratio between a sub-peril loss and the tiv. Sub-peril losses\nin gulitems are always back-allocated proportionally to the losses.\n\nArgs:\n    gulitems (numpy.array[oasis_float]): array containing losses of all items.\n    tiv (oasis_float): total insured value.",
      "content": "# File: oasislmf/pytools/gul/core.py\n# function: split_tiv_multiplicative (lines 124-149)\n\ndef split_tiv_multiplicative(gulitems, tiv):\n    \"\"\"Split the total insured value (TIV) using a multiplicative formula for the\n    total loss as tiv * (1 - (1-A)*(1-B)*(1-C)...), where A, B, C are damage ratios\n    computed as the ratio between a sub-peril loss and the tiv. Sub-peril losses\n    in gulitems are always back-allocated proportionally to the losses.\n\n    Args:\n        gulitems (numpy.array[oasis_float]): array containing losses of all items.\n        tiv (oasis_float): total insured value.\n    \"\"\"\n    Ngulitems = gulitems.shape[0]\n    undamaged_value = 1.\n    sum_loss = 0.\n    for i in range(Ngulitems):\n        undamaged_value *= 1. - gulitems[i] / tiv\n        sum_loss += gulitems[i]\n\n    multiplicative_loss = tiv * (1. - undamaged_value)\n\n    if sum_loss > 0.:\n        # back-allocate proportionally in any case (i.e., not only if total_loss > tiv)\n        f = multiplicative_loss / sum_loss\n\n        for j in range(Ngulitems):\n            # editing in-place the np array\n            gulitems[j] *= f\n\n\"\"\"Docstring (excerpt)\"\"\"\nSplit the total insured value (TIV) using a multiplicative formula for the\ntotal loss as tiv * (1 - (1-A)*(1-B)*(1-C)...), where A, B, C are damage ratios\ncomputed as the ratio between a sub-peril loss and the tiv. Sub-peril losses\nin gulitems are always back-allocated proportionally to the losses.\n\nArgs:\n    gulitems (numpy.array[oasis_float]): array containing losses of all items.\n    tiv (oasis_float): total insured value."
    },
    {
      "chunk_id": "oasislmf/pytools/gul/core.py::compute_mean_loss@153",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/core.py",
      "symbol_type": "function",
      "name": "compute_mean_loss",
      "lineno": 153,
      "end_lineno": 186,
      "business_stage": "gul",
      "docstring": "Compute the mean ground-up loss and some properties.\n\nArgs:\n    bin_scaling (oasis_float): scaling on damage bin values.\n    prob_to (numpy.array[oasis_float]): bin maximum probability\n    bin_mean (numpy.array[oasis_float]): bin mean damage (`interpolation` column in damagebins file).\n    bin_count (int): number of bins.\n    max_damage_bin_to (oasis_float): maximum damage value (i.e., `bin_to` of the last damage bin).\n\nReturns:\n    float64, float64, float64, float64: mean ground-up loss, standard deviation of the ground-up loss,\n      chance of loss, maximum loss",
      "content": "# File: oasislmf/pytools/gul/core.py\n# function: compute_mean_loss (lines 153-186)\n\ndef compute_mean_loss(bin_scaling, prob_to, bin_mean, bin_count, max_damage_bin_to):\n    \"\"\"Compute the mean ground-up loss and some properties.\n\n    Args:\n        bin_scaling (oasis_float): scaling on damage bin values.\n        prob_to (numpy.array[oasis_float]): bin maximum probability\n        bin_mean (numpy.array[oasis_float]): bin mean damage (`interpolation` column in damagebins file).\n        bin_count (int): number of bins.\n        max_damage_bin_to (oasis_float): maximum damage value (i.e., `bin_to` of the last damage bin).\n\n    Returns:\n        float64, float64, float64, float64: mean ground-up loss, standard deviation of the ground-up loss,\n          chance of loss, maximum loss\n    \"\"\"\n    # chance_of_loss = 1. - prob_to[0] if bin_mean[0] == 0. else 1.\n    chance_of_loss = 1 - prob_to[0] * (1 - (bin_mean[0] > 0))\n\n    gul_mean = 0.\n    ctr_var = 0.\n    last_prob_to = 0.\n    for i in range(bin_count):\n        prob_from = last_prob_to\n        new_gul = (prob_to[i] - prob_from) * bin_mean[i]\n        gul_mean += new_gul\n        ctr_var += new_gul * bin_mean[i]\n        last_prob_to = prob_to[i]\n\n    gul_mean *= bin_scaling\n    ctr_var *= bin_scaling**2.\n    # Var(aX) = a**2 E(X^2) - E(aX)**2\n    std_dev = sqrt(max(ctr_var - gul_mean**2., 0.))\n    max_loss = max_damage_bin_to * bin_scaling\n\n    return gul_mean, std_dev, chance_of_loss, max_loss\n\n\"\"\"Docstring (excerpt)\"\"\"\nCompute the mean ground-up loss and some properties.\n\nArgs:\n    bin_scaling (oasis_float): scaling on damage bin values.\n    prob_to (numpy.array[oasis_float]): bin maximum probability\n    bin_mean (numpy.array[oasis_float]): bin mean damage (`interpolation` column in damagebins file).\n    bin_count (int): number of bins.\n    max_damage_bin_to (oasis_float): maximum damage value (i.e., `bin_to` of the last damage bin).\n\nReturns:\n    float64, float64, float64, float64: mean ground-up loss, standard deviation of the ground-up loss,\n      chance of loss, maximum loss"
    },
    {
      "chunk_id": "oasislmf/pytools/gul/io.py::gen_structs@23",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/io.py",
      "symbol_type": "function",
      "name": "gen_structs",
      "lineno": 23,
      "end_lineno": 34,
      "business_stage": "gul",
      "docstring": "Generate some data structures needed for the whole computation.\n\nReturns:\n    Dict(int,int), List: map of group ids to random seeds,\n      list storing the index where a specific cdf record starts in the `rec` numpy array.",
      "content": "# File: oasislmf/pytools/gul/io.py\n# function: gen_structs (lines 23-34)\n\ndef gen_structs():\n    \"\"\"Generate some data structures needed for the whole computation.\n\n    Returns:\n        Dict(int,int), List: map of group ids to random seeds,\n          list storing the index where a specific cdf record starts in the `rec` numpy array.\n\n    \"\"\"\n    group_id_rng_index = Dict.empty(nb_int32, nb_int64)\n    rec_idx_ptr = List([0])\n\n    return group_id_rng_index, rec_idx_ptr\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate some data structures needed for the whole computation.\n\nReturns:\n    Dict(int,int), List: map of group ids to random seeds,\n      list storing the index where a specific cdf record starts in the `rec` numpy array."
    },
    {
      "chunk_id": "oasislmf/pytools/gul/io.py::read_getmodel_stream@46",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/io.py",
      "symbol_type": "function",
      "name": "read_getmodel_stream",
      "lineno": 46,
      "end_lineno": 167,
      "business_stage": "gul",
      "docstring": "Read the getmodel output stream yielding data event by event.\n\nArgs:\n    stream_in (buffer-like): input stream, e.g. `sys.stdin.buffer`.\n    item_map (Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]): dict storing\n      the mapping between areaperil_id, vulnerability_id to item.\n    coverages (numpy.ndarray[coverage_type]): array with coverage data.\n    compute (numpy.array[int]): list of coverages to be computed.\n    seeds (numpy.array[int]): the random seeds for each coverage_id.\n    buff_size (int): size in bytes of the read buffer (see note).\n    valid_area_peril_id (list[int]): list of valid areaperil_ids.\n\nRaises:\n    ValueError: If the stream type is not 1.\n\nYields:\n    int, int, numpy.array[items_data_type], numpy.array[oasis_float], numpy.array[int], int:\n      event_id, index of the last coverage_id stored in compute, item-related data,\n      cdf records, array with the indices of `rec` where each cdf record starts,\n      number of unique random seeds computed so far.\n\nNote:\n    It is advisable to set buff_size as 2x the maximum pipe limit (65536 bytes)\n    to ensure that the stream is always read in the biggest possible chunks,\n    which nominally is the largest between the pipe limit and the remaining memory\n    to fill the memoryview.",
      "content": "# File: oasislmf/pytools/gul/io.py\n# function: read_getmodel_stream (lines 46-167)\n\ndef read_getmodel_stream(stream_in, item_map, coverages, compute, seeds, valid_area_peril_id=None, buff_size=PIPE_CAPACITY):\n    \"\"\"Read the getmodel output stream yielding data event by event.\n\n    Args:\n        stream_in (buffer-like): input stream, e.g. `sys.stdin.buffer`.\n        item_map (Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]): dict storing\n          the mapping between areaperil_id, vulnerability_id to item.\n        coverages (numpy.ndarray[coverage_type]): array with coverage data.\n        compute (numpy.array[int]): list of coverages to be computed.\n        seeds (numpy.array[int]): the random seeds for each coverage_id.\n        buff_size (int): size in bytes of the read buffer (see note).\n        valid_area_peril_id (list[int]): list of valid areaperil_ids.\n\n    Raises:\n        ValueError: If the stream type is not 1.\n\n    Yields:\n        int, int, numpy.array[items_data_type], numpy.array[oasis_float], numpy.array[int], int:\n          event_id, index of the last coverage_id stored in compute, item-related data,\n          cdf records, array with the indices of `rec` where each cdf record starts,\n          number of unique random seeds computed so far.\n\n    Note:\n        It is advisable to set buff_size as 2x the maximum pipe limit (65536 bytes)\n        to ensure that the stream is always read in the biggest possible chunks,\n        which nominally is the largest between the pipe limit and the remaining memory\n        to fill the memoryview.\n\n    \"\"\"\n    # determine stream type\n    stream_source_type, stream_agg_type = bytes_to_stream_types(stream_in.read(4))\n\n    # see https://github.com/OasisLMF/ktools/blob/master/docs/md/CoreComponents.md\n    if stream_source_type != CDF_STREAM_ID:\n        raise ValueError(f\"FATAL: Invalid stream type: expect {CDF_STREAM_ID}, got {stream_source_type}.\")\n\n    # maximum number of entries is buff_size divided by the minimum entry size\n    # (corresponding to a 1-bin only cdf)\n    min_size_cdf_entry = damagecdfrec_stream.size + 4 + ProbMean.size\n\n    # each record from getmodel stream is expected to contain:\n    # 1 damagecdfrec_stream obj, 1 int (Nbins), a number `Nbins` of ProbMean objects\n\n    # init the memory view to store the stream\n    mv = memoryview(bytearray(buff_size))\n    byte_mv = np.frombuffer(buffer=mv, dtype='b')\n\n    valid_buf = 0\n    last_event_id = -1\n    len_read = 1\n\n    if valid_area_peril_id is not None:\n        valid_area_peril_dict = gen_valid_area_peril(valid_area_peril_id)\n    else:\n        valid_area_peril_dict = None\n\n    # init data structures\n    group_id_rng_index, rec_idx_ptr = gen_structs()\n    rng_index = 0\n    damagecdf_i = 0\n    compute_i = 0\n    items_data_i = 0\n    coverages['cur_items'].fill(0)\n    recs = []\n    dmgcdfrecs = []\n\n    items_data = np.empty(2 ** NP_BASE_ARRAY_SIZE, dtype=items_data_type)\n    select_stream_list = [stream_in]\n\n    while True:\n        if valid_buf < buff_size and len_read:\n            select(select_stream_list, [], select_stream_list)\n\n            # read the stream from valid_buf onwards\n            len_read = stream_in.readinto1(mv[valid_buf:])\n            valid_buf += len_read\n\n        if valid_buf == 0:\n            # the stream has ended and all the data has been read\n            if last_event_id != -1:\n\n\"\"\"Docstring (excerpt)\"\"\"\nRead the getmodel output stream yielding data event by event.\n\nArgs:\n    stream_in (buffer-like): input stream, e.g. `sys.stdin.buffer`.\n    item_map (Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]): dict storing\n      the mapping between areaperil_id, vulnerability_id to item.\n    coverages (numpy.ndarray[coverage_type]): array with coverage data.\n    compute (numpy.array[int]): list of coverages to be computed.\n    seeds (numpy.array[int]): the random seeds for each coverage_id.\n    buff_size (int): size in bytes of the read buffer (see note).\n    valid_area_peril_id (list[int]): list of valid areaperil_ids.\n\nRaises:\n    ValueError: If the stream type is not 1.\n\nYields:\n    int, int, numpy.array[items_data_type], numpy.array[oasis_float], numpy.array[int], int:\n      event_id, index of the last coverage_id stored in compute, item-related data,\n      cdf records, array with the indices of `rec` where each cdf record starts,\n      number of unique random seeds computed so far.\n\nNote:\n    It is advisable to set buff_size as 2x the maximum pipe limit (65536 bytes)\n    to ensure that the stream is always read in the biggest possible chunks,\n    which nominally is the largest between the pipe limit and the remaining memory\n    to fill the memoryview."
    },
    {
      "chunk_id": "oasislmf/pytools/gul/io.py::stream_to_data@171",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/io.py",
      "symbol_type": "function",
      "name": "stream_to_data",
      "lineno": 171,
      "end_lineno": 304,
      "business_stage": "gul",
      "docstring": "Parse streamed data into data arrays.\n\nArgs:\n    byte_mv (ndarray): byte view of the buffer\n    valid_buf (int): number of bytes with valid data\n    size_cdf_entry (int): size (in bytes) of a single record\n    last_event_id (int): event_id of the last event that was completed\n    item_map (Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]): dict storing\n      the mapping between areaperil_id, vulnerability_id to item.\n    coverages (numpy.ndarray[coverage_type]): array with coverage data.\n    compute_i (int): index of the last coverage id stored in `compute`.\n    compute (numpy.array[int]): list of coverage ids to be computed.\n    items_data_i (int): index of the last items_data_i stored in `items_data`.\n    items_data (numpy.array[items_data_type]): item-related data.\n    seeds (numpy.array[int]): the random seeds for each coverage_id.\n    rng_index (int): number of unique random seeds computed so far.\n    group_id_rng_index (Dict([int,int])): map of group ids to random seeds.\n    damagecdf_i (int): index of the last cdf record that has been read from stream and stored in `rec`.\n    rec_idx_ptr (numpy.array[int]): array with the indices of `rec` where each cdf record starts.\n\nReturns:\n    int, bool, int, numpy.array[ProbMean], numpy.array[int], int, int, int, numpy.array[items_data_type],\n    int, Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]), int:\n      number of int numbers read from the int32_mv ndarray, whether the current event (id=`event_id`)\n      has been fully read, cdf record, array with the indices of `rec` where each cdf record starts, last or current\n      event id, index of the last coverage id stored in `compute`, index of the last items_data_i stored in `items_data`,\n      item-related data, number of unique random seeds computed so far, map of group ids to random seeds,\n      index of the last cdf record that has been read from stream and stored in `rec`W",
      "content": "# File: oasislmf/pytools/gul/io.py\n# function: stream_to_data (lines 171-304)\n\ndef stream_to_data(byte_mv, valid_buf, size_cdf_entry, last_event_id, item_map, coverages, valid_area_peril_dict,\n                   compute_i, compute, items_data_i, items_data, seeds, rng_index, group_id_rng_index, damagecdf_i, rec_idx_ptr):\n    \"\"\"Parse streamed data into data arrays.\n\n    Args:\n        byte_mv (ndarray): byte view of the buffer\n        valid_buf (int): number of bytes with valid data\n        size_cdf_entry (int): size (in bytes) of a single record\n        last_event_id (int): event_id of the last event that was completed\n        item_map (Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]): dict storing\n          the mapping between areaperil_id, vulnerability_id to item.\n        coverages (numpy.ndarray[coverage_type]): array with coverage data.\n        compute_i (int): index of the last coverage id stored in `compute`.\n        compute (numpy.array[int]): list of coverage ids to be computed.\n        items_data_i (int): index of the last items_data_i stored in `items_data`.\n        items_data (numpy.array[items_data_type]): item-related data.\n        seeds (numpy.array[int]): the random seeds for each coverage_id.\n        rng_index (int): number of unique random seeds computed so far.\n        group_id_rng_index (Dict([int,int])): map of group ids to random seeds.\n        damagecdf_i (int): index of the last cdf record that has been read from stream and stored in `rec`.\n        rec_idx_ptr (numpy.array[int]): array with the indices of `rec` where each cdf record starts.\n\n    Returns:\n        int, bool, int, numpy.array[ProbMean], numpy.array[int], int, int, int, numpy.array[items_data_type],\n        int, Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]), int:\n          number of int numbers read from the int32_mv ndarray, whether the current event (id=`event_id`)\n          has been fully read, cdf record, array with the indices of `rec` where each cdf record starts, last or current\n          event id, index of the last coverage id stored in `compute`, index of the last items_data_i stored in `items_data`,\n          item-related data, number of unique random seeds computed so far, map of group ids to random seeds,\n          index of the last cdf record that has been read from stream and stored in `rec`W\n    \"\"\"\n    yield_event = False\n\n    # `rec` is a temporary buffer to store the cdf being read\n    # conservative choice: size `rec` as if the entire buffer is filled with cdf bins\n    rec = np.zeros(valid_buf // ProbMean_size, dtype=ProbMean)\n    dmgcdfrec = np.zeros(valid_buf // ProbMean_size, dtype=damagecdfrec)\n    dmgcdfrec_idx = 0\n\n    # int32 memoryview cursor\n    cursor = 0\n\n    # init a counter for the local `rec` array\n    last_rec_idx_ptr = 0\n    rec_valid_len = 0\n\n    while cursor + size_cdf_entry <= valid_buf:\n\n        event_cursor = cursor\n        event_id, cursor = mv_read(byte_mv, cursor, oasis_int, oasis_int_size)\n\n        if event_id != last_event_id:\n            # a new event has started\n            if last_event_id > 0:\n                # if this is not the beginning of the very first event, yield the event that was just completed\n                yield_event = True\n                cursor = event_cursor\n                break\n\n            last_event_id = event_id\n\n        areaperil_id, cursor = mv_read(byte_mv, cursor, areaperil_int, areaperil_int_size)\n        vulnerability_id, cursor = mv_read(byte_mv, cursor, oasis_int, oasis_int_size)\n        Nbins_to_read, cursor = mv_read(byte_mv, cursor, oasis_int, oasis_int_size)\n\n        if cursor + Nbins_to_read * ProbMean_size > valid_buf:\n            # if the next cdf record is not fully contained in the valid buf, then\n            # get more data in the buffer and put cursor back at the beginning of this cdf\n            cursor = event_cursor\n            break\n\n        # peril filter\n        if valid_area_peril_dict is not None:\n            if areaperil_id not in valid_area_peril_dict:\n                cursor += ProbMean_size * Nbins_to_read\n                continue\n\n        dmgcdfrec[dmgcdfrec_idx][\"areaperil_id\"] = areaperil_id\n        dmgcdfrec[dmgcdfrec_idx][\"vulnerability_id\"] = vulnerability_id\n        dmgcdfrec_idx += 1\n\n\"\"\"Docstring (excerpt)\"\"\"\nParse streamed data into data arrays.\n\nArgs:\n    byte_mv (ndarray): byte view of the buffer\n    valid_buf (int): number of bytes with valid data\n    size_cdf_entry (int): size (in bytes) of a single record\n    last_event_id (int): event_id of the last event that was completed\n    item_map (Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]): dict storing\n      the mapping between areaperil_id, vulnerability_id to item.\n    coverages (numpy.ndarray[coverage_type]): array with coverage data.\n    compute_i (int): index of the last coverage id stored in `compute`.\n    compute (numpy.array[int]): list of coverage ids to be computed.\n    items_data_i (int): index of the last items_data_i stored in `items_data`.\n    items_data (numpy.array[items_data_type]): item-related data.\n    seeds (numpy.array[int]): the random seeds for each coverage_id.\n    rng_index (int): number of unique random seeds computed so far.\n    group_id_rng_index (Dict([int,int])): map of group ids to random seeds.\n    damagecdf_i (int): index of the last cdf record that has been read from stream and stored in `rec`.\n    rec_idx_ptr (numpy.array[int]): array with the indices of `rec` where each cdf record starts.\n\nReturns:\n    int, bool, int, numpy.array[ProbMean], numpy.array[int], int, int, int, numpy.array[items_data_type],\n    int, Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]), int:\n      number of int numbers read from the int32_mv ndarray, whether the current event (id=`event_id`)\n      has been fully read, cdf record, array with the indices of `rec` where each cdf record starts, last or current\n      event id, index of the last coverage id stored in `compute`, index of the last items_data_i stored in `items_data`,\n      item-related data, number of unique random seeds computed so far, map of group ids to random seeds,\n      index of the last cdf record that has been read from stream and stored in `rec`W"
    },
    {
      "chunk_id": "oasislmf/pytools/gul/manager.py::adjust_byte_mv_size@47",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/manager.py",
      "symbol_type": "function",
      "name": "adjust_byte_mv_size",
      "lineno": 47,
      "end_lineno": 66,
      "business_stage": "gul",
      "docstring": "adjust buff size so that the buffer fits the longest coverage\nArgs:\n    byte_mv: numpy byte array\n    max_bytes_per_coverage: max size possible to accommodate all the coverage in byte_mv\n\nReturns:\n    byte_mv: numpy byte array",
      "content": "# File: oasislmf/pytools/gul/manager.py\n# function: adjust_byte_mv_size (lines 47-66)\n\ndef adjust_byte_mv_size(byte_mv, max_bytes_per_coverage):\n    \"\"\"\n    adjust buff size so that the buffer fits the longest coverage\n    Args:\n        byte_mv: numpy byte array\n        max_bytes_per_coverage: max size possible to accommodate all the coverage in byte_mv\n\n    Returns:\n        byte_mv: numpy byte array\n    \"\"\"\n    #\n    buff_size = byte_mv.shape[0]\n    while buff_size < max_bytes_per_coverage:\n        buff_size *= 2\n\n    if byte_mv.shape[0] < buff_size:\n        # create a new bigger byte_mv\n        byte_mv = np.empty(buff_size, dtype='b')\n\n    return byte_mv\n\n\"\"\"Docstring (excerpt)\"\"\"\nadjust buff size so that the buffer fits the longest coverage\nArgs:\n    byte_mv: numpy byte array\n    max_bytes_per_coverage: max size possible to accommodate all the coverage in byte_mv\n\nReturns:\n    byte_mv: numpy byte array"
    },
    {
      "chunk_id": "oasislmf/pytools/gul/manager.py::gul_get_items@69",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/manager.py",
      "symbol_type": "function",
      "name": "gul_get_items",
      "lineno": 69,
      "end_lineno": 93,
      "business_stage": "gul",
      "docstring": "Load the items from the items file.\n\nArgs:\n    input_path (str): the path pointing to the file\n    ignore_file_type (Set[str]): file extension to ignore when loading.\n\nReturns:\n    Tuple[Dict[int, int], List[int], Dict[int, int], List[Tuple[int, int]], List[int]]\n      vulnerability dictionary, vulnerability IDs, areaperil to vulnerability index dictionary,\n      areaperil ID to vulnerability index array, areaperil ID to vulnerability array",
      "content": "# File: oasislmf/pytools/gul/manager.py\n# function: gul_get_items (lines 69-93)\n\ndef gul_get_items(input_path, ignore_file_type=set()):\n    \"\"\"Load the items from the items file.\n\n    Args:\n        input_path (str): the path pointing to the file\n        ignore_file_type (Set[str]): file extension to ignore when loading.\n\n    Returns:\n        Tuple[Dict[int, int], List[int], Dict[int, int], List[Tuple[int, int]], List[int]]\n          vulnerability dictionary, vulnerability IDs, areaperil to vulnerability index dictionary,\n          areaperil ID to vulnerability index array, areaperil ID to vulnerability array\n    \"\"\"\n    input_files = set(os.listdir(input_path))\n    if \"items.bin\" in input_files and \"bin\" not in ignore_file_type:\n        items_fname = os.path.join(input_path, 'items.bin')\n        logger.debug(f\"loading {items_fname}\")\n        items = np.memmap(items_fname, dtype=items_dtype, mode='r')\n    elif \"items.csv\" in input_files and \"csv\" not in ignore_file_type:\n        items_fname = os.path.join(input_path, 'items.csv')\n        logger.debug(f\"loading {items_fname}\")\n        items = np.loadtxt(items_fname, dtype=items_dtype, delimiter=\",\", skiprows=1, ndmin=1)\n    else:\n        raise FileNotFoundError(f'items file not found at {input_path}')\n\n    return items\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoad the items from the items file.\n\nArgs:\n    input_path (str): the path pointing to the file\n    ignore_file_type (Set[str]): file extension to ignore when loading.\n\nReturns:\n    Tuple[Dict[int, int], List[int], Dict[int, int], List[Tuple[int, int]], List[int]]\n      vulnerability dictionary, vulnerability IDs, areaperil to vulnerability index dictionary,\n      areaperil ID to vulnerability index array, areaperil ID to vulnerability array"
    },
    {
      "chunk_id": "oasislmf/pytools/gul/manager.py::generate_item_map@97",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/manager.py",
      "symbol_type": "function",
      "name": "generate_item_map",
      "lineno": 97,
      "end_lineno": 121,
      "business_stage": "gul",
      "docstring": "Generate item_map; requires items to be sorted.\n\nArgs:\n    items (numpy.ndarray[int32, int32, int32]): 1-d structured array storing\n      `item_id`, `coverage_id`, `group_id` for all items.\n      items need to be sorted by increasing areaperil_id, vulnerability_id\n      in order to output the items in correct order.\n\nReturns:\n    item_map (Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]): dict storing\n      the mapping between areaperil_id, vulnerability_id to item.",
      "content": "# File: oasislmf/pytools/gul/manager.py\n# function: generate_item_map (lines 97-121)\n\ndef generate_item_map(items, coverages):\n    \"\"\"Generate item_map; requires items to be sorted.\n\n    Args:\n        items (numpy.ndarray[int32, int32, int32]): 1-d structured array storing\n          `item_id`, `coverage_id`, `group_id` for all items.\n          items need to be sorted by increasing areaperil_id, vulnerability_id\n          in order to output the items in correct order.\n\n    Returns:\n        item_map (Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]): dict storing\n          the mapping between areaperil_id, vulnerability_id to item.\n    \"\"\"\n    item_map = Dict.empty(ITEM_MAP_KEY_TYPE, List.empty_list(ITEM_MAP_VALUE_TYPE))\n    Nitems = items.shape[0]\n\n    for j in range(Nitems):\n        append_to_dict_value(\n            item_map,\n            tuple((items[j]['areaperil_id'], items[j]['vulnerability_id'])),\n            tuple((items[j]['item_id'], items[j]['coverage_id'], items[j]['group_id'])),\n            ITEM_MAP_VALUE_TYPE\n        )\n        coverages[items[j]['coverage_id']]['max_items'] += 1\n    return item_map\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate item_map; requires items to be sorted.\n\nArgs:\n    items (numpy.ndarray[int32, int32, int32]): 1-d structured array storing\n      `item_id`, `coverage_id`, `group_id` for all items.\n      items need to be sorted by increasing areaperil_id, vulnerability_id\n      in order to output the items in correct order.\n\nReturns:\n    item_map (Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]): dict storing\n      the mapping between areaperil_id, vulnerability_id to item."
    },
    {
      "chunk_id": "oasislmf/pytools/gul/manager.py::run@125",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/manager.py",
      "symbol_type": "function",
      "name": "run",
      "lineno": 125,
      "end_lineno": 326,
      "business_stage": "gul",
      "docstring": "Execute the main gulpy worklow.\n\nArgs:\n    run_dir: (str) the directory of where the process is running\n    ignore_file_type set(str): file extension to ignore when loading\n    sample_size (int): number of random samples to draw.\n    loss_threshold (float): threshold above which losses are printed to the output stream.\n    alloc_rule (int): back-allocation rule.\n    debug (bool): if True, for each random sample, print to the output stream the random value\n      instead of the loss.\n    random_generator (int): random generator function id.\n    file_in (str, optional): filename of input stream. Defaults to None.\n    file_out (str, optional): filename of output stream. Defaults to None.\n    ignore_correlation (bool): if True, do not compute correlated random samples.\n\nRaises:\n    ValueError: if alloc_rule is not 0, 1, or 2.\n\nReturns:\n    int: 0 if no errors occurred.",
      "content": "# File: oasislmf/pytools/gul/manager.py\n# function: run (lines 125-326)\n\ndef run(run_dir, ignore_file_type, sample_size, loss_threshold, alloc_rule, debug,\n        random_generator, peril_filter=[], file_in=None, file_out=None, ignore_correlation=False, **kwargs):\n    \"\"\"Execute the main gulpy worklow.\n\n    Args:\n        run_dir: (str) the directory of where the process is running\n        ignore_file_type set(str): file extension to ignore when loading\n        sample_size (int): number of random samples to draw.\n        loss_threshold (float): threshold above which losses are printed to the output stream.\n        alloc_rule (int): back-allocation rule.\n        debug (bool): if True, for each random sample, print to the output stream the random value\n          instead of the loss.\n        random_generator (int): random generator function id.\n        file_in (str, optional): filename of input stream. Defaults to None.\n        file_out (str, optional): filename of output stream. Defaults to None.\n        ignore_correlation (bool): if True, do not compute correlated random samples.\n\n    Raises:\n        ValueError: if alloc_rule is not 0, 1, or 2.\n\n    Returns:\n        int: 0 if no errors occurred.\n    \"\"\"\n    logger.info(\"starting gulpy\")\n\n    model_storage = get_storage_from_config_path(\n        os.path.join(run_dir, 'model_storage.json'),\n        os.path.join(run_dir, 'static'),\n    )\n    input_path = os.path.join(run_dir, 'input')\n    ignore_file_type = set(ignore_file_type)\n\n    damage_bins = get_damage_bins(model_storage)\n\n    # read coverages from file\n    coverages_tiv = read_coverages(input_path)\n\n    # load keys.csv to determine included AreaPerilID from peril_filter\n    if peril_filter:\n        keys_df = pd.read_csv(os.path.join(input_path, 'keys.csv'), dtype=Keys)\n        valid_area_peril_id = keys_df.loc[keys_df['PerilID'].isin(peril_filter), 'AreaPerilID'].to_numpy()\n        logger.debug(\n            f'Peril specific run: ({peril_filter}), {len(valid_area_peril_id)} AreaPerilID included out of {len(keys_df)}')\n    else:\n        valid_area_peril_id = None\n\n    # init the structure for computation\n    # coverages are numbered from 1, therefore we skip element 0 in `coverages`\n    coverages = np.zeros(coverages_tiv.shape[0] + 1, coverage_type)\n    coverages[1:]['tiv'] = coverages_tiv\n    del coverages_tiv\n\n    items = gul_get_items(input_path)\n\n    # in-place sort items in order to store them in item_map in the desired order\n    # currently numba only supports a simple call to np.sort() with no `order` keyword,\n    # so we do the sort here.\n    items = np.sort(items, order=['areaperil_id', 'vulnerability_id'])\n    item_map = generate_item_map(items, coverages)\n\n    # init array to store the coverages to be computed\n    # coverages are numebered from 1, therefore skip element 0.\n    compute = np.zeros(coverages.shape[0] + 1, items.dtype['coverage_id'])\n\n    with ExitStack() as stack:\n        # set up streams\n        if file_in is None:\n            streams_in = sys.stdin.buffer\n        else:\n            streams_in = stack.enter_context(open(file_in, 'rb'))\n\n        if file_out is None or file_out == '-':\n            stream_out = sys.stdout.buffer\n        else:\n            stream_out = stack.enter_context(open(file_out, 'wb'))\n\n        select_stream_list = [stream_out]\n\n        # prepare output buffer, write stream header\n        stream_out.write(stream_info_to_bytes(LOSS_STREAM_ID, ITEM_STREAM))\n\n\"\"\"Docstring (excerpt)\"\"\"\nExecute the main gulpy worklow.\n\nArgs:\n    run_dir: (str) the directory of where the process is running\n    ignore_file_type set(str): file extension to ignore when loading\n    sample_size (int): number of random samples to draw.\n    loss_threshold (float): threshold above which losses are printed to the output stream.\n    alloc_rule (int): back-allocation rule.\n    debug (bool): if True, for each random sample, print to the output stream the random value\n      instead of the loss.\n    random_generator (int): random generator function id.\n    file_in (str, optional): filename of input stream. Defaults to None.\n    file_out (str, optional): filename of output stream. Defaults to None.\n    ignore_correlation (bool): if True, do not compute correlated random samples.\n\nRaises:\n    ValueError: if alloc_rule is not 0, 1, or 2.\n\nReturns:\n    int: 0 if no errors occurred."
    },
    {
      "chunk_id": "oasislmf/pytools/gul/manager.py::compute_event_losses@330",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/manager.py",
      "symbol_type": "function",
      "name": "compute_event_losses",
      "lineno": 330,
      "end_lineno": 453,
      "business_stage": "gul",
      "docstring": "Compute losses for an event.\n\nArgs:\n    event_id (int32): event id.\n    coverages (numpy.array[oasis_float]): array with the coverage values for each coverage_id.\n    coverage_ids (numpy.array[int]): array of unique coverage ids used in this event.\n    items_data (numpy.array[items_data_type]): items-related data.\n    last_processed_coverage_ids_idx (int): index of the last coverage_id stored in `coverage_ids` that was fully processed\n      and printed to the output stream.\n    sample_size (int): number of random samples to draw.\n    recs (numpy.array[ProbMean]): all the cdfs used in event_id.\n    rec_idx_ptr (numpy.array[int]): array with the indices of `rec` where each cdf record starts.\n    damage_bins (List[Union[damagebindictionaryCsv, damagebindictionary]]): loaded data from the damage_bin_dict file.\n    loss_threshold (float): threshold above which losses are printed to the output stream.\n    losses (numpy.array[oasis_float]): array (to be re-used) to store losses for all item_ids.\n    alloc_rule (int): back-allocation rule.\n    do_correlation (bool): if True, compute correlated random samples.\n    rndms (numpy.array[float64]): 2d array of shape (number of seeds, sample_size) storing the random values\n      drawn for each seed.\n    debug (bool): if True, for each random sample, print to the output stream the random value\n      instead of the loss.\n    max_bytes_per_item (int): maximum bytes to be written in the output stream for an item.\n    byte_mv (numpy.array): byte view of where the output is buffered.\n    cursor (int): index of int32_mv where to start writing.\n\nReturns:\n    int, int: updated value of cursor, last last_processed_coverage_ids_idx",
      "content": "# File: oasislmf/pytools/gul/manager.py\n# function: compute_event_losses (lines 330-453)\n\ndef compute_event_losses(event_id, coverages, coverage_ids, items_data,\n                         last_processed_coverage_ids_idx, sample_size, recs, rec_idx_ptr, damage_bins,\n                         loss_threshold, losses, alloc_rule, do_correlation, rndms_base, eps_ij, corr_data_by_item_id,\n                         arr_min, arr_max, arr_N, norm_inv_cdf, arr_min_cdf, arr_max_cdf, norm_cdf,\n                         z_unif, debug, max_bytes_per_item, byte_mv, cursor):\n    \"\"\"Compute losses for an event.\n\n    Args:\n        event_id (int32): event id.\n        coverages (numpy.array[oasis_float]): array with the coverage values for each coverage_id.\n        coverage_ids (numpy.array[int]): array of unique coverage ids used in this event.\n        items_data (numpy.array[items_data_type]): items-related data.\n        last_processed_coverage_ids_idx (int): index of the last coverage_id stored in `coverage_ids` that was fully processed\n          and printed to the output stream.\n        sample_size (int): number of random samples to draw.\n        recs (numpy.array[ProbMean]): all the cdfs used in event_id.\n        rec_idx_ptr (numpy.array[int]): array with the indices of `rec` where each cdf record starts.\n        damage_bins (List[Union[damagebindictionaryCsv, damagebindictionary]]): loaded data from the damage_bin_dict file.\n        loss_threshold (float): threshold above which losses are printed to the output stream.\n        losses (numpy.array[oasis_float]): array (to be re-used) to store losses for all item_ids.\n        alloc_rule (int): back-allocation rule.\n        do_correlation (bool): if True, compute correlated random samples.\n        rndms (numpy.array[float64]): 2d array of shape (number of seeds, sample_size) storing the random values\n          drawn for each seed.\n        debug (bool): if True, for each random sample, print to the output stream the random value\n          instead of the loss.\n        max_bytes_per_item (int): maximum bytes to be written in the output stream for an item.\n        byte_mv (numpy.array): byte view of where the output is buffered.\n        cursor (int): index of int32_mv where to start writing.\n\n    Returns:\n        int, int: updated value of cursor, last last_processed_coverage_ids_idx\n    \"\"\"\n    for coverage_i in range(last_processed_coverage_ids_idx, coverage_ids.shape[0]):\n        coverage = coverages[coverage_ids[coverage_i]]\n        tiv = coverage['tiv']  # coverages are indexed from 1\n        Nitem_ids = coverage['cur_items']\n        exposureValue = tiv / Nitem_ids\n\n        # estimate max number of bytes needed to output this coverage\n        # conservatively assume all random samples are printed (losses>loss_threshold)\n        # number of records of type gulSampleslevelRec_size is sample_size + 5 (negative sidx) + 1 (terminator line)\n        est_cursor_bytes = Nitem_ids * max_bytes_per_item\n\n        # return before processing this coverage if the number of free bytes left in the buffer\n        # is not sufficient to write out the full coverage\n        if cursor + est_cursor_bytes > byte_mv.shape[0]:\n            return cursor, last_processed_coverage_ids_idx\n\n        items = items_data[coverage['start_items']: coverage['start_items'] + coverage['cur_items']]\n\n        for item_i in range(coverage['cur_items']):\n            item = items[item_i]\n            damagecdf_i = item['damagecdf_i']\n            rng_index = item['rng_index']\n            rec = recs[rec_idx_ptr[damagecdf_i]:rec_idx_ptr[damagecdf_i + 1]]\n            prob_to = rec['prob_to']\n            bin_mean = rec['bin_mean']\n            Nbins = len(prob_to)\n\n            # compute mean values\n            gul_mean, std_dev, chance_of_loss, max_loss = compute_mean_loss(\n                tiv, prob_to, bin_mean, Nbins, damage_bins[Nbins - 1]['bin_to'],\n            )\n\n            losses[MAX_LOSS_IDX, item_i] = max_loss\n            losses[CHANCE_OF_LOSS_IDX, item_i] = chance_of_loss\n            losses[TIV_IDX, item_i] = exposureValue\n            losses[STD_DEV_IDX, item_i] = std_dev\n            losses[MEAN_IDX, item_i] = gul_mean\n\n            if sample_size > 0:\n                if do_correlation and corr_data_by_item_id[item['item_id']]['damage_correlation_value'] > 0:\n                    item_corr_data = corr_data_by_item_id[item['item_id']]\n                    get_corr_rval(\n                        eps_ij[item_corr_data['peril_correlation_group']], rndms_base[rng_index],\n                        item_corr_data['damage_correlation_value'], arr_min, arr_max, arr_N, norm_inv_cdf,\n                        arr_min_cdf, arr_max_cdf, norm_cdf, sample_size, z_unif\n                    )\n                    rndms = z_unif\n\n\"\"\"Docstring (excerpt)\"\"\"\nCompute losses for an event.\n\nArgs:\n    event_id (int32): event id.\n    coverages (numpy.array[oasis_float]): array with the coverage values for each coverage_id.\n    coverage_ids (numpy.array[int]): array of unique coverage ids used in this event.\n    items_data (numpy.array[items_data_type]): items-related data.\n    last_processed_coverage_ids_idx (int): index of the last coverage_id stored in `coverage_ids` that was fully processed\n      and printed to the output stream.\n    sample_size (int): number of random samples to draw.\n    recs (numpy.array[ProbMean]): all the cdfs used in event_id.\n    rec_idx_ptr (numpy.array[int]): array with the indices of `rec` where each cdf record starts.\n    damage_bins (List[Union[damagebindictionaryCsv, damagebindictionary]]): loaded data from the damage_bin_dict file.\n    loss_threshold (float): threshold above which losses are printed to the output stream.\n    losses (numpy.array[oasis_float]): array (to be re-used) to store losses for all item_ids.\n    alloc_rule (int): back-allocation rule.\n    do_correlation (bool): if True, compute correlated random samples.\n    rndms (numpy.array[float64]): 2d array of shape (number of seeds, sample_size) storing the random values\n      drawn for each seed.\n    debug (bool): if True, for each random sample, print to the output stream the random value\n      instead of the loss.\n    max_bytes_per_item (int): maximum bytes to be written in the output stream for an item.\n    byte_mv (numpy.array): byte view of where the output is buffered.\n    cursor (int): index of int32_mv where to start writing.\n\nReturns:\n    int, int: updated value of cursor, last last_processed_coverage_ids_idx"
    },
    {
      "chunk_id": "oasislmf/pytools/gul/manager.py::write_losses@457",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/manager.py",
      "symbol_type": "function",
      "name": "write_losses",
      "lineno": 457,
      "end_lineno": 514,
      "business_stage": "gul",
      "docstring": "Write the computed losses.\n\nArgs:\n    event_id (int32): event id.\n    sample_size (int): number of random samples to draw.\n    loss_threshold (float): threshold above which losses are printed to the output stream.\n    losses (numpy.array[oasis_float]): losses for all item_ids\n    item_ids (numpy.array[ITEM_ID_TYPE]): ids of items whose losses are in `losses`.\n    alloc_rule (int): back-allocation rule.\n    tiv (oasis_float): total insured value.\n    byte_mv (numpy.ndarray): byte view of where the output is buffered.\n    cursor (int): index of int32_mv where to start writing.\n\nReturns:\n    int: updated values of cursor",
      "content": "# File: oasislmf/pytools/gul/manager.py\n# function: write_losses (lines 457-514)\n\ndef write_losses(event_id, sample_size, loss_threshold, losses, item_ids, alloc_rule, tiv,\n                 byte_mv, cursor):\n    \"\"\"Write the computed losses.\n\n    Args:\n        event_id (int32): event id.\n        sample_size (int): number of random samples to draw.\n        loss_threshold (float): threshold above which losses are printed to the output stream.\n        losses (numpy.array[oasis_float]): losses for all item_ids\n        item_ids (numpy.array[ITEM_ID_TYPE]): ids of items whose losses are in `losses`.\n        alloc_rule (int): back-allocation rule.\n        tiv (oasis_float): total insured value.\n        byte_mv (numpy.ndarray): byte view of where the output is buffered.\n        cursor (int): index of int32_mv where to start writing.\n\n    Returns:\n        int: updated values of cursor\n    \"\"\"\n    if alloc_rule == 2:\n        setmaxloss(losses)\n\n    if tiv > 0:\n        # check whether the sum of losses-per-sample exceeds TIV\n        # if so, split TIV in proportion to the losses\n\n        if alloc_rule in [1, 2]:\n            split_tiv_classic(losses[TIV_IDX], tiv)\n            split_tiv_classic(losses[MAX_LOSS_IDX], tiv)\n            split_tiv_classic(losses[MEAN_IDX], tiv)\n            for sample_i in range(1, losses.shape[0] - NUM_IDX):\n                split_tiv_classic(losses[sample_i], tiv)\n\n        elif alloc_rule == 3:\n            split_tiv_multiplicative(losses[TIV_IDX], tiv)\n            split_tiv_multiplicative(losses[MAX_LOSS_IDX], tiv)\n            split_tiv_multiplicative(losses[MEAN_IDX], tiv)\n            for sample_i in range(1, losses.shape[0] - NUM_IDX):\n                split_tiv_multiplicative(losses[sample_i], tiv)\n\n    # output the losses for all the items\n    for item_j in range(item_ids.shape[0]):\n\n        # write header\n        cursor = mv_write_item_header(byte_mv, cursor, event_id, item_ids[item_j])\n\n        # write negative sidx\n        for sample_idx in SPECIAL_SIDX:\n            cursor = mv_write_sidx_loss(byte_mv, cursor, sample_idx, losses[sample_idx, item_j])\n\n        # write the random samples (only those with losses above the threshold)\n        for sample_idx in range(1, sample_size + 1):\n            if losses[sample_idx, item_j] >= loss_threshold:\n                cursor = mv_write_sidx_loss(byte_mv, cursor, sample_idx, losses[sample_idx, item_j])\n\n        # write terminator for the samples for this item\n        cursor = mv_write_delimiter(byte_mv, cursor)\n\n    return cursor\n\n\"\"\"Docstring (excerpt)\"\"\"\nWrite the computed losses.\n\nArgs:\n    event_id (int32): event id.\n    sample_size (int): number of random samples to draw.\n    loss_threshold (float): threshold above which losses are printed to the output stream.\n    losses (numpy.array[oasis_float]): losses for all item_ids\n    item_ids (numpy.array[ITEM_ID_TYPE]): ids of items whose losses are in `losses`.\n    alloc_rule (int): back-allocation rule.\n    tiv (oasis_float): total insured value.\n    byte_mv (numpy.ndarray): byte view of where the output is buffered.\n    cursor (int): index of int32_mv where to start writing.\n\nReturns:\n    int: updated values of cursor"
    },
    {
      "chunk_id": "oasislmf/pytools/gul/random.py::generate_hash@26",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/random.py",
      "symbol_type": "function",
      "name": "generate_hash",
      "lineno": 26,
      "end_lineno": 40,
      "business_stage": "gul",
      "docstring": "Generate hash for a given `group_id`, `event_id` pair for the vulnerability pdf.\n\nArgs:\n    group_id (int): group id.\n    event_id (int]): event id.\n    base_seed (int, optional): base random seed. Defaults to 0.\n\nReturns:\n    int64: hash",
      "content": "# File: oasislmf/pytools/gul/random.py\n# function: generate_hash (lines 26-40)\n\ndef generate_hash(group_id, event_id, base_seed=0):\n    \"\"\"Generate hash for a given `group_id`, `event_id` pair for the vulnerability pdf.\n\n    Args:\n        group_id (int): group id.\n        event_id (int]): event id.\n        base_seed (int, optional): base random seed. Defaults to 0.\n\n    Returns:\n        int64: hash\n    \"\"\"\n    hash = (base_seed + (group_id * GROUP_ID_HASH_CODE) % HASH_MOD_CODE +\n            (event_id * EVENT_ID_HASH_CODE) % HASH_MOD_CODE) % HASH_MOD_CODE\n\n    return hash\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate hash for a given `group_id`, `event_id` pair for the vulnerability pdf.\n\nArgs:\n    group_id (int): group id.\n    event_id (int]): event id.\n    base_seed (int, optional): base random seed. Defaults to 0.\n\nReturns:\n    int64: hash"
    },
    {
      "chunk_id": "oasislmf/pytools/gul/random.py::generate_hash_hazard@44",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/random.py",
      "symbol_type": "function",
      "name": "generate_hash_hazard",
      "lineno": 44,
      "end_lineno": 58,
      "business_stage": "gul",
      "docstring": "Generate hash for a given `hazard_group_id`, `event_id` pair for the hazard pdf.\n\nArgs:\n    hazard_group_id (int): group id.\n    event_id (int]): event id.\n    base_seed (int, optional): base random seed. Defaults to 0.\n\nReturns:\n    int64: hash",
      "content": "# File: oasislmf/pytools/gul/random.py\n# function: generate_hash_hazard (lines 44-58)\n\ndef generate_hash_hazard(hazard_group_id, event_id, base_seed=0):\n    \"\"\"Generate hash for a given `hazard_group_id`, `event_id` pair for the hazard pdf.\n\n    Args:\n        hazard_group_id (int): group id.\n        event_id (int]): event id.\n        base_seed (int, optional): base random seed. Defaults to 0.\n\n    Returns:\n        int64: hash\n    \"\"\"\n    hash = (base_seed + (hazard_group_id * HAZARD_GROUP_ID_HASH_CODE) % HAZARD_HASH_MOD_CODE +\n            (event_id * HAZARD_EVENT_ID_HASH_CODE) % HAZARD_HASH_MOD_CODE) % HAZARD_HASH_MOD_CODE\n\n    return hash\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate hash for a given `hazard_group_id`, `event_id` pair for the hazard pdf.\n\nArgs:\n    hazard_group_id (int): group id.\n    event_id (int]): event id.\n    base_seed (int, optional): base random seed. Defaults to 0.\n\nReturns:\n    int64: hash"
    },
    {
      "chunk_id": "oasislmf/pytools/gul/random.py::get_random_generator@61",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/random.py",
      "symbol_type": "function",
      "name": "get_random_generator",
      "lineno": 61,
      "end_lineno": 81,
      "business_stage": "gul",
      "docstring": "Get the random generator function.\n\nArgs:\n    random_generator (int): random generator function id.\n\nReturns:\n    The random generator function.",
      "content": "# File: oasislmf/pytools/gul/random.py\n# function: get_random_generator (lines 61-81)\n\ndef get_random_generator(random_generator):\n    \"\"\"Get the random generator function.\n\n    Args:\n        random_generator (int): random generator function id.\n\n    Returns:\n        The random generator function.\n\n    \"\"\"\n    # define random generator function\n    if random_generator == 0:\n        logger.info(\"Random generator: MersenneTwister\")\n        return random_MersenneTwister\n\n    elif random_generator == 1:\n        logger.info(\"Random generator: Latin Hypercube\")\n        return random_LatinHypercube\n\n    else:\n        raise ValueError(f\"No random generator exists for random_generator={random_generator}.\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet the random generator function.\n\nArgs:\n    random_generator (int): random generator function id.\n\nReturns:\n    The random generator function."
    },
    {
      "chunk_id": "oasislmf/pytools/gul/random.py::generate_correlated_hash_vector@90",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/random.py",
      "symbol_type": "function",
      "name": "generate_correlated_hash_vector",
      "lineno": 90,
      "end_lineno": 110,
      "business_stage": "gul",
      "docstring": "Generate hashes for all peril correlation groups for a given `event_id`.\n\nArgs:\n    unique_peril_correlation_groups (List[int]): list of the unique peril correlation groups.\n    event_id (int): event id.\n    base_seed (int, optional): base random seed. Defaults to 0.\n    correlated_hashes: empty buffer for the output (size of max group id not the number of group id",
      "content": "# File: oasislmf/pytools/gul/random.py\n# function: generate_correlated_hash_vector (lines 90-110)\n\ndef generate_correlated_hash_vector(unique_peril_correlation_groups, event_id, correlated_hashes, base_seed=0):\n    \"\"\"Generate hashes for all peril correlation groups for a given `event_id`.\n\n    Args:\n        unique_peril_correlation_groups (List[int]): list of the unique peril correlation groups.\n        event_id (int): event id.\n        base_seed (int, optional): base random seed. Defaults to 0.\n        correlated_hashes: empty buffer for the output (size of max group id not the number of group id\n    \"\"\"\n    unique_peril_index = 0\n    unique_peril_len = unique_peril_correlation_groups.shape[0]\n    for i in range(1, correlated_hashes.shape[0]):\n        if unique_peril_correlation_groups[unique_peril_index] == i:\n            correlated_hashes[i] = (\n                base_seed +\n                (unique_peril_correlation_groups[unique_peril_index] * PERIL_CORRELATION_GROUP_HASH) % HASH_MOD_CODE +\n                (event_id * EVENT_ID_HASH_CODE) % HASH_MOD_CODE\n            ) % HASH_MOD_CODE\n            unique_peril_index += 1\n            if unique_peril_index == unique_peril_len:\n                break\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate hashes for all peril correlation groups for a given `event_id`.\n\nArgs:\n    unique_peril_correlation_groups (List[int]): list of the unique peril correlation groups.\n    event_id (int): event id.\n    base_seed (int, optional): base random seed. Defaults to 0.\n    correlated_hashes: empty buffer for the output (size of max group id not the number of group id"
    },
    {
      "chunk_id": "oasislmf/pytools/gul/random.py::get_corr_rval_float@141",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/random.py",
      "symbol_type": "function",
      "name": "get_corr_rval_float",
      "lineno": 141,
      "end_lineno": 156,
      "business_stage": "gul",
      "docstring": "this calculate the new correlated values like in get_corr_rval but with precomputed inv_factor and norm_factor\ninv_factor = (N - 1) // (x_max - x_min)\nnorm_factor = (N - 1) // (cdf_max - cdf_min)",
      "content": "# File: oasislmf/pytools/gul/random.py\n# function: get_corr_rval_float (lines 141-156)\n\ndef get_corr_rval_float(x_unif, y_unif, rho, x_min, norm_inv_cdf, inv_factor, cdf_min,\n                        norm_cdf, norm_factor, Nsamples, z_unif):\n    \"\"\"\n    this calculate the new correlated values like in get_corr_rval but with precomputed inv_factor and norm_factor\n    inv_factor = (N - 1) // (x_max - x_min)\n    norm_factor = (N - 1) // (cdf_max - cdf_min)\n    \"\"\"\n    sqrt_rho = sqrt(rho)\n    sqrt_1_minus_rho = sqrt(1. - rho)\n\n    for i in range(Nsamples):\n        x_norm = norm_inv_cdf[int((x_unif[i] - x_min) * inv_factor)]\n        y_norm = norm_inv_cdf[int((y_unif[i] - x_min) * inv_factor)]\n        z_norm = sqrt_rho * x_norm + sqrt_1_minus_rho * y_norm\n\n        z_unif[i] = norm_cdf[int((z_norm - cdf_min) * norm_factor)]\n\n\"\"\"Docstring (excerpt)\"\"\"\nthis calculate the new correlated values like in get_corr_rval but with precomputed inv_factor and norm_factor\ninv_factor = (N - 1) // (x_max - x_min)\nnorm_factor = (N - 1) // (cdf_max - cdf_min)"
    },
    {
      "chunk_id": "oasislmf/pytools/gul/random.py::random_MersenneTwister@160",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/random.py",
      "symbol_type": "function",
      "name": "random_MersenneTwister",
      "lineno": 160,
      "end_lineno": 189,
      "business_stage": "gul",
      "docstring": "Generate random numbers using the default Mersenne Twister algorithm.\n\nArgs:\n    seeds (List[int64]): List of seeds.\n    n (int): number of random samples to generate for each seed.\n    skip_seeds (int): number of seeds to skip starting from the beginning\n      of the `seeds` array. For skipped seeds no random numbers are generated\n      and the output rndms will contain zeros at their corresponding row.\n      Default is 0, i.e. no seeds are skipped.\n\nReturns:\n    rndms (array[float]): 2-d array of shape (number of seeds, n) \n      containing the random values generated for each seed.\n    rndms_idx (Dict[int64, int]): mapping between `seed` and the \n      row in rndms that stores the corresponding random values.",
      "content": "# File: oasislmf/pytools/gul/random.py\n# function: random_MersenneTwister (lines 160-189)\n\ndef random_MersenneTwister(seeds, n, skip_seeds=0):\n    \"\"\"Generate random numbers using the default Mersenne Twister algorithm.\n\n    Args:\n        seeds (List[int64]): List of seeds.\n        n (int): number of random samples to generate for each seed.\n        skip_seeds (int): number of seeds to skip starting from the beginning\n          of the `seeds` array. For skipped seeds no random numbers are generated\n          and the output rndms will contain zeros at their corresponding row.\n          Default is 0, i.e. no seeds are skipped.\n\n    Returns:\n        rndms (array[float]): 2-d array of shape (number of seeds, n) \n          containing the random values generated for each seed.\n        rndms_idx (Dict[int64, int]): mapping between `seed` and the \n          row in rndms that stores the corresponding random values.\n    \"\"\"\n    Nseeds = len(seeds)\n    rndms = np.zeros((Nseeds, n), dtype='float64')\n\n    for seed_i in range(skip_seeds, Nseeds, 1):\n        # set the seed\n        np.random.seed(seeds[seed_i])\n\n        # draw the random numbers\n        for j in range(n):\n            # by default in numba this should be Mersenne-Twister\n            rndms[seed_i, j] = np.random.uniform(0., 1.)\n\n    return rndms\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate random numbers using the default Mersenne Twister algorithm.\n\nArgs:\n    seeds (List[int64]): List of seeds.\n    n (int): number of random samples to generate for each seed.\n    skip_seeds (int): number of seeds to skip starting from the beginning\n      of the `seeds` array. For skipped seeds no random numbers are generated\n      and the output rndms will contain zeros at their corresponding row.\n      Default is 0, i.e. no seeds are skipped.\n\nReturns:\n    rndms (array[float]): 2-d array of shape (number of seeds, n) \n      containing the random values generated for each seed.\n    rndms_idx (Dict[int64, int]): mapping between `seed` and the \n      row in rndms that stores the corresponding random values."
    },
    {
      "chunk_id": "oasislmf/pytools/gul/random.py::random_LatinHypercube@193",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/random.py",
      "symbol_type": "function",
      "name": "random_LatinHypercube",
      "lineno": 193,
      "end_lineno": 237,
      "business_stage": "gul",
      "docstring": "Generate random numbers using the Latin Hypercube algorithm.\n\nArgs:\n    seeds (List[int64]): List of seeds.\n    n (int): number of random samples to generate for each seed.\n\nReturns:\n    rndms (array[float]): 2-d array of shape (number of seeds, n) \n      containing the random values generated for each seed.\n    rndms_idx (Dict[int64, int]): mapping between `seed` and the \n      row in rndms that stores the corresponding random values.\n    skip_seeds (int): number of seeds to skip starting from the beginning\n      of the `seeds` array. For skipped seeds no random numbers are generated\n      and the output rndms will contain zeros at their corresponding row.\n      Default is 0, i.e. no seeds are skipped.\n\nNotes:\n    Implementation follows scipy.stats.qmc.LatinHypercube v1.8.0.\n    Following scipy notation, here we assume `centered=False` all the times:\n    instead of taking `samples=0.5*np.ones(n)`, here we always\n    draw uniform random samples in order to initialise `samples`.",
      "content": "# File: oasislmf/pytools/gul/random.py\n# function: random_LatinHypercube (lines 193-237)\n\ndef random_LatinHypercube(seeds, n, skip_seeds=0):\n    \"\"\"Generate random numbers using the Latin Hypercube algorithm.\n\n    Args:\n        seeds (List[int64]): List of seeds.\n        n (int): number of random samples to generate for each seed.\n\n    Returns:\n        rndms (array[float]): 2-d array of shape (number of seeds, n) \n          containing the random values generated for each seed.\n        rndms_idx (Dict[int64, int]): mapping between `seed` and the \n          row in rndms that stores the corresponding random values.\n        skip_seeds (int): number of seeds to skip starting from the beginning\n          of the `seeds` array. For skipped seeds no random numbers are generated\n          and the output rndms will contain zeros at their corresponding row.\n          Default is 0, i.e. no seeds are skipped.\n\n    Notes:\n        Implementation follows scipy.stats.qmc.LatinHypercube v1.8.0.\n        Following scipy notation, here we assume `centered=False` all the times:\n        instead of taking `samples=0.5*np.ones(n)`, here we always\n        draw uniform random samples in order to initialise `samples`.\n    \"\"\"\n    Nseeds = len(seeds)\n    rndms = np.zeros((Nseeds, n), dtype='float64')\n    # define arrays here and re-use them later\n    samples = np.zeros(n, dtype='float64')\n    perms = np.zeros(n, dtype='float64')\n\n    for seed_i in range(skip_seeds, Nseeds, 1):\n        # set the seed\n        np.random.seed(seeds[seed_i])\n\n        # draw the random numbers and re-generate permutations array\n        for i in range(n):\n            samples[i] = np.random.uniform(0., 1.)\n            perms[i] = i + 1\n\n        # in-place shuffle permutations\n        np.random.shuffle(perms)\n\n        for j in range(n):\n            rndms[seed_i, j] = (perms[j] - samples[j]) / float(n)\n\n    return rndms\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate random numbers using the Latin Hypercube algorithm.\n\nArgs:\n    seeds (List[int64]): List of seeds.\n    n (int): number of random samples to generate for each seed.\n\nReturns:\n    rndms (array[float]): 2-d array of shape (number of seeds, n) \n      containing the random values generated for each seed.\n    rndms_idx (Dict[int64, int]): mapping between `seed` and the \n      row in rndms that stores the corresponding random values.\n    skip_seeds (int): number of seeds to skip starting from the beginning\n      of the `seeds` array. For skipped seeds no random numbers are generated\n      and the output rndms will contain zeros at their corresponding row.\n      Default is 0, i.e. no seeds are skipped.\n\nNotes:\n    Implementation follows scipy.stats.qmc.LatinHypercube v1.8.0.\n    Following scipy notation, here we assume `centered=False` all the times:\n    instead of taking `samples=0.5*np.ones(n)`, here we always\n    draw uniform random samples in order to initialise `samples`."
    },
    {
      "chunk_id": "oasislmf/pytools/gul/utils.py::binary_search@10",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/utils.py",
      "symbol_type": "function",
      "name": "binary_search",
      "lineno": 10,
      "end_lineno": 36,
      "business_stage": "gul",
      "docstring": "Assuming `array` is a sorted array (increasing order), find the bin\nwhere where `array` gets larger than `value`.\n\n    Args:\n        value (float,int): the value to be searched\n    array (float, int): the array where `value` is to be searched.\n    n (int): number of elements of `array` where `value` is to be searched, starting\n      from the beginning of `array`.\n\nReturns:\n    int: the first index of `array` where `array` is larger than `value`.\n      Returns -1 if this condition never occurs.",
      "content": "# File: oasislmf/pytools/gul/utils.py\n# function: binary_search (lines 10-36)\n\ndef binary_search(value, array, n):\n    \"\"\"\n    Assuming `array` is a sorted array (increasing order), find the bin\n    where where `array` gets larger than `value`.\n\n        Args:\n            value (float,int): the value to be searched\n        array (float, int): the array where `value` is to be searched.\n        n (int): number of elements of `array` where `value` is to be searched, starting\n          from the beginning of `array`.\n\n    Returns:\n        int: the first index of `array` where `array` is larger than `value`.\n          Returns -1 if this condition never occurs.\n\n    \"\"\"\n    lo = 0\n    hi = n\n    while lo < hi:\n        mid = (lo + hi) >> 1  # divide by two\n\n        if array[mid] < value:\n            lo = mid + 1\n        else:\n            hi = mid\n\n    return lo\n\n\"\"\"Docstring (excerpt)\"\"\"\nAssuming `array` is a sorted array (increasing order), find the bin\nwhere where `array` gets larger than `value`.\n\n    Args:\n        value (float,int): the value to be searched\n    array (float, int): the array where `value` is to be searched.\n    n (int): number of elements of `array` where `value` is to be searched, starting\n      from the beginning of `array`.\n\nReturns:\n    int: the first index of `array` where `array` is larger than `value`.\n      Returns -1 if this condition never occurs."
    },
    {
      "chunk_id": "oasislmf/pytools/gul/utils.py::append_to_dict_value@40",
      "source_type": "code",
      "path": "oasislmf/pytools/gul/utils.py",
      "symbol_type": "function",
      "name": "append_to_dict_value",
      "lineno": 40,
      "end_lineno": 67,
      "business_stage": "gul",
      "docstring": "Append a value to the list populating a dictionary value.\nIf the key is not present in the dictionary, populate the entry with a list with\njust the passed value.\nThe dictionary `d` is modified *in-place*, thus it is not returned by the function.\nIf `d` is a dictionary and `d[key]` is a list, this function appends\n`value` to the list. Example: if d = {0: [1, 2], 1: [3]}, then:\n\n   append_to_dict_entry(d, 0, 3, int)\n\nwill modify `d` to:\n\n   d = {0: [1, 2, 3], 1: [3]}\n\nDesigned to be used with numba.typed.Dict and numba.typed.List.\n\nArgs:\n    d (numba.typed.Dict[*,numba.typedList[value_type]]): dictionary to be modified,\n      by appending `value` to the list in d[key].\n    key (same as d.key_type): key of the element to modify.\n    value (value_type): value to be appended to the list in d[key].\n    value_type (built-in Python or numba type): value data type.",
      "content": "# File: oasislmf/pytools/gul/utils.py\n# function: append_to_dict_value (lines 40-67)\n\ndef append_to_dict_value(d, key, value, value_type):\n    \"\"\"Append a value to the list populating a dictionary value.\n    If the key is not present in the dictionary, populate the entry with a list with\n    just the passed value.\n    The dictionary `d` is modified *in-place*, thus it is not returned by the function.\n    If `d` is a dictionary and `d[key]` is a list, this function appends\n    `value` to the list. Example: if d = {0: [1, 2], 1: [3]}, then:\n\n       append_to_dict_entry(d, 0, 3, int)\n\n    will modify `d` to:\n\n       d = {0: [1, 2, 3], 1: [3]}\n\n    Designed to be used with numba.typed.Dict and numba.typed.List.\n\n    Args:\n        d (numba.typed.Dict[*,numba.typedList[value_type]]): dictionary to be modified,\n          by appending `value` to the list in d[key].\n        key (same as d.key_type): key of the element to modify.\n        value (value_type): value to be appended to the list in d[key].\n        value_type (built-in Python or numba type): value data type.\n    \"\"\"\n    def_lst = List.empty_list(value_type)\n    d.setdefault(key, def_lst)\n    lst = d[key]\n    lst.append(value)\n    d[key] = lst\n\n\"\"\"Docstring (excerpt)\"\"\"\nAppend a value to the list populating a dictionary value.\nIf the key is not present in the dictionary, populate the entry with a list with\njust the passed value.\nThe dictionary `d` is modified *in-place*, thus it is not returned by the function.\nIf `d` is a dictionary and `d[key]` is a list, this function appends\n`value` to the list. Example: if d = {0: [1, 2], 1: [3]}, then:\n\n   append_to_dict_entry(d, 0, 3, int)\n\nwill modify `d` to:\n\n   d = {0: [1, 2, 3], 1: [3]}\n\nDesigned to be used with numba.typed.Dict and numba.typed.List.\n\nArgs:\n    d (numba.typed.Dict[*,numba.typedList[value_type]]): dictionary to be modified,\n      by appending `value` to the list in d[key].\n    key (same as d.key_type): key of the element to modify.\n    value (value_type): value to be appended to the list in d[key].\n    value_type (built-in Python or numba type): value data type."
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/aggregate.py::gen_empty_agg_vuln_to_vuln_ids@24",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/aggregate.py",
      "symbol_type": "function",
      "name": "gen_empty_agg_vuln_to_vuln_ids",
      "lineno": 24,
      "end_lineno": 30,
      "business_stage": "gul",
      "docstring": "Generate empty map to store the definitions of aggregate vulnerability functions.\n\nReturns:\n    dict[int, list[int]]: map of aggregate vulnerability id to list of vulnerability ids.",
      "content": "# File: oasislmf/pytools/gulmc/aggregate.py\n# function: gen_empty_agg_vuln_to_vuln_ids (lines 24-30)\n\ndef gen_empty_agg_vuln_to_vuln_ids():\n    \"\"\"Generate empty map to store the definitions of aggregate vulnerability functions.\n\n    Returns:\n        dict[int, list[int]]: map of aggregate vulnerability id to list of vulnerability ids.\n    \"\"\"\n    return Dict.empty(nb_int32, List.empty_list(nb_int32))\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate empty map to store the definitions of aggregate vulnerability functions.\n\nReturns:\n    dict[int, list[int]]: map of aggregate vulnerability id to list of vulnerability ids."
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/aggregate.py::gen_empty_areaperil_vuln_idx_to_weights@34",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/aggregate.py",
      "symbol_type": "function",
      "name": "gen_empty_areaperil_vuln_idx_to_weights",
      "lineno": 34,
      "end_lineno": 40,
      "business_stage": "gul",
      "docstring": "Generate empty map to store the weights of individual vulnerability functions in each aggregate vulnerability.\n\nReturns:\n    dict[AGG_VULN_WEIGHTS_KEY_TYPE, AGG_VULN_WEIGHTS_VAL_TYPE]: map of areaperil_id, vulnerability id to weight.",
      "content": "# File: oasislmf/pytools/gulmc/aggregate.py\n# function: gen_empty_areaperil_vuln_idx_to_weights (lines 34-40)\n\ndef gen_empty_areaperil_vuln_idx_to_weights():\n    \"\"\"Generate empty map to store the weights of individual vulnerability functions in each aggregate vulnerability.\n\n    Returns:\n        dict[AGG_VULN_WEIGHTS_KEY_TYPE, AGG_VULN_WEIGHTS_VAL_TYPE]: map of areaperil_id, vulnerability id to weight.\n    \"\"\"\n    return Dict.empty(AGG_VULN_WEIGHTS_KEY_TYPE, AGG_VULN_WEIGHTS_VAL_TYPE)\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate empty map to store the weights of individual vulnerability functions in each aggregate vulnerability.\n\nReturns:\n    dict[AGG_VULN_WEIGHTS_KEY_TYPE, AGG_VULN_WEIGHTS_VAL_TYPE]: map of areaperil_id, vulnerability id to weight."
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/aggregate.py::read_aggregate_vulnerability@43",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/aggregate.py",
      "symbol_type": "function",
      "name": "read_aggregate_vulnerability",
      "lineno": 43,
      "end_lineno": 70,
      "business_stage": "gul",
      "docstring": "Load the aggregate vulnerability definitions from file.\n\nArgs:\n    storage: (BaseStorage) the storage manager for fetching model data\n    ignore_file_type (Set[str]): file extension to ignore when loading.\n\nReturns:\n    np.array[AggregateVulnerability]: aggregate vulnerability table.",
      "content": "# File: oasislmf/pytools/gulmc/aggregate.py\n# function: read_aggregate_vulnerability (lines 43-70)\n\ndef read_aggregate_vulnerability(storage: BaseStorage, ignore_file_type=set()):\n    \"\"\"Load the aggregate vulnerability definitions from file.\n\n    Args:\n        storage: (BaseStorage) the storage manager for fetching model data\n        ignore_file_type (Set[str]): file extension to ignore when loading.\n\n    Returns:\n        np.array[AggregateVulnerability]: aggregate vulnerability table.\n    \"\"\"\n    input_files = set(storage.listdir())\n\n    if \"aggregate_vulnerability.bin\" in input_files and \"bin\" not in ignore_file_type:\n        logger.debug(f\"loading {storage.get_storage_url('aggregate_vulnerability.bin', encode_params=False)}\")\n        with storage.open('aggregate_vulnerability.bin') as f:\n            aggregate_vulnerability = np.memmap(f, dtype=aggregatevulnerability_dtype, mode='r')\n\n    elif \"aggregate_vulnerability.csv\" in input_files and \"csv\" not in ignore_file_type:\n        logger.debug(f\"loading {storage.get_storage_url('aggregate_vulnerability.csv', encode_params=False)}\")\n        with storage.open('aggregate_vulnerability.csv') as f:\n            aggregate_vulnerability = np.loadtxt(f, dtype=aggregatevulnerability_dtype, delimiter=\",\", skiprows=1, ndmin=1)\n\n    else:\n        aggregate_vulnerability = None\n        logging.warning(\n            f\"Aggregate vulnerability table not found at {storage.get_storage_url('', encode_params=False)[0]}. Continuing without aggregate vulnerability definitions.\")\n\n    return aggregate_vulnerability\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoad the aggregate vulnerability definitions from file.\n\nArgs:\n    storage: (BaseStorage) the storage manager for fetching model data\n    ignore_file_type (Set[str]): file extension to ignore when loading.\n\nReturns:\n    np.array[AggregateVulnerability]: aggregate vulnerability table."
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/aggregate.py::read_vulnerability_weights@73",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/aggregate.py",
      "symbol_type": "function",
      "name": "read_vulnerability_weights",
      "lineno": 73,
      "end_lineno": 100,
      "business_stage": "gul",
      "docstring": "Load the vulnerability weights definitions from file.\n\nArgs:\n    storage: (BaseStorage) the storage manager for fetching model data\n    ignore_file_type (Set[str]): file extension to ignore when loading.\n\nReturns:\n    np.array[VulnerabilityWeight]: vulnerability weights table.",
      "content": "# File: oasislmf/pytools/gulmc/aggregate.py\n# function: read_vulnerability_weights (lines 73-100)\n\ndef read_vulnerability_weights(storage: BaseStorage, ignore_file_type=set()):\n    \"\"\"Load the vulnerability weights definitions from file.\n\n    Args:\n        storage: (BaseStorage) the storage manager for fetching model data\n        ignore_file_type (Set[str]): file extension to ignore when loading.\n\n    Returns:\n        np.array[VulnerabilityWeight]: vulnerability weights table.\n    \"\"\"\n    input_files = set(storage.listdir())\n\n    if \"weights.bin\" in input_files and \"bin\" not in ignore_file_type:\n        logger.debug(f\"loading {storage.get_storage_url('weights.bin', encode_params=False)}\")\n        with storage.open(\"weights.bin\") as f:\n            aggregate_weights = np.memmap(f, dtype=vulnerability_weight_dtype, mode='r')\n\n    elif \"weights.csv\" in input_files and \"csv\" not in ignore_file_type:\n        logger.debug(f\"loading {storage.get_storage_url('weights.csv', encode_params=False)}\")\n        with storage.open(\"weights.csv\") as f:\n            aggregate_weights = np.loadtxt(f, dtype=vulnerability_weight_dtype, delimiter=\",\", skiprows=1, ndmin=1)\n\n    else:\n        aggregate_weights = None\n        logging.warning(\n            f\"Vulnerability weights not found at {storage.get_storage_url('', encode_params=False)[0]}. Continuing without vulnerability weights definitions.\")\n\n    return aggregate_weights\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoad the vulnerability weights definitions from file.\n\nArgs:\n    storage: (BaseStorage) the storage manager for fetching model data\n    ignore_file_type (Set[str]): file extension to ignore when loading.\n\nReturns:\n    np.array[VulnerabilityWeight]: vulnerability weights table."
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/aggregate.py::process_aggregate_vulnerability@103",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/aggregate.py",
      "symbol_type": "function",
      "name": "process_aggregate_vulnerability",
      "lineno": 103,
      "end_lineno": 133,
      "business_stage": "gul",
      "docstring": "Rearrange aggregate vulnerability definitions from tabular format to a map between aggregate\nvulnerability id and the list of vulnerability ids that it is made of.\n\nArgs:\n    aggregate_vulnerability (np.array[AggregateVulnerability]): aggregate vulnerability table.\n\nReturns:\n    dict[int, list[int]]: map of aggregate vulnerability id to list of vulnerability ids.",
      "content": "# File: oasislmf/pytools/gulmc/aggregate.py\n# function: process_aggregate_vulnerability (lines 103-133)\n\ndef process_aggregate_vulnerability(aggregate_vulnerability):\n    \"\"\"Rearrange aggregate vulnerability definitions from tabular format to a map between aggregate\n    vulnerability id and the list of vulnerability ids that it is made of.\n\n    Args:\n        aggregate_vulnerability (np.array[AggregateVulnerability]): aggregate vulnerability table.\n\n    Returns:\n        dict[int, list[int]]: map of aggregate vulnerability id to list of vulnerability ids.\n    \"\"\"\n    agg_vuln_to_vuln_ids = gen_empty_agg_vuln_to_vuln_ids()\n\n    if aggregate_vulnerability is not None:\n\n        agg_vuln_df = pd.DataFrame(aggregate_vulnerability)\n        # init agg_vuln_to_vuln_ids to allow numba to compile later functions\n        # vulnerability_id and aggregate_vulnerability_id are remapped to the internal ids\n        # using the vulnd_dict map that contains only the vulnerability_id used in this portfolio.\n\n        # here we read all aggregate vulnerability_id, then, after processing the items file,\n        # we will filter out the aggregate vulnerability that are not used in this portfolio.\n        for agg, grp in agg_vuln_df.groupby('aggregate_vulnerability_id'):\n            agg_vuln_id = nb_int32(agg)\n\n            if agg_vuln_id not in agg_vuln_to_vuln_ids:\n                agg_vuln_to_vuln_ids[agg_vuln_id] = List.empty_list(nb_int32)\n\n            for entry in grp['vulnerability_id'].to_list():\n                agg_vuln_to_vuln_ids[agg_vuln_id].append(nb_int32(entry))\n\n    return agg_vuln_to_vuln_ids\n\n\"\"\"Docstring (excerpt)\"\"\"\nRearrange aggregate vulnerability definitions from tabular format to a map between aggregate\nvulnerability id and the list of vulnerability ids that it is made of.\n\nArgs:\n    aggregate_vulnerability (np.array[AggregateVulnerability]): aggregate vulnerability table.\n\nReturns:\n    dict[int, list[int]]: map of aggregate vulnerability id to list of vulnerability ids."
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/aggregate.py::process_vulnerability_weights@137",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/aggregate.py",
      "symbol_type": "function",
      "name": "process_vulnerability_weights",
      "lineno": 137,
      "end_lineno": 151,
      "business_stage": "gul",
      "docstring": "Polpulate the useful (areaperil_id, vulnerability_i) in areaperil_vuln_i_to_weight with the weight from aggregate_weights\n\nArgs:\n    areaperil_vuln_i_to_weight: dict of useful (areaperil_id, vulnerability_i) to 0. (weight placeholder to be updated)\n    vuln_dict: vuln_dict (Tuple[Dict[int, int]): vulnerability dictionary, vuln_id => vuln_i.\n    aggregate_weights (np.array[VulnerabilityWeight]): vulnerability weights table.",
      "content": "# File: oasislmf/pytools/gulmc/aggregate.py\n# function: process_vulnerability_weights (lines 137-151)\n\ndef process_vulnerability_weights(areaperil_vuln_i_to_weight, vuln_dict, aggregate_weights):\n    \"\"\"\n    Polpulate the useful (areaperil_id, vulnerability_i) in areaperil_vuln_i_to_weight with the weight from aggregate_weights\n\n    Args:\n        areaperil_vuln_i_to_weight: dict of useful (areaperil_id, vulnerability_i) to 0. (weight placeholder to be updated)\n        vuln_dict: vuln_dict (Tuple[Dict[int, int]): vulnerability dictionary, vuln_id => vuln_i.\n        aggregate_weights (np.array[VulnerabilityWeight]): vulnerability weights table.\n    \"\"\"\n    for i in range(len(aggregate_weights)):\n        rec = aggregate_weights[i]\n        if rec['vulnerability_id'] in vuln_dict:\n            key = (nb_areaperil_int(rec['areaperil_id']), vuln_dict[rec['vulnerability_id']])\n            if key in areaperil_vuln_i_to_weight:\n                areaperil_vuln_i_to_weight[key] = nb_oasis_float(rec['weight'])\n\n\"\"\"Docstring (excerpt)\"\"\"\nPolpulate the useful (areaperil_id, vulnerability_i) in areaperil_vuln_i_to_weight with the weight from aggregate_weights\n\nArgs:\n    areaperil_vuln_i_to_weight: dict of useful (areaperil_id, vulnerability_i) to 0. (weight placeholder to be updated)\n    vuln_dict: vuln_dict (Tuple[Dict[int, int]): vulnerability dictionary, vuln_id => vuln_i.\n    aggregate_weights (np.array[VulnerabilityWeight]): vulnerability weights table."
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/items.py::read_items@22",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/items.py",
      "symbol_type": "function",
      "name": "read_items",
      "lineno": 22,
      "end_lineno": 49,
      "business_stage": "gul",
      "docstring": "Load the items from the items file.\n\nArgs:\n    input_path (str): the path pointing to the file\n    ignore_file_type (Set[str]): file extension to ignore when loading.\n\nReturns:\n    Tuple[Dict[int, int], List[int], Dict[int, int], List[Tuple[int, int]], List[int]]\n      vulnerability dictionary, vulnerability IDs, areaperil to vulnerability index dictionary,\n      areaperil ID to vulnerability index array, areaperil ID to vulnerability array",
      "content": "# File: oasislmf/pytools/gulmc/items.py\n# function: read_items (lines 22-49)\n\ndef read_items(input_path, ignore_file_type=set(), dynamic_footprint=False, legacy=False):\n    \"\"\"Load the items from the items file.\n\n    Args:\n        input_path (str): the path pointing to the file\n        ignore_file_type (Set[str]): file extension to ignore when loading.\n\n    Returns:\n        Tuple[Dict[int, int], List[int], Dict[int, int], List[Tuple[int, int]], List[int]]\n          vulnerability dictionary, vulnerability IDs, areaperil to vulnerability index dictionary,\n          areaperil ID to vulnerability index array, areaperil ID to vulnerability array\n    \"\"\"\n    input_files = set(os.listdir(input_path))\n\n    if \"items.bin\" in input_files and \"bin\" not in ignore_file_type:\n        items_fname = os.path.join(input_path, 'items.bin')\n        logger.debug(f\"loading {items_fname}\")\n        items = np.memmap(items_fname, dtype=items_dtype, mode='r')\n\n    elif \"items.csv\" in input_files and \"csv\" not in ignore_file_type:\n        items_fname = os.path.join(input_path, 'items.csv')\n        logger.debug(f\"loading {items_fname}\")\n        items = np.loadtxt(items_fname, dtype=items_dtype, delimiter=\",\", skiprows=1, ndmin=1)\n\n    else:\n        raise FileNotFoundError(f'items file not found at {input_path}')\n\n    return items\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoad the items from the items file.\n\nArgs:\n    input_path (str): the path pointing to the file\n    ignore_file_type (Set[str]): file extension to ignore when loading.\n\nReturns:\n    Tuple[Dict[int, int], List[int], Dict[int, int], List[Tuple[int, int]], List[int]]\n      vulnerability dictionary, vulnerability IDs, areaperil to vulnerability index dictionary,\n      areaperil ID to vulnerability index array, areaperil ID to vulnerability array"
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/items.py::generate_item_map@53",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/items.py",
      "symbol_type": "function",
      "name": "generate_item_map",
      "lineno": 53,
      "end_lineno": 118,
      "business_stage": "gul",
      "docstring": "Generate item_map; requires items to be sorted.\n\nArgs:\n    items (numpy.ndarray[int32, int32, int32]): 1-d structured array storing\n        `item_id`, `coverage_id`, `group_id` for all items.\n        items need to be sorted by increasing areaperil_id, vulnerability_id\n        in order to output the items in correct order.\n    coverages (numpy.ndarray): coverage id to information on items\n    valid_areaperil_id (numpy.ndarray[int32]): list of non-filtered area_peril_id (None is no filter)\n    agg_vuln_to_vulns (dict[int, list[int]]): map of aggregate vulnerability id to list of vulnerability ids.\n\nReturns:\n    item_map (Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]): dict storing\n        the mapping between areaperil_id, vulnerability_id to item.\n    areaperil_ids_map (Dict[int, Dict[int, int]]) dict storing the mapping between each\n        areaperil_id and all the vulnerability ids associated with it.\n    vuln id to vuln idx for each vulnerability in each areaperil, list of all used vulnerability ids.\n    agg_vuln_to_vuln_idxs dict[int, list[int]]: map between aggregate vulnerability id and the list of indices where the individual vulnerability_ids\n      that compose it are stored in `vuln_array`.\n    areaperil_vuln_idx_to_weight dict[AGG_VULN_WEIGHTS_KEY_TYPE, AGG_VULN_WEIGHTS_VAL_TYPE]: map between the areaperil id and the index where the vulnerability function\n      is stored in `vuln_array` and the vulnerability weight.",
      "content": "# File: oasislmf/pytools/gulmc/items.py\n# function: generate_item_map (lines 53-118)\n\ndef generate_item_map(items, coverages, valid_areaperil_id, agg_vuln_to_vulns):\n    \"\"\"Generate item_map; requires items to be sorted.\n\n    Args:\n        items (numpy.ndarray[int32, int32, int32]): 1-d structured array storing\n            `item_id`, `coverage_id`, `group_id` for all items.\n            items need to be sorted by increasing areaperil_id, vulnerability_id\n            in order to output the items in correct order.\n        coverages (numpy.ndarray): coverage id to information on items\n        valid_areaperil_id (numpy.ndarray[int32]): list of non-filtered area_peril_id (None is no filter)\n        agg_vuln_to_vulns (dict[int, list[int]]): map of aggregate vulnerability id to list of vulnerability ids.\n\n    Returns:\n        item_map (Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]): dict storing\n            the mapping between areaperil_id, vulnerability_id to item.\n        areaperil_ids_map (Dict[int, Dict[int, int]]) dict storing the mapping between each\n            areaperil_id and all the vulnerability ids associated with it.\n        vuln id to vuln idx for each vulnerability in each areaperil, list of all used vulnerability ids.\n        agg_vuln_to_vuln_idxs dict[int, list[int]]: map between aggregate vulnerability id and the list of indices where the individual vulnerability_ids\n          that compose it are stored in `vuln_array`.\n        areaperil_vuln_idx_to_weight dict[AGG_VULN_WEIGHTS_KEY_TYPE, AGG_VULN_WEIGHTS_VAL_TYPE]: map between the areaperil id and the index where the vulnerability function\n          is stored in `vuln_array` and the vulnerability weight.\n\n    \"\"\"\n    item_map = Dict.empty(ITEM_MAP_KEY_TYPE, List.empty_list(ITEM_MAP_VALUE_TYPE))\n    areaperil_ids_map = Dict.empty(nb_areaperil_int, Dict.empty(nb_int32, nb_int8))\n    vuln_dict = Dict()\n    vuln_idx = 0\n    areaperil_vuln_idx_to_weight = gen_empty_areaperil_vuln_idx_to_weights()\n\n    agg_vuln_to_vuln_idxs = Dict.empty(nb_int32, List.empty_list(nb_int32))\n\n    for j, item in enumerate(items):\n        areaperil_id = item['areaperil_id']\n        vulnerability_id = item['vulnerability_id']\n\n        if valid_areaperil_id is not None and item['areaperil_id'] not in valid_areaperil_id:\n            continue\n\n        append_to_dict_value(item_map, tuple((areaperil_id, vulnerability_id)), j, ITEM_MAP_VALUE_TYPE)\n        coverages[item['coverage_id']]['max_items'] += 1\n\n        # Populate vuln_dict, to map all used vuln_id to vuln_i\n        if item['vulnerability_id'] not in vuln_dict:\n            if item['vulnerability_id'] in agg_vuln_to_vulns:  # vulnerability is an aggregate\n                for sub_vuln_id in agg_vuln_to_vulns[item['vulnerability_id']]:\n                    if sub_vuln_id not in vuln_dict:\n                        vuln_dict[sub_vuln_id] = nb_oasis_int(vuln_idx)\n                        vuln_idx += 1\n                agg_vuln_to_vuln_idxs[item['vulnerability_id']] = List([vuln_dict[vuln] for vuln in agg_vuln_to_vulns[item['vulnerability_id']]])\n\n            else:  # single vulnerability\n                vuln_dict[item['vulnerability_id']] = nb_oasis_int(vuln_idx)\n                vuln_idx += 1\n\n        if item['vulnerability_id'] in agg_vuln_to_vulns:\n            for sub_vuln_id in agg_vuln_to_vulns[item['vulnerability_id']]:\n                areaperil_vuln_idx_to_weight[(nb_areaperil_int(areaperil_id), vuln_dict[sub_vuln_id])] = nb_oasis_float(0)\n        else:\n            item['vulnerability_idx'] = vuln_dict[item['vulnerability_id']]\n\n        if areaperil_id not in areaperil_ids_map:\n            areaperil_ids_map[areaperil_id] = Dict.empty(nb_int32, nb_int8)\n        areaperil_ids_map[areaperil_id][vulnerability_id] = 0\n\n    return item_map, areaperil_ids_map, vuln_dict, agg_vuln_to_vuln_idxs, areaperil_vuln_idx_to_weight\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate item_map; requires items to be sorted.\n\nArgs:\n    items (numpy.ndarray[int32, int32, int32]): 1-d structured array storing\n        `item_id`, `coverage_id`, `group_id` for all items.\n        items need to be sorted by increasing areaperil_id, vulnerability_id\n        in order to output the items in correct order.\n    coverages (numpy.ndarray): coverage id to information on items\n    valid_areaperil_id (numpy.ndarray[int32]): list of non-filtered area_peril_id (None is no filter)\n    agg_vuln_to_vulns (dict[int, list[int]]): map of aggregate vulnerability id to list of vulnerability ids.\n\nReturns:\n    item_map (Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]): dict storing\n        the mapping between areaperil_id, vulnerability_id to item.\n    areaperil_ids_map (Dict[int, Dict[int, int]]) dict storing the mapping between each\n        areaperil_id and all the vulnerability ids associated with it.\n    vuln id to vuln idx for each vulnerability in each areaperil, list of all used vulnerability ids.\n    agg_vuln_to_vuln_idxs dict[int, list[int]]: map between aggregate vulnerability id and the list of indices where the individual vulnerability_ids\n      that compose it are stored in `vuln_array`.\n    areaperil_vuln_idx_to_weight dict[AGG_VULN_WEIGHTS_KEY_TYPE, AGG_VULN_WEIGHTS_VAL_TYPE]: map between the areaperil id and the index where the vulnerability function\n      is stored in `vuln_array` and the vulnerability weight."
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/manager.py::gen_empty_vuln_cdf_lookup@70",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/manager.py",
      "symbol_type": "function",
      "name": "gen_empty_vuln_cdf_lookup",
      "lineno": 70,
      "end_lineno": 87,
      "business_stage": "gul",
      "docstring": "Generate structures needed to store and retrieve vulnerability cdf in the cache.\n\nArgs:\n    list_size (int): maximum number of cdfs to be stored in the cache.\n\nReturns:\n    cached_vuln_cdf_lookup (Dict[VULN_LOOKUP_KEY_TYPE, VULN_LOOKUP_VALUE_TYPE]): dict to store\n      the map between vuln_id and intensity bin id and the location of the cdf in the cache.\n    cached_vuln_cdf_lookup_keys (List[VULN_LOOKUP_VALUE_TYPE]): list of lookup keys.",
      "content": "# File: oasislmf/pytools/gulmc/manager.py\n# function: gen_empty_vuln_cdf_lookup (lines 70-87)\n\ndef gen_empty_vuln_cdf_lookup(list_size, compute_info):\n    \"\"\"Generate structures needed to store and retrieve vulnerability cdf in the cache.\n\n    Args:\n        list_size (int): maximum number of cdfs to be stored in the cache.\n\n    Returns:\n        cached_vuln_cdf_lookup (Dict[VULN_LOOKUP_KEY_TYPE, VULN_LOOKUP_VALUE_TYPE]): dict to store\n          the map between vuln_id and intensity bin id and the location of the cdf in the cache.\n        cached_vuln_cdf_lookup_keys (List[VULN_LOOKUP_VALUE_TYPE]): list of lookup keys.\n    \"\"\"\n    cached_vuln_cdf_lookup = Dict.empty(VULN_LOOKUP_KEY_TYPE, VULN_LOOKUP_VALUE_TYPE)\n    cached_vuln_cdf_lookup_keys = List.empty_list(VULN_LOOKUP_KEY_TYPE)\n    dummy = tuple((nb_int32(-1), nb_areaperil_int(0), nb_int32(-1), nb_int32(-1), nb_int32(-1)))\n    for _ in range(list_size):\n        cached_vuln_cdf_lookup_keys.append(dummy)\n    compute_info['next_cached_vuln_cdf_i'] = 0\n    return cached_vuln_cdf_lookup, cached_vuln_cdf_lookup_keys\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate structures needed to store and retrieve vulnerability cdf in the cache.\n\nArgs:\n    list_size (int): maximum number of cdfs to be stored in the cache.\n\nReturns:\n    cached_vuln_cdf_lookup (Dict[VULN_LOOKUP_KEY_TYPE, VULN_LOOKUP_VALUE_TYPE]): dict to store\n      the map between vuln_id and intensity bin id and the location of the cdf in the cache.\n    cached_vuln_cdf_lookup_keys (List[VULN_LOOKUP_VALUE_TYPE]): list of lookup keys."
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/manager.py::get_dynamic_footprint_adjustments@90",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/manager.py",
      "symbol_type": "function",
      "name": "get_dynamic_footprint_adjustments",
      "lineno": 90,
      "end_lineno": 107,
      "business_stage": "gul",
      "docstring": "Generate intensity adjustment array for dynamic footprint models.\n\nArgs:\n    input_path (str): location of the generated adjustments file.\n\nReturns:\n    numpy array with itemid and adjustment factors",
      "content": "# File: oasislmf/pytools/gulmc/manager.py\n# function: get_dynamic_footprint_adjustments (lines 90-107)\n\ndef get_dynamic_footprint_adjustments(input_path):\n    \"\"\"Generate intensity adjustment array for dynamic footprint models.\n\n    Args:\n        input_path (str): location of the generated adjustments file.\n\n    Returns:\n        numpy array with itemid and adjustment factors\n    \"\"\"\n    adjustments_fn = os.path.join(input_path, 'item_adjustments.csv')\n    if os.path.isfile(adjustments_fn):\n        adjustments_tb = np.loadtxt(adjustments_fn, dtype=ItemAdjustment, delimiter=\",\", skiprows=1, ndmin=1)\n    else:\n        items_fp = os.path.join(input_path, 'items.csv')\n        items_tb = np.loadtxt(items_fp, dtype=items_dtype, delimiter=\",\", skiprows=1, ndmin=1)\n        adjustments_tb = np.array([(i[0], 0, 0) for i in items_tb], dtype=ItemAdjustment)\n\n    return adjustments_tb\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate intensity adjustment array for dynamic footprint models.\n\nArgs:\n    input_path (str): location of the generated adjustments file.\n\nReturns:\n    numpy array with itemid and adjustment factors"
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/manager.py::get_peril_id@110",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/manager.py",
      "symbol_type": "function",
      "name": "get_peril_id",
      "lineno": 110,
      "end_lineno": 139,
      "business_stage": "gul",
      "docstring": "Get peril_id associated with item_id\n\nArgs:\n    input_path (str): The directory path where the 'gul_summary_map.csv' file is located.\n\nReturns:\n    np.ndarray: A structured NumPy array with the following fields:\n        - 'item_id' (oasis_int): The item ID as an integer.\n        - 'peril_id' (oasis_int): The encoded peril ID as an integer.",
      "content": "# File: oasislmf/pytools/gulmc/manager.py\n# function: get_peril_id (lines 110-139)\n\ndef get_peril_id(input_path):\n    \"\"\"\n    Get peril_id associated with item_id\n\n    Args:\n        input_path (str): The directory path where the 'gul_summary_map.csv' file is located.\n\n    Returns:\n        np.ndarray: A structured NumPy array with the following fields:\n            - 'item_id' (oasis_int): The item ID as an integer.\n            - 'peril_id' (oasis_int): The encoded peril ID as an integer.\n    \"\"\"\n\n    dtype = np.dtype([\n        ('item_id', oasis_int),\n        ('peril_id', oasis_int)\n    ])\n\n    item_peril = pd.read_csv(\n        os.path.join(input_path, 'gul_summary_map.csv'),\n        usecols=['item_id', 'peril_id']\n    )[['item_id', 'peril_id']]\n\n    item_peril['peril_id'] = item_peril['peril_id'].apply(encode_peril_id)\n\n    item_peril = np.array(\n        list(item_peril.itertuples(index=False, name=None)),\n        dtype=dtype)\n\n    return item_peril\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet peril_id associated with item_id\n\nArgs:\n    input_path (str): The directory path where the 'gul_summary_map.csv' file is located.\n\nReturns:\n    np.ndarray: A structured NumPy array with the following fields:\n        - 'item_id' (oasis_int): The item ID as an integer.\n        - 'peril_id' (oasis_int): The encoded peril ID as an integer."
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/manager.py::get_vuln_rngadj@142",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/manager.py",
      "symbol_type": "function",
      "name": "get_vuln_rngadj",
      "lineno": 142,
      "end_lineno": 167,
      "business_stage": "gul",
      "docstring": "Loads vulnerability adjustments from the analysis settings file.\n\nArgs:\n    run_dir (str): path to the run directory (used to load the analysis settings)\n\nReturns: (Dict[nb_int32, nb_float64]) vulnerability adjustments dictionary",
      "content": "# File: oasislmf/pytools/gulmc/manager.py\n# function: get_vuln_rngadj (lines 142-167)\n\ndef get_vuln_rngadj(run_dir, vuln_dict):\n    \"\"\"\n    Loads vulnerability adjustments from the analysis settings file.\n\n    Args:\n        run_dir (str): path to the run directory (used to load the analysis settings)\n\n    Returns: (Dict[nb_int32, nb_float64]) vulnerability adjustments dictionary\n    \"\"\"\n    settings_path = os.path.join(run_dir, \"analysis_settings.json\")\n    vuln_adj = np.ones(len(vuln_dict), dtype=oasis_float)\n    if not os.path.exists(settings_path):\n        logger.debug(f\"analysis_settings.json not found in {run_dir}.\")\n        return vuln_adj\n    vulnerability_adjustments_field = analysis_settings_loader(settings_path).get('vulnerability_adjustments', None)\n    if vulnerability_adjustments_field is not None:\n        adjustments = vulnerability_adjustments_field.get('adjustments', None)\n    else:\n        adjustments = None\n    if adjustments is None:\n        logger.debug(f\"vulnerability_adjustments not found in {settings_path}.\")\n        return vuln_adj\n    for key, value in adjustments.items():\n        if nb_int32(key) in vuln_dict.keys():\n            vuln_adj[vuln_dict[nb_int32(key)]] = nb_oasis_float(value)\n    return vuln_adj\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoads vulnerability adjustments from the analysis settings file.\n\nArgs:\n    run_dir (str): path to the run directory (used to load the analysis settings)\n\nReturns: (Dict[nb_int32, nb_float64]) vulnerability adjustments dictionary"
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/manager.py::run@171",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/manager.py",
      "symbol_type": "function",
      "name": "run",
      "lineno": 171,
      "end_lineno": 594,
      "business_stage": "gul",
      "docstring": "Execute the main gulmc workflow.\n\nArgs:\n    run_dir (str): the directory of where the process is running\n    ignore_file_type set(str): file extension to ignore when loading\n    sample_size (int): number of random samples to draw.\n    loss_threshold (float): threshold above which losses are printed to the output stream.\n    alloc_rule (int): back-allocation rule.\n    debug (int): for each random sample, print to the output stream the random loss (if 0), the random value used to draw\n      the hazard intensity sample (if 1), the random value used to draw the damage sample (if 2). Defaults to 0.\n    random_generator (int): random generator function id.\n    peril_filter (list[int], optional): list of perils to include in the computation (if None, all perils will be included). Defaults to [].\n    file_in (str, optional): filename of input stream. Defaults to None.\n    file_out (str, optional): filename of output stream. Defaults to None.\n    data_server (bool, optional): if True, run the data server. Defaults to None.\n    ignore_correlation (bool, optional): if True, do not compute correlated random samples. Defaults to False.\n    effective_damageability (bool, optional): if True, it uses effective damageability to draw damage samples instead of\n      using the full monte carlo approach (i.e., to draw hazard intensity first, then damage).\n    max_cached_vuln_cdf_size_MB (int, optional): size in MB of the in-memory cache to store and reuse vulnerability cdf. Defaults to 200.\n    model_df_engine: (str) The engine to use when loading model dataframes\nRaises:\n    ValueError: if alloc_rule is not 0, 1, 2, or 3.\n    ValueError: if alloc_rule is 1, 2, or 3 when debug is 1 or 2.\n\nReturns:\n    int: 0 if no errors occurred.",
      "content": "# File: oasislmf/pytools/gulmc/manager.py\n# function: run (lines 171-594)\n\ndef run(run_dir,\n        ignore_file_type,\n        sample_size,\n        loss_threshold,\n        alloc_rule,\n        debug,\n        random_generator,\n        peril_filter=[],\n        file_in=None,\n        file_out=None,\n        data_server=None,\n        ignore_correlation=False,\n        ignore_haz_correlation=False,\n        effective_damageability=False,\n        max_cached_vuln_cdf_size_MB=200,\n        model_df_engine=\"oasis_data_manager.df_reader.reader.OasisPandasReader\",\n        dynamic_footprint=False,\n        **kwargs):\n    \"\"\"Execute the main gulmc workflow.\n\n    Args:\n        run_dir (str): the directory of where the process is running\n        ignore_file_type set(str): file extension to ignore when loading\n        sample_size (int): number of random samples to draw.\n        loss_threshold (float): threshold above which losses are printed to the output stream.\n        alloc_rule (int): back-allocation rule.\n        debug (int): for each random sample, print to the output stream the random loss (if 0), the random value used to draw\n          the hazard intensity sample (if 1), the random value used to draw the damage sample (if 2). Defaults to 0.\n        random_generator (int): random generator function id.\n        peril_filter (list[int], optional): list of perils to include in the computation (if None, all perils will be included). Defaults to [].\n        file_in (str, optional): filename of input stream. Defaults to None.\n        file_out (str, optional): filename of output stream. Defaults to None.\n        data_server (bool, optional): if True, run the data server. Defaults to None.\n        ignore_correlation (bool, optional): if True, do not compute correlated random samples. Defaults to False.\n        effective_damageability (bool, optional): if True, it uses effective damageability to draw damage samples instead of\n          using the full monte carlo approach (i.e., to draw hazard intensity first, then damage).\n        max_cached_vuln_cdf_size_MB (int, optional): size in MB of the in-memory cache to store and reuse vulnerability cdf. Defaults to 200.\n        model_df_engine: (str) The engine to use when loading model dataframes\n    Raises:\n        ValueError: if alloc_rule is not 0, 1, 2, or 3.\n        ValueError: if alloc_rule is 1, 2, or 3 when debug is 1 or 2.\n\n    Returns:\n        int: 0 if no errors occurred.\n    \"\"\"\n    logger.info(\"starting gulmc\")\n\n    model_storage = get_storage_from_config_path(\n        os.path.join(run_dir, 'model_storage.json'),\n        os.path.join(run_dir, 'static'),\n    )\n    input_path = os.path.join(run_dir, 'input')\n    ignore_file_type = set(ignore_file_type)\n\n    if alloc_rule not in [0, 1, 2, 3]:\n        raise ValueError(f\"Expect alloc_rule to be 0, 1, 2, or 3, got {alloc_rule}\")\n\n    if debug > 0 and alloc_rule != 0:\n        raise ValueError(f\"Expect alloc_rule to be 0 if debug is 1 or 2, got {alloc_rule}\")\n\n    if data_server:\n        logger.debug(\"data server active\")\n        FootprintLayerClient.register()\n        logger.debug(\"registered with data server\")\n        atexit.register(FootprintLayerClient.unregister)\n    else:\n        logger.debug(\"data server not active\")\n\n    with ExitStack() as stack:\n        if file_in is None:\n            streams_in = sys.stdin.buffer\n        else:\n            streams_in = stack.enter_context(open(file_in, 'rb'))\n\n        event_id_mv = memoryview(bytearray(4))\n        event_ids = np.ndarray(1, buffer=event_id_mv, dtype='i4')\n\n        # load keys.csv to determine included AreaPerilID from peril_filter\n        if os.path.exists(os.path.join(input_path, 'keys.csv')):\n            keys_df = pd.read_csv(os.path.join(input_path, 'keys.csv'), dtype=Keys)\n\n\"\"\"Docstring (excerpt)\"\"\"\nExecute the main gulmc workflow.\n\nArgs:\n    run_dir (str): the directory of where the process is running\n    ignore_file_type set(str): file extension to ignore when loading\n    sample_size (int): number of random samples to draw.\n    loss_threshold (float): threshold above which losses are printed to the output stream.\n    alloc_rule (int): back-allocation rule.\n    debug (int): for each random sample, print to the output stream the random loss (if 0), the random value used to draw\n      the hazard intensity sample (if 1), the random value used to draw the damage sample (if 2). Defaults to 0.\n    random_generator (int): random generator function id.\n    peril_filter (list[int], optional): list of perils to include in the computation (if None, all perils will be included). Defaults to [].\n    file_in (str, optional): filename of input stream. Defaults to None.\n    file_out (str, optional): filename of output stream. Defaults to None.\n    data_server (bool, optional): if True, run the data server. Defaults to None.\n    ignore_correlation (bool, optional): if True, do not compute correlated random samples. Defaults to False.\n    effective_damageability (bool, optional): if True, it uses effective damageability to draw damage samples instead of\n      using the full monte carlo approach (i.e., to draw hazard intensity first, then damage).\n    max_cached_vuln_cdf_size_MB (int, optional): size in MB of the in-memory cache to store and reuse vulnerability cdf. Defaults to 200.\n    model_df_engine: (str) The engine to use when loading model dataframes\nRaises:\n    ValueError: if alloc_rule is not 0, 1, 2, or 3.\n    ValueError: if alloc_rule is 1, 2, or 3 when debug is 1 or 2.\n\nReturns:\n    int: 0 if no errors occurred."
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/manager.py::get_last_non_empty@621",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/manager.py",
      "symbol_type": "function",
      "name": "get_last_non_empty",
      "lineno": 621,
      "end_lineno": 634,
      "business_stage": "gul",
      "docstring": "remove empty bucket from the end\nArgs:\n    cdf: cumulative distribution\n    bin_i: last valid bin index\n\nReturns:\n    last bin index with an increased in the cdf",
      "content": "# File: oasislmf/pytools/gulmc/manager.py\n# function: get_last_non_empty (lines 621-634)\n\ndef get_last_non_empty(cdf, bin_i):\n    \"\"\"\n    remove empty bucket from the end\n    Args:\n        cdf: cumulative distribution\n        bin_i: last valid bin index\n\n    Returns:\n        last bin index with an increased in the cdf\n    \"\"\"\n    last_prob = cdf[bin_i]\n    while bin_i > 0 and cdf[bin_i - 1] == last_prob:\n        bin_i -= 1\n    return bin_i\n\n\"\"\"Docstring (excerpt)\"\"\"\nremove empty bucket from the end\nArgs:\n    cdf: cumulative distribution\n    bin_i: last valid bin index\n\nReturns:\n    last bin index with an increased in the cdf"
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/manager.py::pdf_to_cdf@638",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/manager.py",
      "symbol_type": "function",
      "name": "pdf_to_cdf",
      "lineno": 638,
      "end_lineno": 656,
      "business_stage": "gul",
      "docstring": "return the cumulative distribution from the probality distribution\nArgs:\n    pdf (np.array[float]): probality distribution\n    empty_cdf (np.array[float]): cumulative distribution buffer for output\nReturns:\n     cdf (np.array[float]): here we return only the valid part if needed",
      "content": "# File: oasislmf/pytools/gulmc/manager.py\n# function: pdf_to_cdf (lines 638-656)\n\ndef pdf_to_cdf(pdf, empty_cdf):\n    \"\"\"\n    return the cumulative distribution from the probality distribution\n    Args:\n        pdf (np.array[float]): probality distribution\n        empty_cdf (np.array[float]): cumulative distribution buffer for output\n    Returns:\n         cdf (np.array[float]): here we return only the valid part if needed\n    \"\"\"\n    cumsum = 0\n    i = 0\n    while i < pdf.shape[0]:\n        cumsum += pdf[i]\n        empty_cdf[i] = cumsum\n        i += 1\n        if cumsum >= 0.999999940:\n            break\n    i = get_last_non_empty(empty_cdf, i - 1)\n    return empty_cdf[: i + 1]\n\n\"\"\"Docstring (excerpt)\"\"\"\nreturn the cumulative distribution from the probality distribution\nArgs:\n    pdf (np.array[float]): probality distribution\n    empty_cdf (np.array[float]): cumulative distribution buffer for output\nReturns:\n     cdf (np.array[float]): here we return only the valid part if needed"
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/manager.py::calc_eff_damage_cdf@660",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/manager.py",
      "symbol_type": "function",
      "name": "calc_eff_damage_cdf",
      "lineno": 660,
      "end_lineno": 681,
      "business_stage": "gul",
      "docstring": "calculate the covoluted cumulative distribution between vulnerability damage and hazard probability distribution\nArgs:\n    vuln_pdf (np.array[float]) : vulnerability damage probability distribution\n    haz_pdf (np.array[float]): hazard probability distribution\n    eff_damage_cdf_empty (np.array[float]): output buffer\nReturns:\n    eff_damage_cdf (np.array[float]): cdf is stored in eff_damage_cdf_empty, here we return only the valid part if needed",
      "content": "# File: oasislmf/pytools/gulmc/manager.py\n# function: calc_eff_damage_cdf (lines 660-681)\n\ndef calc_eff_damage_cdf(vuln_pdf, haz_pdf, eff_damage_cdf_empty):\n    \"\"\"\n    calculate the covoluted cumulative distribution between vulnerability damage and hazard probability distribution\n    Args:\n        vuln_pdf (np.array[float]) : vulnerability damage probability distribution\n        haz_pdf (np.array[float]): hazard probability distribution\n        eff_damage_cdf_empty (np.array[float]): output buffer\n    Returns:\n        eff_damage_cdf (np.array[float]): cdf is stored in eff_damage_cdf_empty, here we return only the valid part if needed\n    \"\"\"\n    eff_damage_cdf_cumsum = 0.\n    damage_bin_i = 0\n    while damage_bin_i < vuln_pdf.shape[1]:\n        for haz_i in range(vuln_pdf.shape[0]):\n            eff_damage_cdf_cumsum += vuln_pdf[haz_i, damage_bin_i] * haz_pdf[haz_i]\n\n        eff_damage_cdf_empty[damage_bin_i] = eff_damage_cdf_cumsum\n        damage_bin_i += 1\n        if eff_damage_cdf_cumsum >= 0.999999940:\n            break\n    damage_bin_i = get_last_non_empty(eff_damage_cdf_empty, damage_bin_i - 1)\n    return eff_damage_cdf_empty[:damage_bin_i + 1]\n\n\"\"\"Docstring (excerpt)\"\"\"\ncalculate the covoluted cumulative distribution between vulnerability damage and hazard probability distribution\nArgs:\n    vuln_pdf (np.array[float]) : vulnerability damage probability distribution\n    haz_pdf (np.array[float]): hazard probability distribution\n    eff_damage_cdf_empty (np.array[float]): output buffer\nReturns:\n    eff_damage_cdf (np.array[float]): cdf is stored in eff_damage_cdf_empty, here we return only the valid part if needed"
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/manager.py::compute_event_losses@718",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/manager.py",
      "symbol_type": "function",
      "name": "compute_event_losses",
      "lineno": 718,
      "end_lineno": 1037,
      "business_stage": "gul",
      "docstring": "Compute losses for an event.\n\nArgs:\n    compute_info (ndarray): information on the state of the computation\n    coverages (numpy.array[oasis_float]): array with the coverage values for each coverage_id.\n    coverage_ids (numpy.array[int]): array of unique coverage ids used in this event.\n    items_event_data (numpy.array[items_data_type]): items-related data.\n    items (np.ndarray): items table merged with correlation parameters.\n    sample_size (int): number of random samples to draw.\n    haz_pdf (np.array[oasis_float]): hazard intensity cdf.\n    haz_arr_ptr (np.array[int]): array with the indices where each cdf record starts in `haz_cdf`.\n    vuln_array (np.array[float]): damage pdf for different vulnerability functions, as a function of hazard intensity.\n    damage_bins (List[Union[damagebindictionaryCsv, damagebindictionary]]): loaded data from the damage_bin_dict file.\n    cached_vuln_cdf_lookup (Dict[VULN_LOOKUP_KEY_TYPE, VULN_LOOKUP_VALUE_TYPE]): dict to store\n      the map between vuln_id and intensity bin id and the location of the cdf in the cache.\n    cached_vuln_cdf_lookup_keys (List[VULN_LOOKUP_VALUE_TYPE]): list of lookup keys.\n    cached_vuln_cdfs (np.array[oasis_float]): vulnerability cdf cache.\n    agg_vuln_to_vuln_idxs (dict[int, list[int]]): map between aggregate vulnerability id and the list of indices where the individual vulnerability_ids\n      that compose it are stored in `vuln_array`.\n    areaperil_vuln_idx_to_weight (dict[AGG_VULN_WEIGHTS_KEY_TYPE, AGG_VULN_WEIGHTS_VAL_TYPE]): map between the areaperil id and the index where the vulnerability function\n      is stored in `vuln_array` and the vulnerability weight.\n    losses (numpy.array[oasis_float]): array (to be re-used) to store losses for each item.\n    haz_rndms_base (numpy.array[float64]): 2d array of shape (number of seeds, sample_size) storing the random values\n      drawn for each seed for the hazard intensity sampling.\n    vuln_rndms_base (numpy.array[float64]): 2d array of shape (number of seeds, sample_size) storing the random values\n      drawn for each seed for the damage sampling.\n    vuln_adj (np.array): map array between vulnerability_idx and the adjustment factor to be applied to the (random numbers extracted) vulnerability function.\n    haz_eps_ij (np.array[float]): correlated random values of shape `(number of seeds, sample_size)` for hazard sampling.\n    damage_eps_ij (np.array[float]): correlated random values of shape `(number of seeds, sample_size)` for damage sampling.\n    norm_inv_parameters (NormInversionParameters): parameters for the Normal (Gaussian) inversion functionality.\n    norm_inv_cdf (np.array[float]): inverse Gaussian cdf.\n    norm_cdf (np.array[float]): Gaussian cdf.\n    vuln_z_unif (np.array[float]): buffer to be re-used to store the correlated random values for vuln.\n    haz_z_unif (np.array[float]): buffer to be re-used to store the correlated random values for haz.\n    byte_mv (numpy.array): byte view of where the output is buffered.\n    dynamic_footprint,\n    intensity_bin_dict\n\nReturns:\n    True if processing is done else false",
      "content": "# File: oasislmf/pytools/gulmc/manager.py\n# function: compute_event_losses (lines 718-1037)\n\ndef compute_event_losses(compute_info,\n                         coverages,\n                         coverage_ids,\n                         items_event_data,\n                         items,\n                         sample_size,\n                         haz_pdf,\n                         haz_arr_ptr,\n                         vuln_array,\n                         damage_bins,\n                         cached_vuln_cdf_lookup,\n                         cached_vuln_cdf_lookup_keys,\n                         cached_vuln_cdfs,\n                         agg_vuln_to_vuln_idxs,\n                         areaperil_vuln_idx_to_weight,\n                         losses,\n                         haz_rndms_base,\n                         vuln_rndms_base,\n                         vuln_adj,\n                         haz_eps_ij,\n                         damage_eps_ij,\n                         norm_inv_parameters,\n                         norm_inv_cdf,\n                         norm_cdf,\n                         vuln_z_unif,\n                         haz_z_unif,\n                         byte_mv,\n                         dynamic_footprint,\n                         intensity_bin_dict\n                         ):\n    \"\"\"Compute losses for an event.\n\n    Args:\n        compute_info (ndarray): information on the state of the computation\n        coverages (numpy.array[oasis_float]): array with the coverage values for each coverage_id.\n        coverage_ids (numpy.array[int]): array of unique coverage ids used in this event.\n        items_event_data (numpy.array[items_data_type]): items-related data.\n        items (np.ndarray): items table merged with correlation parameters.\n        sample_size (int): number of random samples to draw.\n        haz_pdf (np.array[oasis_float]): hazard intensity cdf.\n        haz_arr_ptr (np.array[int]): array with the indices where each cdf record starts in `haz_cdf`.\n        vuln_array (np.array[float]): damage pdf for different vulnerability functions, as a function of hazard intensity.\n        damage_bins (List[Union[damagebindictionaryCsv, damagebindictionary]]): loaded data from the damage_bin_dict file.\n        cached_vuln_cdf_lookup (Dict[VULN_LOOKUP_KEY_TYPE, VULN_LOOKUP_VALUE_TYPE]): dict to store\n          the map between vuln_id and intensity bin id and the location of the cdf in the cache.\n        cached_vuln_cdf_lookup_keys (List[VULN_LOOKUP_VALUE_TYPE]): list of lookup keys.\n        cached_vuln_cdfs (np.array[oasis_float]): vulnerability cdf cache.\n        agg_vuln_to_vuln_idxs (dict[int, list[int]]): map between aggregate vulnerability id and the list of indices where the individual vulnerability_ids\n          that compose it are stored in `vuln_array`.\n        areaperil_vuln_idx_to_weight (dict[AGG_VULN_WEIGHTS_KEY_TYPE, AGG_VULN_WEIGHTS_VAL_TYPE]): map between the areaperil id and the index where the vulnerability function\n          is stored in `vuln_array` and the vulnerability weight.\n        losses (numpy.array[oasis_float]): array (to be re-used) to store losses for each item.\n        haz_rndms_base (numpy.array[float64]): 2d array of shape (number of seeds, sample_size) storing the random values\n          drawn for each seed for the hazard intensity sampling.\n        vuln_rndms_base (numpy.array[float64]): 2d array of shape (number of seeds, sample_size) storing the random values\n          drawn for each seed for the damage sampling.\n        vuln_adj (np.array): map array between vulnerability_idx and the adjustment factor to be applied to the (random numbers extracted) vulnerability function.\n        haz_eps_ij (np.array[float]): correlated random values of shape `(number of seeds, sample_size)` for hazard sampling.\n        damage_eps_ij (np.array[float]): correlated random values of shape `(number of seeds, sample_size)` for damage sampling.\n        norm_inv_parameters (NormInversionParameters): parameters for the Normal (Gaussian) inversion functionality.\n        norm_inv_cdf (np.array[float]): inverse Gaussian cdf.\n        norm_cdf (np.array[float]): Gaussian cdf.\n        vuln_z_unif (np.array[float]): buffer to be re-used to store the correlated random values for vuln.\n        haz_z_unif (np.array[float]): buffer to be re-used to store the correlated random values for haz.\n        byte_mv (numpy.array): byte view of where the output is buffered.\n        dynamic_footprint,\n        intensity_bin_dict\n\n    Returns:\n        True if processing is done else false\n    \"\"\"\n    haz_cdf_empty = np.empty(compute_info['Ndamage_bins_max'], dtype=oasis_float)\n    vuln_pdf_empty = np.empty((vuln_array.shape[2], compute_info['Ndamage_bins_max']), dtype=vuln_array.dtype)\n    eff_damage_cdf_empty = np.empty(compute_info['Ndamage_bins_max'], dtype=oasis_float)\n    haz_i_to_Ndamage_bins_empty = np.empty(vuln_array.shape[2], dtype=oasis_int)\n    haz_i_to_vuln_cdf_empty = np.empty((vuln_array.shape[2], compute_info['Ndamage_bins_max']), dtype=vuln_array.dtype)\n\n    # we process at least one full coverage at a time, so when we write to stream, we write the whole buffer\n    compute_info['cursor'] = 0\n\n\"\"\"Docstring (excerpt)\"\"\"\nCompute losses for an event.\n\nArgs:\n    compute_info (ndarray): information on the state of the computation\n    coverages (numpy.array[oasis_float]): array with the coverage values for each coverage_id.\n    coverage_ids (numpy.array[int]): array of unique coverage ids used in this event.\n    items_event_data (numpy.array[items_data_type]): items-related data.\n    items (np.ndarray): items table merged with correlation parameters.\n    sample_size (int): number of random samples to draw.\n    haz_pdf (np.array[oasis_float]): hazard intensity cdf.\n    haz_arr_ptr (np.array[int]): array with the indices where each cdf record starts in `haz_cdf`.\n    vuln_array (np.array[float]): damage pdf for different vulnerability functions, as a function of hazard intensity.\n    damage_bins (List[Union[damagebindictionaryCsv, damagebindictionary]]): loaded data from the damage_bin_dict file.\n    cached_vuln_cdf_lookup (Dict[VULN_LOOKUP_KEY_TYPE, VULN_LOOKUP_VALUE_TYPE]): dict to store\n      the map between vuln_id and intensity bin id and the location of the cdf in the cache.\n    cached_vuln_cdf_lookup_keys (List[VULN_LOOKUP_VALUE_TYPE]): list of lookup keys.\n    cached_vuln_cdfs (np.array[oasis_float]): vulnerability cdf cache.\n    agg_vuln_to_vuln_idxs (dict[int, list[int]]): map between aggregate vulnerability id and the list of indices where the individual vulnerability_ids\n      that compose it are stored in `vuln_array`.\n    areaperil_vuln_idx_to_weight (dict[AGG_VULN_WEIGHTS_KEY_TYPE, AGG_VULN_WEIGHTS_VAL_TYPE]): map between the areaperil id and the index where the vulnerability function\n      is stored in `vuln_array` and the vulnerability weight.\n    losses (numpy.array[oasis_float]): array (to be re-used) to store losses for each item.\n    haz_rndms_base (numpy.array[float64]): 2d array of shape (number of seeds, sample_size) storing the random values\n      drawn for each seed for the hazard intensity sampling.\n    vuln_rndms_base (numpy.array[float64]): 2d array of shape (number of seeds, sample_size) storing the random values\n      drawn for each seed for the damage sampling.\n    vuln_adj (np.array): map array between vulnerability_idx and the adjustment factor to be applied to the (random numbers extracted) vulnerability function.\n    haz_eps_ij (np.array[float]): correlated random values of shape `(number of seeds, sample_size)` for hazard sampling.\n    damage_eps_ij (np.array[float]): correlated random values of shape `(number of seeds, sample_size)` for damage sampling.\n    norm_inv_parameters (NormInversionParameters): parameters for the Normal (Gaussian) inversion functionality.\n    norm_inv_cdf (np.array[float]): inverse Gaussian cdf.\n    norm_cdf (np.array[float]): Gaussian cdf.\n    vuln_z_unif (np.array[float]): buffer to be re-used to store the correlated random values for vuln.\n    haz_z_unif (np.array[float]): buffer to be re-used to store the correlated random values for haz.\n    byte_mv (numpy.array): byte view of where the output is buffered.\n    dynamic_footprint,\n    intensity_bin_dict\n\nReturns:\n    True if processing is done else false"
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/manager.py::process_areaperils_in_footprint@1041",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/manager.py",
      "symbol_type": "function",
      "name": "process_areaperils_in_footprint",
      "lineno": 1041,
      "end_lineno": 1115,
      "business_stage": "gul",
      "docstring": "Process all the areaperils in the footprint, filtering and retaining only those who have associated vulnerability functions\n\nArgs:\n    event_footprint (np.array[Event or footprint_event_dtype]): footprint, made of one or more event entries.\n    present_areaperils (dict[int, int]): areaperil to vulnerability index dictionary.\n    dynamic_footprint (boolean): true if there is dynamic_footprint\n\nReturns:\n    areaperil_ids (List[int]): list of all areaperil_ids present in the footprint.\n    Nhaz_arr_this_event (int): number of hazard stored for this event. If zero, it means no items have losses in such event.\n    areaperil_to_haz_arr_i (dict[int, int]): map between the areaperil_id and the hazard index in haz_arr_ptr.\n    haz_pdf (np.array[oasis_float]): hazard intensity pdf.\n    haz_arr_ptr (np.array[int]): array with the indices where each hazard intensities record starts in haz arrays (ie, haz_pdf).",
      "content": "# File: oasislmf/pytools/gulmc/manager.py\n# function: process_areaperils_in_footprint (lines 1041-1115)\n\ndef process_areaperils_in_footprint(event_footprint,\n                                    present_areaperils,\n                                    dynamic_footprint):\n    \"\"\"\n    Process all the areaperils in the footprint, filtering and retaining only those who have associated vulnerability functions\n\n    Args:\n        event_footprint (np.array[Event or footprint_event_dtype]): footprint, made of one or more event entries.\n        present_areaperils (dict[int, int]): areaperil to vulnerability index dictionary.\n        dynamic_footprint (boolean): true if there is dynamic_footprint\n\n    Returns:\n        areaperil_ids (List[int]): list of all areaperil_ids present in the footprint.\n        Nhaz_arr_this_event (int): number of hazard stored for this event. If zero, it means no items have losses in such event.\n        areaperil_to_haz_arr_i (dict[int, int]): map between the areaperil_id and the hazard index in haz_arr_ptr.\n        haz_pdf (np.array[oasis_float]): hazard intensity pdf.\n        haz_arr_ptr (np.array[int]): array with the indices where each hazard intensities record starts in haz arrays (ie, haz_pdf).\n    \"\"\"\n    # init data structures\n    haz_prob_start_in_footprint = List.empty_list(nb_int64)\n    areaperil_ids = List.empty_list(nb_areaperil_int)\n\n    footprint_i = 0\n    last_areaperil_id = nb_areaperil_int(0)\n    last_areaperil_id_start = nb_int64(0)\n    haz_arr_i = 0\n    areaperil_to_haz_arr_i = Dict.empty(nb_areaperil_int, nb_oasis_int)\n\n    Nevent_footprint_entries = len(event_footprint)\n    haz_pdf = np.empty(Nevent_footprint_entries, dtype=haz_arr_type)  # max size\n\n    arr_ptr_start = 0\n    arr_ptr_end = 0\n    haz_arr_ptr = List([0])\n\n    while footprint_i <= Nevent_footprint_entries:\n\n        if footprint_i < Nevent_footprint_entries:\n            areaperil_id = event_footprint[footprint_i]['areaperil_id']\n        else:\n            areaperil_id = nb_areaperil_int(0)\n\n        if areaperil_id != last_areaperil_id:\n            # one areaperil_id is completed\n\n            if last_areaperil_id > 0:\n                if last_areaperil_id in present_areaperils:\n                    # if items with this areaperil_id exist, process and store this areaperil_id\n                    areaperil_ids.append(last_areaperil_id)\n                    haz_prob_start_in_footprint.append(last_areaperil_id_start)\n                    areaperil_to_haz_arr_i[last_areaperil_id] = nb_int32(haz_arr_i)\n                    haz_arr_i += 1\n\n                    # store the hazard intensity pdf\n                    arr_ptr_end = arr_ptr_start + (footprint_i - last_areaperil_id_start)\n                    haz_pdf['probability'][arr_ptr_start: arr_ptr_end] = event_footprint['probability'][last_areaperil_id_start: footprint_i]\n                    haz_pdf['intensity_bin_id'][arr_ptr_start: arr_ptr_end] = event_footprint['intensity_bin_id'][last_areaperil_id_start: footprint_i]\n                    if dynamic_footprint is not None:\n                        haz_pdf['intensity'][arr_ptr_start: arr_ptr_end] = event_footprint['intensity'][last_areaperil_id_start: footprint_i]\n\n                    haz_arr_ptr.append(arr_ptr_end)\n                    arr_ptr_start = arr_ptr_end\n\n            last_areaperil_id = areaperil_id\n            last_areaperil_id_start = footprint_i\n\n        footprint_i += 1\n\n    Nhaz_arr_this_event = haz_arr_i\n\n    return (areaperil_ids,\n            Nhaz_arr_this_event,\n            areaperil_to_haz_arr_i,\n            haz_pdf[:arr_ptr_end],\n            haz_arr_ptr)\n\n\"\"\"Docstring (excerpt)\"\"\"\nProcess all the areaperils in the footprint, filtering and retaining only those who have associated vulnerability functions\n\nArgs:\n    event_footprint (np.array[Event or footprint_event_dtype]): footprint, made of one or more event entries.\n    present_areaperils (dict[int, int]): areaperil to vulnerability index dictionary.\n    dynamic_footprint (boolean): true if there is dynamic_footprint\n\nReturns:\n    areaperil_ids (List[int]): list of all areaperil_ids present in the footprint.\n    Nhaz_arr_this_event (int): number of hazard stored for this event. If zero, it means no items have losses in such event.\n    areaperil_to_haz_arr_i (dict[int, int]): map between the areaperil_id and the hazard index in haz_arr_ptr.\n    haz_pdf (np.array[oasis_float]): hazard intensity pdf.\n    haz_arr_ptr (np.array[int]): array with the indices where each hazard intensities record starts in haz arrays (ie, haz_pdf)."
    },
    {
      "chunk_id": "oasislmf/pytools/gulmc/manager.py::reconstruct_coverages@1119",
      "source_type": "code",
      "path": "oasislmf/pytools/gulmc/manager.py",
      "symbol_type": "function",
      "name": "reconstruct_coverages",
      "lineno": 1119,
      "end_lineno": 1245,
      "business_stage": "gul",
      "docstring": "Register each item to its coverage, with the location of the corresponding hazard intensity cdf\nin the footprint, compute the random seeds for the hazard intensity and vulnerability samples.\n\nArgs:\n    dynamic_compute_info (dynamic_compute_info_type): ndarray that store all dynamic info on computation\n    areaperil_ids (List[int]): list of all areaperil_ids present in the footprint.\n    areaperil_ids_map (Dict[int, Dict[int, int]]) dict storing the mapping between each\n      areaperil_id and all the vulnerability ids associated with it.\n    areaperil_to_haz_arr_i (dict[int, int]): map between the areaperil_id and the hazard arr index.\n    item_map (Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]): dict storing\n      the mapping between areaperil_id, vulnerability_id to item.\n    items (np.ndarray): items table merged with correlation parameters.\n    coverages (numpy.array[oasis_float]): array with the coverage values for each coverage_id.\n    compute (numpy.array[int]): list of coverage ids to be computed.\n    haz_seeds (numpy.array[int]): the random seeds to draw the hazard intensity samples.\n    haz_peril_correlation_groups (numpy.array[int]): unique peril_correlation_groups for hazard\n    haz_corr_seeds (numpy.array[int]): empty buffer to write hazard_corr_seeds\n    vuln_seeds (numpy.array[int]): the random seeds to draw the damage samples.\n    damage_peril_correlation_groups (numpy.array[int]): unique peril_correlation_groups for damage\n    damage_corr_seeds (numpy.array[int]): empty buffer to write damage_corr_seeds\n    dynamic_footprint (bollean): true if dynamic_footprint is on\n    byte_mv : writing buffer\n\nReturns:\n    compute_i (int): index of the last coverage id stored in `compute`.\n    items_data (numpy.array[items_MC_data_type]): item-related data.\n    rng_index (int): number of unique random seeds for damage sampling computed so far.\n    hazard_rng_index (int): number of unique random seeds for hazard intensity sampling computed so far.\n    byte_mv : writing buffer with increased size if needed",
      "content": "# File: oasislmf/pytools/gulmc/manager.py\n# function: reconstruct_coverages (lines 1119-1245)\n\ndef reconstruct_coverages(compute_info,\n                          areaperil_ids,\n                          areaperil_ids_map,\n                          areaperil_to_haz_arr_i,\n                          item_map,\n                          items,\n                          coverages,\n                          compute,\n                          haz_seeds,\n                          haz_peril_correlation_groups,\n                          haz_corr_seeds,\n                          vuln_seeds,\n                          damage_peril_correlation_groups,\n                          damage_corr_seeds,\n                          dynamic_footprint,\n                          byte_mv):\n    \"\"\"Register each item to its coverage, with the location of the corresponding hazard intensity cdf\n    in the footprint, compute the random seeds for the hazard intensity and vulnerability samples.\n\n    Args:\n        dynamic_compute_info (dynamic_compute_info_type): ndarray that store all dynamic info on computation\n        areaperil_ids (List[int]): list of all areaperil_ids present in the footprint.\n        areaperil_ids_map (Dict[int, Dict[int, int]]) dict storing the mapping between each\n          areaperil_id and all the vulnerability ids associated with it.\n        areaperil_to_haz_arr_i (dict[int, int]): map between the areaperil_id and the hazard arr index.\n        item_map (Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]): dict storing\n          the mapping between areaperil_id, vulnerability_id to item.\n        items (np.ndarray): items table merged with correlation parameters.\n        coverages (numpy.array[oasis_float]): array with the coverage values for each coverage_id.\n        compute (numpy.array[int]): list of coverage ids to be computed.\n        haz_seeds (numpy.array[int]): the random seeds to draw the hazard intensity samples.\n        haz_peril_correlation_groups (numpy.array[int]): unique peril_correlation_groups for hazard\n        haz_corr_seeds (numpy.array[int]): empty buffer to write hazard_corr_seeds\n        vuln_seeds (numpy.array[int]): the random seeds to draw the damage samples.\n        damage_peril_correlation_groups (numpy.array[int]): unique peril_correlation_groups for damage\n        damage_corr_seeds (numpy.array[int]): empty buffer to write damage_corr_seeds\n        dynamic_footprint (bollean): true if dynamic_footprint is on\n        byte_mv : writing buffer\n\n    Returns:\n        compute_i (int): index of the last coverage id stored in `compute`.\n        items_data (numpy.array[items_MC_data_type]): item-related data.\n        rng_index (int): number of unique random seeds for damage sampling computed so far.\n        hazard_rng_index (int): number of unique random seeds for hazard intensity sampling computed so far.\n        byte_mv : writing buffer with increased size if needed\n    \"\"\"\n    # init data structures\n    group_id_rng_index = Dict.empty(nb_int32, nb_int64)\n    hazard_group_id_hazard_rng_index = Dict.empty(nb_int32, nb_int64)\n    rng_index = 0\n    hazard_rng_index = 0\n    compute_i = 0\n    items_data_i = 0\n    coverages['cur_items'].fill(0)\n    items_event_data = np.empty(2 ** NP_BASE_ARRAY_SIZE, dtype=items_MC_data_type)\n\n    # for each areaperil_id, loop over all vulnerability functions used in that areaperil_id and,\n    # for each item:\n    #  - compute the seeds for the hazard intensity sampling and for the damage sampling\n    #  - store data for later processing (hazard cdf index, etc.)\n    for areaperil_id in areaperil_ids:\n\n        for vuln_id in areaperil_ids_map[areaperil_id]:\n            # register the items to their coverage\n            item_key = tuple((areaperil_id, vuln_id))\n\n            for item_idx in item_map[item_key]:\n                # if this group_id was not seen yet, process it.\n                # it assumes that hash only depends on event_id and group_id\n                # and that only 1 event_id is processed at a time.\n                group_id = items[item_idx]['group_id']\n                if group_id not in group_id_rng_index:\n                    group_id_rng_index[group_id] = rng_index\n                    vuln_seeds[rng_index] = generate_hash(group_id, compute_info['event_id'])\n                    this_rng_index = rng_index\n                    rng_index += 1\n\n                else:\n                    this_rng_index = group_id_rng_index[group_id]\n\n\"\"\"Docstring (excerpt)\"\"\"\nRegister each item to its coverage, with the location of the corresponding hazard intensity cdf\nin the footprint, compute the random seeds for the hazard intensity and vulnerability samples.\n\nArgs:\n    dynamic_compute_info (dynamic_compute_info_type): ndarray that store all dynamic info on computation\n    areaperil_ids (List[int]): list of all areaperil_ids present in the footprint.\n    areaperil_ids_map (Dict[int, Dict[int, int]]) dict storing the mapping between each\n      areaperil_id and all the vulnerability ids associated with it.\n    areaperil_to_haz_arr_i (dict[int, int]): map between the areaperil_id and the hazard arr index.\n    item_map (Dict[ITEM_MAP_KEY_TYPE, ITEM_MAP_VALUE_TYPE]): dict storing\n      the mapping between areaperil_id, vulnerability_id to item.\n    items (np.ndarray): items table merged with correlation parameters.\n    coverages (numpy.array[oasis_float]): array with the coverage values for each coverage_id.\n    compute (numpy.array[int]): list of coverage ids to be computed.\n    haz_seeds (numpy.array[int]): the random seeds to draw the hazard intensity samples.\n    haz_peril_correlation_groups (numpy.array[int]): unique peril_correlation_groups for hazard\n    haz_corr_seeds (numpy.array[int]): empty buffer to write hazard_corr_seeds\n    vuln_seeds (numpy.array[int]): the random seeds to draw the damage samples.\n    damage_peril_correlation_groups (numpy.array[int]): unique peril_correlation_groups for damage\n    damage_corr_seeds (numpy.array[int]): empty buffer to write damage_corr_seeds\n    dynamic_footprint (bollean): true if dynamic_footprint is on\n    byte_mv : writing buffer\n\nReturns:\n    compute_i (int): index of the last coverage id stored in `compute`.\n    items_data (numpy.array[items_MC_data_type]): item-related data.\n    rng_index (int): number of unique random seeds for damage sampling computed so far.\n    hazard_rng_index (int): number of unique random seeds for hazard intensity sampling computed so far.\n    byte_mv : writing buffer with increased size if needed"
    },
    {
      "chunk_id": "oasislmf/pytools/join_summary_info/manager.py::load_summary_info@17",
      "source_type": "code",
      "path": "oasislmf/pytools/join_summary_info/manager.py",
      "symbol_type": "function",
      "name": "load_summary_info",
      "lineno": 17,
      "end_lineno": 64,
      "business_stage": "aggregation",
      "docstring": "Load summary-info data into an array as strings to maintain sigfigs/formatting\nArgs:\n    stack (ExitStack): Exit Stack\n    summaryinfo_file (str | os.PathLike): Path to summary-info csv file\nReturns:\n    full_summary_data (ndarray[object]): Array of strings, indexed by Summary Id\n    headers (List[str]): List of strings for summary info headers to add to data\n    max_summary_id (int): Max Summary ID",
      "content": "# File: oasislmf/pytools/join_summary_info/manager.py\n# function: load_summary_info (lines 17-64)\n\ndef load_summary_info(stack, summaryinfo_file):\n    \"\"\"Load summary-info data into an array as strings to maintain sigfigs/formatting\n    Args:\n        stack (ExitStack): Exit Stack\n        summaryinfo_file (str | os.PathLike): Path to summary-info csv file\n    Returns:\n        full_summary_data (ndarray[object]): Array of strings, indexed by Summary Id\n        headers (List[str]): List of strings for summary info headers to add to data\n        max_summary_id (int): Max Summary ID\n    \"\"\"\n    summaryinfo_file = Path(summaryinfo_file)\n\n    if summaryinfo_file.suffix == \".csv\":\n        summary_ids = []\n        summary_data = []\n\n        with stack.enter_context(open(summaryinfo_file, \"r\")) as fin:\n            headers = fin.readline().strip().split(\",\")\n            summary_id_col_idx = headers.index(\"summary_id\")\n\n            for line in fin:\n                row = line.strip().split(\",\")\n                summary_ids.append(row[summary_id_col_idx])\n                summary_data.append(\",\".join(row[:summary_id_col_idx] + row[summary_id_col_idx + 1:]))\n\n        headers = headers[:summary_id_col_idx] + headers[summary_id_col_idx + 1:]\n\n        summary_ids = np.array(summary_ids, dtype=np.int64)\n        summary_data = np.array(summary_data, dtype=object)\n    elif summaryinfo_file.suffix == \".parquet\":\n        table = pq.read_table(summaryinfo_file)\n        df = table.to_pandas()\n\n        if \"summary_id\" not in df.columns:\n            raise ValueError(\"Missing 'summary_id' column in summary info file.\")\n\n        summary_ids = df[\"summary_id\"].to_numpy(dtype=np.int64)\n        summary_data = df.drop(columns=[\"summary_id\"]).astype(str).agg(\",\".join, axis=1).to_numpy(dtype=object)\n        headers = [col for col in df.columns if col != \"summary_id\"]\n    else:\n        raise ValueError(f\"Unsupported file format {summaryinfo_file.suffix}.\")\n\n    max_summary_id = summary_ids.max()\n    full_summary_data = np.full((max_summary_id + 1,), \",\" * (len(headers) - 1), dtype=object)\n    for i in range(len(summary_ids)):\n        full_summary_data[summary_ids[i]] = summary_data[i]\n\n    return full_summary_data, headers, max_summary_id\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoad summary-info data into an array as strings to maintain sigfigs/formatting\nArgs:\n    stack (ExitStack): Exit Stack\n    summaryinfo_file (str | os.PathLike): Path to summary-info csv file\nReturns:\n    full_summary_data (ndarray[object]): Array of strings, indexed by Summary Id\n    headers (List[str]): List of strings for summary info headers to add to data\n    max_summary_id (int): Max Summary ID"
    },
    {
      "chunk_id": "oasislmf/pytools/join_summary_info/manager.py::run@67",
      "source_type": "code",
      "path": "oasislmf/pytools/join_summary_info/manager.py",
      "symbol_type": "function",
      "name": "run",
      "lineno": 67,
      "end_lineno": 136,
      "business_stage": "aggregation",
      "docstring": "Join the summary-info file to the given ORD data file based on SummaryId\nArgs:\n    summaryinfo_file (str | os.PathLike): Path to summary-info file\n    data_file (str | os.PathLike): Path to ORD output data file (e.g. SELT, MPLT, AAL, PSEPT) \n    output_file (str | os.PathLike): Path to combined output file",
      "content": "# File: oasislmf/pytools/join_summary_info/manager.py\n# function: run (lines 67-136)\n\ndef run(\n    summaryinfo_file,\n    data_file,\n    output_file,\n):\n    \"\"\"Join the summary-info file to the given ORD data file based on SummaryId\n    Args:\n        summaryinfo_file (str | os.PathLike): Path to summary-info file\n        data_file (str | os.PathLike): Path to ORD output data file (e.g. SELT, MPLT, AAL, PSEPT) \n        output_file (str | os.PathLike): Path to combined output file\n    \"\"\"\n    summaryinfo_file = Path(summaryinfo_file)\n    data_file = Path(data_file)\n    output_file = Path(output_file)\n\n    valid_file_suffix = [\".csv\", \".parquet\"]\n\n    if summaryinfo_file.suffix not in valid_file_suffix or \\\n       data_file.suffix not in valid_file_suffix or \\\n       output_file.suffix not in valid_file_suffix:\n        raise ValueError(f\"All files must be in one of {valid_file_suffix} formats.\")\n\n    if not (summaryinfo_file.suffix == data_file.suffix == output_file.suffix):\n        raise ValueError(\"Summary info, data, and output files must all have the same format.\")\n\n    with ExitStack() as stack:\n        summary_data, summary_headers, max_summary_id = load_summary_info(stack, summaryinfo_file)\n\n        # Use a temporary file if input and output are the same\n        same_file = data_file.resolve() == output_file.resolve()\n        temp_output = None\n\n        if same_file:\n            temp_output = tempfile.NamedTemporaryFile(mode=\"w\", delete=False)\n            temp_output_file = Path(temp_output.name)\n        else:\n            temp_output_file = output_file\n\n        if data_file.suffix == \".csv\":\n            with stack.enter_context(open(data_file, \"r\")) as data_fin, stack.enter_context(open(temp_output_file, \"w\")) as fout:\n                data_headers = data_fin.readline().strip().split(\",\")\n                fout.write(\",\".join(data_headers + summary_headers) + \"\\n\")\n\n                summary_id_col_idx = data_headers.index(\"SummaryId\")\n                for line in data_fin:\n                    row = line.strip().split(\",\")\n                    summary_id = int(row[summary_id_col_idx])\n                    if summary_id > max_summary_id:\n                        fout.write(line.strip() + (\",\" * len(summary_headers)) + \"\\n\")\n                    else:\n                        fout.write(line.strip() + \",\" + summary_data[summary_id] + \"\\n\")\n        elif data_file.suffix == \".parquet\":\n            table = pq.read_table(data_file)\n            df = table.to_pandas()\n\n            if \"SummaryId\" not in df.columns:\n                raise ValueError(\"Missing 'SummaryId' column in data file.\")\n\n            df[summary_headers] = df[\"SummaryId\"].apply(\n                lambda sid: summary_data[sid] if sid <= max_summary_id else \"\"\n            ).str.split(\",\", expand=True)\n            pq.write_table(pa.Table.from_pandas(df), temp_output_file)\n        else:\n            raise ValueError(f\"Unsupported file format {data_file.suffix}.\")\n\n        # Replace the original file if needed\n        if same_file and temp_output:\n            temp_output.close()\n            temp_output_file.replace(output_file)\n            temp_output_file.unlink(missing_ok=True)\n\n\"\"\"Docstring (excerpt)\"\"\"\nJoin the summary-info file to the given ORD data file based on SummaryId\nArgs:\n    summaryinfo_file (str | os.PathLike): Path to summary-info file\n    data_file (str | os.PathLike): Path to ORD output data file (e.g. SELT, MPLT, AAL, PSEPT) \n    output_file (str | os.PathLike): Path to combined output file"
    },
    {
      "chunk_id": "oasislmf/pytools/kat/manager.py::check_file_extensions@73",
      "source_type": "code",
      "path": "oasislmf/pytools/kat/manager.py",
      "symbol_type": "function",
      "name": "check_file_extensions",
      "lineno": 73,
      "end_lineno": 84,
      "business_stage": "other",
      "docstring": "Check file path extensions are all identical\nArgs:\n    file_paths (List[str | os.PathLike]): List of csv file paths.\nReturns:\n    ext (str): file extension as a str",
      "content": "# File: oasislmf/pytools/kat/manager.py\n# function: check_file_extensions (lines 73-84)\n\ndef check_file_extensions(file_paths):\n    \"\"\"Check file path extensions are all identical\n    Args:\n        file_paths (List[str | os.PathLike]): List of csv file paths.\n    Returns:\n        ext (str): file extension as a str\n    \"\"\"\n    first_ext = file_paths[0].suffix\n\n    if all(fp.suffix == first_ext for fp in file_paths):\n        return first_ext\n    raise RuntimeError(\"ERROR: katpy has input files with different file extensions. Make sure all input files are of the same type.\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nCheck file path extensions are all identical\nArgs:\n    file_paths (List[str | os.PathLike]): List of csv file paths.\nReturns:\n    ext (str): file extension as a str"
    },
    {
      "chunk_id": "oasislmf/pytools/kat/manager.py::find_csv_with_header@87",
      "source_type": "code",
      "path": "oasislmf/pytools/kat/manager.py",
      "symbol_type": "function",
      "name": "find_csv_with_header",
      "lineno": 87,
      "end_lineno": 117,
      "business_stage": "other",
      "docstring": "Find and check csv files for consistent and present headers\nArgs:\n    stack (ExitStack): Exit Stack.\n    file_paths (List[str | os.PathLike]): List of csv file paths.\nReturns:\n    files_with_header (List[bool]): Bool list of files with header present\n    header (str): Header to write",
      "content": "# File: oasislmf/pytools/kat/manager.py\n# function: find_csv_with_header (lines 87-117)\n\ndef find_csv_with_header(\n    stack,\n    file_paths,\n):\n    \"\"\"Find and check csv files for consistent and present headers\n    Args:\n        stack (ExitStack): Exit Stack.\n        file_paths (List[str | os.PathLike]): List of csv file paths.\n    Returns:\n        files_with_header (List[bool]): Bool list of files with header present\n        header (str): Header to write\n    \"\"\"\n    headers = {}\n    files_with_header = [False] * len(file_paths)\n    for idx, fp in enumerate(file_paths):\n        with stack.enter_context(fp.open(\"r\", newline=\"\", encoding=\"utf-8\")) as f:\n            first_row = f.readline().strip()\n            if first_row:\n                if \"EventId\" in first_row:\n                    headers[fp] = first_row\n                    files_with_header[idx] = True\n\n    if not headers:\n        raise ValueError(\"ERROR: katpy, no valid header found in any CSV file.\")\n\n    header_counts = Counter(headers.values())\n\n    if len(header_counts) > 1:\n        raise ValueError(f\"ERROR: katpy, conflicting headers found: {header_counts}\")\n\n    return files_with_header, list(headers.values())[0]\n\n\"\"\"Docstring (excerpt)\"\"\"\nFind and check csv files for consistent and present headers\nArgs:\n    stack (ExitStack): Exit Stack.\n    file_paths (List[str | os.PathLike]): List of csv file paths.\nReturns:\n    files_with_header (List[bool]): Bool list of files with header present\n    header (str): Header to write"
    },
    {
      "chunk_id": "oasislmf/pytools/kat/manager.py::check_correct_headers@120",
      "source_type": "code",
      "path": "oasislmf/pytools/kat/manager.py",
      "symbol_type": "function",
      "name": "check_correct_headers",
      "lineno": 120,
      "end_lineno": 132,
      "business_stage": "other",
      "docstring": "Checks headers found in csv file matches excpected headers for file type\nArgs:\n    headers (List[str]): Headers\n    file_type (int): File type int matching KAT_NAMES index",
      "content": "# File: oasislmf/pytools/kat/manager.py\n# function: check_correct_headers (lines 120-132)\n\ndef check_correct_headers(headers, file_type):\n    \"\"\"Checks headers found in csv file matches excpected headers for file type\n    Args:\n        headers (List[str]): Headers\n        file_type (int): File type int matching KAT_NAMES index\n    \"\"\"\n    if file_type in KAT_MAP:\n        expected_headers = KAT_MAP[file_type][\"headers\"]\n    else:\n        file_type_names = [v[\"name\"] for v in KAT_MAP.values()]\n        raise ValueError(f\"ERROR: katpy, unknown file_type {file_type}, not in {file_type_names}\")\n    if headers != expected_headers:\n        raise RuntimeError(f\"ERROR: katpy, incorrect headers found in csv file, expected {expected_headers} but got {headers}\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nChecks headers found in csv file matches excpected headers for file type\nArgs:\n    headers (List[str]): Headers\n    file_type (int): File type int matching KAT_NAMES index"
    },
    {
      "chunk_id": "oasislmf/pytools/kat/manager.py::get_header_idxs@135",
      "source_type": "code",
      "path": "oasislmf/pytools/kat/manager.py",
      "symbol_type": "function",
      "name": "get_header_idxs",
      "lineno": 135,
      "end_lineno": 152,
      "business_stage": "other",
      "docstring": "Search for index of headers_to_search in headers of csv file\nArgs:\n    headers (List[str]): Headers\n    headers_to_search (List[str]): Headers to search\nReturns:\n    idxs (List[int]): Indexes of searched headers",
      "content": "# File: oasislmf/pytools/kat/manager.py\n# function: get_header_idxs (lines 135-152)\n\ndef get_header_idxs(\n    headers,\n    headers_to_search,\n):\n    \"\"\"Search for index of headers_to_search in headers of csv file\n    Args:\n        headers (List[str]): Headers\n        headers_to_search (List[str]): Headers to search\n    Returns:\n        idxs (List[int]): Indexes of searched headers\n    \"\"\"\n    idxs = []\n    for header in headers_to_search:\n        try:\n            idxs.append(headers.index(header))\n        except ValueError as e:\n            raise ValueError(f\"ERROR: katpy, cannot sort by header {header}, not found. {e}\")\n    return idxs\n\n\"\"\"Docstring (excerpt)\"\"\"\nSearch for index of headers_to_search in headers of csv file\nArgs:\n    headers (List[str]): Headers\n    headers_to_search (List[str]): Headers to search\nReturns:\n    idxs (List[int]): Indexes of searched headers"
    },
    {
      "chunk_id": "oasislmf/pytools/kat/manager.py::csv_concat_unsorted@155",
      "source_type": "code",
      "path": "oasislmf/pytools/kat/manager.py",
      "symbol_type": "function",
      "name": "csv_concat_unsorted",
      "lineno": 155,
      "end_lineno": 183,
      "business_stage": "other",
      "docstring": "Concats CSV files in order they are passed in.\nArgs:\n    stack (ExitStack): Exit Stack.\n    file_paths (List[str | os.PathLike]): List of csv file paths.\n    files_with_header (List[bool]): Bool list of files with header present\n    headers (List[str]): Headers to write\n    out_file (str | os.PathLike): Output Concatenated CSV file.",
      "content": "# File: oasislmf/pytools/kat/manager.py\n# function: csv_concat_unsorted (lines 155-183)\n\ndef csv_concat_unsorted(\n    stack,\n    file_paths,\n    files_with_header,\n    headers,\n    out_file,\n):\n    \"\"\"Concats CSV files in order they are passed in.\n    Args:\n        stack (ExitStack): Exit Stack.\n        file_paths (List[str | os.PathLike]): List of csv file paths.\n        files_with_header (List[bool]): Bool list of files with header present\n        headers (List[str]): Headers to write\n        out_file (str | os.PathLike): Output Concatenated CSV file.\n    \"\"\"\n    first_header_written = False\n\n    with stack.enter_context(out_file.open(\"wb\")) as out:\n        for i, fp in enumerate(file_paths):\n            with stack.enter_context(fp.open(\"rb\")) as csv_file:\n                if files_with_header[i]:\n                    # Read first line (header)\n                    first_line = csv_file.readline()\n\n                    # Write the expected header at the start of the file\n                    if not first_header_written:\n                        out.write(\",\".join(headers).encode() + b\"\\n\")\n                        first_header_written = True\n                shutil.copyfileobj(csv_file, out)\n\n\"\"\"Docstring (excerpt)\"\"\"\nConcats CSV files in order they are passed in.\nArgs:\n    stack (ExitStack): Exit Stack.\n    file_paths (List[str | os.PathLike]): List of csv file paths.\n    files_with_header (List[bool]): Bool list of files with header present\n    headers (List[str]): Headers to write\n    out_file (str | os.PathLike): Output Concatenated CSV file."
    },
    {
      "chunk_id": "oasislmf/pytools/kat/manager.py::csv_concat_sort_by_headers@186",
      "source_type": "code",
      "path": "oasislmf/pytools/kat/manager.py",
      "symbol_type": "function",
      "name": "csv_concat_sort_by_headers",
      "lineno": 186,
      "end_lineno": 242,
      "business_stage": "other",
      "docstring": "Concats CSV files in order determined by the header_idxs and sort_fn\nArgs:\n    stack (ExitStack): Exit Stack.\n    file_paths (List[str | os.PathLike]): List of csv file paths.\n    files_with_header (List[bool]): Bool list of files with header present\n    headers (List[str]): Headers to write\n    header_idxs (List[int]): Indices of headers to sort by\n    sort_fn (Callable[[List[int]], Any]): Sort function to apply to header_idxs\n    out_file (str | os.PathLike): Output Concatenated CSV file.",
      "content": "# File: oasislmf/pytools/kat/manager.py\n# function: csv_concat_sort_by_headers (lines 186-242)\n\ndef csv_concat_sort_by_headers(\n    stack,\n    file_paths,\n    files_with_header,\n    headers,\n    header_idxs,\n    sort_fn,\n    out_file,\n    **sort_kwargs\n):\n    \"\"\"Concats CSV files in order determined by the header_idxs and sort_fn\n    Args:\n        stack (ExitStack): Exit Stack.\n        file_paths (List[str | os.PathLike]): List of csv file paths.\n        files_with_header (List[bool]): Bool list of files with header present\n        headers (List[str]): Headers to write\n        header_idxs (List[int]): Indices of headers to sort by\n        sort_fn (Callable[[List[int]], Any]): Sort function to apply to header_idxs\n        out_file (str | os.PathLike): Output Concatenated CSV file.\n    \"\"\"\n    # Open all csv files\n    csv_files = [stack.enter_context(open(fp, \"r\", newline=\"\")) for fp in file_paths]\n    readers = [csv.reader(f) for f in csv_files]\n\n    # Skip headers\n    for idx, reader in enumerate(readers):\n        if files_with_header[idx]:\n            next(reader, None)\n\n    def row_generator(reader, file_idx):\n        for row in reader:\n            sort_key = sort_fn([row[i] for i in header_idxs], **sort_kwargs)\n            yield (sort_key, file_idx, row)\n\n    # Create iterator for each file\n    iterators = [row_generator(reader, idx) for idx, reader in enumerate(readers)]\n\n    # Merge all iterators\n    merged_iterator = heapq.merge(*iterators, key=lambda x: x[0])\n\n    buffer_size = DEFAULT_BUFFER_SIZE\n    with stack.enter_context(out_file.open(\"w\")) as out:\n        writer = csv.writer(out, lineterminator=\"\\n\")\n        writer.writerow(headers)\n\n        buffer = []\n        for _, _, row in merged_iterator:\n            buffer.append(row)\n            if len(buffer) > buffer_size:\n                writer.writerows(buffer)\n                buffer.clear()\n\n        if buffer:\n            writer.writerows(buffer)\n\n    for f in csv_files:\n        f.close()\n\n\"\"\"Docstring (excerpt)\"\"\"\nConcats CSV files in order determined by the header_idxs and sort_fn\nArgs:\n    stack (ExitStack): Exit Stack.\n    file_paths (List[str | os.PathLike]): List of csv file paths.\n    files_with_header (List[bool]): Bool list of files with header present\n    headers (List[str]): Headers to write\n    header_idxs (List[int]): Indices of headers to sort by\n    sort_fn (Callable[[List[int]], Any]): Sort function to apply to header_idxs\n    out_file (str | os.PathLike): Output Concatenated CSV file."
    },
    {
      "chunk_id": "oasislmf/pytools/kat/manager.py::bin_concat_unsorted@245",
      "source_type": "code",
      "path": "oasislmf/pytools/kat/manager.py",
      "symbol_type": "function",
      "name": "bin_concat_unsorted",
      "lineno": 245,
      "end_lineno": 259,
      "business_stage": "other",
      "docstring": "Concats Binary files in order they are passed in.\nArgs:\n    stack (ExitStack): Exit Stack.\n    file_paths (List[str | os.PathLike]): List of bin file paths.\n    out_file (str | os.PathLike): Output Concatenated Binary file.",
      "content": "# File: oasislmf/pytools/kat/manager.py\n# function: bin_concat_unsorted (lines 245-259)\n\ndef bin_concat_unsorted(\n    stack,\n    file_paths,\n    out_file,\n):\n    \"\"\"Concats Binary files in order they are passed in.\n    Args:\n        stack (ExitStack): Exit Stack.\n        file_paths (List[str | os.PathLike]): List of bin file paths.\n        out_file (str | os.PathLike): Output Concatenated Binary file.\n    \"\"\"\n    with stack.enter_context(out_file.open('wb')) as out:\n        for fp in file_paths:\n            with fp.open('rb') as bin_file:\n                shutil.copyfileobj(bin_file, out)\n\n\"\"\"Docstring (excerpt)\"\"\"\nConcats Binary files in order they are passed in.\nArgs:\n    stack (ExitStack): Exit Stack.\n    file_paths (List[str | os.PathLike]): List of bin file paths.\n    out_file (str | os.PathLike): Output Concatenated Binary file."
    },
    {
      "chunk_id": "oasislmf/pytools/kat/manager.py::merge_elt_data@263",
      "source_type": "code",
      "path": "oasislmf/pytools/kat/manager.py",
      "symbol_type": "function",
      "name": "merge_elt_data",
      "lineno": 263,
      "end_lineno": 305,
      "business_stage": "other",
      "docstring": "Merge sorted chunks using a k-way merge algorithm\nArgs:\n    memmaps (List[np.memmap]): List of temporary file memmaps\nYields:\n    buffer (ndarray): yields sorted buffer from memmaps",
      "content": "# File: oasislmf/pytools/kat/manager.py\n# function: merge_elt_data (lines 263-305)\n\ndef merge_elt_data(memmaps):\n    \"\"\"Merge sorted chunks using a k-way merge algorithm\n    Args:\n        memmaps (List[np.memmap]): List of temporary file memmaps\n    Yields:\n        buffer (ndarray): yields sorted buffer from memmaps\n    \"\"\"\n    min_heap = init_heap(num_compare=1)\n    size = 0\n    # Initialize the min_heap with the first row of each memmap\n    for i, mmap in enumerate(memmaps):\n        if len(mmap) > 0:\n            first_row = mmap[0]\n            min_heap, size = heap_push(min_heap, size, np.array(\n                [first_row[\"EventId\"], i, 0], dtype=np.int32\n            ))\n\n    buffer_size = DEFAULT_BUFFER_SIZE\n    buffer = np.empty(buffer_size, dtype=memmaps[0].dtype)\n    bidx = 0\n\n    # Perform the k-way merge\n    while size > 0:\n        # The min heap will store the smallest row at the top when popped\n        element, min_heap, size = heap_pop(min_heap, size)\n        file_idx = element[-2]\n        row_num = element[-1]\n        smallest_row = memmaps[file_idx][row_num]\n\n        # Add to buffer and yield when full\n        buffer[bidx] = smallest_row\n        bidx += 1\n        if bidx >= buffer_size:\n            yield buffer[:bidx]\n            bidx = 0\n\n        # Push the next row from the same file into the heap if there are any more rows\n        if row_num + 1 < len(memmaps[file_idx]):\n            next_row = memmaps[file_idx][row_num + 1]\n            min_heap, size = heap_push(min_heap, size, np.array(\n                [next_row[\"EventId\"], file_idx, row_num + 1], dtype=np.int32\n            ))\n    yield buffer[:bidx]\n\n\"\"\"Docstring (excerpt)\"\"\"\nMerge sorted chunks using a k-way merge algorithm\nArgs:\n    memmaps (List[np.memmap]): List of temporary file memmaps\nYields:\n    buffer (ndarray): yields sorted buffer from memmaps"
    },
    {
      "chunk_id": "oasislmf/pytools/kat/manager.py::merge_plt_data@309",
      "source_type": "code",
      "path": "oasislmf/pytools/kat/manager.py",
      "symbol_type": "function",
      "name": "merge_plt_data",
      "lineno": 309,
      "end_lineno": 351,
      "business_stage": "other",
      "docstring": "Merge sorted chunks using a k-way merge algorithm\nArgs:\n    memmaps (List[np.memmap]): List of temporary file memmaps\nYields:\n    buffer (ndarray): yields sorted buffer from memmaps",
      "content": "# File: oasislmf/pytools/kat/manager.py\n# function: merge_plt_data (lines 309-351)\n\ndef merge_plt_data(memmaps):\n    \"\"\"Merge sorted chunks using a k-way merge algorithm\n    Args:\n        memmaps (List[np.memmap]): List of temporary file memmaps\n    Yields:\n        buffer (ndarray): yields sorted buffer from memmaps\n    \"\"\"\n    min_heap = init_heap(num_compare=2)\n    size = 0\n    # Initialize the min_heap with the first row of each memmap\n    for i, mmap in enumerate(memmaps):\n        if len(mmap) > 0:\n            first_row = mmap[0]\n            min_heap, size = heap_push(min_heap, size, np.array(\n                [first_row[\"EventId\"], first_row[\"Period\"], i, 0], dtype=np.int32\n            ))\n\n    buffer_size = DEFAULT_BUFFER_SIZE\n    buffer = np.empty(buffer_size, dtype=memmaps[0].dtype)\n    bidx = 0\n\n    # Perform the k-way merge\n    while size > 0:\n        # The min heap will store the smallest row at the top when popped\n        element, min_heap, size = heap_pop(min_heap, size)\n        file_idx = element[-2]\n        row_num = element[-1]\n        smallest_row = memmaps[file_idx][row_num]\n\n        # Add to buffer and yield when full\n        buffer[bidx] = smallest_row\n        bidx += 1\n        if bidx >= buffer_size:\n            yield buffer[:bidx]\n            bidx = 0\n\n        # Push the next row from the same file into the heap if there are any more rows\n        if row_num + 1 < len(memmaps[file_idx]):\n            next_row = memmaps[file_idx][row_num + 1]\n            min_heap, size = heap_push(min_heap, size, np.array(\n                [next_row[\"EventId\"], next_row[\"Period\"], file_idx, row_num + 1], dtype=np.int32\n            ))\n    yield buffer[:bidx]\n\n\"\"\"Docstring (excerpt)\"\"\"\nMerge sorted chunks using a k-way merge algorithm\nArgs:\n    memmaps (List[np.memmap]): List of temporary file memmaps\nYields:\n    buffer (ndarray): yields sorted buffer from memmaps"
    },
    {
      "chunk_id": "oasislmf/pytools/kat/manager.py::bin_concat_sort_by_headers@354",
      "source_type": "code",
      "path": "oasislmf/pytools/kat/manager.py",
      "symbol_type": "function",
      "name": "bin_concat_sort_by_headers",
      "lineno": 354,
      "end_lineno": 380,
      "business_stage": "other",
      "docstring": "Concats Binary files in order determined out_type and their respective merge functions\nArgs:\n    stack (ExitStack): Exit Stack.\n    file_paths (List[str | os.PathLike]): List of bin file paths.\n    file_type (int): File type int matching KAT_NAMES index\n    out_type (str): Out type str between \"elt\" and \"plt\"\n    out_file (str | os.PathLike): Output Concatenated Binary file.",
      "content": "# File: oasislmf/pytools/kat/manager.py\n# function: bin_concat_sort_by_headers (lines 354-380)\n\ndef bin_concat_sort_by_headers(\n    stack,\n    file_paths,\n    file_type,\n    out_type,\n    out_file,\n):\n    \"\"\"Concats Binary files in order determined out_type and their respective merge functions\n    Args:\n        stack (ExitStack): Exit Stack.\n        file_paths (List[str | os.PathLike]): List of bin file paths.\n        file_type (int): File type int matching KAT_NAMES index\n        out_type (str): Out type str between \"elt\" and \"plt\"\n        out_file (str | os.PathLike): Output Concatenated Binary file.\n    \"\"\"\n    files = [np.memmap(fp, dtype=KAT_MAP[file_type][\"dtype\"]) for fp in file_paths]\n\n    if out_type == \"elt\":\n        gen = merge_elt_data(files)\n    elif out_type == \"plt\":\n        gen = merge_plt_data(files)\n    else:\n        raise RuntimeError(f\"ERROR: katpy, unknown out_type {out_type}\")\n\n    with stack.enter_context(out_file.open(\"wb\")) as out:\n        for data in gen:\n            data.tofile(out)\n\n\"\"\"Docstring (excerpt)\"\"\"\nConcats Binary files in order determined out_type and their respective merge functions\nArgs:\n    stack (ExitStack): Exit Stack.\n    file_paths (List[str | os.PathLike]): List of bin file paths.\n    file_type (int): File type int matching KAT_NAMES index\n    out_type (str): Out type str between \"elt\" and \"plt\"\n    out_file (str | os.PathLike): Output Concatenated Binary file."
    },
    {
      "chunk_id": "oasislmf/pytools/kat/manager.py::parquet_concat_unsorted@383",
      "source_type": "code",
      "path": "oasislmf/pytools/kat/manager.py",
      "symbol_type": "function",
      "name": "parquet_concat_unsorted",
      "lineno": 383,
      "end_lineno": 402,
      "business_stage": "other",
      "docstring": "Concats Parquet files in order they are passed in.\nArgs:\n    file_paths (List[str | os.PathLike]): List of parquet file paths.\n    out_file (str | os.PathLike): Output Concatenated Parquet file.",
      "content": "# File: oasislmf/pytools/kat/manager.py\n# function: parquet_concat_unsorted (lines 383-402)\n\ndef parquet_concat_unsorted(\n    file_paths,\n    out_file,\n):\n    \"\"\"Concats Parquet files in order they are passed in.\n    Args:\n        file_paths (List[str | os.PathLike]): List of parquet file paths.\n        out_file (str | os.PathLike): Output Concatenated Parquet file.\n    \"\"\"\n    writer = None\n    for fp in file_paths:\n        pq_file = pq.ParquetFile(fp)\n        for rg in range(pq_file.num_row_groups):\n            table = pq_file.read_row_group(rg)\n            if writer is None:\n                writer = pq.ParquetWriter(out_file, table.schema)\n            writer.write_table(table)\n\n    if writer:\n        writer.close()\n\n\"\"\"Docstring (excerpt)\"\"\"\nConcats Parquet files in order they are passed in.\nArgs:\n    file_paths (List[str | os.PathLike]): List of parquet file paths.\n    out_file (str | os.PathLike): Output Concatenated Parquet file."
    },
    {
      "chunk_id": "oasislmf/pytools/kat/manager.py::parquet_kway_merge@405",
      "source_type": "code",
      "path": "oasislmf/pytools/kat/manager.py",
      "symbol_type": "function",
      "name": "parquet_kway_merge",
      "lineno": 405,
      "end_lineno": 485,
      "business_stage": "other",
      "docstring": "Merge sorted chunks using a k-way merge algorithm\nArgs:\n    file_paths (List[str | os.PathLike]): List of parquet file paths.\n    keys (List[str]): List of keys to sort by\n    chunk_size (int): Chunk size for reading parquet files. Defaults to 100000.\nYields:\n    buffer (pa.Table): yields sorted pyarrow table from input files",
      "content": "# File: oasislmf/pytools/kat/manager.py\n# function: parquet_kway_merge (lines 405-485)\n\ndef parquet_kway_merge(\n    file_paths,\n    keys,\n    chunk_size=100000,\n):\n    \"\"\"Merge sorted chunks using a k-way merge algorithm\n    Args:\n        file_paths (List[str | os.PathLike]): List of parquet file paths.\n        keys (List[str]): List of keys to sort by\n        chunk_size (int): Chunk size for reading parquet files. Defaults to 100000.\n    Yields:\n        buffer (pa.Table): yields sorted pyarrow table from input files\n    \"\"\"\n    # Helper to read batches and manage current state for each file\n    class FileStream:\n        def __init__(self, path):\n            self.reader = pq.ParquetFile(str(path)).iter_batches(batch_size=chunk_size)\n            self.chunk = None\n            self.table = None\n            self.index = 0\n            self._load_next_chunk()\n\n        def _load_next_chunk(self):\n            try:\n                self.chunk = next(self.reader)\n                self.table = self.chunk.to_pydict()\n                self.index = 0\n            except StopIteration:\n                self.chunk = None\n                self.table = None\n\n        def has_data(self):\n            return self.table is not None\n\n        def current_key(self, keys):\n            return tuple(self.table[k][self.index] for k in keys)\n\n        def current_row(self):\n            return {k: self.table[k][self.index] for k in self.table}\n\n        def advance(self):\n            self.index += 1\n            if self.index >= len(next(iter(self.table.values()))):\n                self._load_next_chunk()\n\n    streams = [FileStream(fp) for fp in file_paths]\n    heap = [\n        (stream.current_key(keys), i)\n        for i, stream in enumerate(streams) if stream.has_data()\n    ]\n    heapq.heapify(heap)\n\n    buffer = {}\n    schema = None\n\n    while heap:\n        _, i = heapq.heappop(heap)\n        stream = streams[i]\n\n        # Initialize schema and buffer\n        if schema is None:\n            schema = stream.chunk.schema\n            buffer = {name: [] for name in schema.names}\n\n        # Append current row to buffer\n        row = stream.current_row()\n        for col in buffer:\n            buffer[col].append(row[col])\n\n        stream.advance()\n        if stream.has_data():\n            heapq.heappush(heap, (stream.current_key(keys), i))\n\n        # Output buffer if full\n        if len(buffer[keys[0]]) >= chunk_size:\n            yield pa.table(buffer, schema=schema)\n            buffer = {name: [] for name in schema.names}\n\n    # Yield remaining buffer\n    if buffer and buffer[keys[0]]:\n\n\"\"\"Docstring (excerpt)\"\"\"\nMerge sorted chunks using a k-way merge algorithm\nArgs:\n    file_paths (List[str | os.PathLike]): List of parquet file paths.\n    keys (List[str]): List of keys to sort by\n    chunk_size (int): Chunk size for reading parquet files. Defaults to 100000.\nYields:\n    buffer (pa.Table): yields sorted pyarrow table from input files"
    },
    {
      "chunk_id": "oasislmf/pytools/kat/manager.py::parquet_concat_sorted@488",
      "source_type": "code",
      "path": "oasislmf/pytools/kat/manager.py",
      "symbol_type": "function",
      "name": "parquet_concat_sorted",
      "lineno": 488,
      "end_lineno": 516,
      "business_stage": "other",
      "docstring": "Concats Parquet files in order determined out_type and their respective merge functions\nArgs:\n    file_paths (List[str | os.PathLike]): List of parquet file paths.\n    out_type (str): Out type str between \"elt\" and \"plt\"\n    out_file (str | os.PathLike): Output Concatenated Parquet file.\n    chunk_size (int): Chunk size for reading parquet files. Defaults to 100000.",
      "content": "# File: oasislmf/pytools/kat/manager.py\n# function: parquet_concat_sorted (lines 488-516)\n\ndef parquet_concat_sorted(\n    file_paths,\n    out_type,\n    out_file,\n    chunk_size=100000,\n):\n    \"\"\"Concats Parquet files in order determined out_type and their respective merge functions\n    Args:\n        file_paths (List[str | os.PathLike]): List of parquet file paths.\n        out_type (str): Out type str between \"elt\" and \"plt\"\n        out_file (str | os.PathLike): Output Concatenated Parquet file.\n        chunk_size (int): Chunk size for reading parquet files. Defaults to 100000.\n    \"\"\"\n    if out_type == \"elt\":\n        keys = [\"EventId\"]\n    elif out_type == \"plt\":\n        keys = [\"EventId\", \"Period\"]\n    else:\n        raise RuntimeError(f\"Unknown out_type: {out_type}\")\n\n    writer = None\n\n    for table in parquet_kway_merge(file_paths, keys, chunk_size):\n        if writer is None:\n            writer = pq.ParquetWriter(out_file, table.schema)\n        writer.write_table(table)\n\n    if writer:\n        writer.close()\n\n\"\"\"Docstring (excerpt)\"\"\"\nConcats Parquet files in order determined out_type and their respective merge functions\nArgs:\n    file_paths (List[str | os.PathLike]): List of parquet file paths.\n    out_type (str): Out type str between \"elt\" and \"plt\"\n    out_file (str | os.PathLike): Output Concatenated Parquet file.\n    chunk_size (int): Chunk size for reading parquet files. Defaults to 100000."
    },
    {
      "chunk_id": "oasislmf/pytools/kat/manager.py::run@519",
      "source_type": "code",
      "path": "oasislmf/pytools/kat/manager.py",
      "symbol_type": "function",
      "name": "run",
      "lineno": 519,
      "end_lineno": 711,
      "business_stage": "other",
      "docstring": "Concatenate CSV files (optionally sorted)\nArgs:\n    out_file (str | os.PathLike): Output Concatenated CSV file.\n    file_type (str, optional): Input file type suffix, if not discernible from input files. Defaults to None.\n    files_in (List[str | os.PathLike], optional): Individual CSV file paths to concatenate. Defaults to None.\n    dir_in (str | os.PathLike, optional): Path to the directory containing files for concatenation. Defaults to None.\n    concatenate_selt (bool, optional): Concatenate SELT CSV file. Defaults to False.\n    concatenate_melt (bool, optional): Concatenate MELT CSV file. Defaults to False.\n    concatenate_qelt (bool, optional): Concatenate QELT CSV file. Defaults to False.\n    concatenate_splt (bool, optional): Concatenate SPLT CSV file. Defaults to False.\n    concatenate_mplt (bool, optional): Concatenate MPLT CSV file. Defaults to False.\n    concatenate_qplt (bool, optional): Concatenate QPLT CSV file. Defaults to False.\n    unsorted (bool, optional): Do not sort by event/period ID. Defaults to False.",
      "content": "# File: oasislmf/pytools/kat/manager.py\n# function: run (lines 519-711)\n\ndef run(\n    out_file,\n    file_type=None,\n    files_in=None,\n    dir_in=None,\n    concatenate_selt=False,\n    concatenate_melt=False,\n    concatenate_qelt=False,\n    concatenate_splt=False,\n    concatenate_mplt=False,\n    concatenate_qplt=False,\n    unsorted=False,\n):\n    \"\"\"Concatenate CSV files (optionally sorted)\n    Args:\n        out_file (str | os.PathLike): Output Concatenated CSV file.\n        file_type (str, optional): Input file type suffix, if not discernible from input files. Defaults to None.\n        files_in (List[str | os.PathLike], optional): Individual CSV file paths to concatenate. Defaults to None.\n        dir_in (str | os.PathLike, optional): Path to the directory containing files for concatenation. Defaults to None.\n        concatenate_selt (bool, optional): Concatenate SELT CSV file. Defaults to False.\n        concatenate_melt (bool, optional): Concatenate MELT CSV file. Defaults to False.\n        concatenate_qelt (bool, optional): Concatenate QELT CSV file. Defaults to False.\n        concatenate_splt (bool, optional): Concatenate SPLT CSV file. Defaults to False.\n        concatenate_mplt (bool, optional): Concatenate MPLT CSV file. Defaults to False.\n        concatenate_qplt (bool, optional): Concatenate QPLT CSV file. Defaults to False.\n        unsorted (bool, optional): Do not sort by event/period ID. Defaults to False.\n    \"\"\"\n    input_files = []\n\n    # Check and add files from dir_in\n    if dir_in:\n        dir_in = Path(dir_in)\n        if not dir_in.exists():\n            raise FileNotFoundError(f\"ERROR: Directory \\'{dir_in}\\' does not exist\")\n        if not dir_in.is_dir():\n            raise ValueError(f\"ERROR: \\'{dir_in}\\' is not a directory.\")\n\n        dir_csv_input_files = glob.glob(str(dir_in / \"*.csv\"))\n        dir_bin_input_files = glob.glob(str(dir_in / \"*.bin\"))\n        dir_parquet_input_files = glob.glob(str(dir_in / \"*.parquet\"))\n        if not dir_csv_input_files and not dir_bin_input_files and not dir_parquet_input_files:\n            logger.warning(f\"Warning: No valid files found in directory \\'{dir_in}\\'\")\n        input_files += [Path(file).resolve() for file in dir_csv_input_files + dir_bin_input_files + dir_parquet_input_files]\n\n    input_files.sort()\n\n    # Check and add files from files_in\n    if files_in:\n        for file in files_in:\n            path = Path(file).resolve()\n            if not path.exists():\n                raise FileNotFoundError(f\"ERROR: File \\'{path}\\' does not exist.\")\n            if not path.is_file():\n                raise FileNotFoundError(f\"ERROR: File \\'{path}\\' is not a valid file.\")\n            input_files.append(path)\n\n    if not input_files:\n        raise RuntimeError(\"ERROR: katpy has no input CSV files to join\")\n\n    out_file = Path(out_file).resolve()\n    input_type = check_file_extensions(input_files)\n    output_type = out_file.suffix\n\n    if input_type == \"\":\n        if not file_type:\n            raise RuntimeError(\"ERROR: katpy, no discernible file type suffix found from input files, please provide a file_type\")\n        input_type = \".\" + file_type\n\n    # If out_file is a csv and input_files are not csvs, then output to temporary outfile\n    # of type input_type, and convert to csv after\n    bin_to_csv = False\n    bin_to_parquet = False\n    if output_type != input_type:\n        if input_type == \".bin\":\n            if output_type not in [\".csv\", \".parquet\"]:\n                raise RuntimeError(f\"ERROR: katpy does not support concatenating input files of type {input_type} to output type {output_type}\")\n            final_out_file_path = out_file\n            temp_file = tempfile.NamedTemporaryFile(suffix=input_type, delete=False)\n            out_file = Path(temp_file.name)\n            bin_to_csv = output_type == \".csv\"\n\n\"\"\"Docstring (excerpt)\"\"\"\nConcatenate CSV files (optionally sorted)\nArgs:\n    out_file (str | os.PathLike): Output Concatenated CSV file.\n    file_type (str, optional): Input file type suffix, if not discernible from input files. Defaults to None.\n    files_in (List[str | os.PathLike], optional): Individual CSV file paths to concatenate. Defaults to None.\n    dir_in (str | os.PathLike, optional): Path to the directory containing files for concatenation. Defaults to None.\n    concatenate_selt (bool, optional): Concatenate SELT CSV file. Defaults to False.\n    concatenate_melt (bool, optional): Concatenate MELT CSV file. Defaults to False.\n    concatenate_qelt (bool, optional): Concatenate QELT CSV file. Defaults to False.\n    concatenate_splt (bool, optional): Concatenate SPLT CSV file. Defaults to False.\n    concatenate_mplt (bool, optional): Concatenate MPLT CSV file. Defaults to False.\n    concatenate_qplt (bool, optional): Concatenate QPLT CSV file. Defaults to False.\n    unsorted (bool, optional): Do not sort by event/period ID. Defaults to False."
    },
    {
      "chunk_id": "oasislmf/pytools/lec/manager.py::read_input_files@26",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/manager.py",
      "symbol_type": "function",
      "name": "read_input_files",
      "lineno": 26,
      "end_lineno": 75,
      "business_stage": "other",
      "docstring": "Reads all input files and returns a dict of relevant data\nArgs:\n    run_dir (str | os.PathLike): Path to directory containing required files structure\n    use_return_period (bool): Use Return Period file.\n    agg_wheatsheaf_mean (bool): Aggregate Wheatsheaf Mean.\n    occ_wheatsheaf_mean (bool): Occurrence Wheatsheaf Mean.\n    sample_size (int): Sample Size.\nReturns:\n    file_data (Dict[str, Any]): A dict of relevent data extracted from files\n    use_return_period (bool): Use Return Period file.\n    agg_wheatsheaf_mean (bool): Aggregate Wheatsheaf Mean.\n    occ_wheatsheaf_mean (bool): Occurrence Wheatsheaf Mean.",
      "content": "# File: oasislmf/pytools/lec/manager.py\n# function: read_input_files (lines 26-75)\n\ndef read_input_files(\n    run_dir,\n    use_return_period,\n    agg_wheatsheaf_mean,\n    occ_wheatsheaf_mean,\n    sample_size\n):\n    \"\"\"Reads all input files and returns a dict of relevant data\n    Args:\n        run_dir (str | os.PathLike): Path to directory containing required files structure\n        use_return_period (bool): Use Return Period file.\n        agg_wheatsheaf_mean (bool): Aggregate Wheatsheaf Mean.\n        occ_wheatsheaf_mean (bool): Occurrence Wheatsheaf Mean.\n        sample_size (int): Sample Size.\n    Returns:\n        file_data (Dict[str, Any]): A dict of relevent data extracted from files\n        use_return_period (bool): Use Return Period file.\n        agg_wheatsheaf_mean (bool): Aggregate Wheatsheaf Mean.\n        occ_wheatsheaf_mean (bool): Occurrence Wheatsheaf Mean.\n    \"\"\"\n    input_dir = Path(run_dir, \"input\")\n    occ_map, date_algorithm, granular_date, no_of_periods = read_occurrence(input_dir)\n    period_weights = read_periods(no_of_periods, input_dir)\n    periods_fp = Path(input_dir, PERIODS_FILE)\n    if not periods_fp.exists():\n        period_weights = np.array([], dtype=period_weights.dtype)\n    else:  # Normalise period weights\n        period_weights[\"weighting\"] /= sample_size\n    returnperiods, use_return_period = read_returnperiods(use_return_period, input_dir)\n\n    # User must define return periods if he/she wishes to use non-uniform period weights for\n    # Wheatsheaf/per sample mean output\n    if agg_wheatsheaf_mean or occ_wheatsheaf_mean:\n        if len(period_weights) > 0 and not use_return_period:\n            logger.warning(\"WARNING: Return periods file must be present if you wish to use non-uniform period weights for Wheatsheaf mean/per sample mean output.\")\n            logger.info(\"INFO: Wheatsheaf mean/per sample mean output will not be produced.\")\n            agg_wheatsheaf_mean = False\n            occ_wheatsheaf_mean = False\n        elif len(period_weights) > 0:\n            logger.info(\"INFO: Tail Value at Risk for Wheatsheaf mean/per sample mean is not supported if you wish to use non-uniform period weights.\")\n\n    file_data = {\n        \"occ_map\": occ_map,\n        \"date_algorithm\": date_algorithm,\n        \"granular_date\": granular_date,\n        \"no_of_periods\": no_of_periods,\n        \"period_weights\": period_weights,\n        \"returnperiods\": returnperiods,\n    }\n    return file_data, use_return_period, agg_wheatsheaf_mean, occ_wheatsheaf_mean\n\n\"\"\"Docstring (excerpt)\"\"\"\nReads all input files and returns a dict of relevant data\nArgs:\n    run_dir (str | os.PathLike): Path to directory containing required files structure\n    use_return_period (bool): Use Return Period file.\n    agg_wheatsheaf_mean (bool): Aggregate Wheatsheaf Mean.\n    occ_wheatsheaf_mean (bool): Occurrence Wheatsheaf Mean.\n    sample_size (int): Sample Size.\nReturns:\n    file_data (Dict[str, Any]): A dict of relevent data extracted from files\n    use_return_period (bool): Use Return Period file.\n    agg_wheatsheaf_mean (bool): Aggregate Wheatsheaf Mean.\n    occ_wheatsheaf_mean (bool): Occurrence Wheatsheaf Mean."
    },
    {
      "chunk_id": "oasislmf/pytools/lec/manager.py::get_max_summary_id@79",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/manager.py",
      "symbol_type": "function",
      "name": "get_max_summary_id",
      "lineno": 79,
      "end_lineno": 103,
      "business_stage": "other",
      "docstring": "Get max summary_id from all summary files\nArgs:\n    file_handles (List[np.memmap]): List of memmaps for summary files data\nReturns:\n    max_summary_id (int): Max summary ID",
      "content": "# File: oasislmf/pytools/lec/manager.py\n# function: get_max_summary_id (lines 79-103)\n\ndef get_max_summary_id(file_handles):\n    \"\"\"Get max summary_id from all summary files\n    Args:\n        file_handles (List[np.memmap]): List of memmaps for summary files data\n    Returns:\n        max_summary_id (int): Max summary ID\n    \"\"\"\n    max_summary_id = -1\n    for fin in file_handles:\n        cursor = oasis_int_size * 3\n\n        valid_buff = len(fin)\n        while cursor < valid_buff:\n            _, cursor = mv_read(fin, cursor, oasis_int, oasis_int_size)\n            summary_id, cursor = mv_read(fin, cursor, oasis_int, oasis_int_size)\n            _, cursor = mv_read(fin, cursor, oasis_float, oasis_float_size)\n\n            max_summary_id = max(max_summary_id, summary_id)\n\n            while cursor < valid_buff:\n                sidx, cursor = mv_read(fin, cursor, oasis_int, oasis_int_size)\n                _, cursor = mv_read(fin, cursor, oasis_float, oasis_float_size)\n                if sidx == 0:\n                    break\n    return max_summary_id\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet max summary_id from all summary files\nArgs:\n    file_handles (List[np.memmap]): List of memmaps for summary files data\nReturns:\n    max_summary_id (int): Max summary ID"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/manager.py::do_lec_output_agg_summary@107",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/manager.py",
      "symbol_type": "function",
      "name": "do_lec_output_agg_summary",
      "lineno": 107,
      "end_lineno": 139,
      "business_stage": "other",
      "docstring": "Populate outloss_mean and outloss_sample with aggregate and max losses\nArgs:\n    summary_id (oasis_int): summary_id\n    sidx (oasis_int): Sample ID\n    loss (oasis_float): Loss value\n    filtered_occ_map (ndarray[occ_map_dtype]): Filtered numpy map of event_id, period_no, occ_date_id from the occurrence file_\n    outloss_mean (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, period_no containing aggregate and max losses\n    outloss_sample (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, sidx, period_no containing aggregate and max losses\n    num_sidxs (int): Number of sidxs to consider for outloss_sample\n    max_summary_id (int): Max summary ID",
      "content": "# File: oasislmf/pytools/lec/manager.py\n# function: do_lec_output_agg_summary (lines 107-139)\n\ndef do_lec_output_agg_summary(\n    summary_id,\n    sidx,\n    loss,\n    filtered_occ_map,\n    outloss_mean,\n    outloss_sample,\n    num_sidxs,\n    max_summary_id,\n):\n    \"\"\"Populate outloss_mean and outloss_sample with aggregate and max losses\n    Args:\n        summary_id (oasis_int): summary_id\n        sidx (oasis_int): Sample ID\n        loss (oasis_float): Loss value\n        filtered_occ_map (ndarray[occ_map_dtype]): Filtered numpy map of event_id, period_no, occ_date_id from the occurrence file_\n        outloss_mean (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, period_no containing aggregate and max losses\n        outloss_sample (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, sidx, period_no containing aggregate and max losses\n        num_sidxs (int): Number of sidxs to consider for outloss_sample\n        max_summary_id (int): Max summary ID\n    \"\"\"\n    for row in filtered_occ_map:\n        period_no = row[\"period_no\"]\n        if sidx == MEAN_IDX:\n            idx = get_outloss_mean_idx(period_no, summary_id, max_summary_id)\n            outloss = outloss_mean\n        else:\n            idx = get_outloss_sample_idx(period_no, sidx, summary_id, num_sidxs, max_summary_id)\n            outloss = outloss_sample\n        outloss[idx][\"row_used\"] = True\n        outloss[idx][\"agg_out_loss\"] += loss\n        max_out_loss = max(outloss[idx][\"max_out_loss\"], loss)\n        outloss[idx][\"max_out_loss\"] = max_out_loss\n\n\"\"\"Docstring (excerpt)\"\"\"\nPopulate outloss_mean and outloss_sample with aggregate and max losses\nArgs:\n    summary_id (oasis_int): summary_id\n    sidx (oasis_int): Sample ID\n    loss (oasis_float): Loss value\n    filtered_occ_map (ndarray[occ_map_dtype]): Filtered numpy map of event_id, period_no, occ_date_id from the occurrence file_\n    outloss_mean (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, period_no containing aggregate and max losses\n    outloss_sample (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, sidx, period_no containing aggregate and max losses\n    num_sidxs (int): Number of sidxs to consider for outloss_sample\n    max_summary_id (int): Max summary ID"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/manager.py::process_input_file@143",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/manager.py",
      "symbol_type": "function",
      "name": "process_input_file",
      "lineno": 143,
      "end_lineno": 203,
      "business_stage": "other",
      "docstring": "Process summary file and populate outloss_mean and outloss_sample with losses\nArgs:\n    fin (np.memmap): summary binary memmap\n    outloss_mean (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, period_no containing aggregate and max losses\n    outloss_sample (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, sidx, period_no containing aggregate and max losses\n    summary_ids (ndarray[bool]): bool array marking which summary_ids are used\n    occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file\n    use_return_period (bool): Use Return Period file.\n    num_sidxs (int): Number of sidxs to consider for outloss_sample\n    max_summary_id (int): Max summary ID",
      "content": "# File: oasislmf/pytools/lec/manager.py\n# function: process_input_file (lines 143-203)\n\ndef process_input_file(\n    fin,\n    outloss_mean,\n    outloss_sample,\n    summary_ids,\n    occ_map,\n    use_return_period,\n    num_sidxs,\n    max_summary_id,\n):\n    \"\"\"Process summary file and populate outloss_mean and outloss_sample with losses\n    Args:\n        fin (np.memmap): summary binary memmap\n        outloss_mean (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, period_no containing aggregate and max losses\n        outloss_sample (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, sidx, period_no containing aggregate and max losses\n        summary_ids (ndarray[bool]): bool array marking which summary_ids are used\n        occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file\n        use_return_period (bool): Use Return Period file.\n        num_sidxs (int): Number of sidxs to consider for outloss_sample\n        max_summary_id (int): Max summary ID\n    \"\"\"\n    # Set cursor to end of stream header (stream_type, sample_size, summary_set_id)\n    cursor = oasis_int_size * 3\n\n    # Read all samples\n    valid_buff = len(fin)\n    while cursor < valid_buff:\n        event_id, cursor = mv_read(fin, cursor, oasis_int, oasis_int_size)\n        summary_id, cursor = mv_read(fin, cursor, oasis_int, oasis_int_size)\n        expval, cursor = mv_read(fin, cursor, oasis_float, oasis_float_size)\n\n        # Discard samples if event_id not found\n        if event_id not in occ_map:\n            while cursor < valid_buff:\n                sidx, cursor = mv_read(fin, cursor, oasis_int, oasis_int_size)\n                _, cursor = mv_read(fin, cursor, oasis_float, oasis_float_size)\n                if sidx == 0:\n                    break\n            continue\n\n        filtered_occ_map = occ_map[event_id]\n        summary_ids[summary_id - 1] = True\n\n        while cursor < valid_buff:\n            sidx, cursor = mv_read(fin, cursor, oasis_int, oasis_int_size)\n            loss, cursor = mv_read(fin, cursor, oasis_float, oasis_float_size)\n            if sidx == 0:\n                break\n            if sidx == NUMBER_OF_AFFECTED_RISK_IDX or sidx == MAX_LOSS_IDX:\n                continue\n            if loss > 0 or use_return_period:\n                do_lec_output_agg_summary(\n                    summary_id,\n                    sidx,\n                    loss,\n                    filtered_occ_map,\n                    outloss_mean,\n                    outloss_sample,\n                    num_sidxs,\n                    max_summary_id,\n                )\n\n\"\"\"Docstring (excerpt)\"\"\"\nProcess summary file and populate outloss_mean and outloss_sample with losses\nArgs:\n    fin (np.memmap): summary binary memmap\n    outloss_mean (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, period_no containing aggregate and max losses\n    outloss_sample (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, sidx, period_no containing aggregate and max losses\n    summary_ids (ndarray[bool]): bool array marking which summary_ids are used\n    occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file\n    use_return_period (bool): Use Return Period file.\n    num_sidxs (int): Number of sidxs to consider for outloss_sample\n    max_summary_id (int): Max summary ID"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/manager.py::run_lec@207",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/manager.py",
      "symbol_type": "function",
      "name": "run_lec",
      "lineno": 207,
      "end_lineno": 238,
      "business_stage": "other",
      "docstring": "Process each summary file and populate outloss_mean and outloss_sample\nArgs:\n    file_handles (List[np.memmap]): List of memmaps for summary files data\n    outloss_mean (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, period_no containing aggregate and max losses\n    outloss_sample (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, sidx, period_no containing aggregate and max losses\n    summary_ids (ndarray[bool]): bool array marking which summary_ids are used\n    occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file\n    use_return_period (bool): Use Return Period file.\n    num_sidxs (int): Number of sidxs to consider for outloss_sample\n    max_summary_id (int): Max summary ID",
      "content": "# File: oasislmf/pytools/lec/manager.py\n# function: run_lec (lines 207-238)\n\ndef run_lec(\n    file_handles,\n    outloss_mean,\n    outloss_sample,\n    summary_ids,\n    occ_map,\n    use_return_period,\n    num_sidxs,\n    max_summary_id,\n):\n    \"\"\"Process each summary file and populate outloss_mean and outloss_sample\n    Args:\n        file_handles (List[np.memmap]): List of memmaps for summary files data\n        outloss_mean (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, period_no containing aggregate and max losses\n        outloss_sample (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, sidx, period_no containing aggregate and max losses\n        summary_ids (ndarray[bool]): bool array marking which summary_ids are used\n        occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file\n        use_return_period (bool): Use Return Period file.\n        num_sidxs (int): Number of sidxs to consider for outloss_sample\n        max_summary_id (int): Max summary ID\n    \"\"\"\n    for fin in file_handles:\n        process_input_file(\n            fin,\n            outloss_mean,\n            outloss_sample,\n            summary_ids,\n            occ_map,\n            use_return_period,\n            num_sidxs,\n            max_summary_id,\n        )\n\n\"\"\"Docstring (excerpt)\"\"\"\nProcess each summary file and populate outloss_mean and outloss_sample\nArgs:\n    file_handles (List[np.memmap]): List of memmaps for summary files data\n    outloss_mean (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, period_no containing aggregate and max losses\n    outloss_sample (ndarray[OUTLOSS_DTYPE]): ndarray indexed by summary_id, sidx, period_no containing aggregate and max losses\n    summary_ids (ndarray[bool]): bool array marking which summary_ids are used\n    occ_map (nb.typed.Dict): numpy map of event_id, period_no, occ_date_id from the occurrence file\n    use_return_period (bool): Use Return Period file.\n    num_sidxs (int): Number of sidxs to consider for outloss_sample\n    max_summary_id (int): Max summary ID"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/manager.py::run@241",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/manager.py",
      "symbol_type": "function",
      "name": "run",
      "lineno": 241,
      "end_lineno": 473,
      "business_stage": "other",
      "docstring": "Runs LEC calculations\nArgs:\n    run_dir (str | os.PathLike): Path to directory containing required files structure\n    subfolder (str): Workspace subfolder inside <run_dir>/work/<subfolder>\n    ept_output_file (str, optional): Path to EPT output file. Defaults to None\n    psept_output_file (str, optional): Path to PSEPT output file. Defaults to None\n    agg_full_uncertainty (bool, optional): Aggregate Full Uncertainty. Defaults to False.\n    agg_wheatsheaf (bool, optional): Aggregate Wheatsheaf. Defaults to False.\n    agg_sample_mean (bool, optional): Aggregate Sample Mean. Defaults to False.\n    agg_wheatsheaf_mean (bool, optional): Aggregate Wheatsheaf Mean. Defaults to False.\n    occ_full_uncertainty (bool, optional): Occurrence Full Uncertainty. Defaults to False.\n    occ_wheatsheaf (bool, optional): Occurrence Wheatsheaf. Defaults to False.\n    occ_sample_mean (bool, optional): Occurrence Sample Mean. Defaults to False.\n    occ_wheatsheaf_mean (bool, optional): Occurrence Wheatsheaf Mean. Defaults to False.\n    use_return_period (bool, optional): Use Return Period file. Defaults to False.\n    noheader (bool): Boolean value to skip header in output file\n    output_format (str): Output format extension. Defaults to \"csv\".",
      "content": "# File: oasislmf/pytools/lec/manager.py\n# function: run (lines 241-473)\n\ndef run(\n    run_dir,\n    subfolder,\n    ept_output_file=None,\n    psept_output_file=None,\n    agg_full_uncertainty=False,\n    agg_wheatsheaf=False,\n    agg_sample_mean=False,\n    agg_wheatsheaf_mean=False,\n    occ_full_uncertainty=False,\n    occ_wheatsheaf=False,\n    occ_sample_mean=False,\n    occ_wheatsheaf_mean=False,\n    use_return_period=False,\n    noheader=False,\n    output_format=\"csv\",\n):\n    \"\"\"Runs LEC calculations\n    Args:\n        run_dir (str | os.PathLike): Path to directory containing required files structure\n        subfolder (str): Workspace subfolder inside <run_dir>/work/<subfolder>\n        ept_output_file (str, optional): Path to EPT output file. Defaults to None\n        psept_output_file (str, optional): Path to PSEPT output file. Defaults to None\n        agg_full_uncertainty (bool, optional): Aggregate Full Uncertainty. Defaults to False.\n        agg_wheatsheaf (bool, optional): Aggregate Wheatsheaf. Defaults to False.\n        agg_sample_mean (bool, optional): Aggregate Sample Mean. Defaults to False.\n        agg_wheatsheaf_mean (bool, optional): Aggregate Wheatsheaf Mean. Defaults to False.\n        occ_full_uncertainty (bool, optional): Occurrence Full Uncertainty. Defaults to False.\n        occ_wheatsheaf (bool, optional): Occurrence Wheatsheaf. Defaults to False.\n        occ_sample_mean (bool, optional): Occurrence Sample Mean. Defaults to False.\n        occ_wheatsheaf_mean (bool, optional): Occurrence Wheatsheaf Mean. Defaults to False.\n        use_return_period (bool, optional): Use Return Period file. Defaults to False.\n        noheader (bool): Boolean value to skip header in output file\n        output_format (str): Output format extension. Defaults to \"csv\".\n    \"\"\"\n    outmap = {\n        \"ept\": {\n            \"compute\": ept_output_file is not None,\n            \"file_path\": ept_output_file,\n            \"fmt\": EPT_fmt,\n            \"headers\": EPT_headers,\n            \"file\": None,\n            \"dtype\": EPT_dtype,\n        },\n        \"psept\": {\n            \"compute\": psept_output_file is not None,\n            \"file_path\": psept_output_file,\n            \"fmt\": PSEPT_fmt,\n            \"headers\": PSEPT_headers,\n            \"file\": None,\n            \"dtype\": PSEPT_dtype,\n        },\n    }\n\n    output_format = \".\" + output_format\n    output_binary = output_format == \".bin\"\n    output_parquet = output_format == \".parquet\"\n    # Check for correct suffix\n    for path in [v[\"file_path\"] for v in outmap.values()]:\n        if path is None:\n            continue\n        if Path(path).suffix == \"\":  # Ignore suffix for pipes\n            continue\n        if (Path(path).suffix != output_format):\n            raise ValueError(f\"Invalid file extension for {output_format}, got {path},\")\n\n    if not all([v[\"compute\"] for v in outmap.values()]):\n        logger.warning(\"No output files specified\")\n\n    with ExitStack() as stack:\n        workspace_folder = Path(run_dir, \"work\", subfolder)\n        if not workspace_folder.is_dir():\n            raise RuntimeError(f\"Error: Unable to open directory {workspace_folder}\")\n\n        # work folder for lec files\n        lec_files_folder = Path(workspace_folder, \"lec_files\")\n        lec_files_folder.mkdir(parents=False, exist_ok=True)\n\n        # Find summary binary files\n        files = [file for file in workspace_folder.glob(\"*.bin\")]\n\n\"\"\"Docstring (excerpt)\"\"\"\nRuns LEC calculations\nArgs:\n    run_dir (str | os.PathLike): Path to directory containing required files structure\n    subfolder (str): Workspace subfolder inside <run_dir>/work/<subfolder>\n    ept_output_file (str, optional): Path to EPT output file. Defaults to None\n    psept_output_file (str, optional): Path to PSEPT output file. Defaults to None\n    agg_full_uncertainty (bool, optional): Aggregate Full Uncertainty. Defaults to False.\n    agg_wheatsheaf (bool, optional): Aggregate Wheatsheaf. Defaults to False.\n    agg_sample_mean (bool, optional): Aggregate Sample Mean. Defaults to False.\n    agg_wheatsheaf_mean (bool, optional): Aggregate Wheatsheaf Mean. Defaults to False.\n    occ_full_uncertainty (bool, optional): Occurrence Full Uncertainty. Defaults to False.\n    occ_wheatsheaf (bool, optional): Occurrence Wheatsheaf. Defaults to False.\n    occ_sample_mean (bool, optional): Occurrence Sample Mean. Defaults to False.\n    occ_wheatsheaf_mean (bool, optional): Occurrence Wheatsheaf Mean. Defaults to False.\n    use_return_period (bool, optional): Use Return Period file. Defaults to False.\n    noheader (bool): Boolean value to skip header in output file\n    output_format (str): Output format extension. Defaults to \"csv\"."
    },
    {
      "chunk_id": "oasislmf/pytools/lec/aggreports/aggreports.py::output_mean_damage_ratio@66",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/aggreports/aggreports.py",
      "symbol_type": "function",
      "name": "output_mean_damage_ratio",
      "lineno": 66,
      "end_lineno": 134,
      "business_stage": "aggregation",
      "docstring": "Output Mean Damage Ratio\nMean Damage Losses - This means do the loss calculation for a year using the event mean\ndamage loss computed by numerical integration of the effective damageability distributions.\nArgs:\n    eptype (int): Exceedance Probability Type\n    eptype_tvar (int): Exceedance Probability Type (Tail Value at Risk)\n    outloss_type (string): Which loss to output",
      "content": "# File: oasislmf/pytools/lec/aggreports/aggreports.py\n# function: output_mean_damage_ratio (lines 66-134)\n\n    def output_mean_damage_ratio(self, eptype, eptype_tvar, outloss_type):\n        \"\"\"Output Mean Damage Ratio\n        Mean Damage Losses - This means do the loss calculation for a year using the event mean\n        damage loss computed by numerical integration of the effective damageability distributions.\n        Args:\n            eptype (int): Exceedance Probability Type\n            eptype_tvar (int): Exceedance Probability Type (Tail Value at Risk)\n            outloss_type (string): Which loss to output\n        \"\"\"\n        epcalc = MEANDR\n\n        # Get row indices that are used\n        row_used_indices = np.where(self.outloss_mean[\"row_used\"])[0]\n\n        # Allocate storage for the flat data array\n        items_fp = Path(self.lec_files_folder, f\"lec_mean_damage_ratio-{outloss_type}-items.bdat\")\n        items = np.memmap(items_fp, dtype=LOSSVEC2MAP_dtype, mode=\"w+\", shape=(len(row_used_indices),))\n        # Track start and end indices for each summary_id\n        items_start_end = np.full((self.max_summary_id, 2), -1, dtype=np.int32)\n\n        # Select the correct outloss values based on type\n        # Required if-else condition as njit cannot resolve outloss_type inside []\n        if outloss_type == \"agg_out_loss\":\n            outloss_vals = self.outloss_mean[\"agg_out_loss\"]\n        elif outloss_type == \"max_out_loss\":\n            outloss_vals = self.outloss_mean[\"max_out_loss\"]\n        else:\n            raise ValueError(f\"Error: Unknown outloss_type: {outloss_type}\")\n\n        # Populate items and items_start_end\n        has_weights, used_period_no = output_mean_damage_ratio(\n            items,\n            items_start_end,\n            row_used_indices,\n            outloss_vals,\n            self.period_weights,\n            self.max_summary_id,\n        )\n        unused_period_weights = self.period_weights[~used_period_no]\n\n        if has_weights:\n            gen = write_ept_weighted(\n                items,\n                items_start_end,\n                self.sample_size,\n                epcalc,\n                eptype,\n                eptype_tvar,\n                unused_period_weights,\n                self.use_return_period,\n                self.returnperiods,\n                self.max_summary_id\n            )\n        else:\n            gen = write_ept(\n                items,\n                items_start_end,\n                self.no_of_periods,\n                epcalc,\n                eptype,\n                eptype_tvar,\n                self.use_return_period,\n                self.returnperiods,\n                self.max_summary_id\n\n            )\n\n        for data in gen:\n            self.output_data(data, \"ept\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nOutput Mean Damage Ratio\nMean Damage Losses - This means do the loss calculation for a year using the event mean\ndamage loss computed by numerical integration of the effective damageability distributions.\nArgs:\n    eptype (int): Exceedance Probability Type\n    eptype_tvar (int): Exceedance Probability Type (Tail Value at Risk)\n    outloss_type (string): Which loss to output"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/aggreports/aggreports.py::output_full_uncertainty@136",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/aggreports/aggreports.py",
      "symbol_type": "function",
      "name": "output_full_uncertainty",
      "lineno": 136,
      "end_lineno": 204,
      "business_stage": "aggregation",
      "docstring": "Output Full Uncertainty\nFull Uncertainty  this means do the calculation across all samples (treating the samples\neffectively as repeat years) - this is the most accurate of all the single EP Curves.\nArgs:\n    eptype (int): Exceedance Probability Type\n    eptype_tvar (int): Exceedance Probability Type (Tail Value at Risk)\n    outloss_type (string): Which loss to output",
      "content": "# File: oasislmf/pytools/lec/aggreports/aggreports.py\n# function: output_full_uncertainty (lines 136-204)\n\n    def output_full_uncertainty(self, eptype, eptype_tvar, outloss_type):\n        \"\"\"Output Full Uncertainty\n        Full Uncertainty  this means do the calculation across all samples (treating the samples\n        effectively as repeat years) - this is the most accurate of all the single EP Curves.\n        Args:\n            eptype (int): Exceedance Probability Type\n            eptype_tvar (int): Exceedance Probability Type (Tail Value at Risk)\n            outloss_type (string): Which loss to output\n        \"\"\"\n        epcalc = FULL\n\n        # Get row indices that are used\n        row_used_indices = np.where(self.outloss_sample[\"row_used\"])[0]\n\n        # Allocate storage for the flat data array\n        items_fp = Path(self.lec_files_folder, f\"lec_full_uncertainty-{outloss_type}-items.bdat\")\n        items = np.memmap(items_fp, dtype=LOSSVEC2MAP_dtype, mode=\"w+\", shape=(len(row_used_indices),))\n        # Track start and end indices for each summary_id\n        items_start_end = np.full((self.max_summary_id, 2), -1, dtype=np.int32)\n\n        # Select the correct outloss values based on type\n        # Required if-else condition as njit cannot resolve outloss_type inside []\n        if outloss_type == \"agg_out_loss\":\n            outloss_vals = self.outloss_sample[\"agg_out_loss\"]\n        elif outloss_type == \"max_out_loss\":\n            outloss_vals = self.outloss_sample[\"max_out_loss\"]\n        else:\n            raise ValueError(f\"Error: Unknown outloss_type: {outloss_type}\")\n\n        # Populate items and items_start_end\n        has_weights, used_period_no = output_full_uncertainty(\n            items,\n            items_start_end,\n            row_used_indices,\n            outloss_vals,\n            self.period_weights,\n            self.max_summary_id,\n            self.num_sidxs,\n        )\n        unused_period_weights = self.period_weights[~used_period_no]\n\n        if has_weights:\n            gen = write_ept_weighted(\n                items,\n                items_start_end,\n                1,\n                epcalc,\n                eptype,\n                eptype_tvar,\n                unused_period_weights,\n                self.use_return_period,\n                self.returnperiods,\n                self.max_summary_id\n            )\n        else:\n            gen = write_ept(\n                items,\n                items_start_end,\n                self.no_of_periods * self.sample_size,\n                epcalc,\n                eptype,\n                eptype_tvar,\n                self.use_return_period,\n                self.returnperiods,\n                self.max_summary_id\n            )\n\n        for data in gen:\n            self.output_data(data, \"ept\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nOutput Full Uncertainty\nFull Uncertainty  this means do the calculation across all samples (treating the samples\neffectively as repeat years) - this is the most accurate of all the single EP Curves.\nArgs:\n    eptype (int): Exceedance Probability Type\n    eptype_tvar (int): Exceedance Probability Type (Tail Value at Risk)\n    outloss_type (string): Which loss to output"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/aggreports/aggreports.py::output_wheatsheaf_and_wheatsheafmean@206",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/aggreports/aggreports.py",
      "symbol_type": "function",
      "name": "output_wheatsheaf_and_wheatsheafmean",
      "lineno": 206,
      "end_lineno": 350,
      "business_stage": "aggregation",
      "docstring": "Output Wheatsheaf and Wheatsheaf Mean\nWheatsheaf, Per Sample EPT (PSEPT)  this means calculate the EP Curve for each sample and\nleave it at the sample level of detail, resulting in multiple curves.\nWheatsheaf Mean, Per Sample mean EPT  this means average the loss at each return period of\nthe Per Sample EPT.\nArgs:\n    eptype (int): Exceedance Probability Type\n    eptype_tvar (int): Exceedance Probability Type (Tail Value at Risk)\n    outloss_type (string): Which loss to output\n    output_wheatsheaf (bool): Bool to Output Wheatsheaf\n    output_wheatsheaf_mean (bool): Bool to Output Wheatsheaf Mean",
      "content": "# File: oasislmf/pytools/lec/aggreports/aggreports.py\n# function: output_wheatsheaf_and_wheatsheafmean (lines 206-350)\n\n    def output_wheatsheaf_and_wheatsheafmean(self, eptype, eptype_tvar, outloss_type, output_wheatsheaf, output_wheatsheaf_mean):\n        \"\"\"Output Wheatsheaf and Wheatsheaf Mean\n        Wheatsheaf, Per Sample EPT (PSEPT)  this means calculate the EP Curve for each sample and\n        leave it at the sample level of detail, resulting in multiple curves.\n        Wheatsheaf Mean, Per Sample mean EPT  this means average the loss at each return period of\n        the Per Sample EPT.\n        Args:\n            eptype (int): Exceedance Probability Type\n            eptype_tvar (int): Exceedance Probability Type (Tail Value at Risk)\n            outloss_type (string): Which loss to output\n            output_wheatsheaf (bool): Bool to Output Wheatsheaf\n            output_wheatsheaf_mean (bool): Bool to Output Wheatsheaf Mean\n        \"\"\"\n        epcalc = PERSAMPLEMEAN\n\n        # Get row indices that are used\n        row_used_indices = np.where(self.outloss_sample[\"row_used\"])[0]\n\n        wheatsheaf_items_file = Path(self.lec_files_folder, f\"lec_wheatsheaf-items-{outloss_type}.bdat\")\n        wheatsheaf_items = np.memmap(\n            wheatsheaf_items_file,\n            dtype=WHEATKEYITEMS_dtype,\n            mode=\"w+\",\n            shape=(len(row_used_indices)),\n        )\n        # Track start and end indices for each summary_id and sidx\n        wheatsheaf_items_start_end = np.full((self.max_summary_id * self.num_sidxs, 2), -1, dtype=np.int32)\n\n        # Select the correct outloss values based on type\n        # Required if-else condition as njit cannot resolve outloss_type inside []\n        if outloss_type == \"agg_out_loss\":\n            outloss_vals = self.outloss_sample[\"agg_out_loss\"]\n        elif outloss_type == \"max_out_loss\":\n            outloss_vals = self.outloss_sample[\"max_out_loss\"]\n        else:\n            raise ValueError(f\"Error: Unknown outloss_type: {outloss_type}\")\n\n        # Populate wheatsheaf_items and wheatsheaf_items_start_end\n        has_weights, used_period_no = fill_wheatsheaf_items(\n            wheatsheaf_items,\n            wheatsheaf_items_start_end,\n            row_used_indices,\n            outloss_vals,\n            self.period_weights,\n            self.max_summary_id,\n            self.num_sidxs,\n        )\n        unused_period_weights = self.period_weights[~used_period_no]\n\n        if has_weights:\n            mean_map = None\n\n            if output_wheatsheaf_mean:\n                mean_map_file = Path(self.lec_files_folder, f\"lec_wheatsheaf_mean-map-{outloss_type}.bdat\")\n                mean_map = np.memmap(\n                    mean_map_file,\n                    dtype=MEANMAP_dtype,\n                    mode=\"w+\",\n                    shape=(self.max_summary_id, len(self.returnperiods)),\n                )\n\n            if output_wheatsheaf:\n                gen = write_psept_weighted(\n                    wheatsheaf_items,\n                    wheatsheaf_items_start_end,\n                    self.no_of_periods,\n                    eptype,\n                    eptype_tvar,\n                    unused_period_weights,\n                    self.use_return_period,\n                    self.returnperiods,\n                    self.max_summary_id,\n                    self.num_sidxs,\n                    self.sample_size,\n                    mean_map=mean_map,\n                )\n                for data in gen:\n                    self.output_data(data, \"psept\")\n\n            if output_wheatsheaf_mean:\n\n\"\"\"Docstring (excerpt)\"\"\"\nOutput Wheatsheaf and Wheatsheaf Mean\nWheatsheaf, Per Sample EPT (PSEPT)  this means calculate the EP Curve for each sample and\nleave it at the sample level of detail, resulting in multiple curves.\nWheatsheaf Mean, Per Sample mean EPT  this means average the loss at each return period of\nthe Per Sample EPT.\nArgs:\n    eptype (int): Exceedance Probability Type\n    eptype_tvar (int): Exceedance Probability Type (Tail Value at Risk)\n    outloss_type (string): Which loss to output\n    output_wheatsheaf (bool): Bool to Output Wheatsheaf\n    output_wheatsheaf_mean (bool): Bool to Output Wheatsheaf Mean"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/aggreports/aggreports.py::output_sample_mean@352",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/aggreports/aggreports.py",
      "symbol_type": "function",
      "name": "output_sample_mean",
      "lineno": 352,
      "end_lineno": 449,
      "business_stage": "aggregation",
      "docstring": "Output Sample Mean\nSample Mean Losses  this means do the loss calculation for a year using the statistical\nsample event mean.\nArgs:\n    eptype (int): Exceedance Probability Type\n    eptype_tvar (int): Exceedance Probability Type (Tail Value at Risk)\n    outloss_type (string): Which loss to output",
      "content": "# File: oasislmf/pytools/lec/aggreports/aggreports.py\n# function: output_sample_mean (lines 352-449)\n\n    def output_sample_mean(self, eptype, eptype_tvar, outloss_type):\n        \"\"\"Output Sample Mean\n        Sample Mean Losses  this means do the loss calculation for a year using the statistical\n        sample event mean.\n        Args:\n            eptype (int): Exceedance Probability Type\n            eptype_tvar (int): Exceedance Probability Type (Tail Value at Risk)\n            outloss_type (string): Which loss to output\n        \"\"\"\n        if self.sample_size == 0:\n            logger.warning(\"aggreports.output_sample_mean, self.sample_size is 0, not outputting any sample mean\")\n            return\n        epcalc = MEANSAMPLE\n\n        # outloss_sample has all SIDXs plus -2 and -3\n        reordered_outlosses_file = Path(self.lec_files_folder, f\"lec_sample_mean-reordered_outlosses-{outloss_type}.bdat\")\n        reordered_outlosses = np.memmap(\n            reordered_outlosses_file,\n            dtype=np.dtype([\n                (\"row_used\", np.bool_),\n                (\"value\", oasis_float),\n            ]),\n            mode=\"w+\",\n            shape=(self.no_of_periods * self.max_summary_id),\n        )\n\n        # Select the correct outloss values based on type\n        # Required if-else condition as njit cannot resolve outloss_type inside []\n        if outloss_type == \"agg_out_loss\":\n            outloss_vals = self.outloss_sample[\"agg_out_loss\"]\n        elif outloss_type == \"max_out_loss\":\n            outloss_vals = self.outloss_sample[\"max_out_loss\"]\n        else:\n            raise ValueError(f\"Error: Unknown outloss_type: {outloss_type}\")\n\n        # Get row indices that are used\n        row_used_indices = np.where(self.outloss_sample[\"row_used\"])[0]\n\n        # Reorder outlosses by summary_id and period_no\n        reorder_losses_by_summary_and_period(\n            reordered_outlosses,\n            row_used_indices,\n            outloss_vals,\n            self.max_summary_id,\n            self.no_of_periods,\n            self.num_sidxs,\n            self.sample_size,\n        )\n\n        # Get row indices that are used\n        row_used_indices = np.where(reordered_outlosses[\"row_used\"])[0]\n\n        # Allocate storage for the flat data array\n        items_fp = Path(self.lec_files_folder, f\"lec_sample_mean-{outloss_type}-items.bdat\")\n        items = np.memmap(items_fp, dtype=LOSSVEC2MAP_dtype, mode=\"w+\", shape=(len(row_used_indices),))\n        # Track start and end indices for each summary_id\n        items_start_end = np.full((self.max_summary_id, 2), -1, dtype=np.int32)\n\n        # Populate items and items_start_end\n        has_weights, used_period_no = output_sample_mean(\n            items,\n            items_start_end,\n            row_used_indices,\n            reordered_outlosses[\"value\"],\n            self.period_weights,\n            self.max_summary_id,\n            self.no_of_periods,\n        )\n        unused_period_weights = self.period_weights[~used_period_no]\n\n        if has_weights:\n            gen = write_ept_weighted(\n                items,\n                items_start_end,\n                self.sample_size,\n                epcalc,\n                eptype,\n                eptype_tvar,\n                unused_period_weights,\n                self.use_return_period,\n\n\"\"\"Docstring (excerpt)\"\"\"\nOutput Sample Mean\nSample Mean Losses  this means do the loss calculation for a year using the statistical\nsample event mean.\nArgs:\n    eptype (int): Exceedance Probability Type\n    eptype_tvar (int): Exceedance Probability Type (Tail Value at Risk)\n    outloss_type (string): Which loss to output"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/aggreports/write_tables.py::get_loss@10",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/aggreports/write_tables.py",
      "symbol_type": "function",
      "name": "get_loss",
      "lineno": 10,
      "end_lineno": 39,
      "business_stage": "aggregation",
      "docstring": "Get loss based on current and next return period\nArgs:\n    next_retperiod (float): Next return period\n    last_retperiod (float): Previous return period\n    last_loss (float): Previous Loss value\n    curr_retperiod (float): Current return period\n    curr_loss (float): Current Loss value\nReturns:\n    loss (float): Loss Value",
      "content": "# File: oasislmf/pytools/lec/aggreports/write_tables.py\n# function: get_loss (lines 10-39)\n\ndef get_loss(\n    next_retperiod,\n    last_retperiod,\n    last_loss,\n    curr_retperiod,\n    curr_loss\n):\n    \"\"\"Get loss based on current and next return period\n    Args:\n        next_retperiod (float): Next return period\n        last_retperiod (float): Previous return period\n        last_loss (float): Previous Loss value\n        curr_retperiod (float): Current return period\n        curr_loss (float): Current Loss value\n    Returns:\n        loss (float): Loss Value\n    \"\"\"\n    if curr_retperiod == 0:\n        return 0\n    if curr_loss == 0:\n        return 0\n    if curr_retperiod == next_retperiod:\n        return curr_loss\n    if curr_retperiod < next_retperiod:\n        # Linear Interpolate\n        return (\n            (next_retperiod - curr_retperiod) * (last_loss - curr_loss) /\n            (last_retperiod - curr_retperiod) + curr_loss\n        )\n    return -1\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet loss based on current and next return period\nArgs:\n    next_retperiod (float): Next return period\n    last_retperiod (float): Previous return period\n    last_loss (float): Previous Loss value\n    curr_retperiod (float): Current return period\n    curr_loss (float): Current Loss value\nReturns:\n    loss (float): Loss Value"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/aggreports/write_tables.py::fill_tvar@43",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/aggreports/write_tables.py",
      "symbol_type": "function",
      "name": "fill_tvar",
      "lineno": 43,
      "end_lineno": 72,
      "business_stage": "aggregation",
      "docstring": "Populate the Tail with retperiod and tvar values for summary_id\nArgs:\n    tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of Summary ID to vector of (return period, tvar) values\n    tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of Summary ID to size of each tail array\n    summary_id (int): Summary ID\n    next_retperiod (float): Next Return Period\n    tvar (float): Tail Value at Risk\nReturns:\n    tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of summary_id to vector of (return period, tvar) values\n    tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of summary_id to size of each tail array",
      "content": "# File: oasislmf/pytools/lec/aggreports/write_tables.py\n# function: fill_tvar (lines 43-72)\n\ndef fill_tvar(\n    tail,\n    tail_sizes,\n    summary_id,\n    next_retperiod,\n    tvar\n):\n    \"\"\"Populate the Tail with retperiod and tvar values for summary_id\n    Args:\n        tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of Summary ID to vector of (return period, tvar) values\n        tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of Summary ID to size of each tail array\n        summary_id (int): Summary ID\n        next_retperiod (float): Next Return Period\n        tvar (float): Tail Value at Risk\n    Returns:\n        tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of summary_id to vector of (return period, tvar) values\n        tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of summary_id to size of each tail array\n    \"\"\"\n    if summary_id not in tail:\n        tail[summary_id] = create_empty_array(TAIL_valtype)\n        tail_sizes[summary_id] = 0\n    tail_arr = tail[summary_id]\n    tail_arr = resize_array(tail_arr, tail_sizes[summary_id])\n    tail_current_size = tail_sizes[summary_id]\n    tail_arr[tail_current_size][\"retperiod\"] = next_retperiod\n    tail_arr[tail_current_size][\"tvar\"] = tvar\n    tail[summary_id] = tail_arr\n    tail_sizes[summary_id] += 1\n\n    return tail, tail_sizes\n\n\"\"\"Docstring (excerpt)\"\"\"\nPopulate the Tail with retperiod and tvar values for summary_id\nArgs:\n    tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of Summary ID to vector of (return period, tvar) values\n    tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of Summary ID to size of each tail array\n    summary_id (int): Summary ID\n    next_retperiod (float): Next Return Period\n    tvar (float): Tail Value at Risk\nReturns:\n    tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of summary_id to vector of (return period, tvar) values\n    tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of summary_id to size of each tail array"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/aggreports/write_tables.py::fill_tvar_wheatsheaf@76",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/aggreports/write_tables.py",
      "symbol_type": "function",
      "name": "fill_tvar_wheatsheaf",
      "lineno": 76,
      "end_lineno": 110,
      "business_stage": "aggregation",
      "docstring": "Populate the Tail with retperiod and tvar values for (summary_id, sidx) pair\nArgs:\n    tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of (summary_id, sidx) pair to vector of (return period, tvar) values\n    tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of (summary_id, sidx) pair to size of each tail array\n    summary_id (int): Summary ID\n    sidx (int): Sample ID\n    num_sidxs (int): Number of sidxs to consider\n    next_retperiod (float): Next Return Period\n    tvar (float): Tail Value at Risk\nReturns:\n    tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of (summary_id, sidx) pair to vector of (return period, tvar) values\n    tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of (summary_id, sidx) pair to size of each tail array",
      "content": "# File: oasislmf/pytools/lec/aggreports/write_tables.py\n# function: fill_tvar_wheatsheaf (lines 76-110)\n\ndef fill_tvar_wheatsheaf(\n    tail,\n    tail_sizes,\n    summary_id,\n    sidx,\n    num_sidxs,\n    next_retperiod,\n    tvar\n):\n    \"\"\"Populate the Tail with retperiod and tvar values for (summary_id, sidx) pair\n    Args:\n        tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of (summary_id, sidx) pair to vector of (return period, tvar) values\n        tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of (summary_id, sidx) pair to size of each tail array\n        summary_id (int): Summary ID\n        sidx (int): Sample ID\n        num_sidxs (int): Number of sidxs to consider\n        next_retperiod (float): Next Return Period\n        tvar (float): Tail Value at Risk\n    Returns:\n        tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of (summary_id, sidx) pair to vector of (return period, tvar) values\n        tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of (summary_id, sidx) pair to size of each tail array\n    \"\"\"\n    idx = get_wheatsheaf_items_idx(summary_id, sidx, num_sidxs)\n    if idx not in tail:\n        tail[idx] = create_empty_array(TAIL_valtype)\n        tail_sizes[idx] = 0\n    tail_arr = tail[idx]\n    tail_arr = resize_array(tail_arr, tail_sizes[idx])\n    tail_current_size = tail_sizes[idx]\n    tail_arr[tail_current_size][\"retperiod\"] = next_retperiod\n    tail_arr[tail_current_size][\"tvar\"] = tvar\n    tail[idx] = tail_arr\n    tail_sizes[idx] += 1\n\n    return tail, tail_sizes\n\n\"\"\"Docstring (excerpt)\"\"\"\nPopulate the Tail with retperiod and tvar values for (summary_id, sidx) pair\nArgs:\n    tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of (summary_id, sidx) pair to vector of (return period, tvar) values\n    tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of (summary_id, sidx) pair to size of each tail array\n    summary_id (int): Summary ID\n    sidx (int): Sample ID\n    num_sidxs (int): Number of sidxs to consider\n    next_retperiod (float): Next Return Period\n    tvar (float): Tail Value at Risk\nReturns:\n    tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of (summary_id, sidx) pair to vector of (return period, tvar) values\n    tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of (summary_id, sidx) pair to size of each tail array"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/aggreports/write_tables.py::write_return_period_out@114",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/aggreports/write_tables.py",
      "symbol_type": "function",
      "name": "write_return_period_out",
      "lineno": 114,
      "end_lineno": 219,
      "business_stage": "aggregation",
      "docstring": "Processes return periods and computes losses for a given summary, updating TVaR and mean map if required.\nArgs:\n    next_returnperiod_idx (int): Index of the next return period to process.\n    last_computed_rp (float): Last computed return period\n    last_computed_loss (float): Last computed loss value\n    curr_retperiod (float): Current return period being processed.\n    curr_loss (float): Loss associated with the current return period.\n    summary_id (int): Identifier for the current summary.\n    eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n    epcalc (int): Type of exceedance probability calculation.\n    max_retperiod (int): Maximum return period to be used in calculations\n    counter (int): Counter used for updating TVaR\n    tvar (float): Tail Value at Risk\n    tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of summary_id or (summary_id, sidx) pair to vector of (return period, tvar) values\n    tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of summary_id or (summary_id, sidx) pair to size of each tail array\n    returnperiods (ndarray[np.int32]): Return Periods array\n    mean_map (ndarray[MEANMAP_dtype], optional): An array mapping used for mean loss calculations per Summary ID. Used for EPT output later. Defaults to None.\n    is_wheatsheaf (bool, optional): If True, update the wheatsheaf TVaR structure.\n    num_sidxs (int, optional): Number of sidxs to consider. Defaults to -1 if not is_wheatsheaf.\nReturns:\n    rets (list[EPT_dtype]): Return period and Loss EPT data\n    tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of summary_id or (summary_id, sidx) pair to vector of (return period, tvar) values\n    tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of summary_id or (summary_id, sidx) pair to size of each tail array\n    last_computed_rp (float): Last computed return period\n    last_computed_loss (float): Last computed loss value",
      "content": "# File: oasislmf/pytools/lec/aggreports/write_tables.py\n# function: write_return_period_out (lines 114-219)\n\ndef write_return_period_out(\n    next_returnperiod_idx,\n    last_computed_rp,\n    last_computed_loss,\n    curr_retperiod,\n    curr_loss,\n    summary_id,\n    eptype,\n    epcalc,\n    max_retperiod,\n    counter,\n    tvar,\n    tail,\n    tail_sizes,\n    returnperiods,\n    mean_map=None,\n    is_wheatsheaf=False,\n    num_sidxs=-1,\n):\n    \"\"\"Processes return periods and computes losses for a given summary, updating TVaR and mean map if required.\n    Args:\n        next_returnperiod_idx (int): Index of the next return period to process.\n        last_computed_rp (float): Last computed return period\n        last_computed_loss (float): Last computed loss value\n        curr_retperiod (float): Current return period being processed.\n        curr_loss (float): Loss associated with the current return period.\n        summary_id (int): Identifier for the current summary.\n        eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n        epcalc (int): Type of exceedance probability calculation.\n        max_retperiod (int): Maximum return period to be used in calculations\n        counter (int): Counter used for updating TVaR\n        tvar (float): Tail Value at Risk\n        tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of summary_id or (summary_id, sidx) pair to vector of (return period, tvar) values\n        tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of summary_id or (summary_id, sidx) pair to size of each tail array\n        returnperiods (ndarray[np.int32]): Return Periods array\n        mean_map (ndarray[MEANMAP_dtype], optional): An array mapping used for mean loss calculations per Summary ID. Used for EPT output later. Defaults to None.\n        is_wheatsheaf (bool, optional): If True, update the wheatsheaf TVaR structure.\n        num_sidxs (int, optional): Number of sidxs to consider. Defaults to -1 if not is_wheatsheaf.\n    Returns:\n        rets (list[EPT_dtype]): Return period and Loss EPT data\n        tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of summary_id or (summary_id, sidx) pair to vector of (return period, tvar) values\n        tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of summary_id or (summary_id, sidx) pair to size of each tail array\n        last_computed_rp (float): Last computed return period\n        last_computed_loss (float): Last computed loss value\n    \"\"\"\n    next_retperiod = 0\n    rets = []\n    while True:\n        if next_returnperiod_idx >= len(returnperiods):\n            return rets, tail, tail_sizes, next_returnperiod_idx, last_computed_rp, last_computed_loss\n\n        next_retperiod = returnperiods[next_returnperiod_idx]\n\n        if curr_retperiod > next_retperiod:\n            break\n\n        if max_retperiod < next_retperiod:\n            next_returnperiod_idx += 1\n            continue\n\n        loss = get_loss(\n            next_retperiod,\n            last_computed_rp,\n            last_computed_loss,\n            curr_retperiod,\n            curr_loss\n        )\n\n        rets.append((summary_id, epcalc, eptype, next_retperiod, loss))\n\n        if mean_map is not None:\n            mean_map[summary_id - 1][next_returnperiod_idx][\"retperiod\"] = next_retperiod\n            mean_map[summary_id - 1][next_returnperiod_idx][\"mean\"] += loss\n            mean_map[summary_id - 1][next_returnperiod_idx][\"count\"] += 1\n\n        if curr_retperiod != 0:\n            tvar = tvar - ((tvar - loss) / counter)\n            if is_wheatsheaf:\n                tail, tail_sizes = fill_tvar_wheatsheaf(\n                    tail,\n\n\"\"\"Docstring (excerpt)\"\"\"\nProcesses return periods and computes losses for a given summary, updating TVaR and mean map if required.\nArgs:\n    next_returnperiod_idx (int): Index of the next return period to process.\n    last_computed_rp (float): Last computed return period\n    last_computed_loss (float): Last computed loss value\n    curr_retperiod (float): Current return period being processed.\n    curr_loss (float): Loss associated with the current return period.\n    summary_id (int): Identifier for the current summary.\n    eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n    epcalc (int): Type of exceedance probability calculation.\n    max_retperiod (int): Maximum return period to be used in calculations\n    counter (int): Counter used for updating TVaR\n    tvar (float): Tail Value at Risk\n    tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of summary_id or (summary_id, sidx) pair to vector of (return period, tvar) values\n    tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of summary_id or (summary_id, sidx) pair to size of each tail array\n    returnperiods (ndarray[np.int32]): Return Periods array\n    mean_map (ndarray[MEANMAP_dtype], optional): An array mapping used for mean loss calculations per Summary ID. Used for EPT output later. Defaults to None.\n    is_wheatsheaf (bool, optional): If True, update the wheatsheaf TVaR structure.\n    num_sidxs (int, optional): Number of sidxs to consider. Defaults to -1 if not is_wheatsheaf.\nReturns:\n    rets (list[EPT_dtype]): Return period and Loss EPT data\n    tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of summary_id or (summary_id, sidx) pair to vector of (return period, tvar) values\n    tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of summary_id or (summary_id, sidx) pair to size of each tail array\n    last_computed_rp (float): Last computed return period\n    last_computed_loss (float): Last computed loss value"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/aggreports/write_tables.py::write_tvar@223",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/aggreports/write_tables.py",
      "symbol_type": "function",
      "name": "write_tvar",
      "lineno": 223,
      "end_lineno": 243,
      "business_stage": "aggregation",
      "docstring": "Get TVaR values for EPT output from tail\nArgs:\n    epcalc (int): Type of exceedance probability calculation.\n    eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n    tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of summary_id to vector of (return period, tvar) values\n    tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of summary_id pair to size of each tail array\nReturns:\n    rets (list[EPT_dtype]): Return period and Loss EPT data",
      "content": "# File: oasislmf/pytools/lec/aggreports/write_tables.py\n# function: write_tvar (lines 223-243)\n\ndef write_tvar(\n    epcalc,\n    eptype_tvar,\n    tail,\n    tail_sizes,\n):\n    \"\"\"Get TVaR values for EPT output from tail\n    Args:\n        epcalc (int): Type of exceedance probability calculation.\n        eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n        tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of summary_id to vector of (return period, tvar) values\n        tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of summary_id pair to size of each tail array\n    Returns:\n        rets (list[EPT_dtype]): Return period and Loss EPT data\n    \"\"\"\n    rets = []\n    for summary_id in sorted(tail.keys()):\n        vals = tail[summary_id][:tail_sizes[summary_id]]\n        for row in vals:\n            rets.append((summary_id, epcalc, eptype_tvar, row[\"retperiod\"], row[\"tvar\"]))\n    return rets\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet TVaR values for EPT output from tail\nArgs:\n    epcalc (int): Type of exceedance probability calculation.\n    eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n    tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of summary_id to vector of (return period, tvar) values\n    tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of summary_id pair to size of each tail array\nReturns:\n    rets (list[EPT_dtype]): Return period and Loss EPT data"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/aggreports/write_tables.py::write_tvar_wheatsheaf@247",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/aggreports/write_tables.py",
      "symbol_type": "function",
      "name": "write_tvar_wheatsheaf",
      "lineno": 247,
      "end_lineno": 268,
      "business_stage": "aggregation",
      "docstring": "Get TVaR values for PSEPT output from tail\nArgs:\n    num_sidxs (int): Number of sidxs to consider.\n    eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n    tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of (summary_id, sidx) pair to vector of (return period, tvar) values\n    tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of (summary_id, sidx) pair to size of each tail array\nReturns:\n    rets (list[PSEPT_dtype]): Return period and Loss PSEPT data",
      "content": "# File: oasislmf/pytools/lec/aggreports/write_tables.py\n# function: write_tvar_wheatsheaf (lines 247-268)\n\ndef write_tvar_wheatsheaf(\n    num_sidxs,\n    eptype_tvar,\n    tail,\n    tail_sizes,\n):\n    \"\"\"Get TVaR values for PSEPT output from tail\n    Args:\n        num_sidxs (int): Number of sidxs to consider.\n        eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n        tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of (summary_id, sidx) pair to vector of (return period, tvar) values\n        tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of (summary_id, sidx) pair to size of each tail array\n    Returns:\n        rets (list[PSEPT_dtype]): Return period and Loss PSEPT data\n    \"\"\"\n    rets = []\n    for idx in sorted(tail.keys()):\n        sidx, summary_id = get_wheatsheaf_items_idx_data(idx, num_sidxs)\n        vals = tail[idx][:tail_sizes[idx]]\n        for row in vals:\n            rets.append((summary_id, sidx, eptype_tvar, row[\"retperiod\"], row[\"tvar\"]))\n    return rets\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet TVaR values for PSEPT output from tail\nArgs:\n    num_sidxs (int): Number of sidxs to consider.\n    eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n    tail (nb.typed.Dict[nb_oasis_int, NB_TAIL_valtype]): Dict of (summary_id, sidx) pair to vector of (return period, tvar) values\n    tail_sizes (nb.typed.Dict[nb_oasis_int, nb.types.int64]): Dict of (summary_id, sidx) pair to size of each tail array\nReturns:\n    rets (list[PSEPT_dtype]): Return period and Loss PSEPT data"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/aggreports/write_tables.py::write_ept@272",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/aggreports/write_tables.py",
      "symbol_type": "function",
      "name": "write_ept",
      "lineno": 272,
      "end_lineno": 437,
      "business_stage": "aggregation",
      "docstring": "Generate Loss Exceedance Curve values and Tail Value at Risk values based on items and epcalc/eptype/eptype_tvar\n\nThe loss calculation follows these principles:\n- For Aggregate Loss Exceedance Curves (AEP): The sum of all losses within a period is calculated.\n- For Occurrence Loss Exceedance Curves (OEP): The maximum loss within a period is taken.\n- TVAR (Tail Conditional Expectation): Calculated as the average of losses exceeding a given return period.\nArgs:\n    items (ndarray[LOSSVEC2MAP_dtype]): Array mapping summary_id to loss value (and period_no/period_weighting where applicable)\n    items_start_end (ndarray[np.int32]): An array marking where the start and end idxs are for each summary_id in the items array \n    max_retperiod (int): Maximum return period to be used in calculations\n    epcalc (int): Specifies the calculation method (mean damage loss, full uncertainty, per sample mean, sample mean).\n    eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n    eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n    use_return_period (bool): Use Return Period file.\n    returnperiods (ndarray[np.int32]): Return Periods array\n    max_summary_id (int): Maximum summary ID\n    sample_size (int, optional): Sample Size. Defaults to 1.\nYields:\n    buffer (ndarray[EPT_dtype]): Buffered chunks of EPT data",
      "content": "# File: oasislmf/pytools/lec/aggreports/write_tables.py\n# function: write_ept (lines 272-437)\n\ndef write_ept(\n    items,\n    items_start_end,\n    max_retperiod,\n    epcalc,\n    eptype,\n    eptype_tvar,\n    use_return_period,\n    returnperiods,\n    max_summary_id,\n    sample_size=1\n):\n    \"\"\"Generate Loss Exceedance Curve values and Tail Value at Risk values based on items and epcalc/eptype/eptype_tvar\n\n    The loss calculation follows these principles:\n    - For Aggregate Loss Exceedance Curves (AEP): The sum of all losses within a period is calculated.\n    - For Occurrence Loss Exceedance Curves (OEP): The maximum loss within a period is taken.\n    - TVAR (Tail Conditional Expectation): Calculated as the average of losses exceeding a given return period.\n    Args:\n        items (ndarray[LOSSVEC2MAP_dtype]): Array mapping summary_id to loss value (and period_no/period_weighting where applicable)\n        items_start_end (ndarray[np.int32]): An array marking where the start and end idxs are for each summary_id in the items array \n        max_retperiod (int): Maximum return period to be used in calculations\n        epcalc (int): Specifies the calculation method (mean damage loss, full uncertainty, per sample mean, sample mean).\n        eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n        eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n        use_return_period (bool): Use Return Period file.\n        returnperiods (ndarray[np.int32]): Return Periods array\n        max_summary_id (int): Maximum summary ID\n        sample_size (int, optional): Sample Size. Defaults to 1.\n    Yields:\n        buffer (ndarray[EPT_dtype]): Buffered chunks of EPT data\n    \"\"\"\n    buffer = np.zeros(DEFAULT_BUFFER_SIZE, dtype=EPT_dtype)\n    bidx = 0\n\n    if len(items) == 0 or sample_size == 0:\n        return\n\n    tail = nb.typed.Dict.empty(nb_oasis_int, NB_TAIL_valtype)\n    tail_sizes = nb.typed.Dict.empty(nb_oasis_int, nb.types.int64)\n\n    for summary_id in range(1, max_summary_id + 1):\n        start, end = items_start_end[summary_id - 1]\n        if start == -1:\n            continue\n        filtered_items = items[start:end]\n        sorted_idxs = np.argsort(filtered_items[\"value\"])[::-1]\n        sorted_items = filtered_items[sorted_idxs]\n        next_returnperiod_idx = 0\n        last_computed_rp = 0\n        last_computed_loss = 0\n        tvar = 0\n        i = 1\n        for item in sorted_items:\n            value = item[\"value\"] / sample_size\n            retperiod = max_retperiod / i\n\n            if use_return_period:\n                if next_returnperiod_idx < len(returnperiods):\n                    rets, tail, tail_sizes, next_returnperiod_idx, last_computed_rp, last_computed_loss = write_return_period_out(\n                        next_returnperiod_idx,\n                        last_computed_rp,\n                        last_computed_loss,\n                        retperiod,\n                        value,\n                        summary_id,\n                        eptype,\n                        epcalc,\n                        max_retperiod,\n                        i,\n                        tvar,\n                        tail,\n                        tail_sizes,\n                        returnperiods,\n                    )\n                    for ret in rets:\n                        if bidx >= len(buffer):\n                            yield buffer[:bidx]\n                            bidx = 0\n                        buffer[bidx][\"SummaryId\"] = ret[0]\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Loss Exceedance Curve values and Tail Value at Risk values based on items and epcalc/eptype/eptype_tvar\n\nThe loss calculation follows these principles:\n- For Aggregate Loss Exceedance Curves (AEP): The sum of all losses within a period is calculated.\n- For Occurrence Loss Exceedance Curves (OEP): The maximum loss within a period is taken.\n- TVAR (Tail Conditional Expectation): Calculated as the average of losses exceeding a given return period.\nArgs:\n    items (ndarray[LOSSVEC2MAP_dtype]): Array mapping summary_id to loss value (and period_no/period_weighting where applicable)\n    items_start_end (ndarray[np.int32]): An array marking where the start and end idxs are for each summary_id in the items array \n    max_retperiod (int): Maximum return period to be used in calculations\n    epcalc (int): Specifies the calculation method (mean damage loss, full uncertainty, per sample mean, sample mean).\n    eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n    eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n    use_return_period (bool): Use Return Period file.\n    returnperiods (ndarray[np.int32]): Return Periods array\n    max_summary_id (int): Maximum summary ID\n    sample_size (int, optional): Sample Size. Defaults to 1.\nYields:\n    buffer (ndarray[EPT_dtype]): Buffered chunks of EPT data"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/aggreports/write_tables.py::write_ept_weighted@441",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/aggreports/write_tables.py",
      "symbol_type": "function",
      "name": "write_ept_weighted",
      "lineno": 441,
      "end_lineno": 628,
      "business_stage": "aggregation",
      "docstring": "Generate Loss Exceedance Curve values and Tail Value at Risk values based on items and epcalc/eptype/eptype_tvar.\n\nThis function calculates weighted exceedance probability tables using cumulative period weightings (`period_weighting`), \nwhich impact the calculation of return periods. The weighting allows for more accurate representation of losses when \nevent periods have different probabilities or frequencies of occurrence.\n\nThe loss calculation follows these principles:\n- For Aggregate Loss Exceedance Curves (AEP): The sum of all losses within a period is calculated.\n- For Occurrence Loss Exceedance Curves (OEP): The maximum loss within a period is taken.\n- TVAR (Tail Conditional Expectation): Calculated as the average of losses exceeding a given return period.\nArgs:\n    items (ndarray[LOSSVEC2MAP_dtype]): Array mapping summary_id to loss value (and period_no/period_weighting where applicable)\n    items_start_end (ndarray[np.int32]): An array marking where the start and end idxs are for each summary_id in the items array \n    cum_weight_constant (float): Constant factor for scaling cumulative period weights.\n    epcalc (int): Specifies the calculation method (mean damage loss, full uncertainty, per sample mean, sample mean).\n    eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n    eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n    unused_period_weights (ndarray[float]): Array of unused period weights\n    use_return_period (bool): Use Return Period file.\n    returnperiods (ndarray[np.int32]): Return Periods array\n    max_summary_id (int): Maximum summary ID\n    sample_size (int, optional): Sample Size. Defaults to 1.\nYields:\n    buffer (ndarray[EPT_dtype]): Buffered chunks of EPT data",
      "content": "# File: oasislmf/pytools/lec/aggreports/write_tables.py\n# function: write_ept_weighted (lines 441-628)\n\ndef write_ept_weighted(\n    items,\n    items_start_end,\n    cum_weight_constant,\n    epcalc,\n    eptype,\n    eptype_tvar,\n    unused_period_weights,\n    use_return_period,\n    returnperiods,\n    max_summary_id,\n    sample_size=1\n):\n    \"\"\"Generate Loss Exceedance Curve values and Tail Value at Risk values based on items and epcalc/eptype/eptype_tvar.\n\n    This function calculates weighted exceedance probability tables using cumulative period weightings (`period_weighting`), \n    which impact the calculation of return periods. The weighting allows for more accurate representation of losses when \n    event periods have different probabilities or frequencies of occurrence.\n\n    The loss calculation follows these principles:\n    - For Aggregate Loss Exceedance Curves (AEP): The sum of all losses within a period is calculated.\n    - For Occurrence Loss Exceedance Curves (OEP): The maximum loss within a period is taken.\n    - TVAR (Tail Conditional Expectation): Calculated as the average of losses exceeding a given return period.\n    Args:\n        items (ndarray[LOSSVEC2MAP_dtype]): Array mapping summary_id to loss value (and period_no/period_weighting where applicable)\n        items_start_end (ndarray[np.int32]): An array marking where the start and end idxs are for each summary_id in the items array \n        cum_weight_constant (float): Constant factor for scaling cumulative period weights.\n        epcalc (int): Specifies the calculation method (mean damage loss, full uncertainty, per sample mean, sample mean).\n        eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n        eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n        unused_period_weights (ndarray[float]): Array of unused period weights\n        use_return_period (bool): Use Return Period file.\n        returnperiods (ndarray[np.int32]): Return Periods array\n        max_summary_id (int): Maximum summary ID\n        sample_size (int, optional): Sample Size. Defaults to 1.\n    Yields:\n        buffer (ndarray[EPT_dtype]): Buffered chunks of EPT data\n    \"\"\"\n    buffer = np.zeros(DEFAULT_BUFFER_SIZE, dtype=EPT_dtype)\n    bidx = 0\n\n    if len(items) == 0 or sample_size == 0:\n        return\n\n    tail = nb.typed.Dict.empty(nb_oasis_int, NB_TAIL_valtype)\n    tail_sizes = nb.typed.Dict.empty(nb_oasis_int, nb.types.int64)\n\n    for summary_id in range(1, max_summary_id + 1):\n        start, end = items_start_end[summary_id - 1]\n        if start == -1:\n            continue\n        filtered_items = items[start:end]\n        sorted_idxs = np.argsort(filtered_items[\"value\"])[::-1]\n        sorted_items = filtered_items[sorted_idxs]\n        next_returnperiod_idx = 0\n        last_computed_rp = 0\n        last_computed_loss = 0\n        tvar = 0\n        i = 1\n        cumulative_weighting = 0\n        max_retperiod = 0\n        largest_loss = False\n\n        for item in sorted_items:\n            value = item[\"value\"] / sample_size\n            cumulative_weighting += (item[\"period_weighting\"] * cum_weight_constant)\n            retperiod = max_retperiod / i\n\n            if item[\"period_weighting\"]:\n                retperiod = 1 / cumulative_weighting\n\n                if not largest_loss:\n                    max_retperiod = retperiod + 0.0001  # Add for floating point errors\n                    largest_loss = True\n\n                if use_return_period:\n                    if next_returnperiod_idx < len(returnperiods):\n                        rets, tail, tail_sizes, next_returnperiod_idx, last_computed_rp, last_computed_loss = write_return_period_out(\n                            next_returnperiod_idx,\n                            last_computed_rp,\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Loss Exceedance Curve values and Tail Value at Risk values based on items and epcalc/eptype/eptype_tvar.\n\nThis function calculates weighted exceedance probability tables using cumulative period weightings (`period_weighting`), \nwhich impact the calculation of return periods. The weighting allows for more accurate representation of losses when \nevent periods have different probabilities or frequencies of occurrence.\n\nThe loss calculation follows these principles:\n- For Aggregate Loss Exceedance Curves (AEP): The sum of all losses within a period is calculated.\n- For Occurrence Loss Exceedance Curves (OEP): The maximum loss within a period is taken.\n- TVAR (Tail Conditional Expectation): Calculated as the average of losses exceeding a given return period.\nArgs:\n    items (ndarray[LOSSVEC2MAP_dtype]): Array mapping summary_id to loss value (and period_no/period_weighting where applicable)\n    items_start_end (ndarray[np.int32]): An array marking where the start and end idxs are for each summary_id in the items array \n    cum_weight_constant (float): Constant factor for scaling cumulative period weights.\n    epcalc (int): Specifies the calculation method (mean damage loss, full uncertainty, per sample mean, sample mean).\n    eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n    eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n    unused_period_weights (ndarray[float]): Array of unused period weights\n    use_return_period (bool): Use Return Period file.\n    returnperiods (ndarray[np.int32]): Return Periods array\n    max_summary_id (int): Maximum summary ID\n    sample_size (int, optional): Sample Size. Defaults to 1.\nYields:\n    buffer (ndarray[EPT_dtype]): Buffered chunks of EPT data"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/aggreports/write_tables.py::write_psept@632",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/aggreports/write_tables.py",
      "symbol_type": "function",
      "name": "write_psept",
      "lineno": 632,
      "end_lineno": 796,
      "business_stage": "aggregation",
      "docstring": "Generate Per Sample Exceedance Probability Tables (PSEPT) for each individual sample, producing a separate loss\nexceedance curve for each sample, eptype, eptype_tvar.\nArgs:\n    items (ndarray[WHEATKEYITEMS_dtype]): Array mapping (summary_id, sidx) to loss value (and period_no/period_weighting where applicable)\n    items_start_end (ndarray[np.int32]): An array marking where the start and end idxs are for each (summary_id, sidx) pair in the items array \n    max_retperiod (int): Maximum return period to be used in calculations\n    eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n    eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n    use_return_period (bool): Use Return Period file.\n    returnperiods (ndarray[np.int32]): Return Periods array\n    max_summary_id (int): Maximum summary ID\n    num_sidxs (int): Number of sidxs to consider\nYields:\n    buffer (ndarray[PSEPT_dtype]): Buffered chunks of PSEPT data",
      "content": "# File: oasislmf/pytools/lec/aggreports/write_tables.py\n# function: write_psept (lines 632-796)\n\ndef write_psept(\n    items,\n    items_start_end,\n    max_retperiod,\n    eptype,\n    eptype_tvar,\n    use_return_period,\n    returnperiods,\n    max_summary_id,\n    num_sidxs\n):\n    \"\"\"Generate Per Sample Exceedance Probability Tables (PSEPT) for each individual sample, producing a separate loss\n    exceedance curve for each sample, eptype, eptype_tvar.\n    Args:\n        items (ndarray[WHEATKEYITEMS_dtype]): Array mapping (summary_id, sidx) to loss value (and period_no/period_weighting where applicable)\n        items_start_end (ndarray[np.int32]): An array marking where the start and end idxs are for each (summary_id, sidx) pair in the items array \n        max_retperiod (int): Maximum return period to be used in calculations\n        eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n        eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n        use_return_period (bool): Use Return Period file.\n        returnperiods (ndarray[np.int32]): Return Periods array\n        max_summary_id (int): Maximum summary ID\n        num_sidxs (int): Number of sidxs to consider\n    Yields:\n        buffer (ndarray[PSEPT_dtype]): Buffered chunks of PSEPT data\n    \"\"\"\n    buffer = np.zeros(DEFAULT_BUFFER_SIZE, dtype=PSEPT_dtype)\n    bidx = 0\n\n    if len(items) == 0:\n        return\n\n    tail = nb.typed.Dict.empty(nb_oasis_int, NB_TAIL_valtype)\n    tail_sizes = nb.typed.Dict.empty(nb_oasis_int, nb.types.int64)\n\n    for idx in range(max_summary_id * num_sidxs):\n        start, end = items_start_end[idx]\n        if start == -1:\n            continue\n        sidx, summary_id = get_wheatsheaf_items_idx_data(idx, num_sidxs)\n        filtered_items = items[start:end]\n        sorted_idxs = np.argsort(filtered_items[\"value\"])[::-1]\n        sorted_items = filtered_items[sorted_idxs]\n        next_returnperiod_idx = 0\n        last_computed_rp = 0\n        last_computed_loss = 0\n        tvar = 0\n        i = 1\n        for item in sorted_items:\n            value = item[\"value\"]\n            retperiod = max_retperiod / i\n\n            if use_return_period:\n                if next_returnperiod_idx < len(returnperiods):\n                    rets, tail, tail_sizes, next_returnperiod_idx, last_computed_rp, last_computed_loss = write_return_period_out(\n                        next_returnperiod_idx,\n                        last_computed_rp,\n                        last_computed_loss,\n                        retperiod,\n                        value,\n                        summary_id,\n                        eptype,\n                        sidx,\n                        max_retperiod,\n                        i,\n                        tvar,\n                        tail,\n                        tail_sizes,\n                        returnperiods,\n                        is_wheatsheaf=True,\n                        num_sidxs=num_sidxs,\n                    )\n                    for ret in rets:\n                        if bidx >= len(buffer):\n                            yield buffer[:bidx]\n                            bidx = 0\n                        buffer[bidx][\"SummaryId\"] = ret[0]\n                        buffer[bidx][\"SampleId\"] = ret[1]\n                        buffer[bidx][\"EPType\"] = ret[2]\n                        buffer[bidx][\"ReturnPeriod\"] = ret[3]\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Per Sample Exceedance Probability Tables (PSEPT) for each individual sample, producing a separate loss\nexceedance curve for each sample, eptype, eptype_tvar.\nArgs:\n    items (ndarray[WHEATKEYITEMS_dtype]): Array mapping (summary_id, sidx) to loss value (and period_no/period_weighting where applicable)\n    items_start_end (ndarray[np.int32]): An array marking where the start and end idxs are for each (summary_id, sidx) pair in the items array \n    max_retperiod (int): Maximum return period to be used in calculations\n    eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n    eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n    use_return_period (bool): Use Return Period file.\n    returnperiods (ndarray[np.int32]): Return Periods array\n    max_summary_id (int): Maximum summary ID\n    num_sidxs (int): Number of sidxs to consider\nYields:\n    buffer (ndarray[PSEPT_dtype]): Buffered chunks of PSEPT data"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/aggreports/write_tables.py::write_psept_weighted@800",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/aggreports/write_tables.py",
      "symbol_type": "function",
      "name": "write_psept_weighted",
      "lineno": 800,
      "end_lineno": 988,
      "business_stage": "aggregation",
      "docstring": "Generate Per Sample Exceedance Probability Tables (PSEPT) for each individual sample, producing a separate loss\nexceedance curve for each sample, eptype, eptype_tvar.\nArgs:\n    items (ndarray[WHEATKEYITEMS_dtype]): Array mapping (summary_id, sidx) to loss value (and period_no/period_weighting where applicable)\n    items_start_end (ndarray[np.int32]): An array marking where the start and end idxs are for each (summary_id, sidx) pair in the items array \n    max_retperiod (int): Maximum return period to be used in calculations\n    eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n    eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n    unused_period_weights (ndarray[float]): Array of unused period weights\n    use_return_period (bool): Use Return Period file.\n    returnperiods (ndarray[np.int32]): Return Periods array\n    max_summary_id (int): Maximum summary ID\n    num_sidxs (int): Number of sidxs to consider\n    sample_size (int): Sample Size. Defaults to 1.\n    mean_map (ndarray[MEANMAP_dtype], optional): An array mapping used for mean loss calculations per Summary ID. Used for EPT output later. Defaults to None.\nYields:\n    buffer (ndarray[PSEPT_dtype]): Buffered chunks of PSEPT data",
      "content": "# File: oasislmf/pytools/lec/aggreports/write_tables.py\n# function: write_psept_weighted (lines 800-988)\n\ndef write_psept_weighted(\n    items,\n    items_start_end,\n    max_retperiod,\n    eptype,\n    eptype_tvar,\n    unused_period_weights,\n    use_return_period,\n    returnperiods,\n    max_summary_id,\n    num_sidxs,\n    sample_size,\n    mean_map=None\n):\n    \"\"\"Generate Per Sample Exceedance Probability Tables (PSEPT) for each individual sample, producing a separate loss\n    exceedance curve for each sample, eptype, eptype_tvar.\n    Args:\n        items (ndarray[WHEATKEYITEMS_dtype]): Array mapping (summary_id, sidx) to loss value (and period_no/period_weighting where applicable)\n        items_start_end (ndarray[np.int32]): An array marking where the start and end idxs are for each (summary_id, sidx) pair in the items array \n        max_retperiod (int): Maximum return period to be used in calculations\n        eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n        eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n        unused_period_weights (ndarray[float]): Array of unused period weights\n        use_return_period (bool): Use Return Period file.\n        returnperiods (ndarray[np.int32]): Return Periods array\n        max_summary_id (int): Maximum summary ID\n        num_sidxs (int): Number of sidxs to consider\n        sample_size (int): Sample Size. Defaults to 1.\n        mean_map (ndarray[MEANMAP_dtype], optional): An array mapping used for mean loss calculations per Summary ID. Used for EPT output later. Defaults to None.\n    Yields:\n        buffer (ndarray[PSEPT_dtype]): Buffered chunks of PSEPT data\n    \"\"\"\n    buffer = np.zeros(DEFAULT_BUFFER_SIZE, dtype=PSEPT_dtype)\n    bidx = 0\n\n    if len(items) == 0:\n        return\n\n    tail = nb.typed.Dict.empty(nb_oasis_int, NB_TAIL_valtype)\n    tail_sizes = nb.typed.Dict.empty(nb_oasis_int, nb.types.int64)\n\n    for idx in range(max_summary_id * num_sidxs):\n        start, end = items_start_end[idx]\n        if start == -1:\n            continue\n        sidx, summary_id = get_wheatsheaf_items_idx_data(idx, num_sidxs)\n        filtered_items = items[start:end]\n        sorted_idxs = np.argsort(filtered_items[\"value\"])[::-1]\n        sorted_items = filtered_items[sorted_idxs]\n        next_returnperiod_idx = 0\n        last_computed_rp = 0\n        last_computed_loss = 0\n        tvar = 0\n        i = 1\n        cumulative_weighting = 0\n        max_retperiod = 0\n        largest_loss = False\n\n        for item in sorted_items:\n            value = item[\"value\"]\n            cumulative_weighting += (item[\"period_weighting\"] * sample_size)\n            retperiod = max_retperiod / i\n\n            if item[\"period_weighting\"]:\n                retperiod = 1 / cumulative_weighting\n\n                if not largest_loss:\n                    max_retperiod = retperiod + 0.0001  # Add for floating point errors\n                    largest_loss = True\n\n                if use_return_period:\n                    if next_returnperiod_idx < len(returnperiods):\n                        rets, tail, tail_sizes, next_returnperiod_idx, last_computed_rp, last_computed_loss = write_return_period_out(\n                            next_returnperiod_idx,\n                            last_computed_rp,\n                            last_computed_loss,\n                            retperiod,\n                            value,\n                            summary_id,\n                            eptype,\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Per Sample Exceedance Probability Tables (PSEPT) for each individual sample, producing a separate loss\nexceedance curve for each sample, eptype, eptype_tvar.\nArgs:\n    items (ndarray[WHEATKEYITEMS_dtype]): Array mapping (summary_id, sidx) to loss value (and period_no/period_weighting where applicable)\n    items_start_end (ndarray[np.int32]): An array marking where the start and end idxs are for each (summary_id, sidx) pair in the items array \n    max_retperiod (int): Maximum return period to be used in calculations\n    eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n    eptype_tvar (int): Type of Tail Value-at-Risk (TVAR) to calculate (0 = OEP TVAR, 1 = AEP TVAR).\n    unused_period_weights (ndarray[float]): Array of unused period weights\n    use_return_period (bool): Use Return Period file.\n    returnperiods (ndarray[np.int32]): Return Periods array\n    max_summary_id (int): Maximum summary ID\n    num_sidxs (int): Number of sidxs to consider\n    sample_size (int): Sample Size. Defaults to 1.\n    mean_map (ndarray[MEANMAP_dtype], optional): An array mapping used for mean loss calculations per Summary ID. Used for EPT output later. Defaults to None.\nYields:\n    buffer (ndarray[PSEPT_dtype]): Buffered chunks of PSEPT data"
    },
    {
      "chunk_id": "oasislmf/pytools/lec/aggreports/write_tables.py::write_wheatsheaf_mean@992",
      "source_type": "code",
      "path": "oasislmf/pytools/lec/aggreports/write_tables.py",
      "symbol_type": "function",
      "name": "write_wheatsheaf_mean",
      "lineno": 992,
      "end_lineno": 1027,
      "business_stage": "aggregation",
      "docstring": "Generate Wheatsheaf Mean Exceedance Probability Table (EPT) by averaging losses for each return period \nfrom a precomputed mean map.\nArgs:\n    mean_map (ndarray[MEANMAP_dtype]): An array mapping used for mean loss calculations per Summary ID.\n    epcalc (int): Specifies the calculation method (mean damage loss, full uncertainty, per sample mean, sample mean).\n    eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n    max_summary_id (int): Maximum summary ID\nYields:\n    buffer (ndarray[EPT_dtype]): Buffered chunks of EPT data",
      "content": "# File: oasislmf/pytools/lec/aggreports/write_tables.py\n# function: write_wheatsheaf_mean (lines 992-1027)\n\ndef write_wheatsheaf_mean(\n    mean_map,\n    eptype,\n    epcalc,\n    max_summary_id,\n):\n    \"\"\"Generate Wheatsheaf Mean Exceedance Probability Table (EPT) by averaging losses for each return period \n    from a precomputed mean map.\n    Args:\n        mean_map (ndarray[MEANMAP_dtype]): An array mapping used for mean loss calculations per Summary ID.\n        epcalc (int): Specifies the calculation method (mean damage loss, full uncertainty, per sample mean, sample mean).\n        eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n        max_summary_id (int): Maximum summary ID\n    Yields:\n        buffer (ndarray[EPT_dtype]): Buffered chunks of EPT data\n    \"\"\"\n    if len(mean_map) == 0:\n        return\n\n    buffer = np.zeros(DEFAULT_BUFFER_SIZE, dtype=EPT_dtype)\n    bidx = 0\n\n    for summary_id in range(1, max_summary_id + 1):\n        if np.sum(mean_map[summary_id - 1][\"count\"]) == 0:\n            continue\n        for mc in mean_map[summary_id - 1]:\n            if bidx >= len(buffer):\n                yield buffer[:bidx]\n                bidx = 0\n            buffer[bidx][\"SummaryId\"] = summary_id\n            buffer[bidx][\"EPCalc\"] = epcalc\n            buffer[bidx][\"EPType\"] = eptype\n            buffer[bidx][\"ReturnPeriod\"] = mc[\"retperiod\"]\n            buffer[bidx][\"Loss\"] = mc[\"mean\"] / max(mc[\"count\"], 1)\n            bidx += 1\n    yield buffer[:bidx]\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate Wheatsheaf Mean Exceedance Probability Table (EPT) by averaging losses for each return period \nfrom a precomputed mean map.\nArgs:\n    mean_map (ndarray[MEANMAP_dtype]): An array mapping used for mean loss calculations per Summary ID.\n    epcalc (int): Specifies the calculation method (mean damage loss, full uncertainty, per sample mean, sample mean).\n    eptype (int): Type of exceedance probability (0 = OEP, 1 = AEP).\n    max_summary_id (int): Maximum summary ID\nYields:\n    buffer (ndarray[EPT_dtype]): Buffered chunks of EPT data"
    },
    {
      "chunk_id": "oasislmf/pytools/pla/manager.py::run@17",
      "source_type": "code",
      "path": "oasislmf/pytools/pla/manager.py",
      "symbol_type": "function",
      "name": "run",
      "lineno": 17,
      "end_lineno": 65,
      "business_stage": "other",
      "docstring": "Execute the main Post Loss Amplification workflow.\n\nArgs:\n    run_dir (str): the directory of where the process is running\n    file_in (str): file name of input stream\n    file_out (str): file name of output streak\n    input_path (str): path to amplifications.bin\n    static_path (str): path to lossfactors.bin\n    secondary_factor (float): secondary factor to apply to post loss\n      amplification\n    uniform_factor (float): uniform factor to apply across all losses\n\nReturns:\n    0 (int): if no errors occurred",
      "content": "# File: oasislmf/pytools/pla/manager.py\n# function: run (lines 17-65)\n\ndef run(\n    run_dir, file_in, file_out, input_path, static_path, secondary_factor,\n    uniform_factor\n):\n    \"\"\"\n    Execute the main Post Loss Amplification workflow.\n\n    Args:\n        run_dir (str): the directory of where the process is running\n        file_in (str): file name of input stream\n        file_out (str): file name of output streak\n        input_path (str): path to amplifications.bin\n        static_path (str): path to lossfactors.bin\n        secondary_factor (float): secondary factor to apply to post loss\n          amplification\n        uniform_factor (float): uniform factor to apply across all losses\n\n    Returns:\n        0 (int): if no errors occurred\n    \"\"\"\n    input_path = os.path.join(run_dir, input_path)\n    model_storage = get_storage_from_config_path(\n        os.path.join(run_dir, 'model_storage.json'),\n        os.path.join(run_dir, static_path),\n    )\n\n    if uniform_factor > 0:\n        items_amps = None\n        plafactors = None\n    else:\n        items_amps = read_amplifications(input_path)\n        plafactors = get_post_loss_amplification_factors(model_storage, secondary_factor)\n\n    # Set default factor should post loss amplification factor be missing\n    default_factor = 1.0 if uniform_factor == 0.0 else uniform_factor\n\n    with ExitStack() as stack:\n        streams_in = get_streams_in(file_in, stack)\n\n        if file_out is None:\n            stream_out = sys.stdout.buffer\n        else:\n            stream_out = stack.enter_context(open(file_out, 'wb'))\n\n        read_and_write_streams(\n            streams_in, stream_out, items_amps, plafactors, default_factor\n        )\n\n    return 0\n\n\"\"\"Docstring (excerpt)\"\"\"\nExecute the main Post Loss Amplification workflow.\n\nArgs:\n    run_dir (str): the directory of where the process is running\n    file_in (str): file name of input stream\n    file_out (str): file name of output streak\n    input_path (str): path to amplifications.bin\n    static_path (str): path to lossfactors.bin\n    secondary_factor (float): secondary factor to apply to post loss\n      amplification\n    uniform_factor (float): uniform factor to apply across all losses\n\nReturns:\n    0 (int): if no errors occurred"
    },
    {
      "chunk_id": "oasislmf/pytools/pla/streams.py::read_buffer@13",
      "source_type": "code",
      "path": "oasislmf/pytools/pla/streams.py",
      "symbol_type": "function",
      "name": "read_buffer",
      "lineno": 13,
      "end_lineno": 62,
      "business_stage": "other",
      "docstring": "read the gul loss stream, apply the post loss amplification factor and load it into out_byte_mv buffer\nThis modified version of the read_buffer template return result when the whole input buffer is read and not when an event is read.\ntherefore it cannot be used to read multiple stream at a time because events would be mixed up.\n\nArgs:\n    byte_mv: input byte array\n    cursor: read cursor\n    valid_buff: valid part of input array\n    event_id: last event id\n    item_id: last item_id\n    items_amps (numpy array): amplification IDs where indexes correspond to item IDs\n    plafactors (dict): event ID and amplification ID pairs mapped to loss factors\n    default_factor (float): post loss reduction/amplification factor to be used if loss factor not found in plafactors\n    out_byte_mv: output byte arrau\n    out_cursor: single value array to store valid part of out_byte_mv",
      "content": "# File: oasislmf/pytools/pla/streams.py\n# function: read_buffer (lines 13-62)\n\ndef read_buffer(byte_mv, cursor, valid_buff, event_id, item_id, items_amps, plafactors, default_factor, out_byte_mv, out_cursor):\n    \"\"\"\n    read the gul loss stream, apply the post loss amplification factor and load it into out_byte_mv buffer\n    This modified version of the read_buffer template return result when the whole input buffer is read and not when an event is read.\n    therefore it cannot be used to read multiple stream at a time because events would be mixed up.\n\n    Args:\n        byte_mv: input byte array\n        cursor: read cursor\n        valid_buff: valid part of input array\n        event_id: last event id\n        item_id: last item_id\n        items_amps (numpy array): amplification IDs where indexes correspond to item IDs\n        plafactors (dict): event ID and amplification ID pairs mapped to loss factors\n        default_factor (float): post loss reduction/amplification factor to be used if loss factor not found in plafactors\n        out_byte_mv: output byte arrau\n        out_cursor: single value array to store valid part of out_byte_mv\n\n    \"\"\"\n    if item_id:\n        factor = plafactors.get((event_id, items_amps[item_id]), default_factor)\n    while True:\n        if item_id:\n            if valid_buff - cursor < (oasis_int_size + oasis_float_size):\n                break\n            sidx, cursor = mv_read(byte_mv, cursor, oasis_int, oasis_int_size)\n            if sidx:\n                loss, _ = mv_read(byte_mv, cursor, oasis_float, oasis_float_size)\n                loss = 0 if np.isnan(loss) else loss\n\n                ###### do loss read ######\n                cursor = mv_write(byte_mv, cursor, oasis_float, oasis_float_size, loss * factor)\n                ##########\n\n            else:\n                ##### do item exit ####\n                ##########\n                cursor += oasis_float_size\n                item_id = 0\n        else:\n            if valid_buff - cursor < 2 * oasis_int_size:\n                break\n            event_id, cursor = mv_read(byte_mv, cursor, oasis_int, oasis_int_size)\n            item_id, cursor = mv_read(byte_mv, cursor, oasis_int, oasis_int_size)\n            ##### do new item setup #####\n            factor = plafactors.get((event_id, items_amps[item_id]), default_factor)\n            ##########\n    out_byte_mv[:cursor] = byte_mv[:cursor]\n    out_cursor[0] = cursor\n    return cursor, event_id, item_id, 1\n\n\"\"\"Docstring (excerpt)\"\"\"\nread the gul loss stream, apply the post loss amplification factor and load it into out_byte_mv buffer\nThis modified version of the read_buffer template return result when the whole input buffer is read and not when an event is read.\ntherefore it cannot be used to read multiple stream at a time because events would be mixed up.\n\nArgs:\n    byte_mv: input byte array\n    cursor: read cursor\n    valid_buff: valid part of input array\n    event_id: last event id\n    item_id: last item_id\n    items_amps (numpy array): amplification IDs where indexes correspond to item IDs\n    plafactors (dict): event ID and amplification ID pairs mapped to loss factors\n    default_factor (float): post loss reduction/amplification factor to be used if loss factor not found in plafactors\n    out_byte_mv: output byte arrau\n    out_cursor: single value array to store valid part of out_byte_mv"
    },
    {
      "chunk_id": "oasislmf/pytools/pla/streams.py::read_and_write_streams@121",
      "source_type": "code",
      "path": "oasislmf/pytools/pla/streams.py",
      "symbol_type": "function",
      "name": "read_and_write_streams",
      "lineno": 121,
      "end_lineno": 160,
      "business_stage": "other",
      "docstring": "Read input stream from gulpy or gulcalc, determine amplification ID from\nitem ID, determine loss factor from event ID and amplification ID pair,\nmultiply losses by relevant factors, and write to output stream.\n\nInput stream is binary file with layout:\n    stream type (oasis_int), maximum sidx value (oasis_int),\n    event ID 1 (oasis_int), item ID 1 (oasis_int),\n    sample ID/sidx 1 (oasis_int), loss for sidx 1 (oasis_float),\n    ...\n    sample ID/sidx n (oasis_int), loss for sidx n (oasis_float),\n    0 (oasis_int), 0.0 (4-byte float),\n    event ID 1 (oasis_int), item ID 2 (oasis_int),\n    ...\n    event ID M (oasis_int), item ID N (oasis_int),\n    sample ID/sidx 1 (oasis_int), loss for sidx 1 (oasis_float),\n    ...\n    sample ID/sidx n (oasis_int), loss for sidx n (oasis_float)\n\nSample ID/sidx of 0 indicates start of next event ID-item ID pair. Output\nstream has same format as input stream.\n\nArgs:\n    stream_in (buffer): input stream\n    stream_out (buffer): output stream\n    items_amps (numpy array): amplification IDs where indexes correspond to item IDs\n    plafactors (dict): event ID and amplification ID pairs mapped to loss factors\n    default_factor (float): post loss reduction/amplification factor to be used if loss factor not found in plafactors",
      "content": "# File: oasislmf/pytools/pla/streams.py\n# function: read_and_write_streams (lines 121-160)\n\ndef read_and_write_streams(\n    stream_in, stream_out, items_amps, plafactors, default_factor\n):\n    \"\"\"\n    Read input stream from gulpy or gulcalc, determine amplification ID from\n    item ID, determine loss factor from event ID and amplification ID pair,\n    multiply losses by relevant factors, and write to output stream.\n\n    Input stream is binary file with layout:\n        stream type (oasis_int), maximum sidx value (oasis_int),\n        event ID 1 (oasis_int), item ID 1 (oasis_int),\n        sample ID/sidx 1 (oasis_int), loss for sidx 1 (oasis_float),\n        ...\n        sample ID/sidx n (oasis_int), loss for sidx n (oasis_float),\n        0 (oasis_int), 0.0 (4-byte float),\n        event ID 1 (oasis_int), item ID 2 (oasis_int),\n        ...\n        event ID M (oasis_int), item ID N (oasis_int),\n        sample ID/sidx 1 (oasis_int), loss for sidx 1 (oasis_float),\n        ...\n        sample ID/sidx n (oasis_int), loss for sidx n (oasis_float)\n\n    Sample ID/sidx of 0 indicates start of next event ID-item ID pair. Output\n    stream has same format as input stream.\n\n    Args:\n        stream_in (buffer): input stream\n        stream_out (buffer): output stream\n        items_amps (numpy array): amplification IDs where indexes correspond to item IDs\n        plafactors (dict): event ID and amplification ID pairs mapped to loss factors\n        default_factor (float): post loss reduction/amplification factor to be used if loss factor not found in plafactors\n\n    \"\"\"\n    stream_source_type, stream_agg_type, len_sample = get_and_check_header_in(stream_in)\n    stream_out.write(stream_info_to_bytes(stream_source_type, stream_agg_type))\n    stream_out.write(len_sample.tobytes())\n\n    pla_reader = PlaReader(items_amps, plafactors, default_factor)\n    for _ in pla_reader.read_streams(stream_in):\n        write_mv_to_stream(stream_out, pla_reader.out_byte_mv, pla_reader.out_cursor[0])\n\n\"\"\"Docstring (excerpt)\"\"\"\nRead input stream from gulpy or gulcalc, determine amplification ID from\nitem ID, determine loss factor from event ID and amplification ID pair,\nmultiply losses by relevant factors, and write to output stream.\n\nInput stream is binary file with layout:\n    stream type (oasis_int), maximum sidx value (oasis_int),\n    event ID 1 (oasis_int), item ID 1 (oasis_int),\n    sample ID/sidx 1 (oasis_int), loss for sidx 1 (oasis_float),\n    ...\n    sample ID/sidx n (oasis_int), loss for sidx n (oasis_float),\n    0 (oasis_int), 0.0 (4-byte float),\n    event ID 1 (oasis_int), item ID 2 (oasis_int),\n    ...\n    event ID M (oasis_int), item ID N (oasis_int),\n    sample ID/sidx 1 (oasis_int), loss for sidx 1 (oasis_float),\n    ...\n    sample ID/sidx n (oasis_int), loss for sidx n (oasis_float)\n\nSample ID/sidx of 0 indicates start of next event ID-item ID pair. Output\nstream has same format as input stream.\n\nArgs:\n    stream_in (buffer): input stream\n    stream_out (buffer): output stream\n    items_amps (numpy array): amplification IDs where indexes correspond to item IDs\n    plafactors (dict): event ID and amplification ID pairs mapped to loss factors\n    default_factor (float): post loss reduction/amplification factor to be used if loss factor not found in plafactors"
    },
    {
      "chunk_id": "oasislmf/pytools/pla/structure.py::get_post_loss_amplification_factors@19",
      "source_type": "code",
      "path": "oasislmf/pytools/pla/structure.py",
      "symbol_type": "function",
      "name": "get_post_loss_amplification_factors",
      "lineno": 19,
      "end_lineno": 56,
      "business_stage": "other",
      "docstring": "Get Post Loss Amplification (PLA) factors mapped to event ID-item ID pair.\nReturns empty dictionary if uniform factor to apply across all losses has\nbeen given.\n\nlossfactors.bin is binary file with layout:\n    reserved header (4-byte int),\n    event ID 1 (4-byte int), number of amplification IDs for event ID 1 (4-byte int),\n    amplification ID 1 (4-byte int), loss factor for amplification ID 1 (4-byte float),\n    ...\n    amplification ID n (4-byte int), loss factor for amplification ID n (4-byte float),\n    event ID 2 (4-byte int), number of amplification IDs for event ID 2 (4-byte int),\n    ...\n    event ID N (4-byte int), number of amplification IDs for event ID N (4-byte int),\n    amplification ID 1 (4-byte int), loss factor for amplification ID 1 (4-byte float),\n    ...\n    amplification ID n (4-byte int), loss factor for amplification ID n (4-byte float)\n\nArgs:\n    storage: (BaseStorage) the storage connector for fetching the model data\n    secondary_factor (float): secondary factor to apply to post loss\n      amplification\n    ignore_file_type: set(str) file extension to ignore when loading\n\nReturns:\n    plafactors (dict): event ID-item ID pairs mapped to amplification IDs",
      "content": "# File: oasislmf/pytools/pla/structure.py\n# function: get_post_loss_amplification_factors (lines 19-56)\n\ndef get_post_loss_amplification_factors(storage: BaseStorage, secondary_factor, ignore_file_type=set()):\n    \"\"\"\n    Get Post Loss Amplification (PLA) factors mapped to event ID-item ID pair.\n    Returns empty dictionary if uniform factor to apply across all losses has\n    been given.\n\n    lossfactors.bin is binary file with layout:\n        reserved header (4-byte int),\n        event ID 1 (4-byte int), number of amplification IDs for event ID 1 (4-byte int),\n        amplification ID 1 (4-byte int), loss factor for amplification ID 1 (4-byte float),\n        ...\n        amplification ID n (4-byte int), loss factor for amplification ID n (4-byte float),\n        event ID 2 (4-byte int), number of amplification IDs for event ID 2 (4-byte int),\n        ...\n        event ID N (4-byte int), number of amplification IDs for event ID N (4-byte int),\n        amplification ID 1 (4-byte int), loss factor for amplification ID 1 (4-byte float),\n        ...\n        amplification ID n (4-byte int), loss factor for amplification ID n (4-byte float)\n\n    Args:\n        storage: (BaseStorage) the storage connector for fetching the model data\n        secondary_factor (float): secondary factor to apply to post loss\n          amplification\n        ignore_file_type: set(str) file extension to ignore when loading\n\n    Returns:\n        plafactors (dict): event ID-item ID pairs mapped to amplification IDs\n    \"\"\"\n    input_files = set(storage.listdir())\n    if PLAFACTORS_FILE in input_files and 'bin' not in ignore_file_type:\n        plafactors = read_lossfactors(storage.root_dir, set([\"csv\"]), PLAFACTORS_FILE)\n        for key, value in plafactors.items():\n            plafactors[key] = max(\n                1 + (value - 1) * secondary_factor, 0.0\n            )\n        return plafactors\n    else:\n        raise FileNotFoundError(f\"lossfactors.bin file not found at {storage.get_storage_url('', encode_params=False)[1]}\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet Post Loss Amplification (PLA) factors mapped to event ID-item ID pair.\nReturns empty dictionary if uniform factor to apply across all losses has\nbeen given.\n\nlossfactors.bin is binary file with layout:\n    reserved header (4-byte int),\n    event ID 1 (4-byte int), number of amplification IDs for event ID 1 (4-byte int),\n    amplification ID 1 (4-byte int), loss factor for amplification ID 1 (4-byte float),\n    ...\n    amplification ID n (4-byte int), loss factor for amplification ID n (4-byte float),\n    event ID 2 (4-byte int), number of amplification IDs for event ID 2 (4-byte int),\n    ...\n    event ID N (4-byte int), number of amplification IDs for event ID N (4-byte int),\n    amplification ID 1 (4-byte int), loss factor for amplification ID 1 (4-byte float),\n    ...\n    amplification ID n (4-byte int), loss factor for amplification ID n (4-byte float)\n\nArgs:\n    storage: (BaseStorage) the storage connector for fetching the model data\n    secondary_factor (float): secondary factor to apply to post loss\n      amplification\n    ignore_file_type: set(str) file extension to ignore when loading\n\nReturns:\n    plafactors (dict): event ID-item ID pairs mapped to amplification IDs"
    },
    {
      "chunk_id": "oasislmf/pytools/pla/structure.py::read_lossfactors@59",
      "source_type": "code",
      "path": "oasislmf/pytools/pla/structure.py",
      "symbol_type": "function",
      "name": "read_lossfactors",
      "lineno": 59,
      "end_lineno": 153,
      "business_stage": "other",
      "docstring": "Load the correlations from the lossfactors file.\nArgs:\n    run_dir (str): path to lossfactors.bin file\n    ignore_file_type (Set[str]): file extension to ignore when loading.\n    filename (str | os.PathLike): lossfactors file name\n    use_stdin (bool): Use standard input for file data, ignores run_dir/filename. Defaults to False.\nReturns:\n    plafactors (dict): event ID-item ID pairs mapped to amplification IDs",
      "content": "# File: oasislmf/pytools/pla/structure.py\n# function: read_lossfactors (lines 59-153)\n\ndef read_lossfactors(run_dir=\"\", ignore_file_type=set(), filename=PLAFACTORS_FILE, use_stdin=False):\n    \"\"\"Load the correlations from the lossfactors file.\n    Args:\n        run_dir (str): path to lossfactors.bin file\n        ignore_file_type (Set[str]): file extension to ignore when loading.\n        filename (str | os.PathLike): lossfactors file name\n        use_stdin (bool): Use standard input for file data, ignores run_dir/filename. Defaults to False.\n    Returns:\n        plafactors (dict): event ID-item ID pairs mapped to amplification IDs\n    \"\"\"\n    int32_itemsize = np.dtype(np.int32).itemsize\n    float32_itemsize = np.dtype(np.float32).itemsize\n\n    @njit(cache=True, error_model=\"numpy\")\n    def _read_bin(lossfactors, plafactors):\n        cursor = 0\n        opts, cursor = mv_read(lossfactors, cursor, np.int32, int32_itemsize)\n\n        valid_buf = len(lossfactors)\n        while cursor + (2 * int32_itemsize) <= valid_buf:\n            event_id, cursor = mv_read(lossfactors, cursor, np.int32, int32_itemsize)\n            count, cursor = mv_read(lossfactors, cursor, np.int32, int32_itemsize)\n            for _ in range(count):\n                if cursor + (int32_itemsize + float32_itemsize) > valid_buf:\n                    break\n                amplification_id, cursor = mv_read(lossfactors, cursor, np.int32, int32_itemsize)\n                factor, cursor = mv_read(lossfactors, cursor, np.float32, float32_itemsize)\n                plafactors[(event_id, amplification_id)] = factor\n\n    @njit(cache=True, error_model=\"numpy\")\n    def _read_csv(lossfactors, plafactors):\n        for row in lossfactors:\n            plafactors[(row[\"event_id\"], row[\"amplification_id\"])] = row[\"factor\"]\n\n    def read_csv_lines(lines):\n        # Check for header\n        first_line_elements = [header.strip() for header in lines[0].strip().split(',')]\n        has_header = first_line_elements == lossfactors_headers\n        data_lines = lines[1:] if has_header else lines\n        return np.loadtxt(\n            data_lines,\n            dtype=lossfactors_dtype,\n            delimiter=\",\",\n            ndmin=1\n        )\n\n    plafactors = Dict.empty(\n        key_type=types.UniTuple(types.int64, 2), value_type=types.float64\n    )\n\n    supported_exts = [\"bin\", \"csv\"]\n\n    if use_stdin:\n        for ext in supported_exts:\n            if ext in ignore_file_type:\n                continue\n\n            if ext == \"bin\":\n                lossfactors = np.frombuffer(sys.stdin.buffer.read(), dtype=np.uint8)\n                _read_bin(lossfactors, plafactors)\n                return plafactors\n            elif ext == \"csv\":\n                lines = sys.stdin.readlines()\n                lossfactors = read_csv_lines(lines)\n                _read_csv(lossfactors, plafactors)\n                return plafactors\n            else:\n                raise RuntimeError(f\"Cannot read lossfactors file of type {ext}. Not Implemented.\")\n        raise RuntimeError(\n            f'lossfactors data not readable with use_stdin={use_stdin}, run_dir={run_dir}, filename={filename}. Ignoring files with ext {ignore_file_type}.')\n\n    for ext in supported_exts:\n        if ext in ignore_file_type:\n            continue\n\n        lossfactors_file = Path(run_dir, filename).with_suffix(\".\" + ext)\n        if not lossfactors_file.exists():\n            continue\n\n        if ext == \"bin\":\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoad the correlations from the lossfactors file.\nArgs:\n    run_dir (str): path to lossfactors.bin file\n    ignore_file_type (Set[str]): file extension to ignore when loading.\n    filename (str | os.PathLike): lossfactors file name\n    use_stdin (bool): Use standard input for file data, ignores run_dir/filename. Defaults to False.\nReturns:\n    plafactors (dict): event ID-item ID pairs mapped to amplification IDs"
    },
    {
      "chunk_id": "oasislmf/pytools/plt/manager.py::read_input_files@432",
      "source_type": "code",
      "path": "oasislmf/pytools/plt/manager.py",
      "symbol_type": "function",
      "name": "read_input_files",
      "lineno": 432,
      "end_lineno": 455,
      "business_stage": "other",
      "docstring": "Reads all input files and returns a dict of relevant data\n\nArgs:\n    run_dir (str | os.PathLike): Path to directory containing required files structure\n    compute_qplt (bool): Compute QPLT bool\n    sample_size (int): Sample size\n\nReturns:\n    file_data (Dict[str, Any]): A dict of relevent data extracted from files",
      "content": "# File: oasislmf/pytools/plt/manager.py\n# function: read_input_files (lines 432-455)\n\ndef read_input_files(run_dir, compute_qplt, sample_size):\n    \"\"\"Reads all input files and returns a dict of relevant data\n\n    Args:\n        run_dir (str | os.PathLike): Path to directory containing required files structure\n        compute_qplt (bool): Compute QPLT bool\n        sample_size (int): Sample size\n\n    Returns:\n        file_data (Dict[str, Any]): A dict of relevent data extracted from files\n    \"\"\"\n    occ_map, date_algorithm, granular_date, no_of_periods = read_occurrence(Path(run_dir, \"input\"))\n    period_weights = read_periods(no_of_periods, Path(run_dir, \"input\"))\n    intervals = read_quantile(sample_size, Path(run_dir, \"input\"), return_empty=not compute_qplt)\n\n    file_data = {\n        \"occ_map\": occ_map,\n        \"date_algorithm\": date_algorithm,\n        \"granular_date\": granular_date,\n        \"no_of_periods\": no_of_periods,\n        \"period_weights\": period_weights,\n        \"intervals\": intervals,\n    }\n    return file_data\n\n\"\"\"Docstring (excerpt)\"\"\"\nReads all input files and returns a dict of relevant data\n\nArgs:\n    run_dir (str | os.PathLike): Path to directory containing required files structure\n    compute_qplt (bool): Compute QPLT bool\n    sample_size (int): Sample size\n\nReturns:\n    file_data (Dict[str, Any]): A dict of relevent data extracted from files"
    },
    {
      "chunk_id": "oasislmf/pytools/plt/manager.py::run@458",
      "source_type": "code",
      "path": "oasislmf/pytools/plt/manager.py",
      "symbol_type": "function",
      "name": "run",
      "lineno": 458,
      "end_lineno": 592,
      "business_stage": "other",
      "docstring": "Runs PLT calculations\n\nArgs:\n    run_dir (str | os.PathLike): Path to directory containing required files structure\n    files_in (list[str]): Path to summary binary input file\n    splt_output_file (str, optional): Path to SPLT output file. Defaults to None.\n    mplt_output_file (str, optional): Path to MPLT output file. Defaults to None.\n    qplt_output_file (str, optional): Path to QPLT output file. Defaults to None.\n    noheader (bool): Boolean value to skip header in output file. Defaults to False.\n    output_format (str): Output format extension. Defaults to \"csv\".",
      "content": "# File: oasislmf/pytools/plt/manager.py\n# function: run (lines 458-592)\n\ndef run(\n    run_dir,\n    files_in,\n    splt_output_file=None,\n    mplt_output_file=None,\n    qplt_output_file=None,\n    noheader=False,\n    output_format=\"csv\",\n):\n    \"\"\"Runs PLT calculations\n\n    Args:\n        run_dir (str | os.PathLike): Path to directory containing required files structure\n        files_in (list[str]): Path to summary binary input file\n        splt_output_file (str, optional): Path to SPLT output file. Defaults to None.\n        mplt_output_file (str, optional): Path to MPLT output file. Defaults to None.\n        qplt_output_file (str, optional): Path to QPLT output file. Defaults to None.\n        noheader (bool): Boolean value to skip header in output file. Defaults to False.\n        output_format (str): Output format extension. Defaults to \"csv\".\n    \"\"\"\n    outmap = {\n        \"splt\": {\n            \"compute\": splt_output_file is not None,\n            \"file_path\": splt_output_file,\n            \"fmt\": SPLT_fmt,\n            \"headers\": SPLT_headers,\n            \"file\": None,\n        },\n        \"mplt\": {\n            \"compute\": mplt_output_file is not None,\n            \"file_path\": mplt_output_file,\n            \"fmt\": MPLT_fmt,\n            \"headers\": MPLT_headers,\n            \"file\": None,\n        },\n        \"qplt\": {\n            \"compute\": qplt_output_file is not None,\n            \"file_path\": qplt_output_file,\n            \"fmt\": QPLT_fmt,\n            \"headers\": QPLT_headers,\n            \"file\": None,\n        },\n    }\n\n    output_format = \".\" + output_format\n    output_binary = output_format == \".bin\"\n    output_parquet = output_format == \".parquet\"\n    # Check for correct suffix\n    for path in [v[\"file_path\"] for v in outmap.values()]:\n        if path is None:\n            continue\n        if Path(path).suffix == \"\":  # Ignore suffix for pipes\n            continue\n        if (Path(path).suffix != output_format):\n            raise ValueError(f\"Invalid file extension for {output_format}, got {path},\")\n\n    if run_dir is None:\n        run_dir = './work'\n\n    if not all([v[\"compute\"] for v in outmap.values()]):\n        logger.warning(\"No output files specified\")\n\n    with ExitStack() as stack:\n        if files_in == [\"-\"]:\n            files_in = None  # init_streams checks for None to read from sys.stdin.buffer\n\n        streams_in, (stream_source_type, stream_agg_type, len_sample) = init_streams_in(files_in, stack)\n        if stream_source_type != SUMMARY_STREAM_ID:\n            raise Exception(f\"unsupported stream type {stream_source_type}, {stream_agg_type}\")\n\n        file_data = read_input_files(\n            run_dir,\n            outmap[\"qplt\"][\"compute\"],\n            len_sample\n        )\n        plt_reader = PLTReader(\n            len_sample,\n            outmap[\"splt\"][\"compute\"],\n            outmap[\"mplt\"][\"compute\"],\n            outmap[\"qplt\"][\"compute\"],\n\n\"\"\"Docstring (excerpt)\"\"\"\nRuns PLT calculations\n\nArgs:\n    run_dir (str | os.PathLike): Path to directory containing required files structure\n    files_in (list[str]): Path to summary binary input file\n    splt_output_file (str, optional): Path to SPLT output file. Defaults to None.\n    mplt_output_file (str, optional): Path to MPLT output file. Defaults to None.\n    qplt_output_file (str, optional): Path to QPLT output file. Defaults to None.\n    noheader (bool): Boolean value to skip header in output file. Defaults to False.\n    output_format (str): Output format extension. Defaults to \"csv\"."
    },
    {
      "chunk_id": "oasislmf/pytools/summary/manager.py::create_summary_object_file@63",
      "source_type": "code",
      "path": "oasislmf/pytools/summary/manager.py",
      "symbol_type": "function",
      "name": "create_summary_object_file",
      "lineno": 63,
      "end_lineno": 66,
      "business_stage": "aggregation",
      "docstring": "create and write summary object into static path",
      "content": "# File: oasislmf/pytools/summary/manager.py\n# function: create_summary_object_file (lines 63-66)\n\ndef create_summary_object_file(static_path, run_type):\n    \"\"\"create and write summary object into static path\"\"\"\n    summary_objects = get_summary_object(static_path, run_type)\n    write_summary_objects(static_path, run_type, *summary_objects)\n\n\"\"\"Docstring (excerpt)\"\"\"\ncreate and write summary object into static path"
    },
    {
      "chunk_id": "oasislmf/pytools/summary/manager.py::load_summary_object@69",
      "source_type": "code",
      "path": "oasislmf/pytools/summary/manager.py",
      "symbol_type": "function",
      "name": "load_summary_object",
      "lineno": 69,
      "end_lineno": 74,
      "business_stage": "aggregation",
      "docstring": "load already prepare summary data structure if present otherwise create them",
      "content": "# File: oasislmf/pytools/summary/manager.py\n# function: load_summary_object (lines 69-74)\n\ndef load_summary_object(static_path, run_type):\n    \"\"\"load already prepare summary data structure if present otherwise create them\"\"\"\n    if os.path.isfile(os.path.join(static_path, run_type, 'summary_info.npy')):\n        return read_summary_objects(static_path, run_type)\n    else:\n        return get_summary_object(static_path, run_type)\n\n\"\"\"Docstring (excerpt)\"\"\"\nload already prepare summary data structure if present otherwise create them"
    },
    {
      "chunk_id": "oasislmf/pytools/summary/manager.py::get_summary_object@77",
      "source_type": "code",
      "path": "oasislmf/pytools/summary/manager.py",
      "symbol_type": "function",
      "name": "get_summary_object",
      "lineno": 77,
      "end_lineno": 114,
      "business_stage": "aggregation",
      "docstring": "read static files to get summary static data structure",
      "content": "# File: oasislmf/pytools/summary/manager.py\n# function: get_summary_object (lines 77-114)\n\ndef get_summary_object(static_path, run_type):\n    \"\"\"read static files to get summary static data structure\"\"\"\n\n    # extract item_id to index in the loss summary\n    if run_type == RUNTYPE_GROUNDUP_LOSS:\n        summary_xref = load_as_ndarray(static_path, 'gulsummaryxref', gul_summary_xref_dtype)\n        summary_map = pd.read_csv(os.path.join(static_path, 'gul_summary_map.csv'),\n                                  usecols=['loc_id', 'item_id', 'building_id', 'coverage_id'],\n                                  dtype=oasis_int)\n    elif run_type == RUNTYPE_INSURED_LOSS:\n        summary_xref = load_as_ndarray(static_path, 'fmsummaryxref', fm_summary_xref_dtype)\n        summary_xref = summary_xref.astype(gul_summary_xref_dtype)  # Change dtype to keep consistent column names\n        summary_map = pd.read_csv(os.path.join(static_path, 'fm_summary_map.csv'),\n                                  usecols=['loc_id', 'output_id', 'building_id', 'coverage_id'],\n                                  dtype=oasis_int,\n                                  ).rename(columns={'output_id': 'item_id'})\n    elif run_type == RUNTYPE_REINSURANCE_LOSS:\n        summary_xref = load_as_ndarray(static_path, 'fmsummaryxref', fm_summary_xref_dtype)\n        summary_xref = summary_xref.astype(gul_summary_xref_dtype)  # Change dtype to keep consistent column names\n        summary_map = None  # numba use none to optimise function when some part are not used\n    else:\n        raise Exception(f\"run type {run_type} not in supported list {SUPPORTED_RUN_TYPE}\")\n\n    summary_sets_id = np.sort(np.unique(summary_xref['summaryset_id']))\n    summary_set_id_to_summary_set_index = get_summary_set_id_to_summary_set_index(summary_sets_id)\n    summary_set_index_to_loss_ptr, item_id_to_summary_id = get_summary_xref_info(summary_xref, summary_sets_id, summary_set_id_to_summary_set_index)\n\n    if summary_map is not None:\n        nb_risk, item_id_to_risks_i = extract_risk_info(item_id_to_summary_id.shape[0], summary_map)\n    else:\n        item_id_to_risks_i = np.zeros(0, dtype=oasis_int)\n        nb_risk = 0\n\n    summary_info = np.empty(1, dtype=summary_info_dtype)\n    info = summary_info[0]\n    info['nb_risk'] = nb_risk\n\n    return summary_info, summary_set_id_to_summary_set_index, summary_set_index_to_loss_ptr, item_id_to_summary_id, item_id_to_risks_i\n\n\"\"\"Docstring (excerpt)\"\"\"\nread static files to get summary static data structure"
    },
    {
      "chunk_id": "oasislmf/pytools/summary/manager.py::extract_risk_info@151",
      "source_type": "code",
      "path": "oasislmf/pytools/summary/manager.py",
      "symbol_type": "function",
      "name": "extract_risk_info",
      "lineno": 151,
      "end_lineno": 168,
      "business_stage": "aggregation",
      "docstring": "extract relevant information regarding item and risk mapping from summary_map\nArgs:\n    len_item_id: number of items\n    summary_map: numpy ndarray view of the summary_map\n\nReturns:\n    (number of risk, mapping array item_id => risks_i)",
      "content": "# File: oasislmf/pytools/summary/manager.py\n# function: extract_risk_info (lines 151-168)\n\ndef extract_risk_info(len_item_id, summary_map):\n    \"\"\"\n    extract relevant information regarding item and risk mapping from summary_map\n    Args:\n        len_item_id: number of items\n        summary_map: numpy ndarray view of the summary_map\n\n    Returns:\n        (number of risk, mapping array item_id => risks_i)\n    \"\"\"\n\n    item_id_to_risks_i = np.zeros(len_item_id, oasis_int)\n    nb_risk = nb_extract_risk_info(\n        item_id_to_risks_i,\n        summary_map['item_id'].astype(oasis_int).to_numpy(),\n        summary_map['loc_id'].astype(oasis_int).to_numpy(),\n        summary_map['building_id'].astype(oasis_int).to_numpy())\n    return nb_risk, item_id_to_risks_i\n\n\"\"\"Docstring (excerpt)\"\"\"\nextract relevant information regarding item and risk mapping from summary_map\nArgs:\n    len_item_id: number of items\n    summary_map: numpy ndarray view of the summary_map\n\nReturns:\n    (number of risk, mapping array item_id => risks_i)"
    },
    {
      "chunk_id": "oasislmf/pytools/summary/manager.py::read_buffer@172",
      "source_type": "code",
      "path": "oasislmf/pytools/summary/manager.py",
      "symbol_type": "function",
      "name": "read_buffer",
      "lineno": 172,
      "end_lineno": 241,
      "business_stage": "aggregation",
      "docstring": "read valid part of byte_mv and load relevant data for one event",
      "content": "# File: oasislmf/pytools/summary/manager.py\n# function: read_buffer (lines 172-241)\n\ndef read_buffer(byte_mv, cursor, valid_buff, event_id, item_id,\n                summary_sets_id, summary_set_index_to_loss_ptr, item_id_to_summary_id,\n                loss_index, loss_summary, present_summary_id, summary_set_index_to_present_loss_ptr_end,\n                item_id_to_risks_i, is_risk_affected, has_affected_risk):\n    \"\"\"read valid part of byte_mv and load relevant data for one event\"\"\"\n    last_event_id = event_id\n    while True:\n        if item_id:\n            if valid_buff - cursor < (oasis_int_size + oasis_float_size):\n                break\n            sidx, cursor = mv_read(byte_mv, cursor, oasis_int, oasis_int_size)\n            if sidx:\n                loss, cursor = mv_read(byte_mv, cursor, oasis_float, oasis_float_size)\n                loss = 0 if np.isnan(loss) else loss\n\n                ###### do loss read ######\n                if sidx > 0 or sidx in [-1, -3, -5]:\n                    for summary_set_index in range(summary_sets_id.shape[0]):\n                        loss_summary[loss_index[summary_set_index], sidx] += loss\n                ##########\n\n            else:\n                ##### do item exit ####\n                ##########\n                cursor += oasis_float_size\n                item_id = 0\n        else:\n            if valid_buff - cursor < 2 * oasis_int_size:\n                break\n            event_id, cursor = mv_read(byte_mv, cursor, oasis_int, oasis_int_size)\n            if event_id != last_event_id:\n                if last_event_id:  # we have a new event we return the one we just finished\n                    for summary_set_index in range(summary_sets_id.shape[0]):  # reorder summary_id for each summary set\n                        summary_set_start = summary_set_index_to_loss_ptr[summary_set_index]\n                        summary_set_end = summary_set_index_to_present_loss_ptr_end[summary_set_index]\n                        present_summary_id[summary_set_start: summary_set_end] = np.sort(present_summary_id[summary_set_start: summary_set_end])\n                    return cursor - oasis_int_size, last_event_id, 0, 1\n                else:  # first pass we store the event we are reading\n                    last_event_id = event_id\n            item_id, cursor = mv_read(byte_mv, cursor, oasis_int, oasis_int_size)\n\n            ##### do new item setup #####\n            if has_affected_risk is not None:\n                risk_i = item_id_to_risks_i[item_id]\n                if is_risk_affected[risk_i]:\n                    new_risk = 0\n                else:\n                    new_risk = is_risk_affected[risk_i] = 1\n\n            for summary_set_index in range(summary_sets_id.shape[0]):\n                loss_index[summary_set_index] = nb_oasis_int(summary_set_index_to_loss_ptr[summary_set_index]\n                                                             + item_id_to_summary_id[item_id, summary_set_index] - 1)\n                # print(summary_set_index_to_loss_ptr[summary_set_index], item_id_to_summary_id[item_id, summary_set_index])\n                # print('new_risk', event_id, item_id, new_risk)\n                # print('loss_index[summary_set_index]', loss_index[summary_set_index])\n                # print('loss_summary[loss_index[summary_set_index], NUMBER_OF_AFFECTED_RISK_IDX]', loss_summary[loss_index[summary_set_index], -4])\n                if loss_summary[loss_index[summary_set_index], NUMBER_OF_AFFECTED_RISK_IDX] == 0:  # we use sidx 0 to check if this summary_id has already been seen\n                    present_summary_id[summary_set_index_to_present_loss_ptr_end[summary_set_index]\n                                       ] = item_id_to_summary_id[item_id, summary_set_index]\n                    summary_set_index_to_present_loss_ptr_end[summary_set_index] += 1\n                    loss_summary[loss_index[summary_set_index], NUMBER_OF_AFFECTED_RISK_IDX] = 1\n\n                elif has_affected_risk is not None:\n                    loss_summary[loss_index[summary_set_index], NUMBER_OF_AFFECTED_RISK_IDX] += new_risk\n            ##########\n    for summary_set_index in range(summary_sets_id.shape[0]):  # reorder summary_id for each summary set\n        summary_set_start = summary_set_index_to_loss_ptr[summary_set_index]\n        summary_set_end = summary_set_index_to_present_loss_ptr_end[summary_set_index]\n        present_summary_id[summary_set_start: summary_set_end] = np.sort(present_summary_id[summary_set_start: summary_set_end])\n    return cursor, event_id, item_id, 0\n\n\"\"\"Docstring (excerpt)\"\"\"\nread valid part of byte_mv and load relevant data for one event"
    },
    {
      "chunk_id": "oasislmf/pytools/summary/manager.py::mv_write_event@245",
      "source_type": "code",
      "path": "oasislmf/pytools/summary/manager.py",
      "symbol_type": "function",
      "name": "mv_write_event",
      "lineno": 245,
      "end_lineno": 301,
      "business_stage": "aggregation",
      "docstring": "load event summary loss into byte_mv\n\nArgs:\n    byte_mv: numpy byte view to write to the stream\n    event_id: event id\n    len_sample: max sample id\n    last_loss_summary_index: last summary index written (used to restart from the last summary when buffer was full)\n    last_sidx: last sidx written in the buffer (used to restart from the correct sidx when buffer was full\n\n    see other args definition in run method",
      "content": "# File: oasislmf/pytools/summary/manager.py\n# function: mv_write_event (lines 245-301)\n\ndef mv_write_event(byte_mv, event_id, len_sample, last_loss_summary_index, last_sidx,\n                   output_zeros, has_affected_risk,\n                   summary_set_index, summary_set_index_to_loss_ptr, summary_set_index_to_present_loss_ptr_end, present_summary_id, loss_summary,\n                   summary_index_cursor, summary_sets_cursor, summary_stream_index):\n    \"\"\"\n        load event summary loss into byte_mv\n\n    Args:\n        byte_mv: numpy byte view to write to the stream\n        event_id: event id\n        len_sample: max sample id\n        last_loss_summary_index: last summary index written (used to restart from the last summary when buffer was full)\n        last_sidx: last sidx written in the buffer (used to restart from the correct sidx when buffer was full\n\n        see other args definition in run method\n    \"\"\"\n    cursor = 0\n    for loss_summary_index in range(max(summary_set_index_to_loss_ptr[summary_set_index], last_loss_summary_index),\n                                    summary_set_index_to_present_loss_ptr_end[summary_set_index]):\n        summary_id = present_summary_id[loss_summary_index]\n        losses = loss_summary[summary_set_index_to_loss_ptr[summary_set_index] + summary_id - 1]\n\n        if not output_zeros and losses[TIV_IDX] == 0 and losses[MEAN_IDX] == 0:\n            continue\n\n        summary_stream_index[summary_index_cursor]['summary_id'] = summary_id\n        # we use offset to temporally store the cursor, we set the correct value later on\n        summary_stream_index[summary_index_cursor]['offset'] = cursor\n\n        if last_sidx == 0:\n            if cursor < PIPE_CAPACITY - SUMMARY_HEADER_SIZE:\n                cursor = mv_write_summary_header(byte_mv, cursor, event_id, summary_id, losses[TIV_IDX])\n                cursor = mv_write_sidx_loss(byte_mv, cursor, MEAN_IDX, losses[MEAN_IDX])\n                if has_affected_risk is not None:\n                    cursor = mv_write_sidx_loss(byte_mv, cursor, NUMBER_OF_AFFECTED_RISK_IDX, losses[NUMBER_OF_AFFECTED_RISK_IDX])\n                cursor = mv_write_sidx_loss(byte_mv, cursor, MAX_LOSS_IDX, losses[MAX_LOSS_IDX])\n                last_sidx = 1\n            else:\n                return cursor, loss_summary_index, last_sidx, summary_index_cursor\n\n        for sidx in range(last_sidx, len_sample + 1):\n            if not output_zeros and losses[sidx] == 0:\n                continue\n            if cursor < PIPE_CAPACITY - SIDX_LOSS_WRITE_SIZE:  # times 2 to accommodate 0,0 if last item\n                cursor = mv_write_sidx_loss(byte_mv, cursor, sidx, losses[sidx])\n            else:\n                return cursor, loss_summary_index, sidx, summary_index_cursor\n\n        cursor = mv_write_delimiter(byte_mv, cursor)\n        # set the correct offset for idx file and update summary_sets_cursor\n        summary_byte_len = cursor - summary_stream_index[summary_index_cursor]['offset']\n        summary_stream_index[summary_index_cursor]['offset'] = summary_sets_cursor[summary_set_index]\n        summary_sets_cursor[summary_set_index] += summary_byte_len\n        summary_index_cursor += 1\n\n        last_sidx = 0\n    return cursor, -1, 0, summary_index_cursor\n\n\"\"\"Docstring (excerpt)\"\"\"\nload event summary loss into byte_mv\n\nArgs:\n    byte_mv: numpy byte view to write to the stream\n    event_id: event id\n    len_sample: max sample id\n    last_loss_summary_index: last summary index written (used to restart from the last summary when buffer was full)\n    last_sidx: last sidx written in the buffer (used to restart from the correct sidx when buffer was full\n\n    see other args definition in run method"
    },
    {
      "chunk_id": "oasislmf/pytools/summary/manager.py::get_summary_set_id_to_summary_set_index@329",
      "source_type": "code",
      "path": "oasislmf/pytools/summary/manager.py",
      "symbol_type": "function",
      "name": "get_summary_set_id_to_summary_set_index",
      "lineno": 329,
      "end_lineno": 334,
      "business_stage": "aggregation",
      "docstring": "create an array mapping summary_set_id => summary_set_index",
      "content": "# File: oasislmf/pytools/summary/manager.py\n# function: get_summary_set_id_to_summary_set_index (lines 329-334)\n\ndef get_summary_set_id_to_summary_set_index(summary_sets_id):\n    \"\"\"create an array mapping summary_set_id => summary_set_index\"\"\"\n    summary_set_id_to_summary_set_index = np.full(np.max(summary_sets_id) + 1, null_index, 'i4')\n    for summary_set_index in range(summary_sets_id.shape[0]):\n        summary_set_id_to_summary_set_index[summary_sets_id[summary_set_index]] = summary_set_index\n    return summary_set_id_to_summary_set_index\n\n\"\"\"Docstring (excerpt)\"\"\"\ncreate an array mapping summary_set_id => summary_set_index"
    },
    {
      "chunk_id": "oasislmf/pytools/summary/manager.py::get_summary_xref_info@337",
      "source_type": "code",
      "path": "oasislmf/pytools/summary/manager.py",
      "symbol_type": "function",
      "name": "get_summary_xref_info",
      "lineno": 337,
      "end_lineno": 363,
      "business_stage": "aggregation",
      "docstring": "extract mapping from summary_xref",
      "content": "# File: oasislmf/pytools/summary/manager.py\n# function: get_summary_xref_info (lines 337-363)\n\ndef get_summary_xref_info(summary_xref, summary_sets_id, summary_set_id_to_summary_set_index):\n    \"\"\"\n    extract mapping from summary_xref\n    \"\"\"\n    summary_set_index_to_loss_ptr = np.zeros(summary_sets_id.shape[0] + 1, oasis_int)\n    max_item_id = 0\n    for i in range(summary_xref.shape[0]):\n        xref = summary_xref[i]\n        summary_set_index = summary_set_id_to_summary_set_index[xref['summaryset_id']]\n        if summary_set_index == null_index:\n            continue\n        if xref['summary_id'] > summary_set_index_to_loss_ptr[summary_set_index + 1]:\n            summary_set_index_to_loss_ptr[summary_set_index + 1] = xref['summary_id']\n        if xref['item_id'] > max_item_id:\n            max_item_id = xref['item_id']\n    for i in range(1, summary_set_index_to_loss_ptr.shape[0]):\n        summary_set_index_to_loss_ptr[i] += summary_set_index_to_loss_ptr[i - 1]\n\n    item_id_to_summary_id = np.full((max_item_id + 1, summary_sets_id.shape[0]), null_index, oasis_int)\n    for i in range(summary_xref.shape[0]):\n        xref = summary_xref[i]\n        summary_set_index = summary_set_id_to_summary_set_index[xref['summaryset_id']]\n        if summary_set_index == null_index:\n            continue\n        item_id_to_summary_id[xref['item_id'], summary_set_index] = xref['summary_id']\n\n    return summary_set_index_to_loss_ptr, item_id_to_summary_id\n\n\"\"\"Docstring (excerpt)\"\"\"\nextract mapping from summary_xref"
    },
    {
      "chunk_id": "oasislmf/pytools/summary/manager.py::run@366",
      "source_type": "code",
      "path": "oasislmf/pytools/summary/manager.py",
      "symbol_type": "function",
      "name": "run",
      "lineno": 366,
      "end_lineno": 477,
      "business_stage": "aggregation",
      "docstring": "Args:\n    files_in: list of file path to read event from\n    run_type: type of the source that is sending the stream\n    static_path: path to the static files\n    low_memory: if true output summary index file\n    output_zeros: if true output 0 loss\n    **kwargs:",
      "content": "# File: oasislmf/pytools/summary/manager.py\n# function: run (lines 366-477)\n\ndef run(files_in, static_path, run_type, low_memory, output_zeros, **kwargs):\n    \"\"\"\n    Args:\n        files_in: list of file path to read event from\n        run_type: type of the source that is sending the stream\n        static_path: path to the static files\n        low_memory: if true output summary index file\n        output_zeros: if true output 0 loss\n        **kwargs:\n\n    \"\"\"\n    summary_sets_path = {}\n    error_msg = (f\"summary_sets_output expected format is a list of -summary_set_id summary_set_path (ex: -1 S1.bin -2 S2.bin')\"\n                 f\", found '{' '.join(kwargs['summary_sets_output'])}'\")\n    for summary_set_id, summary_set_path in zip_longest(*[iter(kwargs['summary_sets_output'])] * 2):\n        if summary_set_id[0] != '-' or summary_set_path is None:\n            raise Exception(error_msg)\n        try:\n            summary_sets_path[int(summary_set_id[1:])] = summary_set_path\n        except ValueError:\n            raise Exception(error_msg)\n    summary_sets_id = np.array(list(summary_sets_path.keys()))\n\n    with ExitStack() as stack:\n        summary_sets_pipe = {i: stack.enter_context(open(summary_set_path, 'wb')) for i, summary_set_path in summary_sets_path.items()}\n\n        if low_memory:\n            summary_sets_index_pipe = {summary_set_id: stack.enter_context(open(setpath.rsplit('.', 1)[0] + '.idx', 'w'))\n                                       for summary_set_id, setpath in summary_sets_path.items()}\n\n        streams_in, (stream_source_type, stream_agg_type, len_sample) = init_streams_in(files_in, stack)\n\n        if stream_source_type not in (GUL_STREAM_ID, FM_STREAM_ID, LOSS_STREAM_ID):\n            raise Exception(f\"unsupported stream type {stream_source_type}, {stream_agg_type}\")\n\n        summary_object = load_summary_object(static_path, run_type)\n        summary_info, summary_set_id_to_summary_set_index, summary_set_index_to_loss_ptr, item_id_to_summary_id, item_id_to_risks_i = summary_object\n\n        # check all summary_set_id required are defined in summaryxref\n        invalid_summary_sets_id = []\n        for summary_set_id in summary_sets_id:\n            if summary_set_id > summary_set_id_to_summary_set_index.shape[0] or summary_set_id_to_summary_set_index[summary_set_id] == null_index:\n                invalid_summary_sets_id.append(summary_set_id)\n        if invalid_summary_sets_id:\n            raise ValueError(\n                f'summary_set_ids {invalid_summary_sets_id} not found in summaryxref available summary_set_id '\n                f'{[summary_set_id for summary_set_id, summary_set_index in enumerate(summary_set_id_to_summary_set_index) if summary_set_index != null_index]}')\n\n        has_affected_risk = True if summary_info[0]['nb_risk'] > 0 else None\n\n        # init temporary\n        present_summary_id = np.zeros(summary_set_index_to_loss_ptr[-1], oasis_int)\n        loss_index = np.empty(summary_sets_id.shape[0], oasis_int)\n        summary_set_index_to_present_loss_ptr_end = np.array(summary_set_index_to_loss_ptr)\n        loss_summary = np.zeros((summary_set_index_to_loss_ptr[-1], len_sample + SPECIAL_SIDX_COUNT), dtype=oasis_float)\n        is_risk_affected = np.zeros(summary_info[0]['nb_risk'], dtype=oasis_int)\n\n        summary_reader = SummaryReader(\n            summary_sets_id, summary_set_index_to_loss_ptr, item_id_to_summary_id,\n            loss_index, loss_summary, present_summary_id, summary_set_index_to_present_loss_ptr_end,\n            item_id_to_risks_i, is_risk_affected, has_affected_risk)\n\n        out_byte_mv = np.frombuffer(buffer=memoryview(bytearray(PIPE_CAPACITY)), dtype='b')\n\n        # data for index file (low_memory==True)\n        summary_sets_cursor = np.zeros(summary_sets_id.shape[0], dtype=np.int64)\n        summary_stream_index = np.empty(summary_set_index_to_loss_ptr[-1], dtype=np.dtype([('summary_id', oasis_int), ('offset', np.int64)]))\n\n        for summary_set_index, summary_set_id in enumerate(summary_sets_id):\n            summary_pipe = summary_sets_pipe[summary_set_id]\n            summary_sets_cursor[summary_set_index] += summary_pipe.write(stream_info_to_bytes(SUMMARY_STREAM_ID, ITEM_STREAM))\n            summary_sets_cursor[summary_set_index] += summary_pipe.write(len_sample.tobytes())\n            summary_sets_cursor[summary_set_index] += summary_pipe.write(nb_oasis_int(summary_set_id).tobytes())\n\n        try:\n            for event_id in summary_reader.read_streams(streams_in):\n                for summary_set_index, summary_set_id in enumerate(summary_sets_id):\n                    summary_pipe = summary_sets_pipe[summary_set_id]\n                    summary_index_cursor = 0\n                    last_loss_summary_index = 0\n\n\"\"\"Docstring (excerpt)\"\"\"\nArgs:\n    files_in: list of file path to read event from\n    run_type: type of the source that is sending the stream\n    static_path: path to the static files\n    low_memory: if true output summary index file\n    output_zeros: if true output 0 loss\n    **kwargs:"
    },
    {
      "chunk_id": "oasislmf/utils/compress.py::compress_string@15",
      "source_type": "code",
      "path": "oasislmf/utils/compress.py",
      "symbol_type": "function",
      "name": "compress_string",
      "lineno": 15,
      "end_lineno": 45,
      "business_stage": "other",
      "docstring": "Compresses strings using the zlib library.\n\nAdapted from a StackOverflow.com solution by Dmitry Skryabin\n\n    https://stackoverflow.com/a/36056646/7556955\n\nwith a modification to set block/chunk size to 500 Mb (5 x 10^8 bytes).\n\n:param s: Input string to be compressed\n:type s: str\n\n:return: Compressed string as bytes\n:rtype: bytes",
      "content": "# File: oasislmf/utils/compress.py\n# function: compress_string (lines 15-45)\n\ndef compress_string(st: str) -> bytes:\n    \"\"\"\n    Compresses strings using the zlib library.\n\n    Adapted from a StackOverflow.com solution by Dmitry Skryabin\n\n        https://stackoverflow.com/a/36056646/7556955\n\n    with a modification to set block/chunk size to 500 Mb (5 x 10^8 bytes).\n\n    :param s: Input string to be compressed\n    :type s: str\n\n    :return: Compressed string as bytes\n    :rtype: bytes\n    \"\"\"\n    _st = ''.join(st).encode('utf-8')\n    compressed = b''\n    begin = 0\n    compressor = zlib.compressobj()\n\n    try:\n        while begin < len(_st):\n            compressed += compressor.compress(_st[begin:begin + CHUNK_SIZE])\n            begin += CHUNK_SIZE\n\n        compressed += compressor.flush()\n    except zlib.error as e:\n        raise OasisException(\"Exception raised in 'compress_string'\", e)\n\n    return compressed\n\n\"\"\"Docstring (excerpt)\"\"\"\nCompresses strings using the zlib library.\n\nAdapted from a StackOverflow.com solution by Dmitry Skryabin\n\n    https://stackoverflow.com/a/36056646/7556955\n\nwith a modification to set block/chunk size to 500 Mb (5 x 10^8 bytes).\n\n:param s: Input string to be compressed\n:type s: str\n\n:return: Compressed string as bytes\n:rtype: bytes"
    },
    {
      "chunk_id": "oasislmf/utils/compress.py::decompress_string@48",
      "source_type": "code",
      "path": "oasislmf/utils/compress.py",
      "symbol_type": "function",
      "name": "decompress_string",
      "lineno": 48,
      "end_lineno": 60,
      "business_stage": "other",
      "docstring": "Decompresses zlib-compressed strings\n\n:param bt: zlib-compressed string\n:type bt: bytes\n\n:return: Decompressed (Unicode) string\n:rtype: str",
      "content": "# File: oasislmf/utils/compress.py\n# function: decompress_string (lines 48-60)\n\ndef decompress_string(bt: bytes) -> str:\n    \"\"\"\n    Decompresses zlib-compressed strings\n\n    :param bt: zlib-compressed string\n    :type bt: bytes\n\n    :return: Decompressed (Unicode) string\n    :rtype: str\n    \"\"\"\n    decompressor = zlib.decompressobj()\n\n    return decompressor.decompress(bt).decode('utf-8')\n\n\"\"\"Docstring (excerpt)\"\"\"\nDecompresses zlib-compressed strings\n\n:param bt: zlib-compressed string\n:type bt: bytes\n\n:return: Decompressed (Unicode) string\n:rtype: str"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::factorize_array@183",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "factorize_array",
      "lineno": 183,
      "end_lineno": 196,
      "business_stage": "other",
      "docstring": "Groups a 1D Numpy array by item value, and optionally enumerates the\ngroups, starting from 1. The default or assumed type is a Nunpy\narray, although a Python list, tuple or Pandas series will work too.\n\n:param arr: 1D Numpy array (or list, tuple, or Pandas series)\n:type arr: numpy.ndarray\n\n:return: A 2-tuple consisting of the enumeration and the value groups\n:rtype: tuple",
      "content": "# File: oasislmf/utils/data.py\n# function: factorize_array (lines 183-196)\n\ndef factorize_array(arr, sort_opt=False):\n    \"\"\"\n    Groups a 1D Numpy array by item value, and optionally enumerates the\n    groups, starting from 1. The default or assumed type is a Nunpy\n    array, although a Python list, tuple or Pandas series will work too.\n\n    :param arr: 1D Numpy array (or list, tuple, or Pandas series)\n    :type arr: numpy.ndarray\n\n    :return: A 2-tuple consisting of the enumeration and the value groups\n    :rtype: tuple\n    \"\"\"\n    enum, groups = pd.factorize(arr, sort=sort_opt)\n    return enum + 1, groups\n\n\"\"\"Docstring (excerpt)\"\"\"\nGroups a 1D Numpy array by item value, and optionally enumerates the\ngroups, starting from 1. The default or assumed type is a Nunpy\narray, although a Python list, tuple or Pandas series will work too.\n\n:param arr: 1D Numpy array (or list, tuple, or Pandas series)\n:type arr: numpy.ndarray\n\n:return: A 2-tuple consisting of the enumeration and the value groups\n:rtype: tuple"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::factorize_ndarray@199",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "factorize_ndarray",
      "lineno": 199,
      "end_lineno": 228,
      "business_stage": "other",
      "docstring": "Groups an n-D Numpy array by item value, and optionally enumerates the\ngroups, starting from 1. The default or assumed type is a Nunpy\narray, although a Python list, tuple or Pandas series will work too.\n\n:param ndarr: n-D Numpy array (or appropriate Python structure or Pandas dataframe)\n:type ndarr: numpy.ndarray\n\n:param row_idxs: A list of row indices to use for factorization (optional)\n:type row_idxs: list\n\n:param col_idxs: A list of column indices to use for factorization (optional)\n:type col_idxs: list\n\n:return: A 2-tuple consisting of the enumeration and the value groups\n:rtype: tuple",
      "content": "# File: oasislmf/utils/data.py\n# function: factorize_ndarray (lines 199-228)\n\ndef factorize_ndarray(ndarr, row_idxs=[], col_idxs=[], sort_opt=False):\n    \"\"\"\n    Groups an n-D Numpy array by item value, and optionally enumerates the\n    groups, starting from 1. The default or assumed type is a Nunpy\n    array, although a Python list, tuple or Pandas series will work too.\n\n    :param ndarr: n-D Numpy array (or appropriate Python structure or Pandas dataframe)\n    :type ndarr: numpy.ndarray\n\n    :param row_idxs: A list of row indices to use for factorization (optional)\n    :type row_idxs: list\n\n    :param col_idxs: A list of column indices to use for factorization (optional)\n    :type col_idxs: list\n\n    :return: A 2-tuple consisting of the enumeration and the value groups\n    :rtype: tuple\n    \"\"\"\n    if not (row_idxs or col_idxs):\n        raise OasisException('A list of row indices or column indices must be provided')\n\n    _ndarr = ndarr[:, col_idxs].transpose() if col_idxs else ndarr[row_idxs, :]\n    rows, _ = _ndarr.shape\n\n    if rows == 1:\n        return factorize_array(_ndarr[0])\n\n    enum, groups = pd.factorize(fast_zip_arrays(*(arr for arr in _ndarr)), sort=sort_opt)\n\n    return enum + 1, groups\n\n\"\"\"Docstring (excerpt)\"\"\"\nGroups an n-D Numpy array by item value, and optionally enumerates the\ngroups, starting from 1. The default or assumed type is a Nunpy\narray, although a Python list, tuple or Pandas series will work too.\n\n:param ndarr: n-D Numpy array (or appropriate Python structure or Pandas dataframe)\n:type ndarr: numpy.ndarray\n\n:param row_idxs: A list of row indices to use for factorization (optional)\n:type row_idxs: list\n\n:param col_idxs: A list of column indices to use for factorization (optional)\n:type col_idxs: list\n\n:return: A 2-tuple consisting of the enumeration and the value groups\n:rtype: tuple"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::factorize_dataframe@231",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "factorize_dataframe",
      "lineno": 231,
      "end_lineno": 267,
      "business_stage": "other",
      "docstring": "Groups a selection of rows or columns of a Pandas DataFrame array by value,\nand optionally enumerates the groups, starting from 1.\n\n:param df: Pandas DataFrame\n:type: pandas.DataFrame\n\n:param by_row_labels: A list or tuple of row labels\n:type by_row_labels: list, tuple\n\n:param by_row_indices: A list or tuple of row indices\n:type by_row_indices: list, tuple\n\n:param by_col_labels: A list or tuple of column labels\n:type by_col_labels: list, tuple\n\n:param by_col_indices: A list or tuple of column indices\n:type by_col_indices: list, tuple\n\n:return: A 2-tuple consisting of the enumeration and the value groups\n:rtype: tuple",
      "content": "# File: oasislmf/utils/data.py\n# function: factorize_dataframe (lines 231-267)\n\ndef factorize_dataframe(\n        df,\n        by_row_labels=None,\n        by_row_indices=None,\n        by_col_labels=None,\n        by_col_indices=None\n):\n    \"\"\"\n    Groups a selection of rows or columns of a Pandas DataFrame array by value,\n    and optionally enumerates the groups, starting from 1.\n\n    :param df: Pandas DataFrame\n    :type: pandas.DataFrame\n\n    :param by_row_labels: A list or tuple of row labels\n    :type by_row_labels: list, tuple\n\n    :param by_row_indices: A list or tuple of row indices\n    :type by_row_indices: list, tuple\n\n    :param by_col_labels: A list or tuple of column labels\n    :type by_col_labels: list, tuple\n\n    :param by_col_indices: A list or tuple of column indices\n    :type by_col_indices: list, tuple\n\n    :return: A 2-tuple consisting of the enumeration and the value groups\n    :rtype: tuple\n    \"\"\"\n    by_row_indices = by_row_indices or (None if not by_row_labels else [df.index.get_loc(label) for label in by_row_labels])\n    by_col_indices = by_col_indices or (None if not by_col_labels else [df.columns.get_loc(label) for label in by_col_labels])\n\n    return factorize_ndarray(\n        df.values,\n        row_idxs=by_row_indices,\n        col_idxs=by_col_indices\n    )\n\n\"\"\"Docstring (excerpt)\"\"\"\nGroups a selection of rows or columns of a Pandas DataFrame array by value,\nand optionally enumerates the groups, starting from 1.\n\n:param df: Pandas DataFrame\n:type: pandas.DataFrame\n\n:param by_row_labels: A list or tuple of row labels\n:type by_row_labels: list, tuple\n\n:param by_row_indices: A list or tuple of row indices\n:type by_row_indices: list, tuple\n\n:param by_col_labels: A list or tuple of column labels\n:type by_col_labels: list, tuple\n\n:param by_col_indices: A list or tuple of column indices\n:type by_col_indices: list, tuple\n\n:return: A 2-tuple consisting of the enumeration and the value groups\n:rtype: tuple"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::fast_zip_arrays@270",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "fast_zip_arrays",
      "lineno": 270,
      "end_lineno": 282,
      "business_stage": "other",
      "docstring": "Speedy zip of a sequence or ordered iterable of Numpy arrays (Python\niterables with ordered elements such as lists and tuples, or iterators\nor generators of these, will also work).\n\n:param arrays: An iterable or iterator or generator of Numpy arrays\n:type arrays: list, tuple, collections.Iterator, types.GeneratorType\n\n:return: A Numpy 1D array of n-tuples of the zipped sequences\n:rtype: np.array",
      "content": "# File: oasislmf/utils/data.py\n# function: fast_zip_arrays (lines 270-282)\n\ndef fast_zip_arrays(*arrays):\n    \"\"\"\n    Speedy zip of a sequence or ordered iterable of Numpy arrays (Python\n    iterables with ordered elements such as lists and tuples, or iterators\n    or generators of these, will also work).\n\n    :param arrays: An iterable or iterator or generator of Numpy arrays\n    :type arrays: list, tuple, collections.Iterator, types.GeneratorType\n\n    :return: A Numpy 1D array of n-tuples of the zipped sequences\n    :rtype: np.array\n    \"\"\"\n    return pd._libs.lib.fast_zip([arr for arr in arrays])\n\n\"\"\"Docstring (excerpt)\"\"\"\nSpeedy zip of a sequence or ordered iterable of Numpy arrays (Python\niterables with ordered elements such as lists and tuples, or iterators\nor generators of these, will also work).\n\n:param arrays: An iterable or iterator or generator of Numpy arrays\n:type arrays: list, tuple, collections.Iterator, types.GeneratorType\n\n:return: A Numpy 1D array of n-tuples of the zipped sequences\n:rtype: np.array"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::fast_zip_dataframe_columns@285",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "fast_zip_dataframe_columns",
      "lineno": 285,
      "end_lineno": 300,
      "business_stage": "other",
      "docstring": "Speedy zip of a sequence or ordered iterable of Pandas DataFrame columns\n(Python iterables with ordered elements such as lists and tuples, or\niterators or generators of these, will also work).\n\n:param df: Pandas DataFrame\n:type df: pandas.DataFrame\n\n:param cols: An iterable or iterator or generator of Pandas DataFrame columns\n:type cols: list, tuple, collections.Iterator, types.GeneratorType\n\n:return: A Numpy 1D array of n-tuples of the dataframe columns to be zipped\n:rtype: np.array",
      "content": "# File: oasislmf/utils/data.py\n# function: fast_zip_dataframe_columns (lines 285-300)\n\ndef fast_zip_dataframe_columns(df, cols):\n    \"\"\"\n    Speedy zip of a sequence or ordered iterable of Pandas DataFrame columns\n    (Python iterables with ordered elements such as lists and tuples, or\n    iterators or generators of these, will also work).\n\n    :param df: Pandas DataFrame\n    :type df: pandas.DataFrame\n\n    :param cols: An iterable or iterator or generator of Pandas DataFrame columns\n    :type cols: list, tuple, collections.Iterator, types.GeneratorType\n\n    :return: A Numpy 1D array of n-tuples of the dataframe columns to be zipped\n    :rtype: np.array\n    \"\"\"\n    return fast_zip_arrays(*(df[col].values for col in cols))\n\n\"\"\"Docstring (excerpt)\"\"\"\nSpeedy zip of a sequence or ordered iterable of Pandas DataFrame columns\n(Python iterables with ordered elements such as lists and tuples, or\niterators or generators of these, will also work).\n\n:param df: Pandas DataFrame\n:type df: pandas.DataFrame\n\n:param cols: An iterable or iterator or generator of Pandas DataFrame columns\n:type cols: list, tuple, collections.Iterator, types.GeneratorType\n\n:return: A Numpy 1D array of n-tuples of the dataframe columns to be zipped\n:rtype: np.array"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::establish_correlations@303",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "establish_correlations",
      "lineno": 303,
      "end_lineno": 322,
      "business_stage": "other",
      "docstring": "Checks the model settings to see if correlations are present.\n\nArgs:\n    model_settings: (dict) the model settings that are going to be checked\n\nReturns: (bool) True if correlations, False if not",
      "content": "# File: oasislmf/utils/data.py\n# function: establish_correlations (lines 303-322)\n\ndef establish_correlations(model_settings: dict) -> bool:\n    \"\"\"\n    Checks the model settings to see if correlations are present.\n\n    Args:\n        model_settings: (dict) the model settings that are going to be checked\n\n    Returns: (bool) True if correlations, False if not\n    \"\"\"\n    key = 'correlation_settings'\n    correlations_legacy: Optional[List[dict]] = model_settings.get(key, [])\n    correlations: Optional[List[dict]] = model_settings.get(\"model_settings\", {}).get(key, correlations_legacy)\n\n    if correlations is None:\n        return False\n    if not isinstance(correlations, list):\n        return False\n    if len(correlations) == 0:\n        return False\n    return True\n\n\"\"\"Docstring (excerpt)\"\"\"\nChecks the model settings to see if correlations are present.\n\nArgs:\n    model_settings: (dict) the model settings that are going to be checked\n\nReturns: (bool) True if correlations, False if not"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::detect_encoding@325",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "detect_encoding",
      "lineno": 325,
      "end_lineno": 344,
      "business_stage": "other",
      "docstring": "Given a path to a CSV of unknown encoding\nread lines to detects its encoding type\n\n:param filepath: Filepath to check\n:type  filepath: str\n\n:return: Example `{'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}`\n:rtype: dict",
      "content": "# File: oasislmf/utils/data.py\n# function: detect_encoding (lines 325-344)\n\ndef detect_encoding(filepath):\n    \"\"\"\n    Given a path to a CSV of unknown encoding\n    read lines to detects its encoding type\n\n    :param filepath: Filepath to check\n    :type  filepath: str\n\n    :return: Example `{'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}`\n    :rtype: dict\n    \"\"\"\n\n    detector = UniversalDetector()\n    with io.open(filepath, 'rb') as f:\n        for line in f:\n            detector.feed(line)\n            if detector.done:\n                break\n    detector.close()\n    return detector.result\n\n\"\"\"Docstring (excerpt)\"\"\"\nGiven a path to a CSV of unknown encoding\nread lines to detects its encoding type\n\n:param filepath: Filepath to check\n:type  filepath: str\n\n:return: Example `{'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}`\n:rtype: dict"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::get_dataframe@347",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "get_dataframe",
      "lineno": 347,
      "end_lineno": 572,
      "business_stage": "other",
      "docstring": "Loads a Pandas dataframe from a source CSV or JSON file, or a text buffer\nof such a file (``io.StringIO``), or another Pandas dataframe.\n\n:param src_fp: Source CSV or JSON file path (optional)\n:type src_fp: str\n\n:param src_type: Type of source file -CSV or JSON (optional; default is csv)\n:param src_type: str\n\n:param src_buf: Text buffer of a source CSV or JSON file (optional)\n:type src_buf: io.StringIO\n\n:param float_precision: Indicates whether to support high-precision numbers\n                        present in the data (optional; default is high)\n:type float_precision: str\n\n:param empty_data_error_msg: The message of the exception that is thrown\n                            there is no data content, i.e no rows\n                            (optional)\n:type empty_data_error_msg: str\n\n:param lowercase_cols: Whether to convert the dataframe columns to lowercase\n                       (optional; default is True)\n:type lowercase_cols: bool\n\n:param required_cols: An iterable of columns required to be present in the\n                      source data (optional)\n:type required_cols: list, tuple, collections.Iterable\n\n:param col_defaults: A dict of column names and their default values. This\n                     can include both existing columns and new columns -\n                     defaults for existing columns are set row-wise using\n                     pd.DataFrame.fillna, while defaults for non-existent\n                     columns are set column-wise using assignment (optional)\n:type col_defaults: dict\n\n:param non_na_cols: An iterable of names of columns which must be dropped\n                    if they contain any null values (optional)\n:type non_na_cols: list, tuple, collections.Iterable\n\n:param col_dtypes: A dict of column names and corresponding data types -\n                   Python built-in datatypes are accepted but are mapped\n                   to the corresponding Numpy datatypes (optional)\n:type col_dtypes: dict\n\n:param sort_cols: An iterable of column names by which to sort the frame\n                  rows (optional)\n:type sort_cols: list, tuple, collections.Iterable\n\n:param sort_ascending: Whether to perform an ascending or descending sort -\n                       is used only in conjunction with the sort_cols\n                       option (optional)\n:type sort_ascending: bool\n\n:param memory_map: Memory-efficient option used when loading a frame from\n                   a file or text buffer - is a direct optional argument\n                   for the pd.read_csv method\n:type memory_map: bool\n\n:param low_memory: Internally process the file in chunks, resulting in lower memory use\n                   while parsing, but possibly mixed type inference.\n                   To ensure no mixed types either set False,\n:type low_memory: bool\n\n:param encoding: Try to read CSV of JSON data with the given encoding type,\n                 if 'None' will try to auto-detect on UnicodeDecodeError\n:type  encoding: str\n\n\n\n:return: A Pandas dataframe\n:rtype: pd.DataFrame",
      "content": "# File: oasislmf/utils/data.py\n# function: get_dataframe (lines 347-572)\n\ndef get_dataframe(\n        src_fp=None,\n        src_type=None,\n        src_buf=None,\n        src_data=None,\n        float_precision='high',\n        empty_data_error_msg=None,\n        lowercase_cols=True,\n        required_cols=(),\n        col_defaults={},\n        non_na_cols=(),\n        col_dtypes={},\n        sort_cols=None,\n        sort_ascending=None,\n        memory_map=False,\n        low_memory=False,\n        encoding=None\n):\n    \"\"\"\n    Loads a Pandas dataframe from a source CSV or JSON file, or a text buffer\n    of such a file (``io.StringIO``), or another Pandas dataframe.\n\n    :param src_fp: Source CSV or JSON file path (optional)\n    :type src_fp: str\n\n    :param src_type: Type of source file -CSV or JSON (optional; default is csv)\n    :param src_type: str\n\n    :param src_buf: Text buffer of a source CSV or JSON file (optional)\n    :type src_buf: io.StringIO\n\n    :param float_precision: Indicates whether to support high-precision numbers\n                            present in the data (optional; default is high)\n    :type float_precision: str\n\n    :param empty_data_error_msg: The message of the exception that is thrown\n                                there is no data content, i.e no rows\n                                (optional)\n    :type empty_data_error_msg: str\n\n    :param lowercase_cols: Whether to convert the dataframe columns to lowercase\n                           (optional; default is True)\n    :type lowercase_cols: bool\n\n    :param required_cols: An iterable of columns required to be present in the\n                          source data (optional)\n    :type required_cols: list, tuple, collections.Iterable\n\n    :param col_defaults: A dict of column names and their default values. This\n                         can include both existing columns and new columns -\n                         defaults for existing columns are set row-wise using\n                         pd.DataFrame.fillna, while defaults for non-existent\n                         columns are set column-wise using assignment (optional)\n    :type col_defaults: dict\n\n    :param non_na_cols: An iterable of names of columns which must be dropped\n                        if they contain any null values (optional)\n    :type non_na_cols: list, tuple, collections.Iterable\n\n    :param col_dtypes: A dict of column names and corresponding data types -\n                       Python built-in datatypes are accepted but are mapped\n                       to the corresponding Numpy datatypes (optional)\n    :type col_dtypes: dict\n\n    :param sort_cols: An iterable of column names by which to sort the frame\n                      rows (optional)\n    :type sort_cols: list, tuple, collections.Iterable\n\n    :param sort_ascending: Whether to perform an ascending or descending sort -\n                           is used only in conjunction with the sort_cols\n                           option (optional)\n    :type sort_ascending: bool\n\n    :param memory_map: Memory-efficient option used when loading a frame from\n                       a file or text buffer - is a direct optional argument\n                       for the pd.read_csv method\n    :type memory_map: bool\n\n    :param low_memory: Internally process the file in chunks, resulting in lower memory use\n                       while parsing, but possibly mixed type inference.\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoads a Pandas dataframe from a source CSV or JSON file, or a text buffer\nof such a file (``io.StringIO``), or another Pandas dataframe.\n\n:param src_fp: Source CSV or JSON file path (optional)\n:type src_fp: str\n\n:param src_type: Type of source file -CSV or JSON (optional; default is csv)\n:param src_type: str\n\n:param src_buf: Text buffer of a source CSV or JSON file (optional)\n:type src_buf: io.StringIO\n\n:param float_precision: Indicates whether to support high-precision numbers\n                        present in the data (optional; default is high)\n:type float_precision: str\n\n:param empty_data_error_msg: The message of the exception that is thrown\n                            there is no data content, i.e no rows\n                            (optional)\n:type empty_data_error_msg: str\n\n:param lowercase_cols: Whether to convert the dataframe columns to lowercase\n                       (optional; default is True)\n:type lowercase_cols: bool\n\n:param required_cols: An iterable of columns required to be present in the\n                      source data (optional)\n:type required_cols: list, tuple, collections.Iterable\n\n:param col_defaults: A dict of column names and their default values. This\n                     can include both existing columns and new columns -\n                     defaults for existing columns are set row-wise using\n                     pd.DataFrame.fillna, while defaults for non-existent\n                     columns are set column-wise using assignment (optional)\n:type col_defaults: dict\n\n:param non_na_cols: An iterable of names of columns which must be dropped\n                    if they contain any null values (optional)\n:type non_na_cols: list, tuple, collections.Iterable\n\n:param col_dtypes: A dict of column names and corresponding data types -\n                   Python built-in datatypes are accepted but are mapped\n                   to the corresponding Numpy datatypes (optional)\n:type col_dtypes: dict\n\n:param sort_cols: An iterable of column names by which to sort the frame\n                  rows (optional)\n:type sort_cols: list, tuple, collections.Iterable\n\n:param sort_ascending: Whether to perform an ascending or descending sort -\n                       is used only in conjunction with the sort_cols\n                       option (optional)\n:type sort_ascending: bool\n\n:param memory_map: Memory-efficient option used when loading a frame from\n                   a file or text buffer - is a direct optional argument\n                   for the pd.read_csv method\n:type memory_map: bool\n\n:param low_memory: Internally process the file in chunks, resulting in lower memory use\n                   while parsing, but possibly mixed type inference.\n                   To ensure no mixed types either set False,\n:type low_memory: bool\n\n:param encoding: Try to read CSV of JSON data with the given encoding type,\n                 if 'None' will try to auto-detect on UnicodeDecodeError\n:type  encoding: str\n\n\n\n:return: A Pandas dataframe\n:rtype: pd.DataFrame"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::get_dtypes_and_required_cols@575",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "get_dtypes_and_required_cols",
      "lineno": 575,
      "end_lineno": 599,
      "business_stage": "other",
      "docstring": "Get OED column data types and required column names from JSON.\n\n:param all_dtypes: If true return every dtype field, otherwise only categoricals\n:type all_dtypes: boolean\n\n:param get_dtypes: method to get dict from JSON\n:type get_dtypes: function",
      "content": "# File: oasislmf/utils/data.py\n# function: get_dtypes_and_required_cols (lines 575-599)\n\ndef get_dtypes_and_required_cols(get_dtypes, all_dtypes=False):\n    \"\"\"\n    Get OED column data types and required column names from JSON.\n\n    :param all_dtypes: If true return every dtype field, otherwise only categoricals\n    :type all_dtypes: boolean\n\n    :param get_dtypes: method to get dict from JSON\n    :type get_dtypes: function\n    \"\"\"\n    dtypes = get_dtypes()\n\n    if all_dtypes:\n        col_dtypes = {k: v['py_dtype'].lower() for k, v in dtypes.items()}\n    else:\n        col_dtypes = {\n            k: v['py_dtype'].lower() for k, v in dtypes.items() if v['py_dtype'] == 'category'\n        }\n\n    required_cols = [\n        k for k, v in dtypes.items()\n        if v['require_field'] == 'R'\n    ]\n\n    return col_dtypes, required_cols\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet OED column data types and required column names from JSON.\n\n:param all_dtypes: If true return every dtype field, otherwise only categoricals\n:type all_dtypes: boolean\n\n:param get_dtypes: method to get dict from JSON\n:type get_dtypes: function"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::get_ids@602",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "get_ids",
      "lineno": 602,
      "end_lineno": 647,
      "business_stage": "other",
      "docstring": "Enumerates (counts) the rows of a given dataframe in a given subset\nof dataframe columns, and optionally does the enumeration with\nrespect to subgroups of the column subset.\n\n:param df: Input dataframe\n:type df: pandas.DataFrame\n\n:param usecols: The column subset\n:param usecols: list\n\n:param group_by: A subset of the column subset to use a subgroup key\n:param group_by: list\n\n:param sort_keys: Sort keys by value before assigning ids\n:param sort_keys: Boolean\n\n    Example if sort_keys=True:\n    -----------------\n    index  PortNumber AccNumber    locnumbera  id (returned)\n        0           1    A11111  10002082049    3\n        1           1    A11111  10002082050    4\n        2           1    A11111  10002082051    5\n        3           1    A11111  10002082053    7\n        4           1    A11111  10002082054    8\n        5           1    A11111  10002082052    6\n        6           1    A11111  10002082046    1\n        7           1    A11111  10002082046    1\n        8           1    A11111  10002082048    2\n        9           1    A11111  10002082055    9\n\n:return: The enumeration\n:rtype: numpy.ndarray",
      "content": "# File: oasislmf/utils/data.py\n# function: get_ids (lines 602-647)\n\ndef get_ids(df, usecols, group_by=[], sort_keys=True):\n    \"\"\"\n    Enumerates (counts) the rows of a given dataframe in a given subset\n    of dataframe columns, and optionally does the enumeration with\n    respect to subgroups of the column subset.\n\n    :param df: Input dataframe\n    :type df: pandas.DataFrame\n\n    :param usecols: The column subset\n    :param usecols: list\n\n    :param group_by: A subset of the column subset to use a subgroup key\n    :param group_by: list\n\n    :param sort_keys: Sort keys by value before assigning ids\n    :param sort_keys: Boolean\n\n        Example if sort_keys=True:\n        -----------------\n        index  PortNumber AccNumber    locnumbera  id (returned)\n            0           1    A11111  10002082049    3\n            1           1    A11111  10002082050    4\n            2           1    A11111  10002082051    5\n            3           1    A11111  10002082053    7\n            4           1    A11111  10002082054    8\n            5           1    A11111  10002082052    6\n            6           1    A11111  10002082046    1\n            7           1    A11111  10002082046    1\n            8           1    A11111  10002082048    2\n            9           1    A11111  10002082055    9\n\n    :return: The enumeration\n    :rtype: numpy.ndarray\n    \"\"\"\n    _usecols = group_by + list(set(usecols).difference(group_by))\n\n    if not group_by:\n        if sort_keys:\n            sorted_df = df.loc[:, usecols].sort_values(by=usecols, kind='stable')\n            sorted_df['ids'] = factorize_ndarray(sorted_df.values, col_idxs=range(len(_usecols)))[0]\n            return sorted_df.sort_index()['ids'].to_list()\n        else:\n            return factorize_ndarray(df.loc[:, usecols].values, col_idxs=range(len(_usecols)))[0]\n    else:\n        return (df[usecols].groupby(group_by, observed=True).cumcount()) + 1\n\n\"\"\"Docstring (excerpt)\"\"\"\nEnumerates (counts) the rows of a given dataframe in a given subset\nof dataframe columns, and optionally does the enumeration with\nrespect to subgroups of the column subset.\n\n:param df: Input dataframe\n:type df: pandas.DataFrame\n\n:param usecols: The column subset\n:param usecols: list\n\n:param group_by: A subset of the column subset to use a subgroup key\n:param group_by: list\n\n:param sort_keys: Sort keys by value before assigning ids\n:param sort_keys: Boolean\n\n    Example if sort_keys=True:\n    -----------------\n    index  PortNumber AccNumber    locnumbera  id (returned)\n        0           1    A11111  10002082049    3\n        1           1    A11111  10002082050    4\n        2           1    A11111  10002082051    5\n        3           1    A11111  10002082053    7\n        4           1    A11111  10002082054    8\n        5           1    A11111  10002082052    6\n        6           1    A11111  10002082046    1\n        7           1    A11111  10002082046    1\n        8           1    A11111  10002082048    2\n        9           1    A11111  10002082055    9\n\n:return: The enumeration\n:rtype: numpy.ndarray"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::get_json@650",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "get_json",
      "lineno": 650,
      "end_lineno": 664,
      "business_stage": "other",
      "docstring": "Loads JSON from file.\n\n:param src_fp: Source JSON file path\n:type src_fp: str\n\n:return: dict\n:rtype: dict",
      "content": "# File: oasislmf/utils/data.py\n# function: get_json (lines 650-664)\n\ndef get_json(src_fp):\n    \"\"\"\n    Loads JSON from file.\n\n    :param src_fp: Source JSON file path\n    :type src_fp: str\n\n    :return: dict\n    :rtype: dict\n    \"\"\"\n    try:\n        with io.open(src_fp, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except (IOError, JSONDecodeError, OSError, TypeError):\n        raise OasisException('Error trying to load JSON from {}'.format(src_fp))\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoads JSON from file.\n\n:param src_fp: Source JSON file path\n:type src_fp: str\n\n:return: dict\n:rtype: dict"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::get_timestamp@667",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "get_timestamp",
      "lineno": 667,
      "end_lineno": 680,
      "business_stage": "other",
      "docstring": "Get a timestamp string from a ``datetime.datetime`` object\n\n:param thedate: ``datetime.datetime`` object\n:type thedate: datetime.datetime\n\n:param fmt: Timestamp format string\n:type fmt: str\n\n:return: Timestamp string\n:rtype: str",
      "content": "# File: oasislmf/utils/data.py\n# function: get_timestamp (lines 667-680)\n\ndef get_timestamp(thedate=datetime.now(), fmt='%Y%m%d%H%M%S'):\n    \"\"\"\n    Get a timestamp string from a ``datetime.datetime`` object\n\n    :param thedate: ``datetime.datetime`` object\n    :type thedate: datetime.datetime\n\n    :param fmt: Timestamp format string\n    :type fmt: str\n\n    :return: Timestamp string\n    :rtype: str\n    \"\"\"\n    return thedate.strftime(fmt)\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet a timestamp string from a ``datetime.datetime`` object\n\n:param thedate: ``datetime.datetime`` object\n:type thedate: datetime.datetime\n\n:param fmt: Timestamp format string\n:type fmt: str\n\n:return: Timestamp string\n:rtype: str"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::get_utctimestamp@683",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "get_utctimestamp",
      "lineno": 683,
      "end_lineno": 696,
      "business_stage": "other",
      "docstring": "Get a UTC timestamp string from a ``datetime.datetime`` object\n\n:param thedate: ``datetime.datetime`` object\n:type thedate: datetime.datetime\n\n:param fmt: Timestamp format string, default is \"%Y-%b-%d %H:%M:%S\"\n:type fmt: str\n\n:return: UTC timestamp string\n:rtype: str",
      "content": "# File: oasislmf/utils/data.py\n# function: get_utctimestamp (lines 683-696)\n\ndef get_utctimestamp(thedate=datetime.utcnow(), fmt='%Y-%b-%d %H:%M:%S'):\n    \"\"\"\n    Get a UTC timestamp string from a ``datetime.datetime`` object\n\n    :param thedate: ``datetime.datetime`` object\n    :type thedate: datetime.datetime\n\n    :param fmt: Timestamp format string, default is \"%Y-%b-%d %H:%M:%S\"\n    :type fmt: str\n\n    :return: UTC timestamp string\n    :rtype: str\n    \"\"\"\n    return thedate.astimezone(pytz.utc).strftime(fmt)\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet a UTC timestamp string from a ``datetime.datetime`` object\n\n:param thedate: ``datetime.datetime`` object\n:type thedate: datetime.datetime\n\n:param fmt: Timestamp format string, default is \"%Y-%b-%d %H:%M:%S\"\n:type fmt: str\n\n:return: UTC timestamp string\n:rtype: str"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::merge_check@699",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "merge_check",
      "lineno": 699,
      "end_lineno": 726,
      "business_stage": "other",
      "docstring": "Check two dataframes for keys intersection, use before performing a merge\n\n:param left: The first of two dataframes to be merged\n:type left: pd.DataFrame\n\n:param right: The second of two dataframes to be merged\n:type left: pd.DataFrame\n\n:param on: column keys to test\n:type on: list\n\n:return: A dict of booleans, True for an intersection between left/right\n:rtype: dict\n\n{'PortNumber': False, 'AccNumber': True, 'layer_id': True, 'condnumber': True}",
      "content": "# File: oasislmf/utils/data.py\n# function: merge_check (lines 699-726)\n\ndef merge_check(left, right, on=[], raise_error=True):\n    \"\"\"\n    Check two dataframes for keys intersection, use before performing a merge\n\n    :param left: The first of two dataframes to be merged\n    :type left: pd.DataFrame\n\n    :param right: The second of two dataframes to be merged\n    :type left: pd.DataFrame\n\n    :param on: column keys to test\n    :type on: list\n\n    :return: A dict of booleans, True for an intersection between left/right\n    :rtype: dict\n\n    {'PortNumber': False, 'AccNumber': True, 'layer_id': True, 'condnumber': True}\n    \"\"\"\n    keys_checked = {}\n    for key in on:\n        key_intersect = set(left[key].unique()).intersection(right[key].unique())\n        keys_checked[key] = bool(key_intersect)\n\n    if raise_error and not all(keys_checked.values()):\n        err_msg = \"Error: Merge mismatch on column(s) {}\".format(\n            [k for k in keys_checked if not keys_checked[k]]\n        )\n        raise OasisException(err_msg)\n\n\"\"\"Docstring (excerpt)\"\"\"\nCheck two dataframes for keys intersection, use before performing a merge\n\n:param left: The first of two dataframes to be merged\n:type left: pd.DataFrame\n\n:param right: The second of two dataframes to be merged\n:type left: pd.DataFrame\n\n:param on: column keys to test\n:type on: list\n\n:return: A dict of booleans, True for an intersection between left/right\n:rtype: dict\n\n{'PortNumber': False, 'AccNumber': True, 'layer_id': True, 'condnumber': True}"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::merge_dataframes@729",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "merge_dataframes",
      "lineno": 729,
      "end_lineno": 774,
      "business_stage": "other",
      "docstring": "Merges two dataframes by ensuring there is no duplication of columns.\n\n:param left: The first of two dataframes to be merged\n:type left: pd.DataFrame\n\n:param right: The second of two dataframes to be merged\n:type left: pd.DataFrame\n\n:param kwargs: Optional keyword arguments passed directly to the underlying\n               pd.merge method that is called, including options for the\n               join keys, join type, etc. - please see the pd.merge\n               documentation for details of these optional arguments\n:type kwargs: dict\n\n:return: A merged dataframe\n:rtype: pd.DataFrame",
      "content": "# File: oasislmf/utils/data.py\n# function: merge_dataframes (lines 729-774)\n\ndef merge_dataframes(left, right, join_on=None, **kwargs):\n    \"\"\"\n    Merges two dataframes by ensuring there is no duplication of columns.\n\n    :param left: The first of two dataframes to be merged\n    :type left: pd.DataFrame\n\n    :param right: The second of two dataframes to be merged\n    :type left: pd.DataFrame\n\n    :param kwargs: Optional keyword arguments passed directly to the underlying\n                   pd.merge method that is called, including options for the\n                   join keys, join type, etc. - please see the pd.merge\n                   documentation for details of these optional arguments\n    :type kwargs: dict\n\n    :return: A merged dataframe\n    :rtype: pd.DataFrame\n    \"\"\"\n    if not join_on:\n        left_keys = kwargs.get('left_on') or kwargs.get('on') or []\n        left_keys = [left_keys] if isinstance(left_keys, str) else left_keys\n\n        drop_cols = [\n            k for k in set(left.columns).intersection(right.columns)\n            if k and k not in left_keys\n        ]\n        drop_duplicates = kwargs.get('drop_duplicates', True)\n        kwargs.pop('drop_duplicates') if 'drop_duplicates' in kwargs else None\n\n        merge = pd.merge(\n            left.drop(drop_cols, axis=1),\n            right,\n            **kwargs\n        )\n\n        return merge if not drop_duplicates else merge.drop_duplicates()\n    else:\n        _join_on = [join_on] if isinstance(join_on, str) else join_on.copy()\n        drop_cols = list(set(left.columns).intersection(right.columns).difference(_join_on))\n        left = left.set_index(_join_on)\n        right = right.drop(drop_cols, axis=1).set_index(_join_on)\n\n        join = left.join(right, how=(kwargs.get('how') or 'left')).reset_index()\n\n        return join\n\n\"\"\"Docstring (excerpt)\"\"\"\nMerges two dataframes by ensuring there is no duplication of columns.\n\n:param left: The first of two dataframes to be merged\n:type left: pd.DataFrame\n\n:param right: The second of two dataframes to be merged\n:type left: pd.DataFrame\n\n:param kwargs: Optional keyword arguments passed directly to the underlying\n               pd.merge method that is called, including options for the\n               join keys, join type, etc. - please see the pd.merge\n               documentation for details of these optional arguments\n:type kwargs: dict\n\n:return: A merged dataframe\n:rtype: pd.DataFrame"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::print_dataframe@901",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "print_dataframe",
      "lineno": 901,
      "end_lineno": 978,
      "business_stage": "other",
      "docstring": "A method to pretty-print a Pandas dataframe - calls on the ``tabulate``\npackage\n\n:param df: The dataframe to pretty-print\n:type df: pd.DataFrame\n\n:param cols: An iterable of names of columns whose values should\n                       be printed (optional). If unset, all columns will be printed.\n:type cols: list, tuple, collections.Iterable\n\n:param string_cols: An iterable of names of columns whose values should\n                       be treated as strings (optional)\n:type string_cols: list, tuple, collections.Iterable\n\n:param show_index: Whether to display the index column in the printout\n                   (optional; default is False)\n:type show_index: bool\n\n:param frame_header: Header string to display on top of the printed\n                     dataframe (optional)\n:type frame_header: str\n\n:param column_headers: Column header format - see the tabulate.tabulate\n                    method documentation (optional, default is 'keys')\n:type column_headers: list, str\n\n:param tablefmt: Table format - see the tabulate.tabulate method\n                 documentation (optional; default is 'psql')\n:type tablefmt: str, list, tuple\n\n:param floatfmt: Floating point format - see the tabulate.tabulate\n                method documnetation (optional; default is \".2f\")\n:type floatfmt: str\n\n:param end: String to append after printing the dataframe\n            (optional; default is newline)\n:type end: str\n\n:param tabulate_kwargs: Additional optional arguments passed directly to\n                        the underlying tabulate.tabulate method - see the\n                        method documentation for more details\n:param tabulate_kwargs: dict",
      "content": "# File: oasislmf/utils/data.py\n# function: print_dataframe (lines 901-978)\n\ndef print_dataframe(\n        df,\n        cols=[],\n        string_cols=[],\n        show_index=False,\n        frame_header=None,\n        column_headers='keys',\n        tablefmt='psql',\n        floatfmt=\",.2f\",\n        end='\\n',\n        **tabulate_kwargs\n):\n    \"\"\"\n    A method to pretty-print a Pandas dataframe - calls on the ``tabulate``\n    package\n\n    :param df: The dataframe to pretty-print\n    :type df: pd.DataFrame\n\n    :param cols: An iterable of names of columns whose values should\n                           be printed (optional). If unset, all columns will be printed.\n    :type cols: list, tuple, collections.Iterable\n\n    :param string_cols: An iterable of names of columns whose values should\n                           be treated as strings (optional)\n    :type string_cols: list, tuple, collections.Iterable\n\n    :param show_index: Whether to display the index column in the printout\n                       (optional; default is False)\n    :type show_index: bool\n\n    :param frame_header: Header string to display on top of the printed\n                         dataframe (optional)\n    :type frame_header: str\n\n    :param column_headers: Column header format - see the tabulate.tabulate\n                        method documentation (optional, default is 'keys')\n    :type column_headers: list, str\n\n    :param tablefmt: Table format - see the tabulate.tabulate method\n                     documentation (optional; default is 'psql')\n    :type tablefmt: str, list, tuple\n\n    :param floatfmt: Floating point format - see the tabulate.tabulate\n                    method documnetation (optional; default is \".2f\")\n    :type floatfmt: str\n\n    :param end: String to append after printing the dataframe\n                (optional; default is newline)\n    :type end: str\n\n    :param tabulate_kwargs: Additional optional arguments passed directly to\n                            the underlying tabulate.tabulate method - see the\n                            method documentation for more details\n    :param tabulate_kwargs: dict\n    \"\"\"\n    _df = df.copy(deep=True)\n\n    if cols is not None and len(cols) > 0:\n        _df = _df[cols]\n\n    for col in string_cols:\n        _df[col] = _df[col].astype(object)\n\n    if frame_header:\n        print('\\n{}'.format(frame_header))\n\n    if tabulate_kwargs:\n        tabulate_kwargs.pop('headers') if 'headers' in tabulate_kwargs else None\n        tabulate_kwargs.pop('tablefmt') if 'tablefmt' in tabulate_kwargs else None\n        tabulate_kwargs.pop('floatfmt') if 'floatfmt' in tabulate_kwargs else None\n        tabulate_kwargs.pop('showindex') if 'showindex' in tabulate_kwargs else None\n\n    print(\n        tabulate(\n            _df, headers=column_headers, tablefmt=tablefmt,\n            showindex=show_index, floatfmt=floatfmt, **tabulate_kwargs),\n        end=end)\n\n\"\"\"Docstring (excerpt)\"\"\"\nA method to pretty-print a Pandas dataframe - calls on the ``tabulate``\npackage\n\n:param df: The dataframe to pretty-print\n:type df: pd.DataFrame\n\n:param cols: An iterable of names of columns whose values should\n                       be printed (optional). If unset, all columns will be printed.\n:type cols: list, tuple, collections.Iterable\n\n:param string_cols: An iterable of names of columns whose values should\n                       be treated as strings (optional)\n:type string_cols: list, tuple, collections.Iterable\n\n:param show_index: Whether to display the index column in the printout\n                   (optional; default is False)\n:type show_index: bool\n\n:param frame_header: Header string to display on top of the printed\n                     dataframe (optional)\n:type frame_header: str\n\n:param column_headers: Column header format - see the tabulate.tabulate\n                    method documentation (optional, default is 'keys')\n:type column_headers: list, str\n\n:param tablefmt: Table format - see the tabulate.tabulate method\n                 documentation (optional; default is 'psql')\n:type tablefmt: str, list, tuple\n\n:param floatfmt: Floating point format - see the tabulate.tabulate\n                method documnetation (optional; default is \".2f\")\n:type floatfmt: str\n\n:param end: String to append after printing the dataframe\n            (optional; default is newline)\n:type end: str\n\n:param tabulate_kwargs: Additional optional arguments passed directly to\n                        the underlying tabulate.tabulate method - see the\n                        method documentation for more details\n:param tabulate_kwargs: dict"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::set_dataframe_column_dtypes@981",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "set_dataframe_column_dtypes",
      "lineno": 981,
      "end_lineno": 1003,
      "business_stage": "other",
      "docstring": "A method to set column datatypes for a Pandas dataframe\n\n:param df: The dataframe to process\n:type df: pd.DataFrame\n\n:param dtypes: A dict of column names and corresponding Numpy datatypes -\n               Python built-in datatypes can be passed in but they will be\n               mapped to the corresponding Numpy datatypes\n:type dtypes: dict\n\n:return: The processed dataframe with column datatypes set\n:rtype: pandas.DataFrame",
      "content": "# File: oasislmf/utils/data.py\n# function: set_dataframe_column_dtypes (lines 981-1003)\n\ndef set_dataframe_column_dtypes(df, dtypes):\n    \"\"\"\n    A method to set column datatypes for a Pandas dataframe\n\n    :param df: The dataframe to process\n    :type df: pd.DataFrame\n\n    :param dtypes: A dict of column names and corresponding Numpy datatypes -\n                   Python built-in datatypes can be passed in but they will be\n                   mapped to the corresponding Numpy datatypes\n    :type dtypes: dict\n\n    :return: The processed dataframe with column datatypes set\n    :rtype: pandas.DataFrame\n    \"\"\"\n    existing_cols = list(set(dtypes).intersection(df.columns))\n    _dtypes = {\n        col: PANDAS_BASIC_DTYPES[getattr(builtins, dtype) if dtype in ('int', 'bool', 'float', 'object', 'str',) else dtype]\n        for col, dtype in [(_col, dtypes[_col]) for _col in existing_cols]\n    }\n    df = df.astype(_dtypes)\n\n    return df\n\n\"\"\"Docstring (excerpt)\"\"\"\nA method to set column datatypes for a Pandas dataframe\n\n:param df: The dataframe to process\n:type df: pd.DataFrame\n\n:param dtypes: A dict of column names and corresponding Numpy datatypes -\n               Python built-in datatypes can be passed in but they will be\n               mapped to the corresponding Numpy datatypes\n:type dtypes: dict\n\n:return: The processed dataframe with column datatypes set\n:rtype: pandas.DataFrame"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::validate_vuln_csv_contents@1006",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "validate_vuln_csv_contents",
      "lineno": 1006,
      "end_lineno": 1042,
      "business_stage": "other",
      "docstring": "Validate the contents of the CSV file for vulnerability replacements.\n\nArgs:\n    file_path (str): Path to the vulnerability CSV file\n\nReturns:\n    bool: True if the file is valid, False otherwise",
      "content": "# File: oasislmf/utils/data.py\n# function: validate_vuln_csv_contents (lines 1006-1042)\n\ndef validate_vuln_csv_contents(file_path):\n    \"\"\"\n    Validate the contents of the CSV file for vulnerability replacements.\n\n    Args:\n        file_path (str): Path to the vulnerability CSV file\n\n    Returns:\n        bool: True if the file is valid, False otherwise\n    \"\"\"\n    expected_columns = ['vulnerability_id', 'intensity_bin_id', 'damage_bin_id', 'probability']\n    try:\n        vuln_df = pd.read_csv(file_path)\n        if list(vuln_df.columns) != expected_columns:\n            logger.warning(f\"CSV file {file_path} does not have the expected columns.\")\n            return False\n\n        # Check data types and constraints\n        if not (\n            np.issubdtype(vuln_df['vulnerability_id'].dtype, np.integer)\n            and np.issubdtype(vuln_df['intensity_bin_id'].dtype, np.integer)\n            and np.issubdtype(vuln_df['damage_bin_id'].dtype, np.integer)\n        ):\n            logger.warning(\"vulnerability_id, intensity_bin_id, and damage_bin_id columns must contain integer values.\")\n            return False\n        if vuln_df['probability'].apply(lambda x: isinstance(x, (int, float))).all():\n            if not (vuln_df['probability'].between(0, 1).all()):\n                logger.warning(\"probability column must contain values between 0 and 1.\")\n                return False\n        else:\n            logger.warning(\"probability column must contain numeric values.\")\n            return False\n        return True\n    except Exception as e:\n        # No fail if the file is not valid, just warn the user\n        logger.warning(f\"Error occurred while validating CSV file: {e}\")\n        return False\n\n\"\"\"Docstring (excerpt)\"\"\"\nValidate the contents of the CSV file for vulnerability replacements.\n\nArgs:\n    file_path (str): Path to the vulnerability CSV file\n\nReturns:\n    bool: True if the file is valid, False otherwise"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::validate_vulnerability_replacements@1045",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "validate_vulnerability_replacements",
      "lineno": 1045,
      "end_lineno": 1086,
      "business_stage": "other",
      "docstring": "Validate vulnerability replacements in analysis settings file.\nIf vulnerability replacements are specified as a file path, check that the file exists.\nThis way the user will be warned early if the vulnerability option selected is not valid.\n\nArgs:\n    analysis_settings_json (str): JSON file path to analysis settings file\n\nReturns:\n    bool: True if the vulnerability replacements are present and valid, False otherwise",
      "content": "# File: oasislmf/utils/data.py\n# function: validate_vulnerability_replacements (lines 1045-1086)\n\ndef validate_vulnerability_replacements(analysis_settings_json):\n    \"\"\"\n    Validate vulnerability replacements in analysis settings file.\n    If vulnerability replacements are specified as a file path, check that the file exists.\n    This way the user will be warned early if the vulnerability option selected is not valid.\n\n    Args:\n        analysis_settings_json (str): JSON file path to analysis settings file\n\n    Returns:\n        bool: True if the vulnerability replacements are present and valid, False otherwise\n\n    \"\"\"\n    if analysis_settings_json is None:\n        return False\n\n    vulnerability_adjustments_key = analysis_settings_loader(analysis_settings_json).get('vulnerability_adjustments')\n    if vulnerability_adjustments_key is None:\n        return False\n\n    vulnerability_replacements = vulnerability_adjustments_key.get('replace_data', None)\n    if vulnerability_replacements is None:\n        vulnerability_replacements = vulnerability_adjustments_key.get('replace_file', None)\n    if vulnerability_replacements is None:\n        return False\n    if isinstance(vulnerability_replacements, dict):\n        logger.info('Vulnerability replacements are specified in the analysis settings file')\n        return True\n    if isinstance(vulnerability_replacements, str):\n        abs_path = os.path.abspath(vulnerability_replacements)\n        if not os.path.isfile(abs_path):\n            logger.warning('Vulnerability replacements file does not exist: {}'.format(abs_path))\n            return False\n\n        if not validate_vuln_csv_contents(abs_path):\n            logger.warning('Vulnerability replacements file is not valid: {}'.format(abs_path))\n            return False\n\n        logger.info('Vulnerability replacements found in file: {}'.format(abs_path))\n        return True\n    logger.warning('Vulnerability replacements must be a dict or a file path, got: {}'.format(vulnerability_replacements))\n    return False\n\n\"\"\"Docstring (excerpt)\"\"\"\nValidate vulnerability replacements in analysis settings file.\nIf vulnerability replacements are specified as a file path, check that the file exists.\nThis way the user will be warned early if the vulnerability option selected is not valid.\n\nArgs:\n    analysis_settings_json (str): JSON file path to analysis settings file\n\nReturns:\n    bool: True if the vulnerability replacements are present and valid, False otherwise"
    },
    {
      "chunk_id": "oasislmf/utils/data.py::fill_na_with_categoricals@1109",
      "source_type": "code",
      "path": "oasislmf/utils/data.py",
      "symbol_type": "function",
      "name": "fill_na_with_categoricals",
      "lineno": 1109,
      "end_lineno": 1141,
      "business_stage": "other",
      "docstring": "Fill NA values in a Pandas DataFrame, with handling for Categorical dtype columns.\n\nThe input dataframe is modified inplace.\n\n:param df: The dataframe to process\n:type df: pd.DataFrame\n\n:param fill_value: A single value to use in all columns, or a dict of column names and\n                   corresponding values to fill.\n:type fill_value: int, float, str, dict",
      "content": "# File: oasislmf/utils/data.py\n# function: fill_na_with_categoricals (lines 1109-1141)\n\ndef fill_na_with_categoricals(df, fill_value):\n    \"\"\"\n    Fill NA values in a Pandas DataFrame, with handling for Categorical dtype columns.\n\n    The input dataframe is modified inplace.\n\n    :param df: The dataframe to process\n    :type df: pd.DataFrame\n\n    :param fill_value: A single value to use in all columns, or a dict of column names and\n                       corresponding values to fill.\n    :type fill_value: int, float, str, dict\n\n    \"\"\"\n    if not isinstance(fill_value, dict):\n        fill_value = {col_name: fill_value for col_name in df.columns}\n\n    for col_name, value in fill_value.items():\n        if col_name not in df:\n            continue\n\n        col = df[col_name]\n        if isinstance(col.dtype, pd.CategoricalDtype):\n            # Force to be a string - using categorical for string columns\n            value = str(value)\n            fill_value[col_name] = value\n            if value not in col.cat.categories:\n                df[col_name] = col.cat.add_categories([value])\n\n    # Note that the following lines do not work properly with Pandas 1.1.0/1.1.1, due to a bug\n    # related to fillna and categorical dtypes. This bug should be fixed in >1.1.2.\n    # https://github.com/pandas-dev/pandas/issues/35731\n    df.fillna(value=fill_value, inplace=True)\n\n\"\"\"Docstring (excerpt)\"\"\"\nFill NA values in a Pandas DataFrame, with handling for Categorical dtype columns.\n\nThe input dataframe is modified inplace.\n\n:param df: The dataframe to process\n:type df: pd.DataFrame\n\n:param fill_value: A single value to use in all columns, or a dict of column names and\n                   corresponding values to fill.\n:type fill_value: int, float, str, dict"
    },
    {
      "chunk_id": "oasislmf/utils/defaults.py::store_exposure_fp@147",
      "source_type": "code",
      "path": "oasislmf/utils/defaults.py",
      "symbol_type": "function",
      "name": "store_exposure_fp",
      "lineno": 147,
      "end_lineno": 168,
      "business_stage": "other",
      "docstring": "Preserve original exposure file extention if its in a pandas supported\ncompressed format\n\ncompression : {infer, gzip, bz2, zip, xz, None}, default infer\n              For on-the-fly decompression of on-disk data. If infer and\n              filepath_or_buffer is path-like, then detect compression from\n              the following extensions: .gz, .bz2, .zip, or .xz\n              (otherwise no decompression).\n\n              If using zip, the ZIP file must contain only one data file\n              to be read in. Set to None for no decompression.\n\n            New in version 0.18.1: support for zip and xz compression.",
      "content": "# File: oasislmf/utils/defaults.py\n# function: store_exposure_fp (lines 147-168)\n\ndef store_exposure_fp(fp, exposure_type):\n    \"\"\"\n    Preserve original exposure file extention if its in a pandas supported\n    compressed format\n\n    compression : {infer, gzip, bz2, zip, xz, None}, default infer\n                  For on-the-fly decompression of on-disk data. If infer and\n                  filepath_or_buffer is path-like, then detect compression from\n                  the following extensions: .gz, .bz2, .zip, or .xz\n                  (otherwise no decompression).\n\n                  If using zip, the ZIP file must contain only one data file\n                  to be read in. Set to None for no decompression.\n\n                New in version 0.18.1: support for zip and xz compression.\n    \"\"\"\n    compressed_ext = ('.gz', '.bz2', '.zip', '.xz', '.parquet')\n    filename = SOURCE_FILENAMES[exposure_type]\n    if fp.endswith(compressed_ext):\n        return '.'.join([filename, fp.rsplit('.')[-1]])\n    else:\n        return filename\n\n\"\"\"Docstring (excerpt)\"\"\"\nPreserve original exposure file extention if its in a pandas supported\ncompressed format\n\ncompression : {infer, gzip, bz2, zip, xz, None}, default infer\n              For on-the-fly decompression of on-disk data. If infer and\n              filepath_or_buffer is path-like, then detect compression from\n              the following extensions: .gz, .bz2, .zip, or .xz\n              (otherwise no decompression).\n\n              If using zip, the ZIP file must contain only one data file\n              to be read in. Set to None for no decompression.\n\n            New in version 0.18.1: support for zip and xz compression."
    },
    {
      "chunk_id": "oasislmf/utils/defaults.py::find_exposure_fp@171",
      "source_type": "code",
      "path": "oasislmf/utils/defaults.py",
      "symbol_type": "function",
      "name": "find_exposure_fp",
      "lineno": 171,
      "end_lineno": 178,
      "business_stage": "other",
      "docstring": "Find an OED exposure file stored in the oasis inputs dir\nwhile preserving the compressed ext",
      "content": "# File: oasislmf/utils/defaults.py\n# function: find_exposure_fp (lines 171-178)\n\ndef find_exposure_fp(input_dir, exposure_type, required=True):\n    \"\"\"\n    Find an OED exposure file stored in the oasis inputs dir\n    while preserving the compressed ext\n    \"\"\"\n    fp = glob.glob(os.path.join(input_dir, SOURCE_FILENAMES[exposure_type].rsplit(\".\", 1)[0] + '*'))\n    if required or fp:\n        return fp.pop()\n\n\"\"\"Docstring (excerpt)\"\"\"\nFind an OED exposure file stored in the oasis inputs dir\nwhile preserving the compressed ext"
    },
    {
      "chunk_id": "oasislmf/utils/defaults.py::get_default_json@181",
      "source_type": "code",
      "path": "oasislmf/utils/defaults.py",
      "symbol_type": "function",
      "name": "get_default_json",
      "lineno": 181,
      "end_lineno": 195,
      "business_stage": "other",
      "docstring": "Loads JSON from file.\n\n:param src_fp: Source JSON file path\n:type src_fp: str\n\n:return: dict\n:rtype: dict",
      "content": "# File: oasislmf/utils/defaults.py\n# function: get_default_json (lines 181-195)\n\ndef get_default_json(src_fp):\n    \"\"\"\n    Loads JSON from file.\n\n    :param src_fp: Source JSON file path\n    :type src_fp: str\n\n    :return: dict\n    :rtype: dict\n    \"\"\"\n    try:\n        with io.open(src_fp, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except (IOError, JSONDecodeError, OSError, TypeError):\n        raise OasisException('Error trying to load JSON from {}'.format(src_fp))\n\n\"\"\"Docstring (excerpt)\"\"\"\nLoads JSON from file.\n\n:param src_fp: Source JSON file path\n:type src_fp: str\n\n:return: dict\n:rtype: dict"
    },
    {
      "chunk_id": "oasislmf/utils/diff.py::unified_diff@29",
      "source_type": "code",
      "path": "oasislmf/utils/diff.py",
      "symbol_type": "function",
      "name": "unified_diff",
      "lineno": 29,
      "end_lineno": 48,
      "business_stage": "other",
      "docstring": "Generates a unified diff of two files: ``a`` and ``b``. The files must\nbe passed in as absolute paths.",
      "content": "# File: oasislmf/utils/diff.py\n# function: unified_diff (lines 29-48)\n\ndef unified_diff(a, b, as_string=False):\n    \"\"\"\n    Generates a unified diff of two files: ``a`` and ``b``. The files must\n    be passed in as absolute paths.\n    \"\"\"\n    try:\n        with io.open(a, 'r') as f1:\n            with io.open(b, 'r') as f2:\n                diff = difflib.unified_diff(\n                    f1.readlines(),\n                    f2.readlines(),\n                    fromfile=f1.name,\n                    tofile=f2.name,\n                )\n    except (OSError, IOError) as e:\n        raise OasisException(\"Exception raised in 'unified_diff'\", e)\n\n    if as_string:\n        return ''.join(diff)\n    return diff\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerates a unified diff of two files: ``a`` and ``b``. The files must\nbe passed in as absolute paths."
    },
    {
      "chunk_id": "oasislmf/utils/inputs.py::InputValues@37",
      "source_type": "code",
      "path": "oasislmf/utils/inputs.py",
      "symbol_type": "class",
      "name": "InputValues",
      "lineno": 37,
      "end_lineno": 176,
      "business_stage": "other",
      "docstring": "Helper class for accessing the input values from either\nthe command line or the configuration file.\n\ninternal_update",
      "content": "# File: oasislmf/utils/inputs.py\n# class: InputValues (lines 37-176)\n\nclass InputValues(object):\n    \"\"\"\n    Helper class for accessing the input values from either\n    the command line or the configuration file.\n\n    internal_update\n\n    \"\"\"\n\n    def __init__(self, args, update_keys=True):\n        self.logger = logging.getLogger(__name__)\n        self.args = args\n        self.config = {}\n        self.config_fp = self.get('config', is_path=True)\n        self.config_mapping = get_config_profile()\n\n        if self.config_fp is not None:\n            try:\n                self.config = update_config(self.load_config_file())\n                self.config_dir = os.path.dirname(self.config_fp)\n                self.list_unknown_keys()\n            except JSONDecodeError as e:\n                raise OasisException(f\"Configuration file {self.config_fp} is not a valid json file\", e)\n\n    def list_unknown_keys(self):\n        \"\"\"\n        List all Unknown keys set in the 'oasislmf.json' file\n        \"\"\"\n        valid_arg_names = set(arg[0] for arg in self.args._get_kwargs())\n        config_arg_names = set(self.config.keys())\n        unknown_args = config_arg_names - valid_arg_names - set(self.config_mapping.keys())\n\n        if unknown_args:\n            self.logger.warning('Warning: Unknown options(s) set in MDK config:')\n            for k in unknown_args:\n                self.logger.warning('   {} : {}'.format(\n                    k,\n                    self.config[k]\n                ))\n\n    def load_config_file(self):\n        try:\n            with io.open(self.config_fp, 'r', encoding='utf-8') as f:\n                return {k.lower(): v for k, v in json.load(f).items()}\n        except FileNotFoundError:\n            raise OasisException('MDK config. file path {} provided does not exist'.format(self.config_fp))\n\n    def write_config_file(self, config_fp):\n        with io.open(config_fp, 'w', encoding='utf-8') as f:\n            f.write(u'{}'.format(json.dumps(self.config, sort_keys=True, indent=4, ensure_ascii=False)))\n\n    def confirm_action(self, question_str, no_confirm=False):\n        self.logger.debug('Prompt user for confirmation')\n        if no_confirm:\n            return True\n        try:\n            check = str(input(\"%s (Y/N): \" % question_str)).lower().strip()\n            if check[:1] == 'y':\n                return True\n            elif check[:1] == 'n':\n                return False\n            else:\n                self.logger.error('Enter \"y\" for Yes, \"n\" for No or Ctrl-C to exit.\\n')\n                return self.confirm_action(question_str)\n        except KeyboardInterrupt:\n            self.logger.error('\\nexiting.')\n\n    def get(self, name, default=None, required=False, is_path=False, dtype=None):\n        \"\"\"\n        Gets the name parameter until found from:\n          - the command line arguments.\n          - the configuration file\n          - the environment variable (put in uppercase)\n\n        If it is not found then ``default`` is returned\n        unless ``required`` is True in which case an ``OasisException`` is raised.\n\n        :param name: The name of the parameter to lookup\n        :type name: str\n\n\"\"\"Docstring (excerpt)\"\"\"\nHelper class for accessing the input values from either\nthe command line or the configuration file.\n\ninternal_update"
    },
    {
      "chunk_id": "oasislmf/utils/inputs.py::str2bool@179",
      "source_type": "code",
      "path": "oasislmf/utils/inputs.py",
      "symbol_type": "function",
      "name": "str2bool",
      "lineno": 179,
      "end_lineno": 202,
      "business_stage": "other",
      "docstring": "Func type for loading strings to boolean values using argparse\nhttps://stackoverflow.com/a/43357954\n\nstep_params:\n    use: `'default': False, 'type': str2bool, 'const':True, 'nargs':'?', ...`\n\nCLI:\n    oasislmf --some-flag\n    oasislmf --some-flag <bool>\n\noasislmf.json\n{\"some_flag\": true, ...}",
      "content": "# File: oasislmf/utils/inputs.py\n# function: str2bool (lines 179-202)\n\ndef str2bool(v):\n    \"\"\" Func type for loading strings to boolean values using argparse\n        https://stackoverflow.com/a/43357954\n\n        step_params:\n            use: `'default': False, 'type': str2bool, 'const':True, 'nargs':'?', ...`\n\n        CLI:\n            oasislmf --some-flag\n            oasislmf --some-flag <bool>\n\n        oasislmf.json\n        {\"some_flag\": true, ...}\n    \"\"\"\n    if v is None:\n        return v\n    elif isinstance(v, bool):\n        return v\n    elif v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise ArgumentTypeError('Boolean value expected.')\n\n\"\"\"Docstring (excerpt)\"\"\"\nFunc type for loading strings to boolean values using argparse\nhttps://stackoverflow.com/a/43357954\n\nstep_params:\n    use: `'default': False, 'type': str2bool, 'const':True, 'nargs':'?', ...`\n\nCLI:\n    oasislmf --some-flag\n    oasislmf --some-flag <bool>\n\noasislmf.json\n{\"some_flag\": true, ...}"
    },
    {
      "chunk_id": "oasislmf/utils/inputs.py::list_unknown_keys@61",
      "source_type": "code",
      "path": "oasislmf/utils/inputs.py",
      "symbol_type": "function",
      "name": "list_unknown_keys",
      "lineno": 61,
      "end_lineno": 75,
      "business_stage": "other",
      "docstring": "List all Unknown keys set in the 'oasislmf.json' file",
      "content": "# File: oasislmf/utils/inputs.py\n# function: list_unknown_keys (lines 61-75)\n\n    def list_unknown_keys(self):\n        \"\"\"\n        List all Unknown keys set in the 'oasislmf.json' file\n        \"\"\"\n        valid_arg_names = set(arg[0] for arg in self.args._get_kwargs())\n        config_arg_names = set(self.config.keys())\n        unknown_args = config_arg_names - valid_arg_names - set(self.config_mapping.keys())\n\n        if unknown_args:\n            self.logger.warning('Warning: Unknown options(s) set in MDK config:')\n            for k in unknown_args:\n                self.logger.warning('   {} : {}'.format(\n                    k,\n                    self.config[k]\n                ))\n\n\"\"\"Docstring (excerpt)\"\"\"\nList all Unknown keys set in the 'oasislmf.json' file"
    },
    {
      "chunk_id": "oasislmf/utils/inputs.py::get@104",
      "source_type": "code",
      "path": "oasislmf/utils/inputs.py",
      "symbol_type": "function",
      "name": "get",
      "lineno": 104,
      "end_lineno": 176,
      "business_stage": "other",
      "docstring": "Gets the name parameter until found from:\n  - the command line arguments.\n  - the configuration file\n  - the environment variable (put in uppercase)\n\nIf it is not found then ``default`` is returned\nunless ``required`` is True in which case an ``OasisException`` is raised.\n\n:param name: The name of the parameter to lookup\n:type name: str\n\n:param default: The default value to return if the name is not\n    found on the command line or in the configuration file.\n\n:param required: Flag whether the value is required, if so and\n    the parameter is not found on the command line or in the\n    configuration file an error is raised.\n:type required: bool\n\n:param is_path: Flag whether the value should be treated as a path and return an abspath,\n    use config_dir as base dir if value comes from the config\n:type is_path: bool\n\n:param dtype: the class <type> of the value, if 'None' load as string by default\n:type: class\n\n:raise OasisException: If the value is not found and ``required``\n    is True\n\n:return: The found value or the default",
      "content": "# File: oasislmf/utils/inputs.py\n# function: get (lines 104-176)\n\n    def get(self, name, default=None, required=False, is_path=False, dtype=None):\n        \"\"\"\n        Gets the name parameter until found from:\n          - the command line arguments.\n          - the configuration file\n          - the environment variable (put in uppercase)\n\n        If it is not found then ``default`` is returned\n        unless ``required`` is True in which case an ``OasisException`` is raised.\n\n        :param name: The name of the parameter to lookup\n        :type name: str\n\n        :param default: The default value to return if the name is not\n            found on the command line or in the configuration file.\n\n        :param required: Flag whether the value is required, if so and\n            the parameter is not found on the command line or in the\n            configuration file an error is raised.\n        :type required: bool\n\n        :param is_path: Flag whether the value should be treated as a path and return an abspath,\n            use config_dir as base dir if value comes from the config\n        :type is_path: bool\n\n        :param dtype: the class <type> of the value, if 'None' load as string by default\n        :type: class\n\n        :raise OasisException: If the value is not found and ``required``\n            is True\n\n        :return: The found value or the default\n        \"\"\"\n        # Load order 0:  Get from CLI flag\n        source = 'arg'\n        value = getattr(self.args, name, None)\n\n        # Load order 1: ENV override (intended for worker images)\n        if str2bool(os.getenv('OASIS_ENV_OVERRIDE', default=False)) and has_oasis_env(name):\n            source = 'env_override'\n            value = get_oasis_env(name, dtype)\n\n        # Load order 2: Get from config JSON\n        if value is None:\n            source = 'config'\n            value = self.config.get(name)\n\n        # Load order 3: Get from environment variable\n        if value is None:\n            source = 'env'\n            value = get_oasis_env(name, dtype)\n\n        if value is None and required:\n            raise OasisException(\n                'Required argument {} could not be found in the command args or the MDK config. file'.format(name)\n            )\n\n        # Load order 4: Get default value\n        if value is None:\n            source = 'default'\n            value = default\n\n        if is_path and value not in [None, \"\"] and not os.path.isabs(value):\n            if source == 'config':\n                value = os.path.join(self.config_dir, value)\n            else:\n                value = os.path.abspath(value)\n\n        # Warn user of environment variable load\n        if source == 'env_override':\n            self.logger.warning(f'Warning - environment variable override: OASIS_{name.upper()}={value}')\n\n        return value\n\n\"\"\"Docstring (excerpt)\"\"\"\nGets the name parameter until found from:\n  - the command line arguments.\n  - the configuration file\n  - the environment variable (put in uppercase)\n\nIf it is not found then ``default`` is returned\nunless ``required`` is True in which case an ``OasisException`` is raised.\n\n:param name: The name of the parameter to lookup\n:type name: str\n\n:param default: The default value to return if the name is not\n    found on the command line or in the configuration file.\n\n:param required: Flag whether the value is required, if so and\n    the parameter is not found on the command line or in the\n    configuration file an error is raised.\n:type required: bool\n\n:param is_path: Flag whether the value should be treated as a path and return an abspath,\n    use config_dir as base dir if value comes from the config\n:type is_path: bool\n\n:param dtype: the class <type> of the value, if 'None' load as string by default\n:type: class\n\n:raise OasisException: If the value is not found and ``required``\n    is True\n\n:return: The found value or the default"
    },
    {
      "chunk_id": "oasislmf/utils/log.py::read_log_config@59",
      "source_type": "code",
      "path": "oasislmf/utils/log.py",
      "symbol_type": "function",
      "name": "read_log_config",
      "lineno": 59,
      "end_lineno": 85,
      "business_stage": "other",
      "docstring": "Read an Oasis standard logging config",
      "content": "# File: oasislmf/utils/log.py\n# function: read_log_config (lines 59-85)\n\ndef read_log_config(config_parser):\n    \"\"\"\n    Read an Oasis standard logging config\n    \"\"\"\n    log_file = config_parser['LOG_FILE']\n    log_level = config_parser['LOG_LEVEL']\n    log_max_size_in_bytes = int(config_parser['LOG_MAX_SIZE_IN_BYTES'])\n    log_backup_count = int(config_parser['LOG_BACKUP_COUNT'])\n\n    log_dir = os.path.dirname(log_file)\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n\n    logger = logging.getLogger('oasislmf')\n    for handler in list(logger.handlers):\n        if handler.name == 'oasislmf':\n            logger.removeHandler(handler)\n            break\n    handler = RotatingFileHandler(\n        log_file, maxBytes=log_max_size_in_bytes,\n        backupCount=log_backup_count)\n    handler.name = 'oasislmf'\n    logging.getLogger('oasislmf').setLevel(log_level)\n    logging.getLogger('oasislmf').addHandler(handler)\n    formatter = logging.Formatter(\n        \"%(asctime)s - %(levelname)s - %(message)s\")\n    handler.setFormatter(formatter)\n\n\"\"\"Docstring (excerpt)\"\"\"\nRead an Oasis standard logging config"
    },
    {
      "chunk_id": "oasislmf/utils/log.py::oasis_log@88",
      "source_type": "code",
      "path": "oasislmf/utils/log.py",
      "symbol_type": "function",
      "name": "oasis_log",
      "lineno": 88,
      "end_lineno": 141,
      "business_stage": "other",
      "docstring": "Decorator that logs the entry, exit and execution time.",
      "content": "# File: oasislmf/utils/log.py\n# function: oasis_log (lines 88-141)\n\ndef oasis_log(*args, **kwargs):\n    \"\"\"\n    Decorator that logs the entry, exit and execution time.\n    \"\"\"\n    logger = logging.getLogger('oasislmf')\n\n    def actual_oasis_log(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            func_name = func.__name__\n            caller_module_name = func.__globals__.get('__name__')\n\n            if func_name == '__init__':\n                logger.debug(\"RUNNING: {}.{}\".format(\n                    caller_module_name, func_name))\n            else:\n                logger.info(\"RUNNING: {}.{}\".format(\n                    caller_module_name, func_name))\n\n            args_name = getargspec(func)[0]\n            args_dict = dict(zip(args_name, args))\n\n            if logger.level <= logging.DEBUG:\n                for key, value in args_dict.items():\n                    if key == \"self\":\n                        continue\n                    logger.debug(\"    {} == {}\".format(key, value))\n\n                if len(args) > len(args_name):\n                    for i in range(len(args_name), len(args)):\n                        logger.debug(\"    {}\".format(args[i]))\n                for key, value in kwargs.items():\n                    logger.debug(\"    {} == {}\".format(key, value))\n\n            start = time.time()\n            result = func(*args, **kwargs)\n            end = time.time()\n\n            # Only log timestamps on functions which took longer than 10ms\n            if (end - start) > 0.01:\n                logger.info(\n                    \"COMPLETED: {}.{} in {}s\".format(\n                        caller_module_name, func_name, round(end - start, 2)))\n            else:\n                logger.debug(\n                    \"COMPLETED: {}.{} in {}s\".format(\n                        caller_module_name, func_name, round(end - start, 2)))\n            return result\n        return wrapper\n\n    if len(args) == 1 and callable(args[0]):\n        return actual_oasis_log(args[0])\n    else:\n        return actual_oasis_log\n\n\"\"\"Docstring (excerpt)\"\"\"\nDecorator that logs the entry, exit and execution time."
    },
    {
      "chunk_id": "oasislmf/utils/log_config.py::OasisLogConfig@6",
      "source_type": "code",
      "path": "oasislmf/utils/log_config.py",
      "symbol_type": "class",
      "name": "OasisLogConfig",
      "lineno": 6,
      "end_lineno": 287,
      "business_stage": "other",
      "docstring": "Configuration handler for OasisLMF CLI/console logging.\n\nHandles log level resolution, format management, and validation for CLI-based logging.\nDesigned to work alongside existing file-based logging in log.py.\n\nEnvironment Variables:\n    OASISLMF_LOG_LEVEL: Override log level (e.g., 'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL')\n\nExample:\n    >>> # Basic usage with config file\n    >>> config = OasisLogConfig({'logging': {'level': 'INFO', 'format': 'compact'}})\n    >>> formatter = config.create_formatter()\n    >>> level = config.get_log_level()\n\n    >>> # CLI override example\n    >>> level = config.get_log_level('DEBUG')  # Override config file\n    >>> available_formats = config.get_available_formats()\n    >>> available_levels = config.get_available_levels()",
      "content": "# File: oasislmf/utils/log_config.py\n# class: OasisLogConfig (lines 6-287)\n\nclass OasisLogConfig:\n    \"\"\"\n    Configuration handler for OasisLMF CLI/console logging.\n\n    Handles log level resolution, format management, and validation for CLI-based logging.\n    Designed to work alongside existing file-based logging in log.py.\n\n    Environment Variables:\n        OASISLMF_LOG_LEVEL: Override log level (e.g., 'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL')\n\n    Example:\n        >>> # Basic usage with config file\n        >>> config = OasisLogConfig({'logging': {'level': 'INFO', 'format': 'compact'}})\n        >>> formatter = config.create_formatter()\n        >>> level = config.get_log_level()\n\n        >>> # CLI override example\n        >>> level = config.get_log_level('DEBUG')  # Override config file\n        >>> available_formats = config.get_available_formats()\n        >>> available_levels = config.get_available_levels()\n    \"\"\"\n\n    # Standard levels for validation and help text\n    STANDARD_LEVELS = [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"]\n\n    FORMAT_TEMPLATES = {\n        \"simple\": \"%(message)s\",\n        \"standard\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n        \"detailed\": \"%(asctime)s - %(processName)s-%(process)d - %(name)s - %(levelname)s - %(message)s\",\n        \"iso_timestamp\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n        \"production\": \"%(asctime)s [%(process)d] %(name)s - %(levelname)s - %(message)s\",\n        \"compact\": \"%(asctime)s [%(levelname)s] %(message)s\",\n    }\n\n    # Date format configurations for different templates\n    DATE_FORMATS = {\n        \"iso_timestamp\": \"%Y-%m-%dT%H:%M:%S\",\n        \"standard\": \"%Y-%m-%d %H:%M:%S\",\n        \"detailed\": \"%Y-%m-%d %H:%M:%S\",\n        \"production\": \"%Y-%m-%d %H:%M:%S\",\n        \"compact\": \"%H:%M:%S\",\n    }\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        \"\"\"\n        Initialize logging configuration manager.\n\n        Args:\n            config: Configuration dictionary (typically from JSON file loaded by command.py)\n        \"\"\"\n        self.config = config or {}\n\n    def get_log_level(\n        self, cli_level: Optional[str] = None, is_verbose: bool = False\n    ) -> int:\n        \"\"\"\n        Get effective log level from various sources.\n\n        Priority order: CLI args > env vars > config file > verbose flag > default\n\n        Args:\n            cli_level: Log level from command line (e.g., 'INFO', 'DEBUG')\n            is_verbose: Legacy verbose flag for backward compatibility\n\n        Returns:\n            Numeric log level (e.g., 20 for INFO, 10 for DEBUG)\n\n        Examples:\n            >>> config = OasisLogConfig()\n            >>> config.get_log_level('DEBUG')\n            10\n            >>> config.get_log_level(is_verbose=True)\n            10\n        \"\"\"\n        # 1. CLI argument takes highest priority\n        if cli_level:\n            return self._parse_level(cli_level)\n\n        # 2. Environment variable\n        env_level = os.environ.get(\"OASISLMF_LOG_LEVEL\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nConfiguration handler for OasisLMF CLI/console logging.\n\nHandles log level resolution, format management, and validation for CLI-based logging.\nDesigned to work alongside existing file-based logging in log.py.\n\nEnvironment Variables:\n    OASISLMF_LOG_LEVEL: Override log level (e.g., 'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL')\n\nExample:\n    >>> # Basic usage with config file\n    >>> config = OasisLogConfig({'logging': {'level': 'INFO', 'format': 'compact'}})\n    >>> formatter = config.create_formatter()\n    >>> level = config.get_log_level()\n\n    >>> # CLI override example\n    >>> level = config.get_log_level('DEBUG')  # Override config file\n    >>> available_formats = config.get_available_formats()\n    >>> available_levels = config.get_available_levels()"
    },
    {
      "chunk_id": "oasislmf/utils/log_config.py::get_log_level@58",
      "source_type": "code",
      "path": "oasislmf/utils/log_config.py",
      "symbol_type": "function",
      "name": "get_log_level",
      "lineno": 58,
      "end_lineno": 99,
      "business_stage": "other",
      "docstring": "Get effective log level from various sources.\n\nPriority order: CLI args > env vars > config file > verbose flag > default\n\nArgs:\n    cli_level: Log level from command line (e.g., 'INFO', 'DEBUG')\n    is_verbose: Legacy verbose flag for backward compatibility\n\nReturns:\n    Numeric log level (e.g., 20 for INFO, 10 for DEBUG)\n\nExamples:\n    >>> config = OasisLogConfig()\n    >>> config.get_log_level('DEBUG')\n    10\n    >>> config.get_log_level(is_verbose=True)\n    10",
      "content": "# File: oasislmf/utils/log_config.py\n# function: get_log_level (lines 58-99)\n\n    def get_log_level(\n        self, cli_level: Optional[str] = None, is_verbose: bool = False\n    ) -> int:\n        \"\"\"\n        Get effective log level from various sources.\n\n        Priority order: CLI args > env vars > config file > verbose flag > default\n\n        Args:\n            cli_level: Log level from command line (e.g., 'INFO', 'DEBUG')\n            is_verbose: Legacy verbose flag for backward compatibility\n\n        Returns:\n            Numeric log level (e.g., 20 for INFO, 10 for DEBUG)\n\n        Examples:\n            >>> config = OasisLogConfig()\n            >>> config.get_log_level('DEBUG')\n            10\n            >>> config.get_log_level(is_verbose=True)\n            10\n        \"\"\"\n        # 1. CLI argument takes highest priority\n        if cli_level:\n            return self._parse_level(cli_level)\n\n        # 2. Environment variable\n        env_level = os.environ.get(\"OASISLMF_LOG_LEVEL\")\n        if env_level:\n            return self._parse_level(env_level)\n\n        # 3. Config file\n        config_level = self.config.get(\"logging\", {}).get(\"level\")\n        if config_level:\n            return self._parse_level(config_level)\n\n        # 4. Verbose flag (backward compatibility)\n        if is_verbose:\n            return logging.DEBUG\n\n        # 5. Default\n        return logging.INFO\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet effective log level from various sources.\n\nPriority order: CLI args > env vars > config file > verbose flag > default\n\nArgs:\n    cli_level: Log level from command line (e.g., 'INFO', 'DEBUG')\n    is_verbose: Legacy verbose flag for backward compatibility\n\nReturns:\n    Numeric log level (e.g., 20 for INFO, 10 for DEBUG)\n\nExamples:\n    >>> config = OasisLogConfig()\n    >>> config.get_log_level('DEBUG')\n    10\n    >>> config.get_log_level(is_verbose=True)\n    10"
    },
    {
      "chunk_id": "oasislmf/utils/log_config.py::get_ods_tools_level@101",
      "source_type": "code",
      "path": "oasislmf/utils/log_config.py",
      "symbol_type": "function",
      "name": "get_ods_tools_level",
      "lineno": 101,
      "end_lineno": 117,
      "business_stage": "other",
      "docstring": "Get ods_tools logger level based on main logger level.\n\nArgs:\n    main_level: Main oasislmf logger level to use as reference\n\nReturns:\n    Numeric log level for ods_tools logger",
      "content": "# File: oasislmf/utils/log_config.py\n# function: get_ods_tools_level (lines 101-117)\n\n    def get_ods_tools_level(self, main_level: int) -> int:\n        \"\"\"\n        Get ods_tools logger level based on main logger level.\n\n        Args:\n            main_level: Main oasislmf logger level to use as reference\n\n        Returns:\n            Numeric log level for ods_tools logger\n        \"\"\"\n        # Check if explicitly configured\n        config_level = self.config.get(\"logging\", {}).get(\"ods_tools_level\")\n        if config_level:\n            return self._parse_level(config_level)\n\n        # Default behavior: WARNING unless main level is DEBUG\n        return logging.DEBUG if main_level <= logging.DEBUG else logging.WARNING\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet ods_tools logger level based on main logger level.\n\nArgs:\n    main_level: Main oasislmf logger level to use as reference\n\nReturns:\n    Numeric log level for ods_tools logger"
    },
    {
      "chunk_id": "oasislmf/utils/log_config.py::get_format_string@119",
      "source_type": "code",
      "path": "oasislmf/utils/log_config.py",
      "symbol_type": "function",
      "name": "get_format_string",
      "lineno": 119,
      "end_lineno": 143,
      "business_stage": "other",
      "docstring": "Get log format string.\n\nArgs:\n    format_name: Format template name from CLI or None for config/default\n\nReturns:\n    Format string for logging.Formatter",
      "content": "# File: oasislmf/utils/log_config.py\n# function: get_format_string (lines 119-143)\n\n    def get_format_string(self, format_name: Optional[str] = None) -> str:\n        \"\"\"\n        Get log format string.\n\n        Args:\n            format_name: Format template name from CLI or None for config/default\n\n        Returns:\n            Format string for logging.Formatter\n        \"\"\"\n        # CLI format name takes priority\n        if format_name and format_name in self.FORMAT_TEMPLATES:\n            return self.FORMAT_TEMPLATES[format_name]\n\n        # Check config file\n        config_format = self.config.get(\"logging\", {}).get(\"format\")\n        if config_format:\n            if config_format in self.FORMAT_TEMPLATES:\n                return self.FORMAT_TEMPLATES[config_format]\n            else:\n                # Custom format string from config\n                return config_format\n\n        # Default: standard format\n        return self.FORMAT_TEMPLATES[\"standard\"]\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet log format string.\n\nArgs:\n    format_name: Format template name from CLI or None for config/default\n\nReturns:\n    Format string for logging.Formatter"
    },
    {
      "chunk_id": "oasislmf/utils/log_config.py::get_date_format@145",
      "source_type": "code",
      "path": "oasislmf/utils/log_config.py",
      "symbol_type": "function",
      "name": "get_date_format",
      "lineno": 145,
      "end_lineno": 165,
      "business_stage": "other",
      "docstring": "Get date format string for the specified template.\n\nArgs:\n    format_name: Format template name\n\nReturns:\n    Date format string or None for logging default",
      "content": "# File: oasislmf/utils/log_config.py\n# function: get_date_format (lines 145-165)\n\n    def get_date_format(self, format_name: Optional[str] = None) -> Optional[str]:\n        \"\"\"\n        Get date format string for the specified template.\n\n        Args:\n            format_name: Format template name\n\n        Returns:\n            Date format string or None for logging default\n        \"\"\"\n        # Check CLI format name first\n        if format_name and format_name in self.DATE_FORMATS:\n            return self.DATE_FORMATS[format_name]\n\n        # Check config format\n        config_format = self.config.get(\"logging\", {}).get(\"format\")\n        if config_format and config_format in self.DATE_FORMATS:\n            return self.DATE_FORMATS[config_format]\n\n        # Default: None (uses logging module default)\n        return None\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet date format string for the specified template.\n\nArgs:\n    format_name: Format template name\n\nReturns:\n    Date format string or None for logging default"
    },
    {
      "chunk_id": "oasislmf/utils/log_config.py::create_formatter@167",
      "source_type": "code",
      "path": "oasislmf/utils/log_config.py",
      "symbol_type": "function",
      "name": "create_formatter",
      "lineno": 167,
      "end_lineno": 180,
      "business_stage": "other",
      "docstring": "Create a logging formatter with appropriate format and date format.\n\nArgs:\n    format_name: Format template name from CLI\n\nReturns:\n    Configured logging.Formatter instance",
      "content": "# File: oasislmf/utils/log_config.py\n# function: create_formatter (lines 167-180)\n\n    def create_formatter(self, format_name: Optional[str] = None) -> logging.Formatter:\n        \"\"\"\n        Create a logging formatter with appropriate format and date format.\n\n        Args:\n            format_name: Format template name from CLI\n\n        Returns:\n            Configured logging.Formatter instance\n        \"\"\"\n        format_str = self.get_format_string(format_name)\n        date_format = self.get_date_format(format_name)\n\n        return logging.Formatter(format_str, datefmt=date_format)\n\n\"\"\"Docstring (excerpt)\"\"\"\nCreate a logging formatter with appropriate format and date format.\n\nArgs:\n    format_name: Format template name from CLI\n\nReturns:\n    Configured logging.Formatter instance"
    },
    {
      "chunk_id": "oasislmf/utils/log_config.py::get_available_formats@182",
      "source_type": "code",
      "path": "oasislmf/utils/log_config.py",
      "symbol_type": "function",
      "name": "get_available_formats",
      "lineno": 182,
      "end_lineno": 195,
      "business_stage": "other",
      "docstring": "Get list of available format template names.\n\nReturns:\n    List of format template names that can be used with get_format_string()\n\nExample:\n    >>> config = OasisLogConfig()\n    >>> formats = config.get_available_formats()\n    >>> 'standard' in formats\n    True",
      "content": "# File: oasislmf/utils/log_config.py\n# function: get_available_formats (lines 182-195)\n\n    def get_available_formats(self) -> List[str]:\n        \"\"\"\n        Get list of available format template names.\n\n        Returns:\n            List of format template names that can be used with get_format_string()\n\n        Example:\n            >>> config = OasisLogConfig()\n            >>> formats = config.get_available_formats()\n            >>> 'standard' in formats\n            True\n        \"\"\"\n        return list(self.FORMAT_TEMPLATES.keys())\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet list of available format template names.\n\nReturns:\n    List of format template names that can be used with get_format_string()\n\nExample:\n    >>> config = OasisLogConfig()\n    >>> formats = config.get_available_formats()\n    >>> 'standard' in formats\n    True"
    },
    {
      "chunk_id": "oasislmf/utils/log_config.py::get_available_levels@197",
      "source_type": "code",
      "path": "oasislmf/utils/log_config.py",
      "symbol_type": "function",
      "name": "get_available_levels",
      "lineno": 197,
      "end_lineno": 210,
      "business_stage": "other",
      "docstring": "Get list of available log level names.\n\nReturns:\n    List of standard log level names that can be used with get_log_level()\n\nExample:\n    >>> config = OasisLogConfig()\n    >>> levels = config.get_available_levels()\n    >>> 'DEBUG' in levels\n    True",
      "content": "# File: oasislmf/utils/log_config.py\n# function: get_available_levels (lines 197-210)\n\n    def get_available_levels(self) -> List[str]:\n        \"\"\"\n        Get list of available log level names.\n\n        Returns:\n            List of standard log level names that can be used with get_log_level()\n\n        Example:\n            >>> config = OasisLogConfig()\n            >>> levels = config.get_available_levels()\n            >>> 'DEBUG' in levels\n            True\n        \"\"\"\n        return self.STANDARD_LEVELS.copy()\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet list of available log level names.\n\nReturns:\n    List of standard log level names that can be used with get_log_level()\n\nExample:\n    >>> config = OasisLogConfig()\n    >>> levels = config.get_available_levels()\n    >>> 'DEBUG' in levels\n    True"
    },
    {
      "chunk_id": "oasislmf/utils/log_config.py::validate_config@212",
      "source_type": "code",
      "path": "oasislmf/utils/log_config.py",
      "symbol_type": "function",
      "name": "validate_config",
      "lineno": 212,
      "end_lineno": 249,
      "business_stage": "other",
      "docstring": "Validate logging configuration and return any issues.\n\nReturns:\n    List of warning messages about configuration issues",
      "content": "# File: oasislmf/utils/log_config.py\n# function: validate_config (lines 212-249)\n\n    def validate_config(self) -> List[str]:\n        \"\"\"\n        Validate logging configuration and return any issues.\n\n        Returns:\n            List of warning messages about configuration issues\n        \"\"\"\n        warnings = []\n        logging_config = self.config.get(\"logging\", {})\n\n        # Validate main log level\n        level = logging_config.get(\"level\")\n        if level:\n            try:\n                self._parse_level(level)\n            except ValueError as e:\n                warnings.append(f\"Invalid log level in config: {e}\")\n\n        # Validate ods_tools level\n        ods_level = logging_config.get(\"ods_tools_level\")\n        if ods_level:\n            try:\n                self._parse_level(ods_level)\n            except ValueError as e:\n                warnings.append(f\"Invalid ods_tools log level in config: {e}\")\n\n        # Validate format (lenient - custom format strings are allowed)\n        format_name = logging_config.get(\"format\")\n        if format_name and format_name not in self.FORMAT_TEMPLATES:\n            # Only warn if it doesn't look like a format string\n            if not (\"%(message)s\" in str(format_name) or \"%(\" in str(format_name)):\n                available = \", \".join(self.get_available_formats())\n                warnings.append(\n                    f\"Format '{format_name}' may not be a valid format template. \"\n                    f\"Available templates: {available}\"\n                )\n\n        return warnings\n\n\"\"\"Docstring (excerpt)\"\"\"\nValidate logging configuration and return any issues.\n\nReturns:\n    List of warning messages about configuration issues"
    },
    {
      "chunk_id": "oasislmf/utils/path.py::as_path@21",
      "source_type": "code",
      "path": "oasislmf/utils/path.py",
      "symbol_type": "function",
      "name": "as_path",
      "lineno": 21,
      "end_lineno": 61,
      "business_stage": "other",
      "docstring": "Processes the path and returns the absolute path.\n\nIf the path does not exist and ``preexists`` is true\nan ``OasisException`` is raised.\n\n:param path: The path to process\n:type path: str\n\n:param label: Human-readable label of the path (used for error reporting)\n:type label: str\n\n:param is_dir: Whether the path is a directory\n:type is_dir: bool\n\n:param preexists: Flag whether to raise an error if the path\n    does not exist.\n:type preexists: bool\n\n:param null_is_valid: flag to indicate if None is a valid value\n:type null_is_valid: bool\n\n:return: The absolute path of the input path",
      "content": "# File: oasislmf/utils/path.py\n# function: as_path (lines 21-61)\n\ndef as_path(path, label, is_dir=False, preexists=True, null_is_valid=True):\n    \"\"\"\n    Processes the path and returns the absolute path.\n\n    If the path does not exist and ``preexists`` is true\n    an ``OasisException`` is raised.\n\n    :param path: The path to process\n    :type path: str\n\n    :param label: Human-readable label of the path (used for error reporting)\n    :type label: str\n\n    :param is_dir: Whether the path is a directory\n    :type is_dir: bool\n\n    :param preexists: Flag whether to raise an error if the path\n        does not exist.\n    :type preexists: bool\n\n    :param null_is_valid: flag to indicate if None is a valid value\n    :type null_is_valid: bool\n\n    :return: The absolute path of the input path\n    \"\"\"\n    if path is None and null_is_valid:\n        return\n\n    if not isinstance(path, str):\n        if preexists:\n            raise OasisException(f'The path {path} ({label}) is indicated as preexisting but is not a valid path')\n        else:\n            return\n    if not os.path.isabs(path):\n        path = os.path.abspath(path)\n    if preexists and not os.path.exists(path):\n        raise OasisException(f'The path {path} ({label}) is indicated as preexisting but does not exist')\n    if is_dir and preexists and not os.path.isdir(path):\n        raise OasisException(f'The path {path} ({label}) is indicated as a preexisting directory but is not actually a directory')\n\n    return os.path.normpath(path)\n\n\"\"\"Docstring (excerpt)\"\"\"\nProcesses the path and returns the absolute path.\n\nIf the path does not exist and ``preexists`` is true\nan ``OasisException`` is raised.\n\n:param path: The path to process\n:type path: str\n\n:param label: Human-readable label of the path (used for error reporting)\n:type label: str\n\n:param is_dir: Whether the path is a directory\n:type is_dir: bool\n\n:param preexists: Flag whether to raise an error if the path\n    does not exist.\n:type preexists: bool\n\n:param null_is_valid: flag to indicate if None is a valid value\n:type null_is_valid: bool\n\n:return: The absolute path of the input path"
    },
    {
      "chunk_id": "oasislmf/utils/path.py::empty_dir@64",
      "source_type": "code",
      "path": "oasislmf/utils/path.py",
      "symbol_type": "function",
      "name": "empty_dir",
      "lineno": 64,
      "end_lineno": 74,
      "business_stage": "other",
      "docstring": "Empties the contents of a directory, but leaves the directory in place.\n\n:param dir_fp: A pre-existing directory path\n:type dir_fp: str",
      "content": "# File: oasislmf/utils/path.py\n# function: empty_dir (lines 64-74)\n\ndef empty_dir(dir_fp):\n    \"\"\"\n    Empties the contents of a directory, but leaves the directory in place.\n\n    :param dir_fp: A pre-existing directory path\n    :type dir_fp: str\n    \"\"\"\n    _dir_fp = as_path(dir_fp, dir_fp, is_dir=True)\n\n    for p in (os.path.join(_dir_fp, fn) for fn in os.listdir(_dir_fp)):\n        os.remove(p) if os.path.isfile(p) else (shutil.rmtree(p) if os.path.isdir(p) else None)\n\n\"\"\"Docstring (excerpt)\"\"\"\nEmpties the contents of a directory, but leaves the directory in place.\n\n:param dir_fp: A pre-existing directory path\n:type dir_fp: str"
    },
    {
      "chunk_id": "oasislmf/utils/path.py::PathCleaner@77",
      "source_type": "code",
      "path": "oasislmf/utils/path.py",
      "symbol_type": "class",
      "name": "PathCleaner",
      "lineno": 77,
      "end_lineno": 95,
      "business_stage": "other",
      "docstring": "A callable that generates the absolute path of the given path and checks\nthat it exists if indicated as preexisting.\n\n:param label: A user-friendly label for the path (used for error reporting)\n:type label: str\n\n:param preexists: Flag whether to raise an error if the path\n    does not exist.\n:type preexists: bool",
      "content": "# File: oasislmf/utils/path.py\n# class: PathCleaner (lines 77-95)\n\nclass PathCleaner(object):\n    \"\"\"\n    A callable that generates the absolute path of the given path and checks\n    that it exists if indicated as preexisting.\n\n    :param label: A user-friendly label for the path (used for error reporting)\n    :type label: str\n\n    :param preexists: Flag whether to raise an error if the path\n        does not exist.\n    :type preexists: bool\n    \"\"\"\n\n    def __init__(self, label, preexists=True):\n        self.label = label\n        self.preexists = preexists\n\n    def __call__(self, path):\n        return as_path(path, self.label, preexists=self.preexists)\n\n\"\"\"Docstring (excerpt)\"\"\"\nA callable that generates the absolute path of the given path and checks\nthat it exists if indicated as preexisting.\n\n:param label: A user-friendly label for the path (used for error reporting)\n:type label: str\n\n:param preexists: Flag whether to raise an error if the path\n    does not exist.\n:type preexists: bool"
    },
    {
      "chunk_id": "oasislmf/utils/path.py::import_from_string@98",
      "source_type": "code",
      "path": "oasislmf/utils/path.py",
      "symbol_type": "function",
      "name": "import_from_string",
      "lineno": 98,
      "end_lineno": 115,
      "business_stage": "other",
      "docstring": "return the object or module from the path given\n>>> import os.path\n>>> mod = import_from_string('os.path')\n>>> os.path is mod\nTrue\n\n>>> from os.path import isabs\n>>> cls = import_from_string('os.path.isabs')\n>>> isabs is cls\nTrue",
      "content": "# File: oasislmf/utils/path.py\n# function: import_from_string (lines 98-115)\n\ndef import_from_string(name):\n    \"\"\"\n    return the object or module from the path given\n    >>> import os.path\n    >>> mod = import_from_string('os.path')\n    >>> os.path is mod\n    True\n\n    >>> from os.path import isabs\n    >>> cls = import_from_string('os.path.isabs')\n    >>> isabs is cls\n    True\n    \"\"\"\n    components = name.split('.')\n    res = __import__(components[0])\n    for comp in components[1:]:\n        res = getattr(res, comp)\n    return res\n\n\"\"\"Docstring (excerpt)\"\"\"\nreturn the object or module from the path given\n>>> import os.path\n>>> mod = import_from_string('os.path')\n>>> os.path is mod\nTrue\n\n>>> from os.path import isabs\n>>> cls = import_from_string('os.path.isabs')\n>>> isabs is cls\nTrue"
    },
    {
      "chunk_id": "oasislmf/utils/path.py::get_custom_module@118",
      "source_type": "code",
      "path": "oasislmf/utils/path.py",
      "symbol_type": "function",
      "name": "get_custom_module",
      "lineno": 118,
      "end_lineno": 150,
      "business_stage": "other",
      "docstring": "return the custom module present at the custom_module_path.\nthe try loop allow for the custom module to work even if it depends on other module of its package\nby testing recursively for the presence of __init__.py file\n\n(ex: this module \"path\" is using from .exceptions import OasisException so it can only be imported as part of the\nutils package => sys.path.insert(0, path_to_utils);importlib.import_module('utils.path'))\n>>> mod = get_custom_module(__file__, \"test module\")\n>>> mod.__name__.rsplit('.', 1)[-1]\n'path'",
      "content": "# File: oasislmf/utils/path.py\n# function: get_custom_module (lines 118-150)\n\ndef get_custom_module(custom_module_path, label):\n    \"\"\"\n    return the custom module present at the custom_module_path.\n    the try loop allow for the custom module to work even if it depends on other module of its package\n    by testing recursively for the presence of __init__.py file\n\n    (ex: this module \"path\" is using from .exceptions import OasisException so it can only be imported as part of the\n    utils package => sys.path.insert(0, path_to_utils);importlib.import_module('utils.path'))\n    >>> mod = get_custom_module(__file__, \"test module\")\n    >>> mod.__name__.rsplit('.', 1)[-1]\n    'path'\n    \"\"\"\n    custom_module_path = as_path(custom_module_path, label, preexists=True, null_is_valid=False)\n\n    package_dir = os.path.dirname(custom_module_path)\n    module_name = re.sub(r'\\.py$', '', os.path.basename(custom_module_path))\n\n    while True:\n        sys.path.insert(0, package_dir)\n        try:\n            custom_module = importlib.import_module(module_name)\n            importlib.reload(custom_module)\n            return custom_module\n        except ImportError:\n            if '__init__.py' in os.listdir(package_dir):\n                module_name = os.path.basename(package_dir) + '.' + module_name\n                package_dir, old_package_dir = os.path.dirname(package_dir), package_dir\n                if package_dir == old_package_dir:\n                    raise\n            else:\n                raise\n        finally:\n            sys.path.pop(0)\n\n\"\"\"Docstring (excerpt)\"\"\"\nreturn the custom module present at the custom_module_path.\nthe try loop allow for the custom module to work even if it depends on other module of its package\nby testing recursively for the presence of __init__.py file\n\n(ex: this module \"path\" is using from .exceptions import OasisException so it can only be imported as part of the\nutils package => sys.path.insert(0, path_to_utils);importlib.import_module('utils.path'))\n>>> mod = get_custom_module(__file__, \"test module\")\n>>> mod.__name__.rsplit('.', 1)[-1]\n'path'"
    },
    {
      "chunk_id": "oasislmf/utils/ping.py::oasis_ping@9",
      "source_type": "code",
      "path": "oasislmf/utils/ping.py",
      "symbol_type": "function",
      "name": "oasis_ping",
      "lineno": 9,
      "end_lineno": 32,
      "business_stage": "other",
      "docstring": "Sends a JSON message to either a websocket server or a socket server.\n\nIf `analysis_pk` is in the data, `OASIS_WEBSOCKET_URL` and `OASIS_WEBSOCKET_URL` are in environment, sends a websocket message.\nIf `analysis_pk` but missing variables, no message sent.\nElse, websocket sent to `OASIS_SOCKET_SERVER_IP` `OASIS_SOCKET_SERVER_PORT` defaulted to 127.0.0.1 8888.\n\nFor a specific socket or websocket, use `oasis_ping_socket` or `oasis_ping_websocket` with the target location.\n\nArgs:\n    data (dict): dictionary of data: JSON serialisable\n\nReturns:\n    Boolean: whether attempted call gets through",
      "content": "# File: oasislmf/utils/ping.py\n# function: oasis_ping (lines 9-32)\n\ndef oasis_ping(data):\n    \"\"\"\n    Sends a JSON message to either a websocket server or a socket server.\n\n    If `analysis_pk` is in the data, `OASIS_WEBSOCKET_URL` and `OASIS_WEBSOCKET_URL` are in environment, sends a websocket message.\n    If `analysis_pk` but missing variables, no message sent.\n    Else, websocket sent to `OASIS_SOCKET_SERVER_IP` `OASIS_SOCKET_SERVER_PORT` defaulted to 127.0.0.1 8888.\n\n    For a specific socket or websocket, use `oasis_ping_socket` or `oasis_ping_websocket` with the target location.\n\n    Args:\n        data (dict): dictionary of data: JSON serialisable\n\n    Returns:\n        Boolean: whether attempted call gets through\n    \"\"\"\n    msg = json.dumps(data)\n    if data.get('analysis_pk', None) is not None:\n        if all(item in os.environ for item in ['OASIS_WEBSOCKET_URL', 'OASIS_WEBSOCKET_PORT']):\n            return oasis_ping_websocket(f\"{os.environ['OASIS_WEBSOCKET_URL']}:{os.environ['OASIS_WEBSOCKET_PORT']}/ws/analysis-status/\", msg)\n        logging.error(\"Missing environment variables `OASIS_WEBSOCKET_URL` and `OASIS_WEBSOCKET_PORT`.\")\n        return False\n    target = (os.environ.get(\"OASIS_SOCKET_SERVER_IP\", SERVER_DEFAULT_IP), int(os.environ.get(\"OASIS_SOCKET_SERVER_PORT\", SERVER_DEFAULT_PORT)))\n    return oasis_ping_socket(target, msg)\n\n\"\"\"Docstring (excerpt)\"\"\"\nSends a JSON message to either a websocket server or a socket server.\n\nIf `analysis_pk` is in the data, `OASIS_WEBSOCKET_URL` and `OASIS_WEBSOCKET_URL` are in environment, sends a websocket message.\nIf `analysis_pk` but missing variables, no message sent.\nElse, websocket sent to `OASIS_SOCKET_SERVER_IP` `OASIS_SOCKET_SERVER_PORT` defaulted to 127.0.0.1 8888.\n\nFor a specific socket or websocket, use `oasis_ping_socket` or `oasis_ping_websocket` with the target location.\n\nArgs:\n    data (dict): dictionary of data: JSON serialisable\n\nReturns:\n    Boolean: whether attempted call gets through"
    },
    {
      "chunk_id": "oasislmf/utils/ping.py::oasis_ping_socket@35",
      "source_type": "code",
      "path": "oasislmf/utils/ping.py",
      "symbol_type": "function",
      "name": "oasis_ping_socket",
      "lineno": 35,
      "end_lineno": 53,
      "business_stage": "other",
      "docstring": "Sends a JSON message to a target socket\n\nArgs:\n    target ((str, int)): IP and port to hit\n    data (str): JSON dumped string\n\nReturns:\n    Boolean: whether attempted call gets through",
      "content": "# File: oasislmf/utils/ping.py\n# function: oasis_ping_socket (lines 35-53)\n\ndef oasis_ping_socket(target, data):\n    \"\"\"\n    Sends a JSON message to a target socket\n\n    Args:\n        target ((str, int)): IP and port to hit\n        data (str): JSON dumped string\n\n    Returns:\n        Boolean: whether attempted call gets through\n    \"\"\"\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as oasis_socket:\n            oasis_socket.connect(target)\n            oasis_socket.sendall(data.encode('utf-8'))\n        return True\n    except ConnectionRefusedError as e:\n        logging.error(f\"oasis_ping_socket could not connect: {e}\")\n        return False\n\n\"\"\"Docstring (excerpt)\"\"\"\nSends a JSON message to a target socket\n\nArgs:\n    target ((str, int)): IP and port to hit\n    data (str): JSON dumped string\n\nReturns:\n    Boolean: whether attempted call gets through"
    },
    {
      "chunk_id": "oasislmf/utils/ping.py::oasis_ping_websocket@56",
      "source_type": "code",
      "path": "oasislmf/utils/ping.py",
      "symbol_type": "function",
      "name": "oasis_ping_websocket",
      "lineno": 56,
      "end_lineno": 76,
      "business_stage": "other",
      "docstring": "Sends a JSON message to a target websocket\n\nArgs:\n    ws_url (str): URL to hit (e.g. \"ws://oasis-websocket:8001/ws/analysis-status/\")\n    data (str): JSON dumped string\n\nReturns:\n    Boolean: whether attempted call gets through",
      "content": "# File: oasislmf/utils/ping.py\n# function: oasis_ping_websocket (lines 56-76)\n\ndef oasis_ping_websocket(ws_url, data):\n    \"\"\"\n    Sends a JSON message to a target websocket\n\n    Args:\n        ws_url (str): URL to hit (e.g. \"ws://oasis-websocket:8001/ws/analysis-status/\")\n        data (str): JSON dumped string\n\n    Returns:\n        Boolean: whether attempted call gets through\n    \"\"\"\n    ws = websocket.WebSocket()\n    try:\n        ws.connect(ws_url)\n        ws.send(data)\n        return True\n    except Exception as e:\n        logging.error(f\"oasis_ping_websocket could not connect: {e}\")\n        return False\n    finally:\n        ws.close()\n\n\"\"\"Docstring (excerpt)\"\"\"\nSends a JSON message to a target websocket\n\nArgs:\n    ws_url (str): URL to hit (e.g. \"ws://oasis-websocket:8001/ws/analysis-status/\")\n    data (str): JSON dumped string\n\nReturns:\n    Boolean: whether attempted call gets through"
    },
    {
      "chunk_id": "oasislmf/utils/profiles.py::get_grouped_fm_profile_by_level_and_term_group@44",
      "source_type": "code",
      "path": "oasislmf/utils/profiles.py",
      "symbol_type": "function",
      "name": "get_grouped_fm_profile_by_level_and_term_group",
      "lineno": 44,
      "end_lineno": 87,
      "business_stage": "fm",
      "docstring": "Get the grouped exposure + accounts profile - this describes the\nfinancial terms found in the source exposure and accounts files,\nwhich are for the following FM levels: site coverage (# 1),\nsite pd (# 2), site all (# 3), cond. all (# 6), policy all (# 9),\npolicy layer (# 10).  It also describes the OED hierarchy terms\npresent in the exposure and accounts files, namely portfolio num.,\nacc. num., loc. num., and cond. num.",
      "content": "# File: oasislmf/utils/profiles.py\n# function: get_grouped_fm_profile_by_level_and_term_group (lines 44-87)\n\ndef get_grouped_fm_profile_by_level_and_term_group(\n    exposure_profile=get_default_exposure_profile(),\n    accounts_profile=get_default_accounts_profile(),\n    grouped_profile_by_level=None\n):\n    \"\"\"\n    Get the grouped exposure + accounts profile - this describes the\n    financial terms found in the source exposure and accounts files,\n    which are for the following FM levels: site coverage (# 1),\n    site pd (# 2), site all (# 3), cond. all (# 6), policy all (# 9),\n    policy layer (# 10).  It also describes the OED hierarchy terms\n    present in the exposure and accounts files, namely portfolio num.,\n    acc. num., loc. num., and cond. num.\n    \"\"\"\n    grouped = grouped_profile_by_level or get_grouped_fm_profile_by_level(exposure_profile, accounts_profile)\n\n    grouped_fm_term_types = OrderedDict({\n        'deductible': FM_TERMS['deductible']['id'],\n        'deductiblecode': FM_TERMS['deductible code']['id'],\n        'deductibletype': FM_TERMS['deductible type']['id'],\n        'deductiblemin': FM_TERMS['min deductible']['id'],\n        'deductiblemax': FM_TERMS['max deductible']['id'],\n        'limit': FM_TERMS['limit']['id'],\n        'limitcode': FM_TERMS['limit code']['id'],\n        'limittype': FM_TERMS['limit type']['id'],\n        'share': FM_TERMS['share']['id']\n    })\n    profile = OrderedDict({\n        level: OrderedDict({\n            FMTermGroupID: OrderedDict({\n                (grouped_fm_term_types.get(term['FMTermType'].lower()) or term['FMTermType'].lower()): term for term in TermGroup\n            }) for FMTermGroupID, TermGroup in groupby(sorted(grouped[level].values(),\n                                                              key=lambda term: term['FMTermGroupID']),\n                                                       key=lambda term: term['FMTermGroupID'])\n        }) for level in sorted(grouped)\n    })\n\n    if not profile:\n        raise OasisException(\n            'Unable to get a unified FM profile by level and term group. '\n            'Canonical loc. and/or acc. profiles are possibly missing FM term information: '\n            'FM term definitions for TIV, deductibles, limit, and/or share.'\n        )\n    return profile\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet the grouped exposure + accounts profile - this describes the\nfinancial terms found in the source exposure and accounts files,\nwhich are for the following FM levels: site coverage (# 1),\nsite pd (# 2), site all (# 3), cond. all (# 6), policy all (# 9),\npolicy layer (# 10).  It also describes the OED hierarchy terms\npresent in the exposure and accounts files, namely portfolio num.,\nacc. num., loc. num., and cond. num."
    },
    {
      "chunk_id": "oasislmf/utils/profiles.py::get_grouped_fm_terms_by_level_and_term_group@90",
      "source_type": "code",
      "path": "oasislmf/utils/profiles.py",
      "symbol_type": "function",
      "name": "get_grouped_fm_terms_by_level_and_term_group",
      "lineno": 90,
      "end_lineno": 117,
      "business_stage": "fm",
      "docstring": "Get the FM terms profile (this is a simplfied view of the main grouped\nprofile, containing only information about the financial terms)",
      "content": "# File: oasislmf/utils/profiles.py\n# function: get_grouped_fm_terms_by_level_and_term_group (lines 90-117)\n\ndef get_grouped_fm_terms_by_level_and_term_group(\n    exposure_profile=get_default_exposure_profile(),\n    accounts_profile=get_default_accounts_profile(),\n    grouped_profile_by_level=None,\n    grouped_profile_by_level_and_term_group=None,\n    lowercase=True\n):\n    \"\"\"\n    Get the FM terms profile (this is a simplfied view of the main grouped\n    profile, containing only information about the financial terms)\n    \"\"\"\n    grouped = (\n        grouped_profile_by_level_and_term_group or\n        get_grouped_fm_profile_by_level_and_term_group(exposure_profile, accounts_profile, grouped_profile_by_level)\n    )\n\n    return OrderedDict({\n        level_id: OrderedDict({\n            tgid: OrderedDict({\n                term_type: (\n                    (\n                        grouped[level_id][tgid][term_type]['ProfileElementName'].lower() if lowercase\n                        else grouped[level_id][tgid][term_type]['ProfileElementName']\n                    ) if grouped[level_id][tgid].get(term_type) else None\n                ) for term_type in [v['id'] for v in FM_TERMS.values()]\n            }) for tgid in grouped[level_id]\n        }) for level_id in sorted(grouped)\n    })\n\n\"\"\"Docstring (excerpt)\"\"\"\nGet the FM terms profile (this is a simplfied view of the main grouped\nprofile, containing only information about the financial terms)"
    },
    {
      "chunk_id": "oasislmf/utils/documentation/markdown.py::get_markdown@16",
      "source_type": "code",
      "path": "oasislmf/utils/documentation/markdown.py",
      "symbol_type": "function",
      "name": "get_markdown",
      "lineno": 16,
      "end_lineno": 25,
      "business_stage": "other",
      "docstring": "Returns markdown string from joined self.sections\nArgs:\n    generate_toc (bool): Generate table of contents bool.\nReturns:\n    str: Markdown string",
      "content": "# File: oasislmf/utils/documentation/markdown.py\n# function: get_markdown (lines 16-25)\n\n    def get_markdown(self, generate_toc=False):\n        \"\"\"Returns markdown string from joined self.sections\n        Args:\n            generate_toc (bool): Generate table of contents bool.\n        Returns:\n            str: Markdown string\n        \"\"\"\n        if generate_toc:\n            self.generate_toc()\n        return \"\".join(self.sections)\n\n\"\"\"Docstring (excerpt)\"\"\"\nReturns markdown string from joined self.sections\nArgs:\n    generate_toc (bool): Generate table of contents bool.\nReturns:\n    str: Markdown string"
    },
    {
      "chunk_id": "oasislmf/utils/documentation/markdown.py::generate_toc@38",
      "source_type": "code",
      "path": "oasislmf/utils/documentation/markdown.py",
      "symbol_type": "function",
      "name": "generate_toc",
      "lineno": 38,
      "end_lineno": 58,
      "business_stage": "other",
      "docstring": "Generate a table of contents from markdown string\nReturns:\n    toc (str): Table of contents markdown string",
      "content": "# File: oasislmf/utils/documentation/markdown.py\n# function: generate_toc (lines 38-58)\n\n    def generate_toc(self, ):\n        \"\"\"Generate a table of contents from markdown string\n        Returns:\n            toc (str): Table of contents markdown string \n        \"\"\"\n        markdown_text = \"\".join(self.sections)\n        lines = markdown_text.split('\\n')\n        toc = []\n        slug_counts = defaultdict(int)\n\n        for line in lines:\n            match = re.match(r'^(#{1,6})\\s+(.*)', line)\n            if match:\n                level = len(match.group(1)) - 1\n                title = match.group(2).strip()\n                base_slug = self._slugify(title)\n                slug_counts[base_slug] += 1\n                anchor = base_slug if slug_counts[base_slug] == 1 else f\"{base_slug}-{slug_counts[base_slug] - 1}\"\n                toc.append(f\"{'  ' * level}- [{title}](#{anchor})\")\n\n        self.sections = [\"## Table of Contents\\n\\n\" + \"\\n\".join(toc) + \"\\n\\n\"] + self.sections\n\n\"\"\"Docstring (excerpt)\"\"\"\nGenerate a table of contents from markdown string\nReturns:\n    toc (str): Table of contents markdown string"
    },
    {
      "chunk_id": "oasislmf/utils/documentation/markdown.py::add_header@60",
      "source_type": "code",
      "path": "oasislmf/utils/documentation/markdown.py",
      "symbol_type": "function",
      "name": "add_header",
      "lineno": 60,
      "end_lineno": 66,
      "business_stage": "other",
      "docstring": "Adds header to markdown\nArgs:\n    title (Any): Title string\n    level (int): Markdown header level. Defaults to 1.",
      "content": "# File: oasislmf/utils/documentation/markdown.py\n# function: add_header (lines 60-66)\n\n    def add_header(self, title, level=1):\n        \"\"\"Adds header to markdown\n        Args:\n            title (Any): Title string\n            level (int): Markdown header level. Defaults to 1.\n        \"\"\"\n        self.sections.append(f\"{'#' * level} {title}\\n\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdds header to markdown\nArgs:\n    title (Any): Title string\n    level (int): Markdown header level. Defaults to 1."
    },
    {
      "chunk_id": "oasislmf/utils/documentation/markdown.py::add_definition@68",
      "source_type": "code",
      "path": "oasislmf/utils/documentation/markdown.py",
      "symbol_type": "function",
      "name": "add_definition",
      "lineno": 68,
      "end_lineno": 75,
      "business_stage": "other",
      "docstring": "Adds definition line to markdown in the following format\n**title**: content\nArgs:\n    title (Any): Name\n    content (Any): Description",
      "content": "# File: oasislmf/utils/documentation/markdown.py\n# function: add_definition (lines 68-75)\n\n    def add_definition(self, title, content):\n        \"\"\"Adds definition line to markdown in the following format\n        **title**: content\n        Args:\n            title (Any): Name\n            content (Any): Description\n        \"\"\"\n        self.sections.append(f\"**{title}**: {content}\\n\\n\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdds definition line to markdown in the following format\n**title**: content\nArgs:\n    title (Any): Name\n    content (Any): Description"
    },
    {
      "chunk_id": "oasislmf/utils/documentation/markdown.py::add_table@77",
      "source_type": "code",
      "path": "oasislmf/utils/documentation/markdown.py",
      "symbol_type": "function",
      "name": "add_table",
      "lineno": 77,
      "end_lineno": 93,
      "business_stage": "other",
      "docstring": "Adds a table to markdown with headers and rows\nArgs:\n    headers (List[str]): Headers\n    rows (List[str]): Rows",
      "content": "# File: oasislmf/utils/documentation/markdown.py\n# function: add_table (lines 77-93)\n\n    def add_table(self, headers, rows):\n        \"\"\"Adds a table to markdown with headers and rows\n        Args:\n            headers (List[str]): Headers\n            rows (List[str]): Rows\n        \"\"\"\n        if len(rows) > 0:\n            assert len(rows[0]) == len(headers), \\\n                f\"Length of rows ({len(rows[0])}) \\\n                does not equal length of headers \\\n                ({len(headers)}) for headers:\\n {headers}\\n\"\n        table = \"| \" + \" | \".join(headers) + \" |\\n\"\n        table += \"|\" + \"|\".join([\"---\"] * len(headers)) + \"|\\n\"\n        for row in rows:\n            table += \"| \" + \" | \".join(row) + \" |\\n\"\n        self.sections.append(table)\n        self.sections.append(\"\\n\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdds a table to markdown with headers and rows\nArgs:\n    headers (List[str]): Headers\n    rows (List[str]): Rows"
    },
    {
      "chunk_id": "oasislmf/utils/documentation/markdown.py::add_list@95",
      "source_type": "code",
      "path": "oasislmf/utils/documentation/markdown.py",
      "symbol_type": "function",
      "name": "add_list",
      "lineno": 95,
      "end_lineno": 102,
      "business_stage": "other",
      "docstring": "Adds list to markdown\nArgs:\n    items (List[str]): List of items",
      "content": "# File: oasislmf/utils/documentation/markdown.py\n# function: add_list (lines 95-102)\n\n    def add_list(self, items):\n        \"\"\"Adds list to markdown\n        Args:\n            items (List[str]): List of items\n        \"\"\"\n        for item in items:\n            self.sections.append(f\"- {item}\\n\")\n        self.sections.append(\"\\n\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdds list to markdown\nArgs:\n    items (List[str]): List of items"
    },
    {
      "chunk_id": "oasislmf/utils/documentation/markdown.py::add_collapsible_section@104",
      "source_type": "code",
      "path": "oasislmf/utils/documentation/markdown.py",
      "symbol_type": "function",
      "name": "add_collapsible_section",
      "lineno": 104,
      "end_lineno": 110,
      "business_stage": "other",
      "docstring": "Adds collapsible section to markdown\nArgs:\n    text (str): contents of collapsible section\n    title (str, optional): Collapsible section title text. Defaults to \"Root\".",
      "content": "# File: oasislmf/utils/documentation/markdown.py\n# function: add_collapsible_section (lines 104-110)\n\n    def add_collapsible_section(self, text, title=\"Root\"):\n        \"\"\"Adds collapsible section to markdown\n        Args:\n            text (str): contents of collapsible section\n            title (str, optional): Collapsible section title text. Defaults to \"Root\".\n        \"\"\"\n        self.add_text(f\"<details><summary>{title}</summary>\\n\\n```json\\n\" + text + \"\\n```\\n</details>\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdds collapsible section to markdown\nArgs:\n    text (str): contents of collapsible section\n    title (str, optional): Collapsible section title text. Defaults to \"Root\"."
    },
    {
      "chunk_id": "oasislmf/utils/documentation/markdown.py::add_text@112",
      "source_type": "code",
      "path": "oasislmf/utils/documentation/markdown.py",
      "symbol_type": "function",
      "name": "add_text",
      "lineno": 112,
      "end_lineno": 117,
      "business_stage": "other",
      "docstring": "Adds text to markdown\nArgs:\n    content (Any): Text content",
      "content": "# File: oasislmf/utils/documentation/markdown.py\n# function: add_text (lines 112-117)\n\n    def add_text(self, content):\n        \"\"\"Adds text to markdown\n        Args:\n            content (Any): Text content\n        \"\"\"\n        self.sections.append(f\"{content}\\n\\n\")\n\n\"\"\"Docstring (excerpt)\"\"\"\nAdds text to markdown\nArgs:\n    content (Any): Text content"
    },
    {
      "chunk_id": "oasislmf/utils/documentation/jsontomd/base.py::BaseJsonToMarkdownGenerator@6",
      "source_type": "code",
      "path": "oasislmf/utils/documentation/jsontomd/base.py",
      "symbol_type": "class",
      "name": "BaseJsonToMarkdownGenerator",
      "lineno": 6,
      "end_lineno": 48,
      "business_stage": "other",
      "docstring": "Base JSON to Markdown Generator class",
      "content": "# File: oasislmf/utils/documentation/jsontomd/base.py\n# class: BaseJsonToMarkdownGenerator (lines 6-48)\n\nclass BaseJsonToMarkdownGenerator(ABC):\n    \"\"\"\n    Base JSON to Markdown Generator class\n    \"\"\"\n\n    def __init__(self, full_schema, data_path, doc_out_dir, markdown_generator=None):\n        \"\"\"\n        Args:\n            full_schema (Dict): Full schema file as dictionary\n            data_path (str | os.PathLike): Path to data folder for any relative file paths\n            doc_out_dir (str | os.PathLike): Path to documentation file output folder for any relative file paths\n            markdown_generator (MarkdownGenerator, optional): MarkdownGenerator class. Defaults to None.\n        \"\"\"\n        self.full_schema = full_schema\n        self.data_path = data_path\n        self.doc_out_dir = doc_out_dir\n        self.md = markdown_generator\n        if not markdown_generator:\n            self.md = MarkdownGenerator()\n\n    def _resolve_internal_ref(self, ref):\n        \"\"\"Resolves a $ref in the schema (only internal refs supported).\n        Args:\n            ref (str): Reference string of format #/$<reftitle>/<refname>\n        Returns:\n            ref_schema (Dict): Data Properties from reference schema as dictionary\n        \"\"\"\n        parts = ref.strip(\"#/\").split(\"/\")\n        ref_schema = self.full_schema\n        for part in parts:\n            ref_schema = ref_schema.get(part, {})\n        return ref_schema\n\n    @abstractmethod\n    def generate(self, json_data, generate_toc=False):\n        \"\"\"Top level function to process entire dict to markdown text\n        Args:\n            json_data (Dict): Json data as dictionary\n            generate_toc (bool, Optional): Generate table of contents bool. Defaults to False.\n        Returns:\n            markdown_txt (str): Markdown text\n        \"\"\"\n        pass\n\n\"\"\"Docstring (excerpt)\"\"\"\nBase JSON to Markdown Generator class"
    },
    {
      "chunk_id": "oasislmf/utils/documentation/jsontomd/base.py::generate@40",
      "source_type": "code",
      "path": "oasislmf/utils/documentation/jsontomd/base.py",
      "symbol_type": "function",
      "name": "generate",
      "lineno": 40,
      "end_lineno": 48,
      "business_stage": "other",
      "docstring": "Top level function to process entire dict to markdown text\nArgs:\n    json_data (Dict): Json data as dictionary\n    generate_toc (bool, Optional): Generate table of contents bool. Defaults to False.\nReturns:\n    markdown_txt (str): Markdown text",
      "content": "# File: oasislmf/utils/documentation/jsontomd/base.py\n# function: generate (lines 40-48)\n\n    def generate(self, json_data, generate_toc=False):\n        \"\"\"Top level function to process entire dict to markdown text\n        Args:\n            json_data (Dict): Json data as dictionary\n            generate_toc (bool, Optional): Generate table of contents bool. Defaults to False.\n        Returns:\n            markdown_txt (str): Markdown text\n        \"\"\"\n        pass\n\n\"\"\"Docstring (excerpt)\"\"\"\nTop level function to process entire dict to markdown text\nArgs:\n    json_data (Dict): Json data as dictionary\n    generate_toc (bool, Optional): Generate table of contents bool. Defaults to False.\nReturns:\n    markdown_txt (str): Markdown text"
    },
    {
      "chunk_id": "oasislmf/utils/documentation/jsontomd/default.py::DefaultJsonToMarkdownGenerator@6",
      "source_type": "code",
      "path": "oasislmf/utils/documentation/jsontomd/default.py",
      "symbol_type": "class",
      "name": "DefaultJsonToMarkdownGenerator",
      "lineno": 6,
      "end_lineno": 80,
      "business_stage": "other",
      "docstring": "Default JSON to Markdown Generator class.\nNaively iterates through the dict and outputs with limited formatting.",
      "content": "# File: oasislmf/utils/documentation/jsontomd/default.py\n# class: DefaultJsonToMarkdownGenerator (lines 6-80)\n\nclass DefaultJsonToMarkdownGenerator(BaseJsonToMarkdownGenerator):\n    \"\"\"\n    Default JSON to Markdown Generator class.\n    Naively iterates through the dict and outputs with limited formatting.\n    \"\"\"\n\n    def json_array_to_mdtable(self, data, ref):\n        array_schema = self._resolve_internal_ref(ref)\n        array_keys = array_schema[\"properties\"].keys()\n        headers = []\n        for k in array_keys:\n            if \"title\" in array_schema[\"properties\"][k]:\n                headers.append(array_schema[\"properties\"][k][\"title\"])\n            else:\n                headers.append(k)\n        rows = []\n        for entry in data:\n            row = []\n            for k in array_keys:\n                v = entry.get(k, \"\")\n                if isinstance(v, list):\n                    rets = []\n                    for v_ in v:\n                        if isinstance(v_, dict):\n                            v_ = json.dumps(v_, indent=4).replace('\\n', '<br>').replace(' ', '&nbsp;')\n                        rets.append(str(v_))\n                    v = \",<br>\".join(rets)\n                elif isinstance(v, dict):\n                    pretty_json = json.dumps(v, indent=4).replace('\\n', '<br>').replace(' ', '&nbsp;')\n                    v = f\"<details><summary>Expand</summary>{pretty_json}</details>\"\n                else:\n                    v = str(v)\n                row.append(v)\n            rows.append(row)\n        self.md.add_table(headers, rows)\n\n    def generate_data(self, data, properties_schema, header_level):\n        for key, value in data.items():\n            key_title = key\n            schema = properties_schema.get(key, {})\n            if \"type\" in schema:\n                if schema[\"type\"] == \"array\":\n                    self.md.add_header(key_title, level=header_level)\n                    items = schema.get(\"items\", {})\n                    if isinstance(items, dict) and \"$ref\" in items:\n                        self.json_array_to_mdtable(value, items[\"$ref\"])\n                    else:\n                        self.md.add_list(value)\n                elif schema[\"type\"] == \"object\":\n                    self.md.add_header(key_title, level=header_level)\n                    self.generate_data(value, schema.get(\"properties\", {}), header_level + 1)\n                else:\n                    self.md.add_header(key_title, level=header_level)\n                    self.md.add_text(value)\n            elif \"$ref\" in schema:\n                self.md.add_header(key_title, level=header_level)\n                ref_schema = self._resolve_internal_ref(schema[\"$ref\"])\n                self.generate_data(value, ref_schema.get(\"properties\", {}), header_level + 1)\n            else:\n                self.md.add_header(key_title, level=header_level)\n                if isinstance(value, list):\n                    for i, v in enumerate(value):\n                        self.md.add_header(f\"Item {i}\", header_level + 1)\n                        self.generate_data(v, properties_schema, header_level + 2)\n                elif isinstance(value, dict):\n                    for k, v in value.items():\n                        self.md.add_header(k, level=header_level + 1)\n                        self.generate_data(v, properties_schema, header_level + 2)\n                else:\n                    self.md.add_text(value)\n\n    def generate(self, json_data, generate_toc=False):\n        self.md.add_header(\"Documentation\", level=1)\n        self.generate_data(json_data, self.full_schema[\"properties\"], header_level=3)\n        return self.md.get_markdown(generate_toc=generate_toc)\n\n\"\"\"Docstring (excerpt)\"\"\"\nDefault JSON to Markdown Generator class.\nNaively iterates through the dict and outputs with limited formatting."
    },
    {
      "chunk_id": "oasislmf/validation/model_data.py::csv_validity_test@31",
      "source_type": "code",
      "path": "oasislmf/validation/model_data.py",
      "symbol_type": "function",
      "name": "csv_validity_test",
      "lineno": 31,
      "end_lineno": 76,
      "business_stage": "other",
      "docstring": "Assess validity of model data.\n\n:param model_data_fp: directory containing csv files\n:type model_data_fp: str\n\n:raises OasisException: if one of the tests fail",
      "content": "# File: oasislmf/validation/model_data.py\n# function: csv_validity_test (lines 31-76)\n\ndef csv_validity_test(model_data_fp):\n    \"\"\"\n    Assess validity of model data.\n\n    :param model_data_fp: directory containing csv files\n    :type model_data_fp: str\n\n    :raises OasisException: if one of the tests fail\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.INFO)\n\n    model_data_dir = os.path.abspath(model_data_fp)\n\n    # Check individual files\n    for input_file in INPUT_FILES.values():\n        validation_tool = input_file['validation_tool']\n        input_file_path = os.path.join(\n            model_data_dir,\n            '{}.csv'.format(input_file['name'])\n        )\n\n        logger.info(\"Testing {}.csv\".format(input_file['name']))\n        cmd_str = \"{} < {}\".format(validation_tool, input_file_path)\n\n        try:\n            subprocess.check_call(cmd_str, stderr=subprocess.STDOUT, shell=True)\n        except subprocess.CalledProcessError as e:\n            raise OasisException(\"Exception raised in 'csv_validity_test'\", e)\n\n    # Execute cross checks\n    logger.info(\"Executing cross checks\")\n    cmd_str = \"crossvalidation\"\n    for input_file in INPUT_FILES.values():\n        flag = input_file['flag']\n        input_file_path = os.path.join(\n            model_data_dir,\n            '{}.csv'.format(input_file['name'])\n        )\n        cmd_str += \" {} {}\".format(flag, input_file_path)\n\n    try:\n        subprocess.check_call(cmd_str, stderr=subprocess.STDOUT, shell=True)\n    except subprocess.CalledProcessError as e:\n        raise OasisException(\"Exception raised in 'csv_validity_test'\", e)\n\n\"\"\"Docstring (excerpt)\"\"\"\nAssess validity of model data.\n\n:param model_data_fp: directory containing csv files\n:type model_data_fp: str\n\n:raises OasisException: if one of the tests fail"
    },
    {
      "chunk_id": "tests/computation/data/fake_pre_analysis.py::ExposurePreAnalysis@1",
      "source_type": "code",
      "path": "tests/computation/data/fake_pre_analysis.py",
      "symbol_type": "class",
      "name": "ExposurePreAnalysis",
      "lineno": 1,
      "end_lineno": 15,
      "business_stage": "other",
      "docstring": "Fake exposure pre-analysis module.",
      "content": "# File: tests/computation/data/fake_pre_analysis.py\n# class: ExposurePreAnalysis (lines 1-15)\n\nclass ExposurePreAnalysis:\n    \"\"\"\n    Fake exposure pre-analysis module.\n    \"\"\"\n\n    def __init__(self, exposure_data, exposure_pre_analysis_setting, **kwargs):\n        self.exposure_data = exposure_data\n        self.exposure_pre_analysis_setting = exposure_pre_analysis_setting\n\n    def run(self):\n        loc_df = self.exposure_data.location.dataframe\n        acc_df = self.exposure_data.account.dataframe\n\n        loc_df['LocNumber'] = self.exposure_pre_analysis_setting['override_loc_num']\n        acc_df['AccNumber'] = self.exposure_pre_analysis_setting['override_acc_num']\n\n\"\"\"Docstring (excerpt)\"\"\"\nFake exposure pre-analysis module."
    }
  ],
  "parse_errors": []
}